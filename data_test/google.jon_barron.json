{
    "82": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://openreview.net/forum?id=AmPeAFzU3a4\">\n                <papertitle>MIRA: Mental Imagery for Robotic Affordances</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://yenchenlin.me/\">Lin Yen-Chen</a>, \n              <a href=\"http://www.peteflorence.com/\">Pete Florence</a>, \n              <a href=\"https://andyzeng.github.io/\">Andy Zeng</a>, <strong>Jonathan T. Barron</strong>, \n              <a href=\"https://yilundu.github.io/\">Yilun Du</a>,\n              <a href=\"https://people.csail.mit.edu/weichium/\">Wei-Chiu Ma</a>,\n              <a href=\"https://anthonysimeonov.github.io/\">Anthony Simeonov</a>,\n              <a href=\"https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU\">Alberto Rodriguez</a>,\n              <a href=\"http://web.mit.edu/phillipi/\">Phillip Isola</a>\n              <br/>\n              <em>CoRL</em>, 2022\n              <p/>\n              <p>\n                NeRF lets us synthesize novel orthographic views that work well with pixel-wise algorithms for robotic manipulation.\n              </p>\n            </td>\n          ",
    "81": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://dreamfusion3d.github.io/\">\n                <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://cs.stanford.edu/~poole/\">Ben Poole</a>,\n              <a href=\"https://www.ajayj.com/\">Ajay Jain</a>,\n              <strong>Jonathan T. Barron</strong>,\n\t\t\t\t\t\t\t<a href=\"https://bmild.github.io/\">Ben Mildenhall</a>\n              <br/>\n              <em>arXiv</em>, 2022\n              <br/>\n              <a href=\"https://dreamfusion3d.github.io/\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2209.14988\">arXiv</a>\n              /\n              <a href=\"https://dreamfusion3d.github.io/gallery.html\">gallery</a>\n              <p/>\n              <p>\n              We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.\n              </p>\n            </td>\n          ",
    "80": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://markboss.me/publication/2022-samurai/\">\n                <papertitle>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image Collections</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://markboss.me\">Mark Boss</a>, \n              <a href=\"\">Andreas Engelhardt</a>, \n              <a href=\"https://abhishekkar.info/\">Abhishek Kar</a>, \n              <a href=\"http://people.csail.mit.edu/yzli/\">Yuanzhen Li</a>, \n              <a href=\"https://deqings.github.io/\">Deqing Sun</a>, \n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/\">Hendrik P. A. Lensch</a>,\n              <a href=\"https://varunjampani.github.io\">Varun Jampani</a>\n              <br/>\n              <em>NeurIPS</em>, 2022\n              <br/>\n              <a href=\"https://markboss.me/publication/2022-samurai/\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=LlYuGDjXp-8\">video</a> /\n              <a href=\"https://arxiv.org/abs/2205.15768\">arXiv</a>\n              <p/>\n              <p>\nA joint optimization framework for estimating shape, BRDF, camera pose, and illumination from in-the-wild image collections.\n              </p>\n            </td>\n          ",
    "79": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n          <a href=\"TODO\">\n          <papertitle>Polynomial Neural Fields for Subband Decomposition</papertitle>\n          </a> <br/>\n          <a href=\"https://www.guandaoyang.com/\">Guandao Yang*</a>,\n          <a href=\"https://sagiebenaim.github.io/\">Sagie Benaim*</a>,\n          <a href=\"https://varunjampani.github.io/\">Varun Jampani</a>,\n          <a href=\"https://www.kylegenova.com/\">Kyle Genova</a>,\n          <strong>Jonathan T. Barron</strong>,\n          <a href=\"https://www.cs.princeton.edu/~funk/\">Thomas Funkhouser</a>,\n          <a href=\"http://home.bharathh.info/\">Bharath Hariharan</a>,\n          <a href=\"https://sergebelongie.github.io/\">Serge Belongie</a>\n          <br/>\n          <em>NeurIPS</em>, 2022\n          <p>\n          Representing neural fields as a composition of manipulable and interpretable components lets you do things like reason about frequencies and scale.\n          </p>\n          </td>\n          ",
    "78": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://yifanjiang.net/MalleConv.html\">\n                <papertitle>Fast and High-quality Image Denoising via Malleable Convolutions</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://yifanjiang.net/\">Yifan Jiang</a>,\n              <a href=\"https://bartwronski.com/\">Bartlomiej Wronski</a>, \n\t\t\t\t\t\t\t<a href=\"https://bmild.github.io/\">Ben Mildenhall</a>, <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://spark.adobe.com/page/CAdrFMJ9QeI2y/\">Zhangyang Wang</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>\n              <br/>\n              <em>ECCV</em>, 2022\n              <br/>\n              <a href=\"https://yifanjiang.net/MalleConv.html\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2201.00392\">arXiv</a>\n              <p/>\n              <p>\n              We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.\n              </p>\n            </td>\n          ",
    "77": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n\t\t\t\t\t\t\t<a href=\"http://yenchenlin.me/nerf-supervision/\">\n                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://yenchenlin.me/\">Lin Yen-Chen</a>, \n              <a href=\"http://www.peteflorence.com/\">Pete Florence</a>, \n              <strong>Jonathan T. Barron</strong>,  <br/>\n              <a href=\"https://scholar.google.com/citations?user=_BPdgV0AAAAJ&amp;hl=en\">Tsung-Yi Lin</a>, \n              <a href=\"https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU\">Alberto Rodriguez</a>,\n              <a href=\"http://web.mit.edu/phillipi/\">Phillip Isola</a>\n              <br/>\n              <em>ICRA</em>, 2022  \n              <br/>\n\t\t\t\t\t\t\t<a href=\"http://yenchenlin.me/nerf-supervision/\">project page</a> / \n\t\t\t\t\t\t\t<a href=\"https://arxiv.org/abs/2203.01913\">arXiv</a> / \n\t\t\t\t\t\t\t<a href=\"https://www.youtube.com/watch?v=_zN-wVwPH1s\">video</a> /\n\t\t\t\t\t\t\t<a href=\"https://github.com/yenchenlin/nerf-supervision-public\">code</a> / \n\t\t\t\t\t\t\t<a href=\"https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing\">colab</a>\t\t\t\t\n              <p/>\n              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>\n            </td>\n          ",
    "76": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n\t\t\t          <a href=\"https://dorverbin.github.io/refnerf/index.html\">\n\t\t\t            <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>\n\t\t\t          </a>\n\t\t\t          <br/>\n\t\t\t          <a href=\"https://scholar.harvard.edu/dorverbin/home\">Dor Verbin</a>,\n\t\t\t          <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n\t\t\t          <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>, <br/>\n\t\t\t          <a href=\"Todd Zickler\">Todd Zickler</a>,\n\t\t\t          <strong>Jonathan T. Barron</strong>,\n\t\t\t          <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>\n\t\t\t          <br/>\n\t\t\t    <em>CVPR</em>, 2022 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>\n\t\t\t          <br/>\n\t\t\t          <a href=\"https://dorverbin.github.io/refnerf/index.html\">project page</a>\n\t\t\t    /\n\t\t\t          <a href=\"https://arxiv.org/abs/2112.03907\">arXiv</a>\n\t\t\t    /\n\t\t\t          <a href=\"https://youtu.be/qrdRH9irAlk\">video</a>\n\t\t\t          <p/>\n\t\t\t          <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>\n\t\t\t        </td>\n\t\t\t      ",
    "75": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://jonbarron.info/mipnerf360\">\n                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <a href=\"https://scholar.harvard.edu/dorverbin/home\">Dor Verbin</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>\n              <br/>\n\t\t\t\t\t\t\t<em>CVPR</em>, 2022 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"http://jonbarron.info/mipnerf360\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2111.12077\">arXiv</a>\n              /\n              <a href=\"https://youtu.be/zBSH-k9GbV4\">video</a>\n              <p/>\n              <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>\n            </td>\n          ",
    "74": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://bmild.github.io/rawnerf/index.html\">\n                <papertitle>NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <a href=\"http://www.ricardomartinbrualla.com/\">Ricardo Martin-Brualla</a>, <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n\t\t\t\t\t\t\t<em>CVPR</em>, 2022 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://bmild.github.io/rawnerf/index.html\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2111.13679\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=JtBS4KBcKVc\">video</a>\n              <p/>\n              <p>\n\t\t\t\t\t\t\t\tProperly training NeRF on raw camera data enables HDR view synthesis and bokeh, and outperforms multi-image denoising.</p>\n            </td>\n          ",
    "72": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n\t\t\t\t\t\t\t<a href=\"https://waymo.com/research/block-nerf/\">\n                <papertitle>Block-NeRF: Scalable Large Scene Neural View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>,\n              <a href=\"http://casser.io/\">Vincent Casser</a>,\n              <a href=\"https://sites.google.com/site/skywalkeryxc/\">Xinchen Yan</a>,\n              <a href=\"https://scholar.google.com/citations?user=5mJUkI4AAAAJ&amp;hl=en\">Sabeek Pradhan</a>, <br/>\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n\t\t\t\t\t\t\t<a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://www.henrikkretzschmar.com/\">Henrik Kretzschmar</a>\n              <br/>\n        <em>CVPR</em>, 2022 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://waymo.com/research/block-nerf/\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2202.05263\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=6lGMCAzBzOQ\">video</a>\n              <p/>\n              <p>We can do city-scale reconstruction by training multiple NeRFs with millions of images.</p>\n            </td>\n          ",
    "71": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://grail.cs.washington.edu/projects/humannerf/\">\n                <papertitle>HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://homes.cs.washington.edu/~chungyi/\">Chung-Yi Weng</a>,\n              <a href=\"https://homes.cs.washington.edu/~curless/\">Brian Curless</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>, <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://www.irakemelmacher.com/\">Ira Kemelmacher-Shlizerman </a>\n              <br/>\n              <em>CVPR</em>, 2022 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://grail.cs.washington.edu/projects/humannerf/\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2201.04127\">arXiv</a>\n              /\n              <a href=\"https://youtu.be/GM-RoZEymmw\">video</a>\n              <p/>\n              <p>Combining NeRF with pose estimation lets you use a monocular video to do free-viewpoint rendering of a human.</p>\n            </td>\n          ",
    "70": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://urban-radiance-fields.github.io/\">\n                <papertitle>Urban Radiance Fields</papertitle>\n              </a>\n              <br/>\n\t\t\t\t\t\t\t<a href=\"http://www.krematas.com/\">Konstantinos Rematas</a>,\n\t\t\t\t\t\t\t<a href=\"https://andrewhliu.github.io/\">Andrew Liu</a>,\n\t\t\t\t\t\t\t<a href=\"https://pratulsrinivasan.github.io/\">Pratul P. Srinivasan</a>,\n\t\t\t\t\t\t\t<strong>Jonathan T. Barron</strong>, <br/>\n\t\t\t\t\t\t\t<a href=\"https://taiya.github.io/\">Andrea Tagliasacchi</a>,\n\t\t\t\t\t\t\t<a href=\"https://www.cs.princeton.edu/~funk/\">Tom Funkhouser</a>,\n\t\t\t\t\t\t\t<a href=\"https://sites.google.com/corp/view/vittoferrari\"> Vittorio Ferrari</a>\n              <br/>\n\t\t\t\t\t\t\t<em>CVPR</em>, 2022\n              <br/>\n              <a href=\"https://urban-radiance-fields.github.io/\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2111.14643\">arXiv</a>\n              /\n              <a href=\"https://www.youtube.com/watch?v=qGlq5DZT6uc\">video</a>\n              <p/>\n              <p>\n\t\t\t\t\t\t\t\tIncorporating lidar and explicitly modeling the sky lets you reconstruct urban environments.</p>\n            </td>\n          ",
    "69": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n      <a href=\"https://arxiv.org/abs/2112.03288\">\n        <papertitle>Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</papertitle>\n      </a>\n      <br/>\n\t\t\t<a href=\"https://niessnerlab.org/members/barbara_roessle/profile.html\">Barbara Roessle</a>,\n\t\t\t<strong>Jonathan T. Barron</strong>,\n\t\t\t<a href=\"https://bmild.github.io/\">Ben Mildenhall</a>, \n\t\t\t<a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>, \n\t\t\t<a href=\"https://www.niessnerlab.org/\">Matthias Nie&#223;ner</a>\n      <br/>\n\t\t\t<em>CVPR</em>, 2022\n      <br/>\n      <a href=\"https://arxiv.org/abs/2112.03288\">arXiv</a>\n      /\n      <a href=\"https://www.youtube.com/watch?v=zzkvvdcvksc\">video</a>\n      <p/>\n      <p>\n      Dense depth completion techniques applied to freely-available sparse stereo data can improve NeRF reconstructions in low-data regimes.\n      </p>\n    </td>\n  ",
    "68": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://ajayj.com/dreamfields\">\n                <papertitle>Zero-Shot Text-Guided Object Generation with Dream Fields</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://www.ajayj.com/\">Ajay Jain</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://people.eecs.berkeley.edu/~pabbeel/\">Pieter Abbeel</a>,\n              <a href=\"https://cs.stanford.edu/~poole/\">Ben Poole</a>\n              <br/>\n        <em>CVPR</em>, 2022\n              <br/>\n              <a href=\"https://ajayj.com/dreamfields\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2112.01455\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=1Fke6w46tv4\">video</a>\n              <p/>\n              <p>Supervising the CLIP embeddings of NeRF renderings lets you to generate 3D objects from text prompts.</p>\n            </td>\n          ",
    "67": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2111.05849\">\n                <papertitle>Advances in Neural Rendering</papertitle>\n              </a>\n              <br/>\n\t\t\t\t\t\t\t<a href=\"https://people.mpi-inf.mpg.de/~atewari/\">Ayush Tewari</a>, \n\t\t\t\t\t\t\t<a href=\"https://justusthies.github.io/\">Justus Thies</a>, \n\t\t\t\t\t\t\t<a href=\"https://bmild.github.io/\">Ben Mildenhall</a>, \n\t\t\t\t\t\t\t<a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>, \n\t\t\t\t\t\t\t<a href=\"https://people.mpi-inf.mpg.de/~tretschk/\">Edgar Tretschk</a>,\n\t\t\t\t\t\t\t<a href=\"https://homes.cs.washington.edu/~yifan1/\">Yifan Wang</a>,\n\t\t\t\t\t\t\t<a href=\"https://christophlassner.de/\">Christoph Lassner</a>,\n\t\t\t\t\t\t\t<a href=\"https://vsitzmann.github.io/\">Vincent Sitzmann</a>,\n\t\t\t\t\t\t\t<a href=\"http://ricardomartinbrualla.com/\">Ricardo Martin-Brualla</a>,\n\t\t\t\t\t\t\t<a href=\"https://stephenlombardi.github.io/\">Stephen Lombardi</a>,\n\t\t\t\t\t\t\t<a href=\"http://www.cs.cmu.edu/~tsimon/\">Tomas Simon</a>,\n\t\t\t\t\t\t\t<a href=\"https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence\">Christian Theobalt</a>,\n\t\t\t\t\t\t\t<a href=\"https://www.niessnerlab.org/\">Matthias Niessner</a>,\n\t\t\t\t\t\t\t<strong>Jonathan T. Barron</strong>,\n\t\t\t\t\t\t\t<a href=\"https://stanford.edu/~gordonwz/\">Gordon Wetzstein</a>,\n\t\t\t\t\t\t\t<a href=\"https://zollhoefer.com/\">Michael Zollhoefer</a>,\n\t\t\t\t\t\t\t<a href=\"https://people.mpi-inf.mpg.de/~golyanik/\">Vladislav Golyanik</a>\n              <br/>\n\t\t\t\t\t\t\t<em>Arxiv</em>, 2021\n              <br/>\n              <p/>\n              <p>\n              A survey of recent progress in neural rendering.\n              </p>\n            </td>\n          ",
    "66": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://markboss.me/publication/2021-neural-pil/\">\n                <papertitle>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</papertitle>\n              </a>\n              <br/>\n\n              <a href=\"https://markboss.me\">Mark Boss</a>, \n              <a href=\"https://varunjampani.github.io\">Varun Jampani</a>,\n              <a href=\"https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/\">Raphael Braun</a>, <br/>\n              <a href=\"http://people.csail.mit.edu/celiu/\">Ce Liu</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/\">Hendrik P. A. Lensch</a>\n              <br/>\n\t\t\t\t\t\t\t<em>NeurIPS</em>, 2021\n              <br/>\n              <a href=\"https://markboss.me/publication/2021-neural-pil/\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=p5cKaNwVp4M\">video</a> /\n              <a href=\"https://arxiv.org/abs/2110.14373\">arXiv</a>\n              <p/>\n              <p>\n              Replacing a costly illumination integral with a simple network query enables more accurate novel view-synthesis and relighting compared to NeRD.\n              </p>\n            </td>\n          ",
    "65": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://hypernerf.github.io/\">\n                <papertitle>HyperNeRF: A Higher-Dimensional Representation\nfor Topologically Varying Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n\t\t\t\t\t\t\t<a href=\"https://keunhong.com\">Keunhong Park</a>,\n\t\t\t\t\t\t\t<a href=\"https://utkarshsinha.com\">Utkarsh Sinha</a>, \n\t\t\t\t\t\t\t<a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <strong>Jonathan T. Barron</strong>, <br/>\n\t\t\t\t\t\t\t<a href=\"http://sofienbouaziz.com\">Sofien Bouaziz</a>,\n\t\t\t\t\t\t\t<a href=\"https://www.danbgoldman.com\">Dan B Goldman</a>,\n\t\t\t\t\t\t\t<a href=\"http://www.ricardomartinbrualla.com\">Ricardo Martin-Brualla</a>, \n\t\t\t\t\t\t\t<a href=\"https://homes.cs.washington.edu/~seitz/\">Steven M. Seitz</a>\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2021 \n              <br/>\n              <a href=\"https://hypernerf.github.io/\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2106.13228\">arXiv</a>\n              <p/>\n              <p>Applying ideas from level set methods to NeRF lets you represent scenes that deform and change shape.</p>\n            </td>\n          ",
    "64": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://people.csail.mit.edu/xiuming/projects/nerfactor/\">\n              <papertitle>NeRFactor: Neural Factorization of Shape and Reflectance<br/>\nUnder an Unknown Illumination</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://people.csail.mit.edu/xiuming/\">Xiuming Zhang</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://boyangdeng.com/\">Boyang Deng</a>,<br/>\n              <a href=\"https://www.pauldebevec.com/\">Paul Debevec</a>,\n              <a href=\"http://billf.mit.edu/\">William T. Freeman</a>,\n\t\t\t\t\t\t\t<strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2021 \n              <br/>\n              <a href=\"https://people.csail.mit.edu/xiuming/projects/nerfactor/\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2106.01970\">arXiv</a>\n              /\n              <a href=\"https://www.youtube.com/watch?v=UUVSPJlwhPg\">video</a>\n              <p/>\n              <p>By placing priors on illumination and materials, we can recover NeRF-like models of the intrinsics of a scene from a single multi-image capture.</p>\n            </td>\n\t\t\t\t\t\t\n          ",
    "63": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2109.06627\">\n                <papertitle>Scalable Font Reconstruction with Dual Latent Manifolds</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.cs.cmu.edu/~asrivats/\">Nikita Srivatsan</a>,\n              <a href=\"http://siwu.io/\">Si Wu</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://cseweb.ucsd.edu/~tberg/\">Taylor Berg-Kirkpatrick</a>\n              <br/>\n              <em>EMNLP</em>, 2021\n              <br/>\n              <p/>\n              <p>VAEs can be used to disentangle a font's style from its content, and to generalize to characters that were never observed during training.</p>\n            </td>\n          ",
    "62": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://jonbarron.info/mipnerf\">\n                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>, <br/>\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <a href=\"http://www.ricardomartinbrualla.com/\">Ricardo Martin-Brualla</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>\n              <br/>\n              <a href=\"http://jonbarron.info/mipnerf\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2103.13415\">arXiv</a>\n              /\n              <a href=\"https://youtu.be/EpH175PY1A0\">video</a>\n\t\t\t\t\t\t\t/\n              <a href=\"https://github.com/google/mipnerf\">code</a>\n              <p/>\n              <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>\n            </td>\n          ",
    "61": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://nerf.live\">\n              <papertitle>Baking Neural Radiance Fields for Real-Time View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://www.pauldebevec.com/\">Paul Debevec</a>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"http://nerf.live\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2103.14645\">arXiv</a>\n              /\n              <a href=\"https://www.youtube.com/watch?v=5jKry8n5YO8\">video</a>\n              /\n              <a href=\"https://nerf.live/#demos\">demo</a>\n              <p/>\n              <p>Baking a trained NeRF into a sparse voxel grid of colors and features lets you render it in real-time in your browser.</p>\n            </td>\n\n\n\n          ",
    "60": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://nerfies.github.io/\">\n                <papertitle>Nerfies: Deformable Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              \n              <a href=\"https://keunhong.com\">Keunhong Park</a>,\n              <a href=\"https://utkarshsinha.com\">Utkarsh Sinha</a>,\n              <strong>Jonathan T. Barron</strong>, <br/>\n              <a href=\"http://sofienbouaziz.com\">Sofien Bouaziz</a>,\n              <a href=\"https://www.danbgoldman.com\">Dan B Goldman</a>,\n              <a href=\"https://homes.cs.washington.edu/~seitz/\">Steven M. Seitz</a>,\n              <a href=\"http://www.ricardomartinbrualla.com\">Ricardo-Martin Brualla</a>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://nerfies.github.io/\">project page</a> /\n              <a href=\"https://arxiv.org/abs/2011.12948\">arXiv</a> /\n              <a href=\"https://www.youtube.com/watch?v=MrKrnHhk8IA\">video</a>\n              <p/>\n              <p>Building deformation fields into NeRF lets you capture non-rigid subjects, like people.\n              </p>\n            </td>\n          ",
    "59": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2011.11890\">\n                <papertitle>Cross-Camera Convolutional Color Constancy</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://sites.google.com/corp/view/mafifi\">Mahmoud Afifi</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://www.chloelegendre.com/\">Chloe LeGendre</a>,\n              <a href=\"https://research.google/people/105312/\">Yun-Ta Tsai</a>,\n              <a href=\"https://www.linkedin.com/in/fbleibel/\">Francois Bleibel</a>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <p/>\n              <p>\n                With some extra (unlabeled) test-set images, you can build a hypernetwork that calibrates itself at test time to previously-unseen cameras.\n              </p>\n            </td>\n          ",
    "58": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://imaging.cs.cmu.edu/dual_pixels/\">\n                <papertitle>Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://shumianxin.github.io/\">Shumian Xin</a>,\n              <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <strong>Jonathan T. Barron</strong>, <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>,\n\t\t\t\t\t\t\t<a href=\"https://www.cs.cmu.edu/~igkioule/\">Ioannis Gkioulekas</a>,\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n\t\t\t\t\t\t\t<br/>\n              <a href=\"https://imaging.cs.cmu.edu/dual_pixels/\">project page</a> /\n              <a href=\"https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring\">code</a>\n              <br/>\n              <p/>\n              <p>\n                Multiplane images can be used to simultaneously deblur dual-pixel images, despite variable defocus due to depth variation in the scene.\n              </p>\n            </td>\n          ",
    "57": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://markboss.me/publication/2021-nerd/\">\n                <papertitle>NeRD: Neural Reflectance Decomposition from Image Collections</papertitle>\n              </a>\n              <br/>\n\n              <a href=\"https://markboss.me\">Mark Boss</a>, \n              <a href=\"https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/\">Raphael Braun</a>,\n              <a href=\"https://varunjampani.github.io\">Varun Jampani</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://people.csail.mit.edu/celiu/\">Ce Liu</a>,\n              <a href=\"https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/\">Hendrik P. A. Lensch</a>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021\n              <br/>\n              <a href=\"https://markboss.me/publication/2021-nerd/\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=JL-qMTXw9VU\">video</a> /\n              <a href=\"https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition\">code</a> /\n              <a href=\"https://arxiv.org/abs/2012.03918\">arXiv</a>\n              <p/>\n              <p>\n              A NeRF-like model that can decompose (and mesh) objects with non-Lambertian reflectances, complex geometry, and unknown illumination.\n              </p>\n            </td>\n          ",
    "56": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2011.12485\">\n                <papertitle>How to Train Neural Networks for Flare Removal</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://yicheng.rice.edu/\">Yicheng Wu</a>,\n              <a href=\"https://scholar.google.com/citations?user=BxqV_RsAAAAJ\">Qiurui He</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>, <br/>\n              <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>,\n              <a href=\"https://computationalimaging.rice.edu/team/ashok-veeraraghavan/\">Ashok Veeraraghavan</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n\t\t\t\t\t\t\t<em>ICCV</em>, 2021\n              <br/>\n              <a href=\"https://yichengwu.github.io/flare-removal/\">project page</a>  / \n              <a href=\"https://arxiv.org/abs/2011.12485\">arXiv</a> \n              <p/>\n              <p>\n                Simulating the optics of a camera's lens lets you train a model that removes lens flare from a single image.\n              </p>\n            </td>\n          ",
    "55": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://yenchenlin.me/inerf/\">\n                <papertitle>iNeRF: Inverting Neural Radiance Fields for Pose Estimation</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://yenchenlin.me/\">Lin Yen-Chen</a>, \n              <a href=\"http://www.peteflorence.com/\">Pete Florence</a>, \n              <strong>Jonathan T. Barron</strong>,  <br/>\n              <a href=\"https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU\">Alberto Rodriguez</a>,\n              <a href=\"http://web.mit.edu/phillipi/\">Phillip Isola</a>,\n              <a href=\"https://scholar.google.com/citations?user=_BPdgV0AAAAJ&amp;hl=en\">Tsung-Yi Lin</a>\n              <br/>\n              <em>IROS</em>, 2021  \n              <br/>\n              <a href=\"http://yenchenlin.me/inerf/\">project page</a> /\n              <a href=\"https://arxiv.org/abs/2012.05877\">arXiv</a> /\n              <a href=\"https://www.youtube.com/watch?v=eQuCZaQN0tI\">video</a>\n              <p/>\n              <p>Given an image of an object and a NeRF of that object, you can estimate that object's pose.\n              </p>\n            </td>\n          ",
    "54": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://ibrnet.github.io/\">\n                <papertitle>IBRNet: Learning Multi-View Image-Based Rendering</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://www.cs.cornell.edu/~qqw/\">Qianqian Wang</a>,\n              <a href=\"https://www.linkedin.com/in/zhicheng-wang-96116897/\">Zhicheng Wang</a>,\n              <a href=\"https://www.kylegenova.com/\">Kyle Genova</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&amp;hl=en\">Howard Zhou</a>, <br/>\n              <strong>Jonathan T. Barron</strong>, \n              <a href=\"http://www.ricardomartinbrualla.com/\">Ricardo Martin-Brualla</a>,\n              <a href=\"https://www.cs.cornell.edu/~snavely/\">Noah Snavely</a>, \n              <a href=\"https://www.cs.princeton.edu/~funk/\">Thomas Funkhouser</a>\n              <br/>\n              <em>CVPR</em>, 2021\n              <br/>\n              <a href=\"https://ibrnet.github.io/\">project page</a> /\n              <a href=\"https://github.com/googleinterns/IBRNet\">code</a> / \n              <a href=\"https://arxiv.org/abs/2102.13090\">arXiv</a>\n              <p/>\n              <p>By learning how to pay attention to input images at render time, \n                  we can amortize inference for view synthesis and reduce error rates by 15%.</p>\n            </td>\n          ",
    "53": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://pratulsrinivasan.github.io/nerv/\">\n                <papertitle>NeRV: Neural Reflection and Visibility Fields for Relighting and View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://boyangdeng.com/\">Boyang Deng</a>,\n              <a href=\"https://people.csail.mit.edu/xiuming/\">Xiuming Zhang</a>, <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>CVPR</em>, 2021\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/nerv/\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=4XyDdvhhjVo\">video</a> /\n              <a href=\"https://arxiv.org/abs/2012.03927\">arXiv</a>\n              <p/>\n              <p>Using neural approximations of expensive visibility integrals lets you recover relightable NeRF-like models.</p>\n            </td>\n          ",
    "52": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://www.matthewtancik.com/learnit\">\n                <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall*</a>,\n              <a href=\"https://www.linkedin.com/in/terrance-wang/\">Terrance Wang</a>,\n              <a href=\"https://www.linkedin.com/in/divi-schmidt-262044180/\">Divi Schmidt</a>, <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n              <em>CVPR</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"http://www.matthewtancik.com/learnit\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=A-r9itCzcyo\">video</a> /\n              <a href=\"https://arxiv.org/abs/2012.02189\">arXiv</a> \n              <p/>\n              <p>Using meta-learning to find weight initializations for coordinate-based MLPs allows them to converge faster and generalize better.</p>\n            </td>\n          ",
    "51": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://nerf-w.github.io/\">\n                <papertitle>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.ricardomartinbrualla.com/\">Ricardo Martin-Brualla*</a>,\n              <a href=\"https://scholar.google.com/citations?user=g98QcZUAAAAJ&amp;hl=en\">Noha Radwan*</a>,\n              <a href=\"https://research.google/people/105804/\">Mehdi S. M. Sajjadi*</a>, <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://scholar.google.com/citations?user=FXNJRDoAAAAJ&amp;hl=en\">Alexey Dosovitskiy</a>,\n              <a href=\"http://www.stronglyconvex.com/about.html\">Daniel Duckworth</a>\n              <br/>\n              <em>CVPR</em>, 2021 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://nerf-w.github.io/\">project page</a> /\n              <a href=\"https://arxiv.org/abs/2008.02268\">arXiv</a> /\n              <a href=\"https://www.youtube.com/watch?v=mRAKVQj5LRA\">video</a>\n              <p/>\n              <p>Letting NeRF reason about occluders and appearance variation produces photorealistic view synthesis using only unstructured internet photos.</p>\n            </td>\n          ",
    "50": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://sniklaus.com/dualref\">\n                <papertitle>Learned Dual-View Reflection Removal</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://sniklaus.com/welcome\">Simon Niklaus</a>,\n              <a href=\"https://people.eecs.berkeley.edu/~cecilia77/\">Xuaner (Cecilia) Zhang</a>,\n              <strong>Jonathan T. Barron</strong>, <br/>\n              <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>,\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>,\n              <a href=\"http://web.cecs.pdx.edu/~fliu/\">Feng Liu</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>\n              <br/>\n              <em>WACV</em>, 2021\n              <br/>\n              <a href=\"http://sniklaus.com/dualref\">project page</a> /\n              <a href=\"https://arxiv.org/abs/2010.00702\">arXiv</a>\n              <p/>\n              <p>\n                Reflections and the things behind them often exhibit parallax, and this lets you remove reflections from stereo pairs.\n              </p>\n            </td>\n          ",
    "49": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://nlt.csail.mit.edu/\">\n                <papertitle>Neural Light Transport for Relighting and View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://people.csail.mit.edu/xiuming/\">Xiuming Zhang</a>,\n              <a href=\"http://www.seanfanello.it/\">Sean Fanello</a>,\n              <a href=\"https://research.google/people/105312/\">Yun-Ta Tsai</a>,\n              <a href=\"http://kevinkingo.com/\">Tiancheng Sun</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <a href=\"https://research.google/people/106687/\">Rohit Pandey</a>,\n              <a href=\"https://www.dtic.ua.es/~sorts/\">Sergio Orts-Escolano</a>,\n              <a href=\"https://dl.acm.org/profile/99659224296\">Philip Davidson</a>,\n              <a href=\"https://scholar.google.com/citations?user=5D0_pjcAAAAJ&amp;hl=en\">Christoph Rhemann</a>,\n              <a href=\"http://www.pauldebevec.com/\">Paul Debevec</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <a href=\"http://billf.mit.edu/\">William T. Freeman</a>\n              <br/>\n              <em>ACM TOG</em>, 2021\n              <br/>\n              <a href=\"http://nlt.csail.mit.edu/\">project page</a> /\n              <a href=\"https://arxiv.org/abs/2008.03806\">arXiv</a> /\n              <a href=\"https://www.youtube.com/watch?v=OGEnCWZihHE\">video</a>\n              <p/>\n              <p>Embedding a convnet within a predefined texture atlas enables simultaneous view synthesis and relighting.</p>\n            </td>\n          ",
    "48": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/\">\n                <papertitle>Light Stage Super-Resolution: Continuous High-Frequency Relighting</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://kevinkingo.com/\">Tiancheng Sun</a>,\n              <a href=\"https://cseweb.ucsd.edu/~zex014/\">Zexiang Xu</a>\n              <a href=\"http://people.csail.mit.edu/xiuming/\">Xiuming Zhang</a>,\n              <a href=\"http://www.seanfanello.it/\">Sean Fanello</a>,\n              <a href=\"https://scholar.google.com/citations?user=5D0_pjcAAAAJ&amp;hl=en\">Christoph Rhemann</a>, <br/>\n              <a href=\"https://www.pauldebevec.com/\">Paul Debevec</a>,\n              <a href=\"https://research.google/people/105312/\">Yun-Ta Tsai</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2020  \n              <br/>\n              <a href=\"http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/\">project page</a> / \n              <a href=\"https://arxiv.org/abs/2010.08888\">arXiv</a>\n              <p/>\n              <p>\n                Scans for light stages are inherently aliased, but we can use learning to super-resolve them.\n              </p>\n            </td>\n          ",
    "47": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://bmild.github.io/fourfeat/index.html\">\n                <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall*</a>,\n              <a href=\"https://people.eecs.berkeley.edu/~sfk/\">Sara Fridovich-Keil</a>, <br/>\n              <a href=\"https://www.linkedin.com/in/nithinraghavan\">Nithin Raghavan</a>,\n              <a href=\"https://scholar.google.com/citations?user=lvA86MYAAAAJ&amp;hl=en\">Utkarsh Singhal</a>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n              <em>NeurIPS</em>, 2020 &amp;nbsp <font color=\"#FF8080\"><strong>(Spotlight)</strong></font>\n              <br/>\n              <a href=\"https://bmild.github.io/fourfeat/\">project page</a> /\n              video: <a href=\"https://www.youtube.com/watch?v=nVA6K6Sn2S4\">3 min</a>, <a href=\"https://www.youtube.com/watch?v=iKyIJ_EtSkw\">10 min</a> /\n              <a href=\"https://arxiv.org/abs/2006.10739\">arXiv</a> /\n              <a href=\"https://github.com/tancik/fourier-feature-networks\">code</a>\n              <p/>\n              <p>Composing neural networks with a simple Fourier feature mapping allows them to learn detailed high-frequency functions.</p>\n            </td>\n          ",
    "46": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2007.07350\">\n                <papertitle>A Generalization of Otsu's Method and Minimum Error Thresholding</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>ECCV</em>, 2020 &amp;nbsp <font color=\"#FF8080\"><strong>(Spotlight)</strong></font>\n              <br/>\n              <a href=\"https://github.com/jonbarron/hist_thresh\">code</a> / \n              <a href=\"https://www.youtube.com/watch?v=rHtQQlQo1Q4\">video</a> / \n              <a href=\"data/BarronECCV2020.bib\">bibtex</a>\n              <br/>\n              <p/>\n              <p>\n              A simple and fast Bayesian algorithm that can be written in ~10 lines of code outperforms or matches giant CNNs on image binarization, and unifies three classic thresholding algorithms.\n              </p>\n            </td>\n          ",
    "45": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2006.04902\">\n                <papertitle>What Matters in Unsupervised Optical Flow</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://ricojonschkowski.com/\">Rico Jonschkowski</a>,\n              <a href=\"https://www.linkedin.com/in/austin-charles-stone-1ba33b138/\">Austin Stone</a>,\n              <strong>Jonathan T. Barron</strong>, <br/>\n              <a href=\"https://research.google/people/ArielGordon/\">Ariel Gordon</a>,\n              <a href=\"https://www.linkedin.com/in/kurt-konolige/\">Kurt Konolige</a>,\n              <a href=\"https://research.google/people/AneliaAngelova/\">Anelia Angelova</a>\n              <br/>\n              <em>ECCV</em>, 2020 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://github.com/google-research/google-research/tree/master/uflow\">code</a>\n              <br/>\n              <p/>\n              <p>\n              Extensive experimentation yields a simple optical flow technique that is trained on only unlabeled videos, but still works as well as supervised techniques.\n              </p>\n            </td>\n          ",
    "44": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://www.matthewtancik.com/nerf\">\n                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall*</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>, <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n              <em>ECCV</em>, 2020 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation, Best Paper Honorable Mention, CACM Research Highlight)</strong></font>\n              <br/>\n              <a href=\"http://www.matthewtancik.com/nerf\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2003.08934\">arXiv</a>\n              /\n              <a href=\"https://www.youtube.com/watch?v=LRAqeM8EjOo&amp;t\">talk video</a>\n              /\n              <a href=\"https://www.youtube.com/watch?v=JuH79E8rdKc\">supp video</a>\n              /\n              <a href=\"https://github.com/bmild/nerf\">code</a>\n              /\n              <a href=\"https://cacm.acm.org/magazines/2022/1/257450-nerf/fulltext\">CACM</a> <a href=\"https://cacm.acm.org/magazines/2022/1/257453-technical-perspective-neural-radiance-fields-explode-on-the-scene/fulltext\">(foreward)</a>\n              <p/>\n              <p>\n              Training a tiny non-convolutional neural network to reproduce a scene using volume rendering achieves photorealistic view synthesis.</p>\n            </td>\n          ",
    "43": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2005.08925\">\n                <papertitle>Portrait Shadow Manipulation</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://people.eecs.berkeley.edu/~cecilia77/\">Xuaner (Cecilia) Zhang</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://ai.google/research/people/105312/\">Yun-Ta Tsai</a>, <br/>\n              <a href=\"https://www.linkedin.com/in/rohit-pandey-bab10b7a/\">Rohit Pandey</a>,\n              <a href=\"http://people.csail.mit.edu/xiuming/\">Xiuming Zhang</a>,\n              <a href=\"http://graphics.stanford.edu/~renng/\">Ren Ng</a>,\n              <a href=\"http://graphics.stanford.edu/~dejacobs/\">David E. Jacobs</a>\n              <br/>\n              <em>SIGGRAPH</em>, 2020  \n              <br/>\n              <a href=\"https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait\">project page</a> / \n              <a href=\"https://www.youtube.com/watch?v=M_qYTXhzyac\">video</a>\n              <p/>\n              <p>Networks can be trained to remove shadows cast on human faces and to soften harsh lighting.</p>\n            </td>\n          ",
    "42": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2004.12260\">\n                <papertitle>Learning to Autofocus</papertitle>\n              </a>\n              <br/>\n              <a href=\"\">Charles Herrmann</a>,\n              <a href=\"\">Richard Strong Bowen</a>,\n              <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>, <br/>\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>,\n              <a href=\"https://scholar.google.com/citations?user=BxqV_RsAAAAJ\">Qiurui He</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://www.cs.cornell.edu/~rdz/index.htm\">Ramin Zabih</a>\n              <br/>\n              <em>CVPR</em>, 2020  \n              <br/>\n\t\t\t\t\t\t\t<a href=\"https://learntoautofocus-google.github.io/\">project page</a>\n\t\t\t\t\t\t\t/\n              <a href=\"https://arxiv.org/abs/2004.12260\">arXiv</a>\n              <p/>\n              <p>Machine learning can be used to train cameras to autofocus (which is not the same problem as \"depth from defocus\").</p>\n            </td>\n          ",
    "41": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://pratulsrinivasan.github.io/lighthouse/\">\n                <papertitle>Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall*</a>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>, <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://research.google/people/RichardTucker/\">Richard Tucker</a>,\n              <a href=\"https://www.cs.cornell.edu/~snavely/\">Noah Snavely</a>\n              <br/>\n        <em>CVPR</em>, 2020  \n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/lighthouse/\">project page</a>\n        /\n              <a href=\"https://github.com/pratulsrinivasan/lighthouse\">code</a>\n        /\n              <a href=\"https://arxiv.org/abs/2003.08367\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=KsiZpUFPqIU\">video</a>\n              <p/>\n              <p>We predict a volume from an input stereo pair that can be used to calculate incident lighting at any 3D point within a scene.</p>\n            </td>\n          ",
    "40": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2006.10172\">\n                <papertitle>Sky Optimization: Semantically Aware Image Processing of Skies in Low-Light Photography</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://sites.google.com/corp/view/orly-liba/\">Orly Liba</a>,\n              <a href=\"https://www.linkedin.com/in/longqicai/en-us\">Longqi Cai</a>,\n              <a href=\"https://ai.google/research/people/105312/\">Yun-Ta Tsai</a>,\n              <a href=\"https://research.google/people/EladEban/\">Elad Eban</a>,\n              <a href=\"https://research.google/people/YairMovshovitzAttias/\">Yair Movshovitz-Attias</a>, <br/>\n              <a href=\"https://scholar.google.com/citations?user=2jXxOYQAAAAJ\">Yael Pritch</a>,\n              <a href=\"https://www.linkedin.com/in/huizhong-chen-00776432\">Huizhong Chen</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>NTIRE CVPRW</em>, 2020  \n              <br/>\n              <a href=\"https://google.github.io/sky-optimization/\">project page</a>\n              <p/>\n              <p>If you want to photograph the sky, it helps to know where the sky is.</p>\n            </td>\n          ",
    "39": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1910.11336\">\n                <papertitle>Handheld Mobile Photography in Very Low Light</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://sites.google.com/site/orlylibaprofessional/\">Orly Liba</a>,\n              <a href=\"https://scholar.google.com/citations?user=6PhlPWMAAAAJ\">Kiran Murthy</a>,\n              <a href=\"https://ai.google/research/people/105312/\">Yun-Ta Tsai</a>,\n              <a href=\"https://www.timothybrooks.com/\">Timothy Brooks</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <a href=\"https://scholar.google.com/citations?user=qgc_jY0AAAAJ\">Nikhil Karnad</a>,\n              <a href=\"https://scholar.google.com/citations?user=BxqV_RsAAAAJ\">Qiurui He</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://ai.google/research/people/105641/\">Dillon Sharlet</a>,\n              <a href=\"http://www.geisswerks.com/\">Ryan Geiss</a>,\n              <a href=\"https://people.csail.mit.edu/hasinoff/\">Samuel W. Hasinoff</a>,\n              <a href=\"https://scholar.google.com/citations?user=2jXxOYQAAAAJ\">Yael Pritch</a>,\n              <a href=\"http://graphics.stanford.edu/~levoy/\">Marc Levoy</a>\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2019\n              <br/>\n              <a href=\"https://github.com/google/night-sight/tree/master/docs\">project page</a>\n              <br/>\n              <p/>\n              <p>By rethinking metering, white balance, and tone mapping, we can take pictures in places too dark for humans to see clearly.</p>\n            </td>\n          ",
    "38": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1910.00748\">\n                <papertitle>A Deep Factorization of Style and Structure in Fonts</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.cs.cmu.edu/~asrivats/\">Nikita Srivatsan</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://people.eecs.berkeley.edu/~klein/\">Dan Klein</a>,\n              <a href=\"http://cseweb.ucsd.edu/~tberg/\">Taylor Berg-Kirkpatrick</a>\n              <br/>\n              <em>EMNLP</em>, 2019 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <p/>\n              <p>Variational auto-encoders can be used to disentangle a characters style from its content.</p>\n            </td>\n          ",
    "37": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1904.05822\">\n                <papertitle>Learning Single Camera Depth Estimation using Dual-Pixels</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>,\n              <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>,\n              <a href=\"\">Sameer Ansari</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>ICCV</em>, 2019 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://github.com/google-research/google-research/tree/master/dual_pixels\">code</a> /\n              <a href=\"data/GargICCV2019.bib\">bibtex</a>\n              <p/>\n              <p>Considering the optics of dual-pixel image sensors improves monocular depth estimation techniques.</p>\n            </td>\n          ",
    "36": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://cseweb.ucsd.edu/~viscomp/projects/SIG19PortraitRelighting/\">\n                <papertitle>Single Image Portrait Relighting</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://kevinkingo.com/\">Tiancheng Sun</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://ai.google/research/people/105312/\">Yun-Ta Tsai</a>,\n              <a href=\"https://cseweb.ucsd.edu/~zex014/\">Zexiang Xu</a>, Xueming Yu,\n              <a href=\"http://ict.usc.edu/profile/graham-fyffe/\">Graham Fyffe</a>, Christoph Rhemann, Jay Busch,\n              <a href=\"https://www.pauldebevec.com/\">Paul Debevec</a>,\n              <a href=\"https://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>\n              <br/>\n              <em>SIGGRAPH</em>, 2019\n              <br/>\n              <a href=\"http://cseweb.ucsd.edu/~viscomp/projects/SIG19PortraitRelighting/\">project page</a> / \n              <a href=\"https://arxiv.org/abs/1905.00824\">arxiv</a> / \n              <a href=\"https://www.youtube.com/watch?v=yxhGWds_g4I\">video</a> /\n              <a href=\"https://petapixel.com/2019/07/16/researchers-developed-an-ai-that-can-relight-portraits-after-the-fact/\">press</a> /\n              <a href=\"data/SunSIGGRAPH2019.bib\">bibtex</a>\n              <br/>\n              <p/>\n              <p>Training a neural network on light stage scans and environment maps produces an effective relighting method.</p>\n            </td>\n          ",
    "35": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq\">\n                <papertitle>A General and Adaptive Robust Loss Function</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>CVPR</em>, 2019 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>\n              <br/>\n              <a href=\"https://arxiv.org/abs/1701.03077\">arxiv</a> /\n              <a href=\"https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4\">supplement</a> /\n              <a href=\"https://youtu.be/BmNKbnF69eY\">video</a> /\n              <a href=\"https://www.youtube.com/watch?v=4IInDT_S0ow&amp;t=37m22s\">talk</a> / \n              <a href=\"https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing\">slides</a> / \n              code: <a href=\"https://github.com/google-research/google-research/tree/master/robust_loss\">TF</a>, <a href=\"https://github.com/google-research/google-research/tree/master/robust_loss_jax\">JAX</a>, <a href=\"https://github.com/jonbarron/robust_loss_pytorch\">pytorch</a> /\n              <a href=\"data/BarronCVPR2019_reviews.txt\">reviews</a> /\n              <a href=\"data/BarronCVPR2019.bib\">bibtex</a>\n              <p/>\n              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>\n            </td>\n          ",
    "34": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1TU5L6fnt4Kd49IUOU7aNxor5NIgdHuNG/view?usp=sharing\">\n                <papertitle>Pushing the Boundaries of View Extrapolation with Multiplane Images</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul P. Srinivasan</a>, Richard Tucker,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <a href=\"http://graphics.stanford.edu/~renng/\">Ren Ng</a>,\n              <a href=\"https://www.cs.cornell.edu/~snavely/\">Noah Snavely</a>\n              <br/>\n              <em>CVPR</em>, 2019 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1GUW_n-BAn9Q4VntEA_OTHNJiHO7XfC62/view?usp=sharing\">supplement</a> /\n              <a href=\"https://www.youtube.com/watch?v=aJqAaMNL2m4\">video</a> /\n              <a href=\"data/SrinivasanCVPR2019.bib\">bibtex</a>\n              <p/>\n              <p>View extrapolation with multiplane images works better if you reason about disocclusions and disparity sampling frequencies.</p>\n            </td>\n          ",
    "33": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1H0Wtd--un2JN76dUJN8iC9fWfkA16n8D/view?usp=sharing\">\n                <papertitle>Unprocessing Images for Learned Raw Denoising</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://timothybrooks.com/\">Tim Brooks</a>,\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>,\n              <a href=\"http://www.dsharlet.com/\">Dillon Sharlet</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>CVPR</em>, 2019 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://arxiv.org/abs/1811.11127\">arxiv</a> /\n              <a href=\"http://timothybrooks.com/tech/unprocessing/\">project page</a> /\n              <a href=\"https://github.com/google-research/google-research/tree/master/unprocessing\">code</a> / \n              <a href=\"data/BrooksCVPR2019.bib\">bibtex</a>\n              <p/>\n              <p>We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p>\n            </td>\n          ",
    "32": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1hWpA4f6iLVcOkZI3zEAAWKARSQhnVgbY/view?usp=sharing\">\n                <papertitle>Learning to Synthesize Motion Blur</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://timothybrooks.com/\">Tim Brooks</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>CVPR</em>, 2019 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://arxiv.org/abs/1811.11745\">arxiv</a> /\n              <a href=\"https://drive.google.com/file/d/1dUQwBMmQdYYIP0zHR_nDQY-uQbaMdcSN/view?usp=sharing\">supplement</a> /\n              <a href=\"http://timothybrooks.com/tech/motion-blur/\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=8T1jjSz-2V8\">video</a> /\n              <a href=\"https://github.com/google-research/google-research/tree/master/motion_blur\">code</a> / \n              <a href=\"data/BrooksBarronCVPR2019.bib\">bibtex</a>\n              <p/>\n              <p>Frame interpolation techniques can be used to train a network that directly synthesizes linear blur kernels.</p>\n            </td>\n          ",
    "31": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1901.01370\">\n                <papertitle>Stereoscopic Dark Flash for Low-light Photography</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://www.andrew.cmu.edu/user/jianwan2/\">Jian Wang</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>\n              <br/>\n              <em>ICCP</em>, 2019\n              <br/>\n              <p/>\n              <p>\n                By making one camera in a stereo pair hyperspectral we can multiplex dark flash pairs in space instead of time.\n              </p>\n            </td>\n          ",
    "30": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing\">\n                <papertitle>Depth from Motion for Smartphone AR</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://www.linkedin.com/in/valentinjulien/\">Julien Valentin</a>,\n              <a href=\"https://www.linkedin.com/in/adarshkowdle/\">Adarsh Kowdle</a>,\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>, and others\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2018\n              <br/>\n              <a href=\"https://github.com/jonbarron/planar_filter\">planar filter toy code</a> / \n              <a href=\"data/Valentin2018.bib\">bibtex</a>\n              <p/>\n              <p>Depth cues from camera motion allow for real-time occlusion effects in augmented reality applications.</p>\n            </td>\n          ",
    "29": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing\">\n                <papertitle>Synthetic Depth-of-Field with a Single-Camera Mobile Phone</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>,\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>,\n              <a href=\"http://graphics.stanford.edu/~dejacobs/\">David E. Jacobs</a>, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,\n              <a href=\"http://www.cs.cmu.edu/~ymovshov/\">Yair Movshovitz-Attias</a>,\n              <strong>Jonathan T. Barron</strong>, Yael Pritch,\n              <a href=\"http://graphics.stanford.edu/~levoy/\">Marc Levoy</a>\n              <br/>\n              <em>SIGGRAPH</em>, 2018\n              <br/>\n              <a href=\"https://arxiv.org/abs/1806.04171\">arxiv</a> /\n              <a href=\"https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html\">blog post</a> /\n              <a href=\"data/Wadhwa2018.bib\">bibtex</a>\n              <p/>\n              <p>Dual pixel cameras and semantic segmentation algorithms can be used for shallow depth of field effects.</p>\n              <p>This system is the basis for \"Portrait Mode\" on the Google Pixel 2 smartphones</p>\n            </td>\n          ",
    "28": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1MpvxcW7OTJP321QL_q4ZLQ8D653bZZzy/view?usp=sharing\">\n                <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul P. Srinivasan</a>,\n              <a href=\"http://rahuldotgarg.appspot.com/\">Rahul Garg</a>,\n              <a href=\"http://nealwadhwa.com\">Neal Wadhwa</a>,\n              <a href=\"http://graphics.stanford.edu/~renng/\">Ren Ng</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>CVPR</em>, 2018\n              <br/>\n              <a href=\"https://github.com/google/aperture_supervision\">code</a> /\n              <a href=\"data/Srinivasan2018.bib\">bibtex</a>\n              <p/>\n              <p>Varying a camera's aperture provides a supervisory signal that can teach a neural network to do monocular depth estimation.</p>\n            </td>\n          ",
    "27": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1GAH8ijyZ7GnoBnQFANEzdXinHrE4vvXn/view?usp=sharing\">\n                <papertitle>Burst Denoising with Kernel Prediction Networks</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://bmild.github.io/\">Ben Mildenhall</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>,\n              <a href=\"http://www.dsharlet.com/\">Dillon Sharlet</a>,\n              <a href=\"http://graphics.stanford.edu/~renng/\">Ren Ng</a>, Robert Carroll\n              <br/>\n              <em>CVPR</em>, 2018 &amp;nbsp <font color=\"#FF8080\"><strong>(Spotlight)</strong></font>\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1aqk3Q-L2spjLZh2yRWKUWIDcZkGjQ7US/view?usp=sharing\">supplement</a> /\n              <a href=\"https://github.com/google/burst-denoising\">code</a> /\n              <a href=\"data/Mildenhall2018.bib\">bibtex</a>\n              <p/>\n              <p>We train a network to predict linear kernels that denoise noisy bursts from cellphone cameras.</p>\n            </td>\n          ",
    "26": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1w_0djhL0QgC_fbehnJ0c-J23_kW_420p/view?usp=sharing\">\n                <papertitle>A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://homes.cs.washington.edu/~amrita/\">Amrita Mazumdar</a>, <a href=\"http://homes.cs.washington.edu/~armin/\">Armin Alaghi</a>, <strong>Jonathan T. Barron</strong>, <a href=\"https://www.cs.unc.edu/~gallup/\">David Gallup</a>, <a href=\"https://homes.cs.washington.edu/~luisceze/\">Luis Ceze</a>, <a href=\"https://homes.cs.washington.edu/~oskin/\">Mark Oskin</a>, <a href=\"http://homes.cs.washington.edu/~seitz/\">Steven M. Seitz</a>\n              <br/>\n              <em>High-Performance Graphics (HPG)</em>, 2017\n              <br/>\n              <a href=\"https://sampa.cs.washington.edu/projects/vr-hw.html\">project page</a>\n              <p/>\n              <p>A reformulation of the bilateral solver can be implemented efficiently on GPUs and FPGAs.</p>\n            </td>\n          ",
    "25": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1jQY3CTMnLX7PeGUzYLso9H1eCsZyWbwg/view?usp=sharing\">\n                <papertitle>Deep Bilateral Learning for Real-Time Image Enhancement</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.mgharbi.com\">Micha&#235;l Gharbi</a>, <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>, <strong>Jonathan T. Barron</strong>, <a href=\"https://people.csail.mit.edu/hasinoff/\">Samuel W. Hasinoff</a>, <a href=\"http://people.csail.mit.edu/fredo/\">Fr&#233;do Durand </a>\n              <br/>\n              <em>SIGGRAPH</em>, 2017\n              <br/>\n              <a href=\"https://groups.csail.mit.edu/graphics/hdrnet/\">project page</a> /\n              <a href=\"https://www.youtube.com/watch?v=GAe0qKKQY_I\">video</a> /\n              <a href=\"data/GharbiSIGGRAPH2017.bib\">bibtex</a> /\n              <a href=\"http://news.mit.edu/2017/automatic-image-retouching-phone-0802\">p</a><a href=\"https://www.wired.com/story/googles-new-algorithm-perfects-photos-before-you-even-take-them/\">r</a><a href=\"https://petapixel.com/2017/08/02/new-ai-can-retouch-photos-snap/\">e</a><a href=\"https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning\">s</a><a href=\"http://gizmodo.com/clever-camera-app-uses-deep-learning-to-perfectly-retou-1797474282\">s</a>\n              <p/>\n              <p>By training a deep network in bilateral space we can learn a model for high-resolution and real-time image enhancement.</p>\n            </td>\n          ",
    "24": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1611.07596\">\n                <papertitle>Fast Fourier Color Constancy</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://ai.google/research/people/105312/\">Yun-Ta Tsai</a>,\n              <br/>\n              <em>CVPR</em>, 2017\n              <br/>\n              <a href=\"https://youtu.be/rZCXSfl13rY\">video</a> /\n              <a href=\"data/BarronTsaiCVPR2017.bib\">bibtex</a> /\n              <a href=\"https://github.com/google/ffcc\">code</a> /\n              <a href=\"https://drive.google.com/open?id=0B4nuwEMaEsnmWkJQMlFPSFNzbEk\">output</a> /\n              <a href=\"https://blog.google/products/photos/six-tips-make-your-photos-pop/\">blog post</a> /\n              <a href=\"https://9to5google.com/2017/03/03/google-photos-auto-white-balance/\">p</a><a href=\"https://www.engadget.com/2017/03/03/google-photos-automatically-fixes-your-pictures-white-balance/\">r</a><a href=\"https://lifehacker.com/google-photos-will-now-automatically-adjust-the-white-b-1793009155\">e</a><a href=\"https://petapixel.com/2017/03/06/google-photos-will-now-automatically-white-balance-snapshots/\">s</a><a href=\"http://www.theverge.com/2017/3/3/14800062/google-photos-auto-white-balance-android\">s</a>\n              <p/>\n              <p>Color space can be aliased, allowing white balance models to be learned and evaluated in the frequency domain. This improves accuracy by 13-20% and speed by 250-3000x.</p>\n              <p>This technology is used by <a href=\"https://store.google.com/product/pixel_compare\">Google Pixel</a>, <a href=\"https://photos.google.com/\">Google Photos</a>, and <a href=\"https://www.google.com/maps\">Google Maps</a>.</p>\n            </td>\n          ",
    "23": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing\">\n                <papertitle>Jump: Virtual Reality Video</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://mi.eng.cam.ac.uk/~ra312/\">Robert Anderson</a>, <a href=\"https://www.cs.unc.edu/~gallup/\">David Gallup</a>, <strong>Jonathan T. Barron</strong>, <a href=\"https://mediatech.aalto.fi/~janne/index.php\">Janne Kontkanen</a>, <a href=\"https://www.cs.cornell.edu/~snavely/\">Noah Snavely</a>, <a href=\"http://carlos-hernandez.org/\">Carlos Hern&amp;aacutendez</a>, <a href=\"https://homes.cs.washington.edu/~sagarwal/\">Sameer Agarwal</a>, <a href=\"https://homes.cs.washington.edu/~seitz/\">Steven M Seitz</a>\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2016\n              <br/>\n              <a href=\"https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing\">supplement</a> /\n              <a href=\"https://www.youtube.com/watch?v=O0qUYynupTI\">video</a> /\n              <a href=\"data/Anderson2016.bib\">bibtex</a> /\n              <a href=\"https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/\">blog post</a>\n              <p/>\n              <p>Using computer vision and a ring of cameras, we can make video for virtual reality headsets that is both stereo and 360&#176;.</p>\n              <p>This technology is used by <a href=\"https://vr.google.com/jump/\">Jump</a>. </p>\n            </td>\n          ",
    "22": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1SSSmVHWbMQ7sZMOredSVWVJXbXobkyzA/view?usp=sharing\">\n                <papertitle>Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://people.csail.mit.edu/hasinoff/\">Samuel W. Hasinoff</a>, <a href=\"http://www.dsharlet.com/\">Dillon Sharlet</a>, <a href=\"http://www.geisswerks.com/\">Ryan Geiss</a>, <a href=\"http://people.csail.mit.edu/abadams/\">Andrew Adams</a>, <strong>Jonathan T. Barron</strong>, Florian Kainz, <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>, <a href=\"http://graphics.stanford.edu/~levoy/\">Marc Levoy</a>\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2016\n              <br/>\n              <a href=\"http://hdrplusdata.org/\">project page</a> /\n              <a href=\"https://drive.google.com/open?id=15EUuSDi1BtHUgQCaiooVrD44qYKIC3vx\">supplement</a> /\n              <a href=\"data/Hasinoff2016.bib\">bibtex</a>\n              <p/>\n              <p>Mobile phones can take beautiful photographs in low-light or high dynamic range environments by aligning and merging a burst of images.</p>\n              <p>This technology is used by the <a href=\"https://research.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html\">Nexus HDR+</a> feature.</p>\n            </td>\n          ",
    "21": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1zFzCaFwkGK1EGmJ_KEqb-ZsRJhfUKN2S/view?usp=sharing\">\n                <papertitle>The Fast Bilateral Solver</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"https://cs.stanford.edu/~poole/\">Ben Poole</a>\n              <br/>\n              <em>ECCV</em>, 2016 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>\n              <br/>\n              <a href=\"http://arxiv.org/abs/1511.03296\">arXiv</a> /\n              <a href=\"data/BarronPooleECCV2016.bib\">bibtex</a> /\n              <a href=\"http://videolectures.net/eccv2016_barron_bilateral_solver/\">video (they messed up my slides, use &#8594;)</a> /\n              <a href=\"https://drive.google.com/file/d/19x1AeN0PFus6Pjrd8nR-vCmJ6bNEefsC/view?usp=sharing\">keynote</a> (or <a href=\"https://drive.google.com/file/d/1p9nduiymK9jUh7WfwlsMjBfW8RoNe_61/view?usp=sharing\">PDF</a>) /\n              <a href=\"https://github.com/poolio/bilateral_solver\">code</a> /\n              <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmaDI3bm5VeDRxams/view?usp=sharing&amp;resourcekey=0-pmkbnOuy8caA7-3GGSfeNQ\">depth super-res results</a> /\n              <a href=\"data/BarronPooleECCV2016_reviews.txt\">reviews</a>\n              <p/>\n              <p>Our solver smooths things better than other filters and faster than other optimization algorithms, and you can backprop through it.</p>\n            </td>\n          ",
    "20": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1mmT-LuK_eBZsl3qp4-fAshEPdgfbgvNE/view?usp=sharing\">\n                <papertitle>Geometric Calibration for Mobile, Stereo, Autofocus Cameras</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.stephendiverdi.com/\">Stephen DiVerdi</a>,\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>WACV</em>, 2016\n              <br/>\n              <a href=\"data/Diverdi2016.bib\">bibtex</a>\n              <p/>\n              <p>Standard techniques for stereo calibration don't work for cheap mobile cameras.</p>\n            </td>\n          ",
    "19": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/178Xj2PZ1w6hZJpucU-TiZOoCemJmvsVQ/view?usp=sharing\">\n                <papertitle>Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</papertitle>\n              </a>\n              <br/>\n              <em>CVPR</em>, 2016\n              <br/>\n              <a href=\"http://liangchiehchen.com/\">Liang-Chieh Chen</a>, <strong>Jonathan T. Barron</strong>, <a href=\"http://ttic.uchicago.edu/~gpapan/\">George Papandreou</a>, <a href=\"http://www.cs.ubc.ca/~murphyk/\">Kevin Murphy</a>, <a href=\"http://www.stat.ucla.edu/~yuille/\">Alan L. Yuille</a>\n              <br/>\n              <a href=\"data/Chen2016.bib\">bibtex</a> /\n              <a href=\"http://liangchiehchen.com/projects/DeepLab.html\">project page</a> /\n              <a href=\"https://bitbucket.org/aquariusjay/deeplab-public-ver2\">code</a>\n              <p/>\n              <p>By integrating an edge-aware filter into a convolutional neural network we can learn an edge-detector while improving semantic segmentation.</p>\n            </td>\n          ",
    "18": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1id74VNDL8ACrrWf6vYgN2M4kS8gd4n7w/view?usp=sharing\">\n                <papertitle>Convolutional Color Constancy</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>\n              <br/>\n              <em>ICCV</em>, 2015\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1vO3sVOMihmpNqsuASeR46Y_iME0lOANR/view?usp=sharing\">supplement</a> / <a href=\"data/BarronICCV2015.bib\">bibtex</a> / <a href=\"https://youtu.be/saHwKY9rfx0\">video</a> (or <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmalBNUzlENUJSVDg/view?usp=sharing\">mp4</a>)\n              <p/>\n              <p>By framing white balance as a chroma localization task we can discriminatively learn a color constancy model that beats the state-of-the-art by 40%.</p>\n            </td>\n          ",
    "17": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1stygV71uBruD7Ck9CaAQr7nREvr3DtUL/view?usp=sharing\">\n                <papertitle>Scene Intrinsics and Depth from a Single Image</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://imaginarynumber.net/\">Evan Shelhamer</a>, <strong>Jonathan T. Barron</strong>, <a href=\"http://www.eecs.berkeley.edu/%7Etrevor/\">Trevor Darrell</a>\n              <br/>\n              <em>ICCV Workshop</em>, 2015\n              <br/>\n              <a href=\"data/Shelhamer2015.bib\">bibtex</a>\n              <p/>\n              <p>The monocular depth estimates produced by fully convolutional networks can be used to inform intrinsic image estimation.</p>\n            </td>\n          ",
    "16": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://drive.google.com/file/d/1R4RdaBZIs-uJobhIFs9yKf3jIsaHQNH0/view?usp=sharing\">\n                <papertitle>Fast Bilateral-Space Stereo for Synthetic Defocus</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://people.csail.mit.edu/abadams/\">Andrew Adams</a>, <a href=\"http://people.csail.mit.edu/yichangshih/\">YiChang Shih</a>, <a href=\"http://carlos-hernandez.org/\">Carlos Hern&amp;aacutendez</a>\n              <br/>\n              <em>CVPR</em>, 2015 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://drive.google.com/file/d/125qgMdqeT1vojMIijIKcOF099LjUgUOL/view?usp=sharing\">abstract</a> /\n              <a href=\"https://drive.google.com/file/d/1HGGvVOGxmPjvgdK5q3UD1Qb5Nttg6kq9/view?usp=sharing\">supplement</a> /\n              <a href=\"data/BarronCVPR2015.bib\">bibtex</a> /\n              <a href=\"http://techtalks.tv/talks/fast-bilateral-space-stereo-for-synthetic-defocus/61624/\">talk</a> /\n              <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmSzZZdUJSMllSUkE/view?usp=sharing\">keynote</a> (or <a href=\"https://drive.google.com/open?id=0B4nuwEMaEsnmZ1ZXUzBCWDJYeFU\">PDF</a>)\n              <p/>\n              <p>By embedding a stereo optimization problem in \"bilateral-space\" we can very quickly solve for an edge-aware depth map, letting us render beautiful depth-of-field effects.</p>\n              <p>This technology is used by the <a href=\"http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html\">Google Camera \"Lens Blur\"</a> feature. </p>\n            </td>\n          ",
    "15": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://arxiv.org/abs/1503.00848\" id=\"MCG_journal\">\n                <papertitle>Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://imatge.upc.edu/web/people/jordi-pont-tuset\">Jordi Pont-Tuset</a>, <a href=\"http://www.cs.berkeley.edu/~arbelaez/\">Pablo Arbel&amp;aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href=\"http://imatge.upc.edu/web/ferran\">Ferran Marqu&amp;eacutes</a>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>TPAMI</em>, 2017\n              <br/>\n              <a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/\">project page</a> /\n              <a href=\"data/PontTusetTPAMI2017.bib\">bibtex</a> /\n              <a href=\"https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing\">fast eigenvector code</a>\n              <p/>\n              <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&amp;times faster.</p>\n              <p>This paper subsumes our CVPR 2014 paper.</p>\n            </td>\n          ",
    "14": "<td width=\"75%\" valign=\"middle\">\n              <p>\n                <a href=\"https://arxiv.org/abs/2010.03592\" id=\"SIRFS\">\n                  <papertitle>Shape, Illumination, and Reflectance from Shading</papertitle>\n                </a>\n                <br/>\n                <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n                <br/>\n                <em>TPAMI</em>, 2015\n                <br/>\n                <a href=\"data/BarronMalikTPAMI2015.bib\">bibtex</a> / <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmVWpfa19mbUxIYW8/view?usp=sharing\">keynote</a> (or <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmazJvLXJUb0NuM1U/view?usp=sharing\">powerpoint</a>, <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmTDBUWE96VHJndjg/view?usp=sharing\">PDF</a>) / <a href=\"http://www.youtube.com/watch?v=NnePYprvFvA\">video</a> / <a href=\"https://drive.google.com/file/d/1vg9Rb-kBntSTnTCzVgFlskkPXvTB_5aq/view?usp=sharing\">code &amp; data</a> / <a href=\"https://drive.google.com/file/d/11X5Zfjy7Q7oP_V2rtqy2f5-x9YgQUAFd/view?usp=sharing\">kudos</a>\n              </p>\n              <p>\n                We present <strong>SIRFS</strong>, which can estimate shape, chromatic illumination, reflectance, and shading from a single image of an masked object.\n              </p>\n              <p>\n                This paper subsumes our CVPR 2011, CVPR 2012, and ECCV 2012 papers.\n              </p>\n            </td>\n          ",
    "13": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1M0wijHY134F9ETBgO8mjeuKUSblTRLG0/view?usp=sharing\">\n                <papertitle>Multiscale Combinatorial Grouping</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.cs.berkeley.edu/~arbelaez/\">Pablo Arbel&amp;aacuteez</a>, <a href=\"http://imatge.upc.edu/web/people/jordi-pont-tuset\">Jordi Pont-Tuset</a>, <strong>Jonathan T. Barron</strong>, <a href=\"http://imatge.upc.edu/web/ferran\">Ferran Marqu&amp;eacutes</a>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>CVPR</em>, 2014\n              <br/>\n              <a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/\">project page</a> /\n              <a href=\"data/ArbelaezCVPR2014.bib\">bibtex</a>\n              <p>This paper is subsumed by <a href=\"#MCG_journal\">our journal paper</a>.</p>\n            </td>\n          ",
    "12": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1shvItvx_8Sb8QNXhrOXkuRmx2618iwNJ/view?usp=sharing\">\n                <papertitle>Volumetric Semantic Segmentation using Pyramid Context Features</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~arbelaez/\">Pablo Arbel&amp;aacuteez</a>, <a href=\"http://big.lbl.gov/\">Soile V. E. Ker&amp;aumlnen</a>, <a href=\"http://www.lbl.gov/gsd/biggin.html\">Mark D. Biggin</a>,\n              <br/> <a href=\"http://dwknowles.lbl.gov/\">David W. Knowles</a>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>ICCV</em>, 2013\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1htiLpMAcYLtuBthmAb4XHnOYxUbkfnqR/view?usp=sharing\">supplement</a> /\n              <a href=\"https://drive.google.com/file/d/1qoYeFNa443myn2SfcdhmCsYBqE9xQrPD/view?usp=sharing\">poster</a> /\n              <a href=\"data/BarronICCV2013.bib\">bibtex</a> / <a href=\"http://www.youtube.com/watch?v=Y56-FcfnlVA&amp;hd=1\">video 1</a> (or <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing\">mp4</a>) / <a href=\"http://www.youtube.com/watch?v=mvRoYuP6-l4&amp;hd=1\">video 2</a> (or <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing\">mp4</a>) / <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmSF9YdWJjQmh4QW8/view?usp=sharing\">code &amp; data</a>\n              <p>\n                We present a technique for efficient per-voxel linear classification, which enables accurate and fast semantic segmentation of volumetric Drosophila imagery.\n              </p>\n            </td>\n          ",
    "11": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing\" id=\"3DSP\">\n                <papertitle>3D Self-Portraits</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.hao-li.com/\">Hao Li</a>, <a href=\"http://www.evouga.com/\">Etienne Vouga</a>, Anton Gudym, <a href=\"http://www.cs.princeton.edu/~linjiel/\">Linjie Luo</a>, <strong>Jonathan T. Barron</strong>, Gleb Gusev\n              <br/>\n              <em>SIGGRAPH Asia</em>, 2013\n              <br/>\n              <a href=\"http://www.youtube.com/watch?v=DmUkbZ0QMCA\">video</a> / <a href=\"http://shapify.me/\">shapify.me</a> / <a href=\"data/3DSP_siggraphAsia2013.bib\">bibtex</a>\n              <p>Our system allows users to create textured 3D models of themselves in arbitrary poses using only a single 3D sensor.</p>\n            </td>\n          ",
    "10": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1snypSLhzC0jXCchJRsWpcDZ7Es5hDmXo/view?usp=sharing\">\n                <papertitle>Intrinsic Scene Properties from a Single RGB-D Image</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>CVPR</em>, 2013 &amp;nbsp <font color=\"red\"><strong>(Oral Presentation)</strong></font>\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1cLUw72WpgdZ_3TQAjJABdgywqjBfn_Mq/view?usp=sharing\">supplement</a> / <a href=\"data/BarronMalikCVPR2013.bib\">bibtex</a> / <a href=\"http://techtalks.tv/talks/intrinsic-scene-properties-from-a-single-rgb-d-image/58614/\">talk</a> / <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmWW1CZGJPbi12R0k/view?usp=sharing\">keynote</a> (or <a href=\"https://drive.google.com/file/d/19q3EFf6GIb4UFcCN2DVU2jVKpxRj5kxf/view?usp=sharing\">powerpoint</a>, <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmMzQ4ZVp1SWdnVkk/view?usp=sharing\">PDF</a>) / <a href=\"https://drive.google.com/open?id=1ZbPScVA6Efqd-ESvojl92sw8K-82Xxry\">code &amp; data</a>\n              <p>By embedding mixtures of shapes &amp; lights into a soft segmentation of an image, and by leveraging the output of the Kinect, we can extend SIRFS to scenes.\n                <br/>\n                <br/>TPAMI Journal version: <a href=\"https://drive.google.com/file/d/1iQiUxZvjPPnb8rFCwXYesTgFSRk7mkAq/view?usp=sharing\">version</a> / <a href=\"data/BarronMalikTPAMI2015B.bib\">bibtex</a>\n              </p>\n            </td>\n          ",
    "9": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1H4YPovfrvcce3HGMEhidwU2l2fTcNR5y/view?usp=sharing\">\n                <papertitle>Boundary Cues for 3D Object Shape Recovery</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.kevinkarsch.com/\">Kevin Karsch</a>,\n              <a href=\"http://web.engr.illinois.edu/~liao17/\">Zicheng Liao</a>,\n              <a href=\"http://web.engr.illinois.edu/~jjrock2/\">Jason Rock</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://www.cs.illinois.edu/homes/dhoiem/\">Derek Hoiem</a>\n              <br/>\n              <em>CVPR</em>, 2013\n              <br/>\n              <a href=\"https://drive.google.com/file/d/0B4nuwEMaEsnmLUQ5SVJTcUZIYXc/view?usp=sharing\">supplement</a> / <a href=\"data/KarschCVPR2013.bib\">bibtex</a>\n              <p>Boundary cues (like occlusions and folds) can be used for shape reconstruction, which improves object recognition for humans and computers.</p>\n            </td>\n          ",
    "8": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1NczR4pJ-s0YBjCe0rCevMt8IM5JPuUrc/view?usp=sharing\">\n                <papertitle>Color Constancy, Intrinsic Images, and Shape Estimation</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>ECCV</em>, 2012\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1zuxhWZ3i6THvuRRBeE7dM_BJfDxO72Fq/view?usp=sharing\">supplement</a> /\n              <a href=\"data/BarronMalikECCV2012.bib\">bibtex</a> /\n              <a href=\"https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing\">poster</a> /\n              <a href=\"http://www.youtube.com/watch?v=NnePYprvFvA\">video</a>\n              <p>This paper is subsumed by <a href=\"#SIRFS\">SIRFS</a>.</p>\n            </td>\n          ",
    "7": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/17RfINbE2dr2EjXp9MtGO0MHJLQmQVhvT/view?usp=sharing\">\n                <papertitle>Shape, Albedo, and Illumination from a Single Image of an Unknown Object</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>CVPR</em>, 2012\n              <br/>\n              <a href=\"https://drive.google.com/file/d/1Im_bUI42AP9VPoNtsjLajvtLRiwv39k3/view?usp=sharing\">supplement</a> /\n              <a href=\"data/BarronMalikCVPR2012.bib\">bibtex</a> /\n              <a href=\"https://drive.google.com/file/d/1IAlSF4k3_CEL9dfbaMiNTFPBoEkLhsRl/view?usp=sharing\">poster</a>\n              <p>This paper is subsumed by <a href=\"#SIRFS\">SIRFS</a>.</p>\n            </td>\n          ",
    "6": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1_S8EQyngbHQrB415o0XkQ4V9SMzdEgWT/view?usp=sharing\">\n                <papertitle>A Category-Level 3-D Object Dataset: Putting the Kinect to Work</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://www.eecs.berkeley.edu/%7Eallie/\">Allison Janoch</a>,\n              <a href=\"http://sergeykarayev.com/\">Sergey Karayev</a>,\n              <a href=\"http://www.eecs.berkeley.edu/%7Ejiayq/\">Yangqing Jia</a>,\n              <strong>Jonathan T. Barron</strong>,\n              <a href=\"http://www.cs.berkeley.edu/%7Emfritz/\">Mario Fritz</a>,\n              <a href=\"http://www.icsi.berkeley.edu/%7Esaenko/\">Kate Saenko</a>,\n              <a href=\"http://www.eecs.berkeley.edu/%7Etrevor/\">Trevor Darrell</a>\n              <br/>\n              <em>ICCV 3DRR Workshop</em>, 2011\n              <br/>\n              <a href=\"data/B3DO_ICCV_2011.bib\">bibtex</a> /\n              <a href=\"https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing\">\"smoothing\" code</a>\n              <p>We present a large RGB-D dataset of indoor scenes and investigate ways to improve object detection using depth information.</p>\n            </td>\n          ",
    "5": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1EZTOO5xezLYcyIFgAzs4KuZFLbTcwTDH/view?usp=sharing\">\n                <papertitle>High-Frequency Shape and Albedo from Shading using Natural Image Statistics</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>CVPR</em>, 2011\n              <br/>\n              <a href=\"data/BarronMalikCVPR2011.bib\">bibtex</a>\n              <p>This paper is subsumed by <a href=\"#SIRFS\">SIRFS</a>.</p>\n            </td>\n          ",
    "4": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1rc05NatkQVmUDlGCAYcHSrvAzTpU9knT/view?usp=sharing\">\n                <papertitle>Discovering Efficiency in Coarse-To-Fine Texture Classification</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.cs.berkeley.edu/~malik/\">Jitendra Malik</a>\n              <br/>\n              <em>Technical Report</em>, 2010\n              <br/>\n              <a href=\"data/BarronTR2010.bib\">bibtex</a>\n              <p>A model and feature representation that allows for sub-linear coarse-to-fine semantic segmentation.\n              </p>\n            </td>\n          ",
    "3": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing\">\n                <papertitle>Parallelizing Reinforcement Learning</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://www.eecs.berkeley.edu/~dsg/\">Dave Golland</a>, <a href=\"http://www.cs.berkeley.edu/~nickjhay/\">Nicholas J. Hay</a>\n              <br/>\n              <em>Technical Report</em>, 2009\n              <br/>\n              <a href=\"data/BarronPRL2009.bib\">bibtex</a>\n              <p>Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.</p>\n            </td>\n          ",
    "2": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing\">\n                <papertitle>Blind Date: Using Proper Motions to Determine the Ages of Historical Images</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://cosmo.nyu.edu/hogg/\">David W. Hogg</a>, <a href=\"http://www.astro.princeton.edu/~dstn/\">Dustin Lang</a>, <a href=\"http://cs.nyu.edu/~roweis/\">Sam Roweis</a>\n              <br/>\n              <em>The Astronomical Journal</em>, 136, 2008\n              <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>\n            </td>\n          ",
    "1": "<td width=\"75%\" valign=\"middle\">\n              <a href=\"https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing\">\n                <papertitle>Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</papertitle>\n              </a>\n              <br/>\n              <strong>Jonathan T. Barron</strong>, <a href=\"http://stumm.ca/\">Christopher Stumm</a>, <a href=\"http://cosmo.nyu.edu/hogg/\">David W. Hogg</a>, <a href=\"http://www.astro.princeton.edu/~dstn/\">Dustin Lang</a>, <a href=\"http://cs.nyu.edu/~roweis/\">Sam Roweis</a>\n              <br/>\n              <em>The Astronomical Journal</em>, 135, 2008\n              <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>\n              <p>In use at <a href=\"http://www.astrometry.net\">Astrometry.net</a></p>\n            </td>\n          "
}