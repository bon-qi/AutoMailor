<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <meta name="generator" content="pandoc">
  <title>Sanja Fidler</title>
  <script src="courses/jquery-1.js"></script>
  <link rel="stylesheet" href="bootstrap.css">
  <link rel="stylesheet" href="bootstrap-theme.css">
  <script src="courses/bootstrap.js"></script>
  <style>
    /* http://stackoverflow.com/questions/18325779/bootstrap-3-collapse-show-state-with-chevron-icon */
    .panel-heading .accordion-toggle:before {
      font-family: 'Glyphicons Halflings';
      content: "\e114";
      float: left;
      color: grey;
      padding-right: 6px;
    }
    .panel-heading .accordion-toggle.collapsed:before {
      content: "\e080";
    }
table, th, td {
    border: 0px solid black;
    border-collapse: collapse;
}
  </style>

<style type="text/css">
</style>

</head>

<body>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11803764-2', 'uchicago.edu');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>


  <nav class="navbar navbar-default navbar-static-top" role="navigation">
    <div class="container">
      <ul class="nav navbar-nav">
        <li><a href="index.html"><font style="font-size:20px; font-weight:500;" color="#003380">Sanja Fidler</font></a></li>
        <li><a href="students.html"><font size="4px">Students</font></a></li>
        <li><a href="research.html"><font size="4px">Research</font></a></li>
        <li><a href="publications.html"><font size="4px" color="#E62E00"><b>Publications</font></b></a></li>
        <li><a href="teaching.html"><font size="4px">Teaching</font></a></li>
       <!-- <li><a href="data.html"><font size="4px">Data</font></a></li>-->
        <li><a href="code.html"><font size="4px">Code</font></a></li>
      </ul>
    </div>
  </nav>

  <div class="container">

    <a name="publications"></a>
        <p style="margin:-15px 0px 0px 0px;"></p>
   <div class="page-header"><h2 id="publications">Publications &nbsp&nbsp<font style="font-size:14px;">(Switch to <a href="publications_simple.html">Simple view</a>)</font></h2></div>

<font size="4">Full list also on <a href="http://scholar.google.com/citations?user=CUlqK5EAAAAJ&hl=en">Google scholar</a></font>
<br/>
<br/>


<h2>Year 2020</h2>
<br/>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 0.9% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/deftet.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning Deformable Tetrahedral Meshes for 3D Reconstruction</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan McGuire, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Neural Information Processing Systems</em> (<strong>NeurIPS</strong>), Vancouver, Canada, 2020</p>

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#deftet" href="#deftetabs-list">Abstract</a>&nbsp
<a href="" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#deftet" href="#deftet-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="deftet-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{DefTet20,<br/>
    title = {Learning Deformable Tetrahedral Meshes for 3D Reconstruction},<br/>
    author = {Jun Gao and Wenzheng Chen and Tommy Xiang and Alec Jacobson and Morgan McGuire and Sanja Fidler},<br/>
    booktitle = {NeurIPS},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="deftetabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations.
We introduce <em>Deformable Tetrahedral Meshes</em> (DefTet) as a particular
parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. 
Unlike existing volumetric approaches,  DefTet
optimizes for  both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing.
We demonstrate that DefTet matches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D results using only a single image as input.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 0.9% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/amodalVAE.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Variational Amodal Object Completion for Interactive Scene Editing</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Huan Ling, David Acuna, Karsten Kreis, Seung Kim, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Neural Information Processing Systems</em> (<strong>NeurIPS</strong>), Vancouver, Canada, 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#amodalvae" href="#amodalvaeabs-list">Abstract</a>&nbsp
<a href="" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#amodalvae" href="#amodalvae-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="amodalvae-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{amodalVAE20,<br/>
    title = {Variational Amodal Object Completion for Interactive Scene Editing},<br/>
    author = {Huan Ling and David Acuna and Karsten Kreis and Seung Kim and Sanja Fidler},<br/>
    booktitle = {NeurIPS},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="amodalvaeabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In images of complex scenes, objects are often occluding each other which makes perception tasks such as object detection and tracking, or robotic control tasks such as planning, challenging. To facilitate downstream tasks, it is thus important to reason about the full extent of objects, i.e., seeing behind occlusion, typically referred to as <em>amodal instance completion</em>. In this paper, we propose a variational generative framework for amodal completion which does not require any amodal labels at training time, as it is able to utilize widely available object instance masks.  We showcase our approach on the downstream task of scene editing where the user is presented with interactive tools to complete and erase objects in photographs. Experiments on complex street scenes demonstrate state-of-the-art performance in amodal mask completion, and showcase higher quality scene editing results over parallel work by Zhan et al. Interestingly, a user study shows that humans prefer our object completions to the human-labeled ones. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 0.9% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/fedsim.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Federated Simulation for Medical Imaging &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a style="color:#E62E00;">(<strong><em>nominated for Young Scientist Award</em></strong>) </a></h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Daiqing Li, Amlan Kar, Nishant Ravikumar, Alejandro F Frangi, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Medical Image Computing and Computer Assisted Intervention</em> (<strong>MICCAI</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2009.00668" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#fedsim" href="#fedsimabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/fed-sim/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#fedsim" href="#fedsim-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="fedsim-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{fedsim20,<br/>
    title = {Federated Simulation for Medical Imaging},<br/>
    author = {Daiqing Li and Amlan Kar and Nishant Ravikumar and Alejandro F Frangi and Sanja Fidler},<br/>
    booktitle = {MICCAI},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="fedsimabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge. Exploiting a larger pool of labeled data available across multiple centers, such as in federated learning, has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers. We aim to address these problems in a common, learning-based image simulation framework which we refer to as Federated Simulation. We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations. Since the model of geometry and material is disentangled from the imaging sensor, it can effectively be trained across multiple medical centers. We show that our data synthesis framework improves the downstream segmentation performance on several datasets.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 0.9% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/liftsplatshoot.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jonah Philion, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2008.05711" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#liftsplat" href="#liftsplatabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/lift-splat-shoot/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#liftsplat" href="#liftsplat-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="liftsplat-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{liftsplat20,<br/>
    title = {Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D},<br/>
    author = {Jonah Philion and Sanja Fidler},<br/>
    booktitle = {ECCV},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="liftsplatabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single "bird's-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to "lift" each image individually into a frustum of features for each camera, then "splat" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by "shooting" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 0.9% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/metasim2.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jeevan Devaranjan*, Amlan Kar*, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2008.09092" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#metasim2" href="#metasim2abs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/meta-sim-structure/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#metasim2" href="#metasim2-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="metasim2-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{metasim20,<br/>
    title = {Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation},<br/>
    author = {Jeevan Devaranjan and Amlan Kar and Sanja Fidler},<br/>
    booktitle = {ECCV},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="metasim2abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/defgrid.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Beyond Fixed Grid: Learning Geometric Image Representation with a Deformable Grid</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jun Gao, Zian Wang, Jinchen Xuan, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2008.09269" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#defgrid" href="#defgridabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~jungao/def-grid/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#defgrid" href="#defgrid-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="defgrid-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{defgrid20,<br/>
    title = {Beyond Fixed Grid: Learning Geometric Image Representation with a Deformable Grid},<br/>
    author = {Jun Gao and Zian Wang and Jinchen Xuan and Sanja Fidler},<br/>
    booktitle = {ECCV},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="defgridabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In modern computer vision, images are typically represented as a fixed uniform grid with some stride and processed via a deep convolutional neural network. We argue that deforming the grid to better align with the high-frequency image content is a more effective strategy. We introduce Deformable Grid (DefGrid), a learnable neural network module that predicts location offsets of vertices of a 2-dimensional triangular grid, such that the edges of the deformed grid align with image boundaries. We showcase our DefGrid in a variety of use cases, i.e., by inserting it as a module at various levels of processing. We utilize DefGrid as an end-to-end learnable geometric downsampling layer that replaces standard pooling methods for reducing feature resolution when feeding images into a deep CNN. We show significantly improved results at the same grid resolution compared to using CNNs on uniform grids for the task of semantic segmentation. We also utilize DefGrid at the output layers for the task of object mask annotation, and show that reasoning about object boundaries on our predicted polygonal grid leads to more accurate results over existing pixel-wise and curve-based approaches. We finally showcase DefGrid as a standalone module for unsupervised image partitioning, showing superior performance over existing approaches.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/scribble3d.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Interactive Annotation of 3D Object Geometry using 2D Scribbles</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tianchang Shen, Jun Gao, Amlan Kar, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2008.10719" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#scribble3d" href="#scribble3dabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~shenti11/scribble3d/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a href="http://aidemos.cs.toronto.edu/3d-annot/" target="_blank" class="buttonPP">Demo</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#scribble3d" href="#scribble3d-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="scribble3d-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{scribble3d20,<br/>
    title = {Interactive Annotation of 3D Object Geometry using 2D Scribbles},<br/>
    author = {Tianchang Shen and Jun Gao and Amlan Kar and Sanja Fidler},<br/>
    booktitle = {ECCV},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="scribble3dabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Inferring detailed 3D geometry of the scene is crucial for robotics applications, simulation, and 3D content creation. However, such information is hard to obtain, and thus very few datasets support it. In this paper, we propose an interactive framework for annotating 3D object geometry from both point cloud data and RGB imagery. The key idea behind our approach is to exploit strong priors that humans have about the 3D world in order to interactively annotate complete 3D shapes. Our framework targets a wide pool of annotators, i.e. naive users without artistic or graphics expertise. In particular, we introduce two simple-to-use interaction modules. First, we make an automatic guess of the 3D shape and allow the user to provide feedback about large errors by drawing scribbles in desired 2D views. Next, we aim to correct minor errors, in which users drag and drop 3D mesh vertices, assisted by a neural interactive module implemented as a Graph Convolutional Network. Experimentally, we show that only a few user interactions are needed to produce good quality 3D shapes on popular benchmarks such as ShapeNet, Pix3D and ScanNet. We implement our framework as a web service and conduct a user study, where we show that user annotated data using our method effectively facilitates real-world learning tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/scribblebox.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">ScribbleBox: Interactive Annotation Framework for Video Object Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Bowen Chen, Huan Ling, Xiaohui Zeng, Jun Gao, Ziyue Xu, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2008.09721" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#scribblebox" href="#scribbleboxabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~linghuan/scribblebox/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#scribblebox" href="#scribblebox-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="scribblebox-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{scribblebox20,<br/>
    title = {ScribbleBox: Interactive Annotation Framework for Video Object Segmentation},<br/>
    author = {Bowen Chen and Huan Ling and Xiaohui Zeng and Jun Gao and Ziyue Xu and Sanja Fidler},<br/>
    booktitle = {ECCV},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="scribbleboxabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Manually labeling video datasets for segmentation tasks is extremely time consuming. In this paper, we introduce ScribbleBox, a novel interactive framework for annotating object instances with masks in videos. In particular, we split annotation into two steps: annotating objects with tracked boxes, and labeling masks inside these tracks. We introduce automation and interaction in both steps. Box tracks are annotated efficiently by approximating the trajectory using a parametric curve with a small number of control points which the annotator can interactively correct. Our approach tolerates a modest amount of noise in the box placements, thus typically only a few clicks are needed to annotate tracked boxes to a sufficient accuracy. Segmentation masks are corrected via scribbles which are efficiently propagated through time. We show significant performance gains in annotation efficiency over past work. We show that our ScribbleBox approach reaches 88.92% J&F on DAVIS2017 with 9.14 clicks per box track, and 4 frames of scribble annotation.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/tele.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Expressive Telepresence via Modular Codec Avatar</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Chu, Shugao Ma, Fernando Torre, Sanja Fidler, Yaser Sheikh</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://research.fb.com/wp-content/uploads/2020/08/Expressive-Telepresence-via-Modular-Codec-Avatars.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#tele" href="#teleabs-list">Abstract</a>&nbsp
<!--<a href="http://www.cs.toronto.edu/~linghuan/scribblebox/" target="_blank" class="buttonPP">Project page</a>&nbsp-->
<a class="buttonSS" data-toggle="collapse" data-parent="#tele" href="#tele-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="tele-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{modularavatar20,<br/>
    title = {Expressive Telepresence via Modular Codec Avatar},<br/>
    author = {Hang Chu and Shugao Ma and Fernando Torre and Sanja Fidler and Yaser Sheikh},<br/>
    booktitle = {ECCV},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="teleabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
VR telepresence consists of interacting with another human
in a virtual space represented by an avatar. Today most avatars are
cartoon-like, but soon the technology will allow video-realistic ones. This
paper aims in this direction, and presents Modular Codec Avatars (MCA),
a method to generate hyper-realistic faces driven by the cameras in the
VR headset. MCA extends traditional Codec Avatars (CA) by replacing
the holistic models with a learned modular representation. It is important to note that traditional person-specific CAs are learned from few
training samples, and typically lack robustness as well as limited expressiveness when transferring facial expressions. MCAs solve these issues by
learning a modulated adaptive blending of different facial components as
well as an exemplar-based latent alignment. We demonstrate that MCA
achieves improved expressiveness and robustness w.r.t to CA in a variety
of real-world datasets and practical scenarios. Finally, we showcase new
applications in VR telepresence enabled by the proposed model.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 1.4% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/color20.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Nonlinear Color Triads for Approximation, Learning and Direct Manipulation of Color Distributions</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Maria Shugrina, Amlan Kar, Sanja Fidler, Karan Singh</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>SIGGRAPH</em>, 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://drive.google.com/file/d/1UmybUdZ1_Pd2m9Y3lh9MTCJ7T1gw2yJf/view?usp=sharing" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#color20" href="#color20abs-list">Abstract</a>&nbsp
<a href="https://colorsandbox.com/research/triads" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#color20" href="#color20-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="color20-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{colortriads20,<br/>
    title = {Nonlinear Color Triads for Approximation, Learning and Direct Manipulation of Color Distributions},<br/>
    author = {Maria Shugrina and Amlan Kar and Sanja Fidler and Karan Singh},<br/>
    booktitle = {SIGGRAPH},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="color20abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present nonlinear color triads, an extension of color gradients able to
approximate a variety of natural color distributions that have no standard
interactive representation. We derive a method to fit this compact parametric
representation to existing images and show its power for tasks such as image
editing and compression. Our color triad formulation can also be included
in standard deep learning architectures, facilitating further research.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/epic20.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Dima Damen, Hazel Doughty, Giovanni Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<strong>T-PAMI</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2005.00343" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#epic20" href="#epic20abs-list">Abstract</a>&nbsp
<a href="https://epic-kitchens.github.io" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#epic20" href="#epic20-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="epic20-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{EPICKITCHENS20,<br/>
    title = {The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines},<br/>
    author = {Dima Damen and Hazel Doughty and Giovanni Farinella and Sanja Fidler and Antonino Furnari and Evangelos Kazakos and Davide Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},<br/>
    journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="epic20abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the largest egocentric video benchmark, offering a unique viewpoint on people's interaction with objects, their attention, and even intention. In this paper, we detail how this large-scale dataset was captured by 32 participants in their native kitchen environments, and densely annotated with actions and object interactions. Our videos depict non-scripted daily activities, as recording is started every time a participant entered their kitchen. Recording took place in 4 countries by participants belonging to 10 different nationalities, resulting in highly diverse kitchen habits and cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.2K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. We introduce new baselines that highlight the multimodal nature of the dataset and the importance of explicit temporal modelling to discriminate fine-grained actions (e.g. "closing a tap" from "opening" it up).
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/nds.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xi Yan*, David Acuna*, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2001.02799" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#nds" href="#ndsabs-list">Abstract</a>&nbsp
<a href="http://aidemos.cs.toronto.edu/nds/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#nds" href="#nds-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="nds-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{nds20,<br/>
    title = {Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data},<br/>
    author = {Xi Yan and David Acuna and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="ndsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Transfer learning has proven to be a successful technique to train deep learning models in the domains where little training data is available. The dominant approach is to pretrain a model on a large generic dataset such as ImageNet and finetune its weights on the target domain. However, in the new era of an ever-increasing number of massive datasets, selecting the relevant data for pretraining is a critical issue. We introduce Neural Data Server (NDS), a large-scale search engine for finding the most useful transfer learning data to the target domain. NDS consists of a dataserver which indexes several large popular image datasets, and aims to recommend data to a client, an end-user with a target application with its own small labeled dataset. The dataserver represents large datasets with a much more compact mixture-of-experts model, and employs it to perform data search in a series of dataserver-client transactions at a low computational cost. We show the effectiveness of NDS in various transfer learning scenarios, demonstrating state-of-the-art performance on several target datasets and tasks such as image classification, object detection and instance segmentation. Neural Data Server is available as a web-service at this <a href="http://aidemos.cs.toronto.edu/nds/" target="_blank">http URL</a>.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/gamegan.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning to Simulate Dynamic Environments with GameGAN</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2005.12126" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#gamegan" href="#gameganabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/gameGAN/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#gamegan" href="#gamegan-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="gamegan-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{gamegan20,<br/>
    title = {Learning to Simulate Dynamic Environments with GameGAN},<br/>
    author = {Seung Wook Kim and Yuhao Zhou and Jonah Philion and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="gameganabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN "renders" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/learneval1.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning to Evaluate Perception Models Using Planner-Centric Metrics</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jonah Philion, Amlan Kar, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://www.dgp.toronto.edu/autotuningsl/ims/7187.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#learneval" href="#learnevalabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/detection-relevance/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#learneval" href="#learneval-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="learneval-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{learneval20,<br/>
    title = {Learning to Evaluate Perception Models Using Planner-Centric Metrics},<br/>
    author = {Jonah Philion and Amlan Kar and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="learnevalabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Variants of accuracy and precision are the gold-standard by which the computer vision community measures progress of perception algorithms. One reason for the ubiquity of these metrics is that they are largely task-agnostic; we in general seek to detect zero false negatives or positives. The downside of these metrics is that, at worst, they penalize all incorrect detections equally without conditioning on the task or scene, and at best, heuristics need to be chosen to ensure that different mistakes count differently. In this paper, we propose a principled metric for 3D object detection specifically for the task of self-driving. The core idea behind our metric is to isolate the task of object detection and measure the impact the produced detections would induce on the downstream task of driving. Without hand-designing it to, we find that our metric penalizes many of the mistakes that other metrics penalize by design. In addition, our metric downweighs detections based on additional factors such as distance from a detection to the ego car and the speed of the detection in intuitive ways that other detection metrics do not. For human evaluation, we generate scenes in which standard metrics and our metric disagree and find that humans side with our metric 79% of the time.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/autostructlight.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Auto-Tuning Structured Light by Optical Stochastic Gradient Descent</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Wenzheng Chen, Parsa Mirdehghan, Sanja Filder, Kiriakos N. Kutulakos</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/2004.08745" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#autostructlight" href="#autostructlightabs-list">Abstract</a>&nbsp
<a href="https://www.dgp.toronto.edu/autotuningsl/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#autostructlight" href="#autostructlight-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="autostructlight-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{autostructlight20,<br/>
    title = {Auto-Tuning Structured Light by Optical Stochastic Gradient Descent},<br/>
    author = {Wenzheng Chen and Parsa Mirdehghan and Sanja Filder and Kiriakos N. Kutulakos},<br/>
    booktitle = {CVPR},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="autostructlightabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We consider the problem of optimizing the performance of an active imaging system by automatically discovering the illuminations it should use, and the way to decode them. Our approach tackles two seemingly incompatible goals: (1) "tuning" the illuminations and decoding algorithm precisely to the devices at hand---to their optical transfer functions, non-linearities, spectral responses, image processing pipelines---and (2) doing so without modeling or calibrating the system; without modeling the scenes of interest; and without prior training data. The key idea is to formulate a stochastic gradient descent (SGD) optimization procedure that puts the actual system in the loop: projecting patterns, capturing images, and calculating the gradient of expected reconstruction error. We apply this idea to structured-light triangulation to "auto-tune" several devices---from smartphones and laser projectors to advanced computational cameras. Our experiments show that despite being modelfree and automatic, optical SGD can boost system 3D accuracy substantially over state-of-the-art coding schemes.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/protoanalysis.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Theoretical Analysis of the Number of Shots in Few-Shot Learning</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tianshi Cao, Marc Law, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1909.11722" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#protoanalysis" href="#protoanalysisabs-list">Abstract</a>&nbsp
<!--<a href="https://www.dgp.toronto.edu/autotuningsl/" target="_blank" class="buttonPP">Project page</a>&nbsp-->
<a class="buttonSS" data-toggle="collapse" data-parent="#protoanalysis" href="#protoanalysis-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="protoanalysis-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{protoanalysis20,<br/>
    title = {A Theoretical Analysis of the Number of Shots in Few-Shot Learning},<br/>
    author = {Tianshi Cao and Marc Law and Sanja Fidler},<br/>
    booktitle = {ICLR},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="protoanalysisabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Few-shot classification is the task of predicting the category of an example from a set of few labeled examples. The number of labeled examples per category is called the number of shots (or shot number). Recent works tackle this task through meta-learning, where a meta-learner extracts information from observed tasks during meta-training to quickly adapt to new tasks during meta-testing. In this formulation, the number of shots exploited during meta-training has an impact on the recognition performance at meta-test time. Generally, the shot number used in meta-training should match the one used in meta-testing to obtain the best performance. We introduce a theoretical analysis of the impact of the shot number on Prototypical Networks, a state-of-the-art few-shot classification method. From our analysis, we propose a simple method that is robust to the choice of shot number used during meta-training, which is a crucial hyperparameter. The performance of our model trained for an arbitrary meta-training shot number shows great performance for different values of meta-testing shot numbers. We experimentally demonstrate our approach on different few-shot classification benchmarks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 1.0% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/crevnet.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Efficient and Information-Preserving Future Frame Prediction and Beyond</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Wei Yu, Yichao Lu, Steve Easterbrook, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2020</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://openreview.net/pdf?id=B1eY_pVYvB" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#crevnet" href="#crevnetabs-list">Abstract</a>&nbsp
<!--<a href="https://www.dgp.toronto.edu/autotuningsl/" target="_blank" class="buttonPP">Project page</a>&nbsp-->
<a class="buttonSS" data-toggle="collapse" data-parent="#crevnet" href="#crevnet-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="crevnet-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{crevnet20,<br/>
    title = {Efficient and Information-Preserving Future Frame Prediction and Beyond},<br/>
    author = {Wei Yu and Yichao Lu and Steve Easterbrook and Sanja Fidler},<br/>
    booktitle = {ICLR},<br/>
    year = {2020}</br>}
</p>      
</div>
<div id="crevnetabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly
limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder
and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much
lower memory consumption and computational efficiency. The lightweight nature of our model enables us to incorporate 3D convolutions without concern of memory
bottleneck, enhancing the modelâs ability to capture both short-term and long-term temporal dependencies. Our proposed approach achieves state-of-the-art results
on Moving MNIST, Traffic4cast and KITTI datasets. We further demonstrate the transferability of our self-supervised learning method by exploiting its learnt features for object detection on KITTI. Our competitive results indicate the potential of using CrevNet as a generative pre-training strategy to guide downstream tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<p style="margin:-10px 0px 0px 0px;"></p>
<p style="margin:65px 0px 0px 0px;"></p>

<h2>Year 2019</h2>
<br/>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.3%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 0.4% 0 0; width:15.4%;" href=""><img width="100%" src="papers/figs/kaolin.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Krishna Murthy Jatavallabhula, Edward Smith, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev Lebaredian, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Technical Report (arXiv:1911.05063), 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1911.05063" target="_blank" class="buttonTT">Tech Report</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#kaolin" href="#kaolinabs-list">Abstract</a>&nbsp
<a href="https://github.com/NVIDIAGameWorks/kaolin" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#kaolin" href="#kaolin-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="kaolin-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Kaolin19,<br/>
    title = {Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research},<br/>
    author = {Krishna Murthy Jatavallabhula and Edward Smith and Jean-Francois Lafleche and Clement Fuji Tsang and Artem Rozantsev and Wenzheng Chen and Tommy Xiang and Rev Lebaredian and Sanja Fidler},<br/>
    booktitle = {arXiv:1911.05063},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="kaolinabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Kaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/dibr.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaako Lehtinen, Alec Jacobson, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Neural Information Processing Systems</em> (<strong>NeurIPS</strong>), Vancouver, Canada, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://nv-tlabs.github.io/DIB-R/files/diff_shader.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#dibr" href="#dibrabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/DIB-R/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#dibr" href="#dibr-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="dibr-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{DIBR19,<br/>
    title = {Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer},<br/>
    author = {Wenzheng Chen and Huan Ling and Jun Gao and Edward Smith and Jaako Lehtinen and Alec Jacobson and Sanja Fidler},<br/>
    booktitle = {NeurIPS},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="dibrabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as an distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.1%;" href=""><img width="100%" src="papers/figs/metasim.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:82%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Meta-Sim: Learning to Generate Synthetic Datasets&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler </p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Seul, Korea, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1904.11621" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#metasim" href="#metasimabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/meta-sim/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#metasim" href="#metasim-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="metasim-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Metasim19,<br/>
    title = {Meta-Sim: Learning to Generate Synthetic Datasets},<br/>
    author = {Amlan Kar and Aayush Prakash and Ming-Yu Liu and Eric Cameracci and Justin Yuan and Matt Rusiniak and David Acuna and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="metasimabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.1%;" href=""><img width="100%" src="papers/figs/ntg.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:82%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Neural Turtle Graphics for Modeling City Road Layouts&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria Shugrina, Xinkai Wei, Ming-Yu Liu, Antonio Torralba, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Seul, Korea, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ntg" href="#ntgabs-list">Abstract</a>&nbsp
<!--<a href="https://nv-tlabs.github.io/meta-sim/" target="_blank" class="buttonPP">Project page</a>&nbsp-->
<a class="buttonSS" data-toggle="collapse" data-parent="#ntg" href="#ntg-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ntg-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{NTG19,<br/>
    title = {Neural Turtle Graphics for Modeling City Road Layouts},<br/>
    author = {Hang Chu and Daiqing Li and David Acuna and Amlan Kar and Maria Shugrina and Xinkai Wei and Ming-Yu Liu and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="ntgabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We propose Neural Turtle Graphics (NTG), a novel generative model for spatial graphs, and demonstrate its applications in modeling city road layouts. Specifically, we represent the road layout using a graph where nodes in the graph represent control points and edges in the graph represents road segments. NTG is a sequential generative model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train NTG on Open Street Map data and show it outperforms existing approaches using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch a part of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/gscnn.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Gated-SCNN: Gated Shape CNNs for Semantic Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Seul, Korea, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1907.05740" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#gscnn" href="#gscnnabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/GSCNN/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#gscnn" href="#gscnn-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="gscnn-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{GSCNN19,<br/>
    title = {Gated-SCNN: Gated Shape CNNs for Semantic Segmentation},<br/>
    author = {Towaki Takikawa and David Acuna and Varun Jampani and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="gscnnabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/asking.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Lifelong Learning for Image Captioning by Asking Natural Language Questions</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Kevin Shen, Amlan Kar, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Seul, Korea, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1812.00235" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#asking" href="#askingabs-list">Abstract</a>&nbsp
<a href="http://aidemos.cs.toronto.edu/lbaq/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#asking" href="#asking-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="asking-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Shen19,<br/>
    title = {Lifelong Learning for Image Captioning by Asking Natural Language Questions},<br/>
    author = {Kevin Shen and Amlan Kar and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="askingabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:2px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/balls.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Video Face Clustering with Unknown Number of Clusters</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Makarand Tapaswi, Marc Law, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Seul, Korea, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1908.03381" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#balls" href="#ballsabs-list">Abstract</a>&nbsp
<a href="https://github.com/makarandtapaswi/BallClustering_ICCV2019" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#gscnn" href="#balls-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="balls-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{BallClust19,<br/>
    title = {Video Face Clustering with Unknown Number of Clusters},<br/>
    author = {Makarand Tapaswi and Marc Law and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="ballsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded.
To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:4px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/dmmnet.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">DMM-Net: Differentiable Mask-Matching Network for Video Instance Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xiaohui Zeng, Renjie Liao, Li Gu, Yuwen Xiong, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Seul, Korea, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1909.12471" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#dmm" href="#dmmabs-list">Abstract</a>&nbsp
<a href="https://github.com/ZENGXH/DMM_Net" target="_blank" class="buttonPP">Code</a>&nbsp-
<a class="buttonSS" data-toggle="collapse" data-parent="#dmm" href="#dmm-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="dmm-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{DMMNet19,<br/>
    title = {DMM-Net: Differentiable Mask-Matching Network for Video Instance Segmentation},<br/>
    author = {Xiaohui Zeng and Renjie Liao and Li Gu and Yuwen Xiong and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="dmmabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we propose the differentiable mask-matching network (DMM-Net) for solving the video object segmentation problem where the initial object masks are provided. Relying on the Mask R-CNN backbone, we extract mask proposals per frame and formulate the matching between object templates and proposals as a linear assignment problem where thA heading inside a blocke cost matrix is predicted by a deep convolutional neural network.
We propose a differentiable matching layer which unrolls a projected gradient descent algorithm in which the projection step exploits the Dykstra's algorithm. We prove that under mild conditions, the matching is guaranteed to converge to the optimal one. In practice, it achieves similar performance compared to the Hungarian algorithm during inference.  Importantly, we can back-propagate through it to learn the cost matrix. After matching, a U-Net style architecture is exploited to refine the matched mask per time step. On DAVIS 2017 dataset, DMM-Net achieves the best performance without online learning on the first frames and the 2nd best with it.  Without any fine-tuning, DMM-Net performs comparably to state-of-the-art methods on SegTrack v2 dataset. At last, our differentiable matching layer is very simple to implement; we attach the PyTorch code in the supplementary material which is less than $50$ lines long.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/steal.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">David Acuna, Amlan Kar, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1904.07934" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#steal" href="#stealabs-list">Abstract</a>&nbsp
<a href="https://nv-tlabs.github.io/STEAL/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#steal" href="#steal-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="steal-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Steal19,<br/>
    title = {Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations},<br/>
    author = {David Acuna and Amlan Kar and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="stealabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/curvegcn.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Fast Interactive Object Annotation with Curve-GCN</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Huan Ling*, Jun Gao*, Amlan Kar, Wenzheng Chen, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1903.06874" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#curvegcn" href="#curvegcnabs-list">Abstract</a>&nbsp
<a href="https://github.com/fidler-lab/curve-gcn" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#curvegcn" href="#curvegcn-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="curvegcn-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{CurveGCN19,<br/>
    title = {Fast Interactive Object Annotation with Curve-GCN},<br/>
    author = {Huan Ling and Jun Gao and Amlan Kar and Wenzheng Chen and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="curvegcnabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Manually labeling objects by tracing their boundaries is a laborious process. In Polygon-RNN++ the authors proposed Polygon-RNN that produces polygonal annotations in a recurrent manner using a CNN-RNN architecture, allowing interactive correction via humans-in-the-loop. We propose a new framework that alleviates the sequential nature of Polygon-RNN, by predicting all vertices simultaneously using a Graph Convolutional Network (GCN). Our model is trained end-to-end. It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. We show that Curve-GCN outperforms all existing approaches in automatic mode, including the powerful PSP-DeepLab and is significantly more efficient in interactive mode than Polygon-RNN++. Our model runs at 29.3ms in automatic, and 2.6ms in interactive mode, making it 10x and 100x faster than Polygon-RNN++.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/delse.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Object Instance Annotation with Deep Extreme Level Set Evolution</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Zian Wang, David Acuna, Huan Ling, Amlan Kar, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Object_Instance_Annotation_With_Deep_Extreme_Level_Set_Evolution_CVPR_2019_paper.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#delse" href="#delseabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#delse" href="#delse-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="delse-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{DELSE19,<br/>
    title = {Object Instance Annotation with Deep Extreme Level Set Evolution},<br/>
    author = {Zian Wang and David Acuna and Huan Ling and Amlan Kar and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="delseabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we tackle the task of interactive object segmentation. We revive the old ideas on level set segmentation
which framed object annotation as curve evolution. Carefully designed energy functions ensured that the curve was
well aligned with image boundaries, and generally "well
behaved". The Level Set Method can handle objects with
complex shapes and topological changes such as merging
and splitting, thus able to deal with occluded objects and
objects with holes. We propose Deep Extreme Level Set Evolution that combines powerful CNN models with level set
optimization in an end-to-end fashion. Our method learns
to predict evolution parameters conditioned on the image
and evolves the predicted initial contour to produce the
final result. We make our model interactive by incorporating user clicks on the extreme boundary points, following
DEXTR. We show that our approach significantly outperforms DEXTR on the static Cityscapes dataset and
the video segmentation benchmark DAVIS, and performs on par on PASCAL and SBD.</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>




<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/vh2.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Synthesizing Environment-Aware Activities via Activity Sketches</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Yuan-Hong Liao, Xavier Puig, Marko Boben, Antonio Torralba, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liao_Synthesizing_Environment-Aware_Activities_via_Activity_Sketches_CVPR_2019_paper.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#vh2" href="#vh2abs-list">Abstract</a>&nbsp
<a href="https://andrewliao11.github.io/project/env-program/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#vh2" href="#vh2-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="vh2-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{VirtualHome2,<br/>
    title = {Synthesizing Environment-Aware Activities via Activity Sketches},<br/>
    author = {Yuan-Hong Liao and Xavier Puig and Marko Boben and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="vh2abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In order to learn to perform activities from demonstrations or descriptions, agents need to distill what the essence
of the given activity is, and how it can be adapted to new
environments. In this work, we address the problem of
environment-aware program generation. Given a visual
demonstration or a description of an activity, we generate program sketches representing the essential instructions
and propose a model to transform these into full programs
representing the actions needed to perform the activity under the presented environmental constraints. To this end,
we build upon VirtualHome to create a new dataset
VirtualHome-Env, where we collect program sketches to
represent activities and match programs with environments
that can afford them. Furthermore, we construct a knowledge base to sample realistic environments and another
knowledge base to seek out the programs under the sampled
environments. Finally, we propose ResActGraph, a network
that generates a program from a given sketch and an environment graph and tracks the changes in the environment
induced by the program.</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>




<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/creativeflow.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Creative Flow+ Dataset</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Maria Shugrina, Ziheng Liang, Amlan Kar, Jiaman Li, Angad Singh, Karan Singh, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Shugrina_Creative_Flow_Dataset_CVPR_2019_paper.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#creativeflow" href="#creativeflowabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/creativeflow/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#creativeflow" href="#creativeflow-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="creativeflow-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{CreativeFlow19,<br/>
    title = {Creative Flow+ Dataset},<br/>
    author = {Maria Shugrina and Ziheng Liang and Amlan Kar and Jiaman Li and Angad Singh and Karan Singh and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="creativeflowabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present the Creative Flow+ Dataset, the first diverse multi-style artistic video dataset richly labeled with
per-pixel optical flow, occlusions, correspondences, segmentation labels, normals, and depth. Our dataset includes 3000 animated sequences rendered using styles randomly selected from 40 textured line styles and 38 shading styles, spanning the range between flat cartoon fill and
wildly sketchy shading. Our dataset includes 124K+ train
set frames and 10K test set frames rendered at 1500x1500
resolution, far surpassing the largest available optical flow
datasets in size. While modern techniques for tasks such as
optical flow estimation achieve impressive performance on
realistic images and video, today there is no way to gauge
their performance on non-photorealistic images. Creative
Flow+ poses a new challenge to generalize real-world
Computer Vision to messy stylized content. We show that
learning-based optical flow methods fail to generalize to
this data and struggle to compete with classical approaches,
and invite new research in this area. Our dataset and a
new optical flow benchmark will be publicly available at:
<a href="http://www.cs.toronto.edu/creativeflow/" target="_blank">www.cs.toronto.edu/creativeflow/</a>. We further release the complete dataset creation pipeline, allowing the community to generate and stylize their own data on
demand.</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/darnet.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">DARNet: Deep Active Ray Network for Building Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Dominic Cheng, Renjie Liao, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1905.05889" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#darnet" href="#darnetabs-list">Abstract</a>&nbsp
<a href="https://github.com/dcheng-utoronto/darnet" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#darnet" href="#darnet-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="darnet-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{DARnet19,<br/>
    title = {DARNet: Deep Active Ray Network for Building Segmentation},<br/>
    author = {Dominic Cheng and Renjie Liao and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="darnetabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we propose a Deep Active Ray Network (DARNet) for automatic building segmentation. Taking an image as input, it first exploits a deep convolutional neural network (CNN) as the backbone to predict energy maps, which are further utilized to construct an energy function. A polygon-based contour is then evolved via minimizing the energy function, of which the minimum defines the final segmentation. Instead of parameterizing the contour using Euclidean coordinates, we adopt polar coordinates, i.e., rays, which not only prevents self-intersection but also simplifies the design of the energy function. Moreover, we propose a loss function that directly encourages the contours to match building boundaries. Our DARNet is trained end-to-end by back-propagating through the energy minimization and the backbone CNN, which makes the CNN adapt to the dynamics of the contour evolution. Experiments on three building instance segmentation datasets demonstrate our DARNet achieves either state-of-the-art or comparable performances to other competitors.</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/actions19.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Action Recognition from Single Timestamp Supervision in Untrimmed Videos</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Davide Moltisanti, Sanja Fidler, Dima Damen</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1904.04689" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#actions19" href="#actions19abs-list">Abstract</a>&nbsp
<a href="http://people.cs.bris.ac.uk/~damen/single_timestamps/index.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#actions19" href="#actions19-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="actions19-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Moltisani19,<br/>
    title = {Action Recognition from Single Timestamp Supervision in Untrimmed Videos},<br/>
    author = {Davide Moltisanti and Sanja Fidler and Dima Damen},<br/>
    booktitle = {CVPR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="actions19abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Recognizing actions in videos relies on labelled supervision during training, typically the start and end times of each action instance. This supervision is not only subjective, but also expensive to acquire. Weak video-level supervision has been successfully exploited for recognition in untrimmed videos, however it is challenged when the number of different actions in training videos increases. We propose a method that is supervised by single timestamps located around each action instance, in untrimmed videos. We replace expensive action bounds with sampling distributions initialized from these timestamps. We then use the classifier's response to iteratively update the sampling distributions. We demonstrate that these distributions converge to the location and extent of discriminative action segments. We evaluate our method on three datasets for fine-grained recognition, with increasing number of different actions per video, and show that single timestamps offer a reasonable compromise between recognition performance and labelling effort, performing comparably to full temporal supervision. Our update method improves top-1 test accuracy by up to 5.4%. across the evaluated datasets.</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/eigendamage.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Chaoqi Wang, Roger Grosse, Sanja Fidler, Guodong Zhang</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>International Conference on Machine Learning</em> (<strong>ICML</strong>), Long Beach, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1905.05934" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#pruning" href="#pruningabs-list">Abstract</a>&nbsp
<a href="https://github.com/alecwangcq/EigenDamage-Pytorch" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#pruning" href="#pruning-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="pruning-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{EigenDamage19,<br/>
    title = {EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis},<br/>
    author = {Chaoqi Wang, Roger Grosse, Sanja Fidler, Guodong Zhang},<br/>
    booktitle = {ICML},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="pruningabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured pruning methods in this basis. As opposed to existing Hessian-based pruning algorithms which do pruning in parameter coordinates, our method works in the KFE where different weights are approximately independent, enabling accurate pruning and fast computation. We demonstrate empirically the effectiveness of the proposed method through extensive experiments. In particular, we highlight that the improvements are especially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-pruning version gives a 10? reduction in model size and a 8? reduction in FLOPs on wide ResNet32.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/pmn.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Visual Reasoning by Progressive Module Networks</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Seung Wook Kim, Makarand Tapaswi, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>International Conference on Learning Representations</em> (<strong>ICLR</strong>), New Orleans, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1806.02453" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#pmn" href="#pmnabs-list">Abstract</a>&nbsp
<a href="https://github.com/seung-kim/pmn_demo" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#pmn" href="#pmn-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="pmn-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{PMN2018,<br/>
    title = {Visual Reasoning by Progressive Module Networks},<br/>
    author = {Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},<br/>
    booktitle = {ICLR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="pmnabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn - most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. ThUSA, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>




<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/nte.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Neural Graph Evolution: Automatic Robot Design</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tingwu Wang, Yuhao Zhou, Sanja Fidler, Jimmy Ba</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>International Conference on Learning Representations</em> (<strong>ICLR</strong>), New Orleans, USA, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1906.05370" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#nte" href="#nteabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~henryzhou/NGE_website/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#nte" href="#nte-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="nte-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{NGE2019,<br/>
    title = {Neural Graph Evolution: Automatic Robot Design},<br/>
    author = {Tingwu Wang and Yuhao Zhou and Sanja Fidler and Jimmy Ba},<br/>
    booktitle = {ICLR},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="nteabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods by an order of magnitude. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs. Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2 machine.</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/chi_color.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Color Builder: A Direct Manipulation Interface for Versatile Color Theme Authoring</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Maria Shugrina, Wenjia Zhang, Fanny Chevalier, Sanja Fidler, Karan Singh</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>CHI Conference on Human Factors in Computing Systems</em> (<strong>CHI</strong>), Glasgow, UK, 2019</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://dl.acm.org/citation.cfm?id=3300686&dl=ACM&coll=DL" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#chicolor" href="#chicolorabs-list">Abstract</a>&nbsp
<a href="http://www.colorsails.com" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#chicolor" href="#chicolor-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="chicolor-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{CHI-Masha2019,<br/>
    title = {Color Builder: A Direct Manipulation Interface for Versatile Color Theme Authoring},<br/>
    author = {Maria Shugrina and Wenjia Zhang and Fanny Chevalier and Sanja Fidler and Karan Singh},<br/>
    booktitle = {CHI},<br/>
    year = {2019}</br>}
</p>      
</div>
<div id="chicolorabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Color themes or palettes are popular for sharing color combinations across many visual domains. We present a novel interface for creating color themes through direct manipulation of color swatches. Users can create and rearrange swatches, and combine them into smooth and step-based gradients and three-color blends -- all using a seamless touch or mouse input. Analysis of existing solutions reveals a fragmented color design workflow, where separate software is used for swatches, smooth and discrete gradients and for in-context color visualization. Our design unifies these tasks, while encouraging playful creative exploration. Adjusting a color using standard color pickers can break this interaction flow with mechanical slider manipulation. To keep interaction seamless, we additionally design an in situ color tweaking interface for freeform exploration of an entire color neighborhood. We evaluate our interface with a group of professional designers and students majoring in this field.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<p style="margin:-10px 0px 0px 0px;"></p>
<p style="margin:65px 0px 0px 0px;"></p>


<h2>Year 2018</h2>
<br/>

<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/ade20k.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Semantic Understanding of Scenes Through the ADE20K Dataset</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>International Journal of Computer Vision</em> (<strong>IJCV</strong>)</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://link.springer.com/article/10.1007/s11263-018-1140-0" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#adeijcv" href="#adeijcvabs-list">Abstract</a>&nbsp
<a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" class="buttonPP">Dataset</a>&nbsp
<a href="https://github.com/CSAILVision/semantic-segmentation-pytorch" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#adeijcv" href="#adeijcv-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="adeijcv-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@article{ADEIJCV,<br/>
    title = {Semantic Understanding of Scenes Through the ADE20K Dataset},<br/>
    author = {Bolei Zhou and Hang Zhao and Xavier Puig and Tete Xiao and Sanja Fidler and Adela Barriuso and Antonio Torralba},<br/>
    journal = {IJCV},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="adeijcvabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Semantic understanding of visual scenes is one of the holy grails of computer vision. Despite efforts of the community in data collection, there are still few image datasets covering a wide range of scenes and object categories with pixel-wise annotations for scene understanding. In this work, we present a densely annotated dataset ADE20K, which spans diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. Totally there are 25k images of the complex everyday scenes containing a variety of objects in their natural spatial context. On average there are 19.5 instances and 10.5 object classes per image. Based on ADE20K, we construct benchmarks for scene parsing and instance segmentation. We provide baseline performances on both of the benchmarks and re-implement state-of-the-art models for open source. We further evaluate the effect of synchronized batch normalization and find that a reasonably large batch size is crucial for the semantic segmentation performance. We show that the networks trained on ADE20K are able to segment a wide variety of scenes and objects.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 3.3% 0 0; width:14.1%;" href=""><img width="100%" src="papers/figs/sails.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Color Sails: Discrete-Continuous Palettes for Deep Color Exploration</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Maria Shugrina, Amlan Kar, Karan Singh, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Arxiv preprint arXiv:1806.02918</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1806.02918" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#sails" href="#sailsabs-list">Abstract</a>&nbsp
<a href="http://www.colorsails.com" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#sails" href="#sails-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="sails-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{sails2018,<br/>
    title = {Color Sails: Discrete-Continuous Palettes for Deep Color Exploration},<br/>
    author = {Maria Shugrina and Amlan Kar and Karan Singh and Sanja Fidler},<br/>
    booktitle = {arXiv:1806.02918},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="sailsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present color sails, a discrete-continuous color gamut representation that extends the color gradient analogy to three dimensions and allows interactive control of the color blending behavior. Our representation models a wide variety of color distributions in a compact manner, and lends itself to applications such as color exploration for graphic design, illustration and similar fields. We propose a Neural Network that can fit a color sail to any image. Then, the user can adjust color sail parameters to change the base colors, their blending behavior and the number of colors, exploring a wide range of options for the original design. In addition, we propose a Deep Learning model that learns to automatically segment an image into color-compatible alpha masks, each equipped with its own color sail. This allows targeted color exploration by either editing their corresponding color sails or using standard software packages. Our model is trained on a custom diverse dataset of art and design. We provide both quantitative evaluations, and a user study, demonstrating the effectiveness of color sail interaction. Interactive demos are available at www.colorsails.com.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/compcaption.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Neural Compositional Paradigm for Image Captioning</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Bo Dai, Sanja Fidler, Dahua Lin</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Neural Information Processing Systems</em> (<strong>NeurIPS</strong>), Montreal, Canada, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1810.09630" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#neurips18" href="#neurips18abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#neurips18" href="#neurips18-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="neurips18-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Dai18neurips,<br/>
    title = {
A Neural Compositional Paradigm for Image Captioning},<br/>
    author = {Bo Dai and Sanja Fidler and Dahua Lin},<br/>
    booktitle = {NeurIPS},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="neurips18abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Mainstream captioning models often follow a sequential structure to generate captions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/poseiros1.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Pose Estimation for Objects with Rotational Symmetry</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Enric Corona, Kaustav Kundu, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Intelligent Robots and Systems</em> (<strong>IROS</strong>), Madrid, Spain</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1810.05780" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#poseiros" href="#poseirosabs-list">Abstract</a>&nbsp
<a href="http://www.cs.utoronto.ca/~ecorona/symmetry_pose_estimation/index.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#poseiros" href="#poseiros-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="poseiros-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{pose2018,<br/>
    title = {Pose Estimation for Objects with Rotational Symmetry},<br/>
    author = {Enric Corona and Kaustav Kundu and Sanja Fidler},<br/>
    booktitle = {IROS},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="poseirosabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Pose estimation is a widely explored problem, enabling many robotic tasks such as grasping and manipulation. In this paper, we tackle the problem of pose estimation for objects that exhibit rotational symmetry, which are common in man-made and industrial environments. In particular, our aim is to infer poses for objects not seen at training time, but for which their 3D CAD models are available at test time. Previous work has tackled this problem by learning to compare captured views of real objects with the rendered views of their 3D CAD models, by embedding them in a joint latent space using neural networks. We show that sidestepping the issue of symmetry in this scenario during training leads to poor performance at test time. We propose a model that reasons about rotational symmetry during training by having access to only a small set of symmetry-labeled objects, whereby exploiting a large collection of unlabeled CAD models. We demonstrate that our approach significantly outperforms a naively trained neural network on a new pose dataset containing images of tools and hardware.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/epic.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Scaling Egocentric Vision: The EPIC-KITCHENS Datasets &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Dima Damen,  Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler,  
           Antonino Furnari, Evangelos Kazakos, Davide Moltisanti,  Jonathan Munro,  Toby Perrett, Will Price, Michael Wray</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), Munich, Germany</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1804.02748" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#epic" href="#epicabs-list">Abstract</a>&nbsp
<a href="https://epic-kitchens.github.io/2018" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#epic" href="#epic-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="epic-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Damen2018EPICKITCHENS,<br/>
    title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},<br/>
    author = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},<br/>
    booktitle = {ECCV},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="epicabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict non-scripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/vsepp.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>spotlight present.</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>British Machine Vision Conference</em> (<strong>BMVC</strong>), Newcastle upon Tyne, UK</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://bmvc2018.org/contents/papers/0344.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#vsepp" href="#vseppabs-list">Abstract</a>&nbsp
<a href="https://github.com/fartashf/vsepp" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#vsepp" href="#vsepp-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="vsepp-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{vsepp2018,<br/>
    title = {VSE++: Improving Visual-Semantic Embeddings with Hard Negatives},<br/>
    author = {Fartash Faghri and David J. Fleet and Jamie Ryan Kiros and Sanja Fidler},<br/>
    booktitle = {BMVC},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="vseppabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present a new technique for learning visual-semantic embeddings for cross-modal
retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction,
and ranking loss functions, we introduce a simple change to common loss functions
used for multi-modal embeddings. That, combined with fine-tuning and use of
augmented data, yields significant gains in retrieval performance. We showcase our
approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and
comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art
methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1).
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/virtualhome.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:82%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">VirtualHome: Simulating Household Activities via Programs&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/VirtualHomeCVPR18.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Virtualhome" href="#Virtualhomeabs-list">Abstract</a>&nbsp
<a href="http://virtual-home.org/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonMM" data-toggle="collapse" data-parent="#Virtualhome" href="#Virtualhomemedia">Press</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Virtualhome" href="#Virtualhome-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Virtualhome-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{VirtualHome2018,<br/>
    title = {VirtualHome: Simulating Household Activities via Programs},<br/>
    author = {Xavier Puig and Kevin Ra and Marko Boben and Jiaman Li and Tingwu Wang and Sanja Fidler and Antonio Torralba},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Virtualhomeabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use <em>programs</em>, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos.  We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to ``drive'' an artificial agent to execute tasks in a simulated household environment. Our <em>VirtualHome</em> simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our <em>VirtualHome</em> based on language descriptions.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>

<div id="Virtualhomemedia" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">
<p style="margin:10px 0px 0px 0px;"></p>
      <div class="media">
      <p style="margin:0px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: left;">
            <tbody>
<tr><td width=180><a href="http://news.mit.edu/2018/mit-csail-teaching-chores-artificial-agent-0530"><img style="height:45px;" src="news/mit.jpg"></a></td>
</tr></tbody></table>
<p style="margin:-15px 0px 0px 0px;"></p>
<a data-toggle="collapse" data-parent="#Virtualhome" href="#Virtualhomemedia"><font color=#000000 size=3>close window</font></a>
</div>


      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/moviegraphs.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">MovieGraphs: Towards Understanding Human-Centric Situations from Videos&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>spotlight present.</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Paul Vicol, Makarand Tapaswi, Lluis Castrejon, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1712.06761" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Moviegraphs" href="#Moviegraphsabs-list">Abstract</a>&nbsp
<a href="http://moviegraphs.cs.toronto.edu/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Moviegraphs" href="#Moviegraphs-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Moviegraphs-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Moviegraphs2018,<br/>
    title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},<br/>
    author = {Paul Vicol and Makarand Tapaswi and Lluis Castrejon and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Moviegraphsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to "read" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reasoning about the social scene. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/movie4d.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Now You Shake Me: Towards Automatic 4D Cinema&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>spotlight presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Yuhao Zhou, Makarand Tapaswi, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://www.cs.toronto.edu/~henryzhou/movie4d/sources/cvpr2018.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Movie4d" href="#Movie4dabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~henryzhou/movie4d/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Movie4d" href="#Movie4d-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Movie4d-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Movie4D2018,<br/>
    title = {Now You Shake Me: Towards Automatic 4D Cinema},<br/>
    author = {Yuhao Zhou and Makarand Tapaswi and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Movie4dabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We are interested in enabling automatic 4D cinema by parsing physical and special effects from untrimmed movies. These include effects such as physical interactions, water splashing, light, and shaking, and are grounded to either a character in the scene or the camera. We collect a new dataset referred to as the Movie4D dataset which annotates over 9K effects in 63 movies. We propose a Conditional Random Field model atop a neural network that brings together visual and audio information, as well as semantics in the form of person tracks. Our model further exploits correlations of effects between different characters in the clip as well as across movie threads. We propose effect detection and classification as two tasks, and present results along with ablation studies on our dataset, paving the way towards 4D cinema in everyone's homes.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/polygonPP.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Efficient Annotation of Segmentation Datasets with Polygon-RNN++</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">David Acuna*, Huan Ling*, Amlan Kar*, Sanja Fidler &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<font style="font-size:10.5px">* <em>(Denotes equal contribution)</em></font></p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1803.09693" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#PolygonPP" href="#PolygonPPabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/polyrnn/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#PolygonPP" href="#PolygonPP-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="PolygonPP-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{PolygonPP2018,<br/>
    title = {Efficient Annotation of Segmentation Datasets with Polygon-RNN++},<br/>
    author = {Acuna, David and Ling, Huan and Kar, Amlan and Fidler, Sanja},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="PolygonPPabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of PolygonRNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/actproperly.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning to Act Properly: Predicting and Explaining Affordances from Images</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ching-Yao Chuang, Jiaman Li, Antonio Torralba, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1712.07576" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Actproperly" href="#Actproperlyabs-list">Abstract</a>&nbsp
<a href="http://www.cs.utoronto.ca/~cychuang/learning2act/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Actproperly" href="#Actproperly-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Actproperly-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ActProperly2018,<br/>
    title = {Learning to Act Properly: Predicting and Explaining Affordances from Images},<br/>
    author = {Chuang, Ching-Yao and Li, Jiaman and Torralba, Antonio and Fidler, Sanja},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Actproperlyabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent's actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which contains annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:12px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/face2face.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Face-to-Face Neural Conversation Model</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Chu, Daiqing Li, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/face2face.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Face2face" href="#Face2faceabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/face2face" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Face2face" href="#Face2face-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Face2face-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{F2F2018,<br/>
    title = {A Face-to-Face Neural Conversation Model},<br/>
    author = {Hang Chu and Daiqing Li and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Face2faceabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Neural networks have recently become good at engaging in dialog. However, current approaches are based solely on verbal text, lacking the richness of a real face-to-face conversation. We propose a neural conversation model that aims to read and generate facial gestures alongside with text. This allows our model to adapt its response based on the ``mood'' of the conversation. In particular, we introduce an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation. The decoder consists of two layers, where the lower layer aims at generating the verbal response and coarse facial expressions, while the second layer fills in the subtle gestures, making the generated output more smooth and natural. We train our neural network by having it "watch'' 250 movies.
We showcase our joint face-text model in generating more natural conversations through automatic metrics and a human study.
We demonstrate an example application with a face-to-face chatting avatar. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:12px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/surfconv.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">SurfConv: Bridging 3D and 2D Convolution for RGBD Images</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Chu, Wei-Chiu Ma, Kaustav Kundu, Raquel Urtasun,  Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>),  Salt Lake City, USA, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/surfconvCVPR18.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Face2face" href="#Surfconvabs-list">Abstract</a>&nbsp
<a href="https://github.com/chuhang/SurfConv" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Surfconv" href="#Surfconv-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Surfconv-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SurfConv2018,<br/>
    title = {SurfConv: Bridging 3D and 2D Convolution for RGBD Images},<br/>
    author = {Hang Chu and Wei-Chiu Ma and Kaustav Kundu and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Surfconvabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The last few years have seen approaches trying to combine the increasing popularity of depth sensors and the success of the convolutional neural networks.  Using depth as an additional channel alongside the RGB input has the scale variance problem present in image convolution based approaches. On the other hand, 3D convolution wastes a large amount of memory on mostly unoccupied 3D space, which consists of only the surface visible to the sensor.  Instead,we  propose  SurfConv,  which  "slides"  compact  2D  filters along  the  visible  3D  surface.   SurfConv  is  formulated  asa simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth Discretization (D4) scheme.  We demonstrate the effectiveness of our method on indoor and outdoor 3D semantic segmentation datasets.  Our method achieves state-of-the-art performance while using less than30%  parameters  used  by  the  3D  convolution  based  approaches.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:12px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/nervenet.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">NerveNet: Learning Structured Policy with Graph Neural Networks</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Learning Representations</em> (<b>ICLR</b>), Vancouver, Canada, 2018</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://www.cs.toronto.edu/~tingwuwang/nervenet.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Nervenet" href="#Nervenetabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~tingwuwang/nervenet.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Nervenet" href="#Nervenet-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Nervenet-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{WangICLR2018,<br/>
    title = {NerveNet: Learning Structured Policy with Graph Neural Networks},<br/>
    author = {Tingwu Wang and Renjie Liao and Jimmy Ba and Sanja Fidler},<br/>
    booktitle = {ICLR},<br/>
    year = {2018}</br>}
</p>      
</div>
<div id="Nervenetabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<p style="margin:-10px 0px 0px 0px;"></p>
<p style="margin:65px 0px 0px 0px;"></p>

<h2>Year 2017</h2>
<br/>

<p style="margin:-12.5px 0px 0px 0px;"></p>

<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:15px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/feedback.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Teaching Machines to Describe Images via Natural Language Feedback</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Huan Ling, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Neural Information Processing Systems</em> (<strong>NIPS</strong>), Long Beach, USA, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1706.00130" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Feedback" href="#Feedbackabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~linghuan/feedbackImageCaption/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Feedback" href="#Feedback-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Feedback-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{LingNIPS2017,<br/>
    title = {Teaching Machines to Describe Images via Natural Language Feedback},<br/>
    author = {Huan Ling and Sanja Fidler},<br/>
    booktitle = {NIPS},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="Feedbackabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
 Robots will eventually be part of every household. 
It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that descriptive sentence can provide a stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.  We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a <i>feedback network</i> that provides reward to the learner by conditioning on the human-provided feedback. We show  that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/captioningRL.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Towards Diverse and Natural Image Descriptions via a Conditional GAN&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1703.06029" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#captioningRLabs" href="#captioningRLabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#captioningRL" href="#captioningRL-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="captioningRL-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{DaiICCV17,<br/>
    title = {Towards Diverse and Natural Image Descriptions via a Conditional GAN},<br/>
    author = {Bo Dai and Sanja Fidler and Raquel Urtasun and Dahua Lin},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="captioningRLabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.  Sentences produced by existing methods, eg those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples.
This principle encourages high resemblance to the "ground-truth'' captions, while suppressing other reasonable descriptions. Conventional evaluation metrics, eg BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression.  Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is
nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. 
 We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>

<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/3dggnn.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">3D Graph Neural Networks for RGBD Semantic Segmentation&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/3dggnn_iccv17.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#GGNN3Dabs" href="#GGNN3Dabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#GGNN3D" href="#GGNN3D-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="GGNN3D-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{3dggnnICCV17,<br/>
    title = {3D Graph Neural Networks for RGBD Semantic Segmentation},<br/>
    author = {Xiaojuan Qi and Renjie Liao and Jiaya Jia and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="GGNN3Dabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
RGBD semantic segmentation requires joint reasoning about 2D appearance and 3D geometric information. In this paper we propose a 3D graph neural network (3DGNN) that builds a k-nearest neighbor graph on top of 3D point cloud. Each node in the graph corresponds to a set of points and is associated with a hidden representation vector initialized with an  appearance feature extracted by a unary CNN from 2D images. Relying on recurrent functions, every node dynamically updates its hidden representation based on the current status and incoming messages from its neighbors. This propagation model is unrolled for a certain number of time steps and the final per-node representation is used for predicting the semantic class of each pixel. We use back-propagation through time to train the model. Extensive experiments on NYUD2 and SUN-RGBD datasets demonstrate the effectiveness of our approach. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/tcity.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">TorontoCity: Seeing the World with a Million Eyes&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>spotlight presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Shenlong Wang, Min Bai, Gellert MattyUSA, Hang Chu, Wenjie Luo, Bin Yang, Justin Liang, Joel Cheverie, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://www.cs.toronto.edu/~urtasun/publications/wang_etal_iccv17.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#TCityabs" href="#TCityabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#TCity" href="#TCity-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="TCity-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{TCity2017,<br/>
    title = {TorontoCity: Seeing the World with a Million Eyes},<br/>
    author = {Shenlong Wang and Min Bai and Gellert Mattyus and Hang Chu and Wenjie Luo and Bin Yang and Justin Liang and Joel Cheverie and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="TCityabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we introduce the TorontoCity benchmark, which covers the full greater Toronto area (GTA) with 712.5km2 of land, 8439km of road and around 400,000
buildings. Our benchmark provides different perspectives of the world captured from airplanes, drones and cars driving around the city. Manually labeling such a large scale dataset is infeasible. Instead, we propose to utilize different sources of high-precision maps to create our ground truth. Towards this goal, we develop algorithms that allow us to align all data sources with the maps while requiring minimal human supervision. We have designed a wide variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction (reorganization), semantic labeling and scene type classification (recognition). Our pilot study shows that most of these tasks are still difficult for modern convolutional neural networks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/situationlearning.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:78%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Situation Recognition with Graph Neural Networks</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ruiyu Li, Makarand Tapaswi, Renjie Liao,  Jiaya Jia, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1708.04320" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Situation17abs" href="#Situation17abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Situation17" href="#Situation17-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Situation17-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SituationsICCV17,<br/>
    title = {Situation Recognition with Graph Neural Networks},<br/>
    author = {Ruiyu Li and Makarand Tapaswi and Renjie Liao and  Jiaya Jia and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="Situation17abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We address the problem of recognizing situations in images. Given an image, the task is to predict the most salient verb (action), and fill its semantic roles such as who is performing the action, what is the source and target of the action, etc. Different verbs have different roles (eg <em>attacking</em> has <em>weapon</em>), and each role can take on many possible values (nouns). We propose a model based on Graph Neural Networks that allows us to efficiently capture joint dependencies between roles using neural networks defined on a graph. Experiments with different graph connectivities show that our approach that propagates information between roles significantly outperforms existing work, as well as multiple baselines. We obtain roughly 3-5% improvement over previous work in predicting the full situation. We also provide a thorough qualitative analysis of our model and influence of different roles in the verbs.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/openvoc.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Open Vocabulary Scene Parsing</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, Antonio Torralba</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1703.08769" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#openvocabs" href="#openvocabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#openvoc" href="#openvoc-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="openvoc-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{openvoc17,<br/>
    title = {Open Vocabulary Scene Parsing},<br/>
    author = {Hang Zhao and Xavier Puig and Bolei Zhou and Sanja Fidler and Antonio Torralba},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="openvocabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/sgn.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Sequential Grouping Networks for Instance Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Shu Liu, Jiaya Jia, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/sgn_iccv17.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SGNabs" href="#SGNabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SGN" href="#SGN-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="SGN-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SGN17,<br/>
    title = {Sequential Grouping Networks for Instance Segmentation},<br/>
    author = {Shu Liu and Jiaya Jia and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="SGNabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we propose Sequential Grouping Networks (SGN)  to tackle the problem of object instance segmentation. SGNs employ a sequence of neural networks, each solving a sub-grouping problem of increasing semantic complexity in order to gradually compose objects out of pixels. In particular, the first network aims to group pixels along each image row and column by predicting horizontal and vertical object breakpoints. These breakpoints are then used to create line segments.  By exploiting two-directional information, the second network groups horizontal and vertical lines  into connected components. Finally, the third network groups the connected components into object instances.
Our experiments show that our SGN significantly outperforms state-of-the-art approaches in both, the Cityscapes dataset as well as PASCAL VOC.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>



<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/ganprada.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:75%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Be Your Own Prada: Fashion Synthesis with Structural Coherence</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Shizhan Zhu, Sanja Fidler, Raquel Urtasun, Dahua Lin, Chen Change Loy</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Venice, Italy, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/ganprada_iccv17.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ganpradaabs" href="#ganpradaabs-list">Abstract</a>&nbsp
<a href="http://mmlab.ie.cuhk.edu.hk/projects/FashionGAN/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ganprada" href="#ganprada-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ganprada-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{GANprada17,<br/>
    title = {Be Your Own Prada: Fashion Synthesis with Structural Coherence},<br/>
    author = {Shizhan Zhu and Sanja Fidler and  Raquel Urtasun and Dahua Lin and Chen Change Loy},<br/>
    booktitle = {ICCV},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="ganpradaabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present a novel and effective approach for generating new clothing on a wearer through generative adversarial
learning. Given an input image of a person and a sentence describing a different outfit, our model "redresses" the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer's body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer's pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>




<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/polyrnn.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:69%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Annotating Object Instances with a Polygon-RNN &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a style="color:#E62E00;">(<strong><em>best paper honorable mention</em></strong>)</a></h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Honolulu, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/paper_polyrnn.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Polyrnnabs" href="#Polyrnnabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/polyrnn/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Polyrnn" href="#Polyrnn-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Polyrnn-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{CastrejonCVPR17,<br/>
    title = {Annotating Object Instances with a Polygon-RNN},<br/>
    author = {Lluis Castrejon and Kaustav Kundu and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="Polyrnnabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We propose an approach for semi-automatic annotation of object instances. 
While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. 
For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We further show generalization capabilities of our approach to unseen datasets.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:12%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="https://www.youtube.com/watch?v=S1UUR4FlJ84" target="_blank"><img width="100%" src="images/polytalk.jpg"></a><br/><font style="font-size:12.0px;">[<a href="https://www.youtube.com/watch?v=S1UUR4FlJ84" target="_blank">talk</a>]&nbsp[<a href="https://medium.com/@Synced/cvpr2017-honorable-mention-paper-annotating-object-instances-with-a-polygon-rnn-c472975667df" target="_blank">press</a>]</font></div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/sports17.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Sports Field Localization via Deep Structured Models</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Namdar Homayounfar, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Honolulu, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1604.02715" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Sports17abs" href="#Sports17abs-list">Abstract</a>&nbsp
<!--<a href="https://github.com/ivendrov/order-embedding" target="_blank" class="buttonPP">Code</a>&nbsp-->
<a class="buttonSS" data-toggle="collapse" data-parent="#Sports17" href="#Sports17-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Sports17-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{NamdarCVPR17,<br/>
    title = {Sports Field Localization via Deep Structured Models},<br/>
    author = {Namdar Homayounfar and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="Sports17abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this work, we propose a novel way of efficiently localizing a sports field from a single broadcast image of the game. Related work in this area relies on manually annotating a few key frames and extending the localization to similar images, or installing fixed specialized cameras in the stadium from which the layout of the field can be obtained. In contrast, we formulate this problem as a branch and bound inference in a Markov random field where an energy function is defined in terms of  semantic cues such as the field surface, lines and circles obtained from a deep semantic segmentation network. Moreover, our approach is fully automatic and depends only on a single image from the broadcast video of the game. We demonstrate the effectiveness of our method by applying it to soccer and hockey. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/ade20k.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Scene Parsing through ADE20K Dataset</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Honolulu, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Ade20kabs" href="#Ade20kabs-list">Abstract</a>&nbsp
<a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" class="buttonPP">Dataset</a>&nbsp
<a href="https://github.com/CSAILVision/semantic-segmentation-pytorch" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Ade20k" href="#Ade20k-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Ade20k-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Ade20k,<br/>
    title = {Scene Parsing through ADE20K Dataset},<br/>
    author = {Bolei Zhou and Hang Zhao and Xavier Puig and Sanja Fidler and Adela Barriuso and Antonio Torralba},<br/>
    booktitle = {CVPR},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="Ade20kabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Scene parsing, or recognizing and segmenting objects
and stuff in an image, is one of the key problems in computer
vision. Despite the community's efforts in data collection,
there are still few image datasets covering a wide range of
scenes and object categories with dense and detailed annotations
for scene parsing. In this paper, we introduce and
analyze the ADE20K dataset, spanning diverse annotations
of scenes, objects, parts of objects, and in some cases even
parts of parts. A scene parsing benchmark is built upon the
ADE20K with 150 object and stuff classes included. Several
segmentation baseline models are evaluated on the benchmark.
A novel network design called Cascade Segmentation
Module is proposed to parse a scene into stuff, objects, and
object parts in a cascade and improve over the baselines.
We further show that the trained scene parsing networks
can lead to applications such as image content removal and
scene synthesis.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 3.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:3px 1.6% 0 0.2%; width:13%;" href=""><img width="100%" src="papers/figs/lostsun.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:84%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Find Your Way by Observing the Sun and Other Semantic Cues</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Wei-Chiu Ma, Shenlong Wang, Marcus A. Brubaker, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Robotics and Automation</em> (<strong>ICRA</strong>), Singapore, 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/pdf/1606.07415.pdf" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Icra17abs" href="#Icra17abs-list">Abstract</a>&nbsp
<!--<a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" class="buttonPP">Dataset</a>&nbsp-->
<a class="buttonSS" data-toggle="collapse" data-parent="#Icra17" href="#Icra17-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Icra17-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{WeiChiuICRA17,<br/>
    title = {Find Your Way by Observing the Sun and Other Semantic Cues},<br/>
    author = {Wei-Chiu Ma and Shenlong Wang and Marcus A. Brubaker and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICRA},<br/>
    year = {2017}</br>}
</p>      
</div>
<div id="Icra17abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we present a robust, efficient and affordable
approach to self-localization which does not require
neither GPS nor knowledge about the appearance of the
world. Towards this goal, we utilize freely available cartographic
maps and derive a probabilistic model that exploits
semantic cues in the form of sun direction, presence of an
intersection, road type, speed limit as well as the ego-car
trajectory in order to produce very reliable localization results.
Our experimental evaluation shows that our approach
can localize much faster (in terms of driving time) with less
computation and more robustly than competing approaches,
which ignore semantic information.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
</div>
      </li>
    </ul>
    

 <p style="margin:-8.5px 0px 0px 0px;"></p> 
 
 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.6%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.3% 0 0; width:15.2%;" href=""><img width="100%" src="papers/figs/proposalsArxiv16.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">3D Object Proposals using Stereo Imagery for Accurate Object Class Detection</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Huimin Ma, Sanja Fidler, Raquel Urtasun</p>
            
	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Transactions on Pattern Analysis and Machine Intelligence</em> (<strong>TPAMI</strong>), 2017</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1608.07711" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ProposalsArxiv16abs" href="#ProposalsArxiv16abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/objprop3d/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ProposalsArxiv16" href="#ProposalsArxiv16-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ProposalsArxiv16-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ChenArxiv16,<br/>
    title = {3D Object Proposals using Stereo Imagery for Accurate Object Class Detection},<br/>
    author = {Xiaozhi Chen and Kaustav Kundu and Yukun Zhu and Huimin Ma and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {arXiv:1608.07711},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="ProposalsArxiv16abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The goal of this paper is to perform 3D object detection in the context of autonomous driving. Our method first aims at generating a set of high-quality 3D object proposals by exploiting stereo imagery. We formulate the problem as minimizing an energy function that encodes object size priors, placement of objects on the ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. We then exploit a CNN on top of these proposals to perform object detection. In particular, we employ a convolutional neural net (CNN) that exploits context and depth information to jointly regress to 3D bounding box coordinates and object pose. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When combined with the CNN, our approach outperforms all existing results in object detection and orientation estimation tasks for all three KITTI object classes. Furthermore, we experiment also with the setting where LIDAR information is available, and show that using both LIDAR and stereo leads to the best result.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                        <!--      <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>
 
 
<p style="margin:-10px 0px 0px 0px;"></p>
<p style="margin:65px 0px 0px 0px;"></p>

<h2>Year 2016</h2>
<br/>

<p style="margin:-12.5px 0px 0px 0px;"></p>


 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.8%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 1.3% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/ai_music.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Song From PI: A Musically Plausible Network for Pop Music Generation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Chu, Raquel Urtasun, Sanja Fidler</p>
            
	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">arXiv:1611.03477, ICLR Workshop track, 2017</p>
            
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Generation of pop songs</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>

            <a href="https://arxiv.org/abs/1611.03477" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SongOfPIabs" href="#SongOfPIabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/songfrompi/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonMM" data-toggle="collapse" data-parent="#SongOfPI" href="#SongOfPImedia">Press</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SongOfPI" href="#SongOfPI-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="SongOfPI-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SongOfPI,<br/>
    title = {Song From PI: A Musically Plausible Network for Pop Music Generation},<br/>
    author = {Hang Chu and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {arXiv:1611.03477},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="SongOfPIabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
            <!--                  <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
                              
                              <div id="SongOfPImedia" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">
<p style="margin:10px 0px 0px 0px;"></p>
      <div class="media">
      <p style="margin:0px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: left;">
            <tbody>
<tr><td width=80px;><a href="https://www.theguardian.com/technology/2016/nov/29/its-no-christmas-no-1-but-ai-generated-song-brings-festive-cheer-to-researchers"><img style="height:35px;" src="news/theguardian.jpg"></a></td><td width=280><a href="http://www.theregister.co.uk/2016/11/11/ai_pop_music_maker/"><img style="height:35px;" src="news/register.jpg"></a></td></td>
</tr><tr><td style="font-size:15px;"><a href="https://www.theguardian.com/technology/2016/nov/29/its-no-christmas-no-1-but-ai-generated-song-brings-festive-cheer-to-researchers">The Guardian</a><td style="font-size:15px;"><a href="http://www.theregister.co.uk/2016/11/11/ai_pop_music_maker/">The Register</a></td></td>
</tr></tbody></table>
<p style="margin:-15px 0px 0px 0px;"></p>
<a data-toggle="collapse" data-parent="#SongOfPI" href="#SongOfPImedia"><font color=#000000 size=3>close window</font></a>
</div>

      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p>
 
 
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:14px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/summarization16.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Efficient Summarization with Read-Again and Copy Mechanism</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Wenyuan Zeng, Wenjie Luo, Sanja Fidler, Raquel Urtasun</p>
            
	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Arxiv preprint arXiv:1611.03382</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="https://arxiv.org/abs/1611.03382" target="_blank" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Summarization16abs" href="#Summarization16abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Summarization16" href="#Summarization16-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Summarization16-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{WenyuanArxiv16,<br/>
    title = {Efficient Summarization with Read-Again and Copy Mechanism},<br/>
    author = {Wenyuan Zeng and Wenjie Luo and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {arXiv:1611.03382},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="Summarization16abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                            <!--  <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p> 



 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:15px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/proximal.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Proximal Deep Structured Models</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Shenlong Wang, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, Barcelona, Spain, 2016</p>
            

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/shenlongNIPS16.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Proximalabs" href="#Proximalabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Proximal" href="#Proximal-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Proximal-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ShenlongNIPS16,<br/>
    title = {Proximal Deep Structured Models},<br/>
    author = {Shenlong Wang and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {NIPS},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="Proximalabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Many problems in real-world applications involve predicting continuous-valued random variables that are statistically related. 
In this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the   dependencies between continuous output variables.
We show that  inference in our  model using proximal methods can be efficiently solved as a feed-forward pass of a special  type of  deep recurrent neural network. We demonstrate the  effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                            <!--  <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p> 
 
 
 
 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/house3d.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">HouseCraft: Building Houses from Rental Ads and Street Views</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Hang Chu, Shenlong Wang, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference on Computer Vision</em> (<strong>ECCV</strong>), Amsterdam, Netherlands, 2016</p>
            
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Creating 3D models of houses</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/chu_eccv16.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#HouseCraftabs" href="#HouseCraftabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/housecraft/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#HouseCraft" href="#HouseCraft-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="HouseCraft-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ChuECCV16,<br/>
    title = {HouseCraft: Building Houses from Rental Ads and Street Views},<br/>
    author = {Hang Chu and Shenlong Wang and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {ECCV},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="HouseCraftabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
    In this paper, we utilize rental ads to create  realistic textured 3D models of building exteriors. In particular, we exploit the address of the property and its floorplan, which are typically available in the ad. The address allows us to extract Google StreetView images around the building, while the building's floorplan allows for an efficient parametrization of the building in 3D via a small set of random variables. We propose an energy minimization framework which jointly reasons about the height of each floor, the vertical positions of windows and doors, as well as the precise location of the building in the world's map, by exploiting several geometric and semantic cues from the StreetView imagery. To demonstrate the effectiveness of our approach, we collected a new dataset with 174 houses by crawling a popular rental website. Our experiments show that our   approach is able to  precisely estimate the geometry and location of the property, and can create realistic 3D building models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
    <!--                          <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p> 
 
 
 
 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/movieqa.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:69.0%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">MovieQA: Understanding Stories in Movies through Question-Answering &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>spotlight</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Makarand Tapaswi, Yukun Zhu, Reiner Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Las Vegas, 2016</p>
            
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Benchmark on question-answering about movies</p>

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1512.02902" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#MovieQAabs" href="#MovieQAabs-list">Abstract</a>&nbsp
<a href="http://movieqa.cs.toronto.edu" target="_blank" class="buttonPP">Benchmark</a>&nbsp
<a class="buttonMM" data-toggle="collapse" data-parent="#MovieQA" href="#MovieQAmedia">Press</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MovieQA" href="#MovieQA-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="MovieQA-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{TapaswiCVPR16,<br/>
    title = {MovieQA: Understanding Stories in Movies through Question-Answering},<br/>
    author = {Makarand Tapaswi and Yukun Zhu and Reiner Stiefelhagen and Antonio Torralba and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="MovieQAabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 15,000 questions about 408 movies with high semantic diversity. The questions range from simpler "Who" did "What" to "Whom", to "Why" and "How" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- full-length movies, plots, subtitles, scripts and for a subset DVS. We analyze our data through various statistics and intelligent baselines. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We plan to create a benchmark with an active leader board, to encourage inspiring work in this challenging domain.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:12%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="https://www.youtube.com/watch?v=aO8Tt9GcQak"><img width="100%" src="images/makarand.jpg"></a><br/><font style="font-size:12.0px;">[<a href="https://www.youtube.com/watch?v=aO8Tt9GcQak">talk</a>]</font></div>
</div>

<div id="MovieQAmedia" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">
<p style="margin:10px 0px 0px 0px;"></p>
      <div class="media">
      <p style="margin:0px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: left;">
            <tbody>
<tr><td width=180><a href="https://www.technologyreview.com/s/544506/now-ai-machines-are-learning-to-understand-stories/"><img style="height:45px;" src="news/mittech.jpg"></a></td><td><a href="https://news.developer.nvidia.com/gpu-trained-system-understands-movies/"><img style="height:45px;" src="news/nvidia.jpeg"></a></td>
</tr><tr><td style="font-size:15px;"><a href="https://www.technologyreview.com/s/544506/now-ai-machines-are-learning-to-understand-stories/">MIT Tech Review</a></td>
<td style="font-size:15px;"><a href="https://news.developer.nvidia.com/gpu-trained-system-understands-movies/">NVIDIA News</a></td>
</tr></tbody></table>
<p style="margin:-15px 0px 0px 0px;"></p>
<a data-toggle="collapse" data-parent="#MovieQA" href="#MovieQAmedia"><font color=#000000 size=3>close window</font></a>
</div>

      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>
 


 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7.5px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/cvpr16_inst.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ziyu Zhang, Sanja Fidler, Raquel Urtasun</p>
            
	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Las Vegas, 2016</p>
            
<p style="margin:-10.0px 0px 0px 0px;"></p>

           <!-- <p style="font-size:13.4px; color:#E62E00;">Object instance segmentation</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1512.06735" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#InstanceCVPR16abs" href="#InstanceCVPR16abs-list">Abstract</a>&nbsp
<a href="http://www.cs.utoronto.ca/kitti-instance/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#InstanceCVPR16" href="#InstanceCVPR16-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="InstanceCVPR16-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ZhangCVPR16,<br/>
    title = {Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs},<br/>
    author = {Ziyu Zhang and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="InstanceCVPR16abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Our aim is to provide a pixel-level object instance labeling of a monocular image. We build on recent work [Zhang et al., ICCV15] that trained a convolutional neural net to predict instance labeling in local image patches, extracted exhaustively in a stride from an image. A simple Markov random field model using several heuristics was then proposed in [Zhang et al., ICCV15] to derive a globally consistent instance labeling of the image. In this paper, we formulate the global labeling problem with a novel densely connected Markov random field and show how to encode various intuitive potentials in a way that is amenable to efficient mean field inference [Krahenbuhl et al., NIPS11]. Our potentials encode the compatibility between the global labeling and the patch-level predictions, contrast-sensitive smoothness as well as the fact that separate regions form different instances. Our experiments on the challenging KITTI benchmark [Geiger et al., CVPR12] demonstrate that our method achieves a significant performance boost over the baseline [Zhang et al., ICCV15].
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                              </div>
      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p> 
 
 
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/cvpr16_det.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Monocular 3D Object Detection for Autonomous Driving</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, Raquel Urtasun</p>
            
	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Las Vegas, 2016</p>
            
<p style="margin:-10.0px 0px 0px 0px;"></p>

           <!-- <p style="font-size:13.4px; color:#E62E00;">Object instance segmentation</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/chenCVPR16.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Det3DCVPR16abs" href="#Det3DCVPR16abs-list">Abstract</a>&nbsp
<a href="http://3dimage.ee.tsinghua.edu.cn/cxz/mono3d" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Det3DCVPR16" href="#Det3DCVPR16-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Det3DCVPR16-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ChenCVPR16,<br/>
    title = {Monocular 3D Object Detection for Autonomous Driving},<br/>
    author = {Xiaozhi Chen and Kaustav Kundu and Ziyu Zhang and Huimin Ma and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="Det3DCVPR16abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The goal of this paper is to perform 3D object detection from a single monocular image in the domain of autonomous driving. 
Our method first aims to generate a set of candidate class-specific object proposals, which are then run through a standard CNN pipeline to obtain high-quality object detections. The focus of this paper is on proposal generation. In particular, we propose an energy minimization approach   that places object candidates in 3D using the fact that objects should be on the ground-plane. We then score each candidate box projected to the image plane via several intuitive potentials encoding  semantic segmentation, contextual information, size and location priors and typical object shape. Our experimental evaluation demonstrates that our object proposal generation approach significantly outperforms all monocular approaches, and achieves the best detection performance on the challenging KITTI benchmark, among  published monocular competitors. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                              </div>
      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p> 
 
 
   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/cvpr16_aerial.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:76.4%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">HD Maps: Fine-grained Road Segmentation by Parsing Ground and Aerial Images</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Gellert MattyUSA, Shenlong Wang, Sanja Fidler, Raquel Urtasun</p>
            
	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Las Vegas, 2016</p>
            
<p style="margin:-10.0px 0px 0px 0px;"></p>

           <!-- <p style="font-size:13.4px; color:#E62E00;">Object instance segmentation</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/gellertCVPR16.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#AerialCVPR16abs" href="#AerialCVPR16abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#AerialCVPR16" href="#AerialCVPR16-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="AerialCVPR16-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{MattyusCVPR16,<br/>
    title = {HD Maps: Fine-grained Road Segmentation by Parsing Ground and Aerial Images},<br/>
    author = {Gellert Mattyus and Shenlong Wang and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="AerialCVPR16abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we present an approach to enhance existing
maps with fine grained segmentation categories such as
parking spots and sidewalk, as well as the number and location
of road lanes. Towards this goal, we propose an efficient approach that is able to estimate these fine grained
categories by doing joint inference over both, monocular
aerial imagery, as well as ground images taken from a
stereo camera pair mounted on top of a car. Important to
this is reasoning about the alignment between the two types
of imagery, as even when the measurements are taken with
sophisticated GPS+IMU systems, this alignment is not sufficiently accurate. We demonstrate the effectiveness of our
approach on a new dataset which enhances KITTI with
aerial images taken with a camera mounted on an airplane
and flying around the city of Karlsruhe, Germany.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                              </div>
      </li>
    </ul>
 <p style="margin:-8.5px 0px 0px 0px;"></p> 
 

 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.5% 0 0; width:14.6%;" href=""><img width="100%" src="papers/figs/orderemb.jpg"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:69%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Order-Embeddings of Images and Language &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Learning Representations</em> (<b>ICLR</b>), Puerto Rico, 2016</p>
            
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">State-of-the-art in caption-image retrieval on COCO</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1511.06361" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#OrderEmbabs" href="#OrderEmbabs-list">Abstract</a>&nbsp
<a href="https://github.com/ivendrov/order-embedding" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#OrderEmb" href="#OrderEmb-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="OrderEmb-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{VendrovArxiv15,<br/>
    title = {Order-Embeddings of Images and Language},<br/>
    author = {Ivan Vendrov and Ryan Kiros and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICLR},<br/>
    year = {2016}</br>}
</p>      
</div>
<div id="OrderEmbabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:12%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="http://videolectures.net/iclr2016_vendrov_order_embeddings/"><img width="100%" src="images/ivantalk.jpg"></a><br/><font style="font-size:12.0px;">[<a href="http://videolectures.net/iclr2016_vendrov_order_embeddings/">talk</a>]</font></div>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 1.6% 0 0; width:14.9%;" href=""><img width="100%" src="papers/figs/pami15small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Roozbeh Mottaghi, Sanja Fidler, Alan Yuille, Raquel Urtasun, Devi Parikh</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Transactions on Pattern Analysis and Machine Intelligence</em> (<strong>TPAMI</strong>), 38(1), 2016, pages 74-87</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>

    <a href="papers/mottaghiPAMI15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Mottaghi15abs" href="#Mottaghi15abs-list">Abstract</a>&nbsp
    <a href="papers/mottaghiPAMI15supmat.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Mottaghi15" href="#Mottaghi15-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Mottaghi15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@article{MottaghiPAMI16,<br/>
    title = {Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding},<br/>
    author = {Roozbeh Mottaghi and Sanja Fidler and Alan Yuille and Raquel Urtasun and Devi Parikh},<br/>
    journal = {Trans. on Pattern Analysis and Machine Intelligence},<br/>
    volume= {38},<br/>
    number= {1},<br/>
    pages= {74--87},<br/>
    year = {2016}<br/>}
</p>      
</div>
<div id="Mottaghi15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Recent trends in image understanding have pushed for scene understanding models that jointly reason about various
tasks such as object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers.
In this work, we are interested in understanding the roles of these different tasks in improved scene understanding, in particular
semantic segmentation, object detection and scene recognition. Towards this goal, we "plug-in" human subjects for each of the
various components in a conditional random field model. Comparisons among various hybrid human-machine CRFs give us
indications of how much "head room" there is to improve scene understanding by focusing research efforts on various individual
tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.1% 1% 0% 0%; width:11%;">
    <a href="papers/mottaghiPAMI15.pdf" class="buttonT" style="width:100%; height:14.6px; font-size:11px;">Paper</a><br/>
    <p style="margin:-10.0px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#Mottaghi15abs" href="#Mottaghi15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF; height:14.6px; font-size:11px;">Abstract</a><br/>
<p style="margin:1px 0px 0px 0px;"></p>
    <a href="papers/mottaghiPAMI15supmat.pdf" class="buttonM" style="width:100%; padding:0 0 -5px 0; height:14.6px; font-size:10.5px;">Suppl. Mat.</a><br/>
    <p style="margin:-9.0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#Mottaghi15" href="#Mottaghi15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933; height:14.6px; font-size:11px;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>


<p style="margin:-10px 0px 0px 0px;"></p>
<p style="margin:65px 0px 0px 0px;"></p>



<h2>Year 2015</h2>
<br/>

<p style="margin:-12.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 1.5% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/moviebook.jpg"><br/><p style="margin:4px 0px 0px 0px;"></p><img width="100%" src="papers/figs/harrypotter_small.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:82%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books<br/>(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Yukun Zhu*, Ryan Kiros*, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Santiago, Chile, 2015</p>
                        <p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:11.5px">* <em>Denotes equal contribution</em></p>
            
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;"> Aligning movies and books for story-like captioning</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.06724" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#MovieBookabs" href="#MovieBookabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~mbweb/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MovieBook" href="#MovieBook-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="MovieBook-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ZhuICCV15,<br/>
    title = {Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},<br/>
    author = {Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {ICCV},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="MovieBookabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in  current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>

<!--                              <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>


<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 2.2% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/shoppingICCV15_small.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Lost Shopping! Monocular Localization in Large Indoor Spaces&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Shenlong Wang, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Santiago, Chile, 2015</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

         <!--   <p style="font-size:13.4px; color:#E62E00;">Classification of unseen categories from their textual description (Wiki articles)</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
          <a href="papers/wangICCV15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Shoppingabs" href="#Shoppingabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Shopping" href="#Shopping-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Shopping-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{WangICCV15,<br/>
    title = {Lost Shopping! Monocular Localization in Large Indoor Spaces},<br/>
    author = {Shenlong Wang and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="Shoppingabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we propose a novel approach to  localization  in very large indoor spaces (i.e., 200+ store shopping malls)  that takes   a single image and a floor plan of the environment as input. 
We formulate  the localization problem as inference in a Markov random field, which jointly reasons about text detection (localizing shop's names in the image with precise bounding boxes), shop facade segmentation, as well as   camera's  rotation and translation within the entire shopping mall. The power of our approach is that it does not use any prior  information about appearance  and instead  exploits text detections corresponding to the shop names. This makes our method applicable to a variety of domains and robust to store appearance variation across countries, seasons, and illumination conditions.
We demonstrate the performance of our approach in a new dataset we collected of two very large shopping malls, and show the power of holistic reasoning. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
            <!--                    <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>



    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.2%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 3.1% 0 0; width:14.7%;" href=""><img width="100%" src="papers/figs/zeroshot_small.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jimmy Ba, Kevin Swersky, Sanja Fidler, Ruslan Salakhutdinov</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Santiago, Chile, 2015</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Classification of unseen categories from their textual description (Wiki articles)</p>

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.00511" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ZeroShotabs" href="#ZeroShotabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ZeroShot" href="#ZeroShot-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ZeroShot-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{BaICCV15,<br/>
    title = {Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions},<br/>
    author = {Jimmy Ba and Kevin Swersky and Sanja Fidler and Ruslan Salakhutdinov},<br/>
    booktitle = {ICCV},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="ZeroShotabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
               <!--                 <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 2.3% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/aerialICCV15_small.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Enhancing World Maps by Parsing Aerial Images</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Gellert MatthyUSA, Shenlong Wang, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Santiago, Chile, 2015</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

         <!--   <p style="font-size:13.4px; color:#E62E00;">Classification of unseen categories from their textual description (Wiki articles)</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
          <a href="papers/aerialICCV15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#AerialICCV15abs" href="#AerialICCV15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#AerialICCV15" href="#AerialICCV15-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="AerialICCV15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{MatthyusICCV15,<br/>
    title = {Enhancing World Maps by Parsing Aerial Images},<br/>
    author = {Gellert Matthyus and Shenlong Wang  and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="AerialICCV15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In recent years, contextual models that exploit maps have been shown to be very effective for many recognition and localization tasks. 
In this paper, we propose to exploit aerial images in order to enhance  freely available world maps. Towards this goal, we make use of OpenStreetMap and formulate the problem as the one of inference in a Markov random field parameterized in terms of  the location of the road-segment centerlines as well as  their width. 
This parameterization enables very efficient inference and  returns only topologically correct roads. In particular, we can segment all OSM roads in the  world in a single day using a small cluster of 10 computers. Importantly, our approach generalizes very well; it can be trained using a single  aerial image and produces very accurate results in any location across the globe.  
We demonstrate the effectiveness of our approach over the previous state-of-the-art on two new benchmarks that we collect. We additionally show how our enhanced maps can be exploited for semantic segmentation of ground images. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                          <!--      <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 2.3% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/instanceICCV15_small.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Monocular Object Instance Segmentation and Depth Ordering with CNNs</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ziyu Zhang, Alex Schwing, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Santiago, Chile, 2015</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

         <!--   <p style="font-size:13.4px; color:#E62E00;">Classification of unseen categories from their textual description (Wiki articles)</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
         <a href="papers/instanceICCV15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#InstanceICCV15abs" href="#InstanceICCV15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#InstanceICCV15" href="#InstanceICCV15-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="InstanceICCV15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ZhangICCV15,<br/>
    title = {Monocular Object Instance Segmentation and Depth Ordering with CNNs},<br/>
    author = {Ziyu Zhang and Alex Schwing  and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {ICCV},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="InstanceICCV15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we tackle the problem of   instance level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict  instance level segmentations where the instance ID encodes   depth ordering from large image patches. 
To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of  convolutional nets applied at overlapping patches of different resolutions as well as the output of a connected component algorithm and predicts very accurate instance level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show very good performance on both tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                      <!--          <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 2.3% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/midlevelICCV15_small.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Learning Framework for Generating Region Proposals with Mid-level Cues</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tom Lee, Sanja Fidler, Sven Dickinson</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Santiago, Chile, 2015</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

         <!--   <p style="font-size:13.4px; color:#E62E00;">Classification of unseen categories from their textual description (Wiki articles)</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
           <a href="papers/leeICCV15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ProposalsICCV15abs" href="#ProposalsICCV15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ProposalsICCV15" href="#ProposalsICCV15-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ProposalsICCV15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{TLeeICCV15,<br/>
    title = {A Learning Framework for Generating Region Proposals with Mid-level Cues},<br/>
    author = {Tom Lee  and Sanja Fidler and Sven Dickinson},<br/>
    booktitle = {ICCV},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="ProposalsICCV15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The object categorization community's migration from object detection to large-scale object categorization has seen a shift from sliding window approaches to bottom-up region segmentation, with the resulting <em>region proposals</em> offering discriminating shape and appearance features through an attempt to explicitly segment the objects in a scene from their background.   One powerful class of region proposal techniques is based on parametric energy minimization (PEM) via parametric maxflow.  In this paper, we incorporate PEM into a novel structured learning framework that learns how to combine a set of mid-level grouping cues to yield a small set of region proposals with high recall.
Second, we diversify our region proposals and rank them with region-based convolutional neural network features.  Our novel approach, called <em>parametric min-loss</em>, casts perceptual grouping and cue combination in a learning framework which yields encouraging results on VOC'2012. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
     <!--                           <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.4%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 2.8% 0 0; width:14.5%;" href=""><img width="100%" src="papers/figs/skipthoughts.png"><br/><p style="margin:12px 0px 0px 0px;"></p><img width="100%" src="papers/figs/skipthoughts_res.jpg"></div>          
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Skip-Thought Vectors</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, Montreal, Canada, 2015</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;"> A neural representation of sentences trained on 11K books</p>

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.06726" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SkipThoughtsabs" href="#SkipThoughtsabs-list">Abstract</a>&nbsp
<a href="https://github.com/ryankiros/skip-thoughts" class="buttonPP">Code</a>&nbsp
<a href="https://github.com/ryankiros/neural-storyteller" class="buttonPP" style="width:100px;">Neural storyteller</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SkipThoughts" href="#SkipThoughts-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="SkipThoughts-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{KirosNIPS15,<br/>
    title = {Skip-Thought Vectors},<br/>
    author = {Ryan Kiros and Yukun Zhu and Ruslan Salakhutdinov and Richard Zemel and Antonio Torralba and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {NIPS},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="SkipThoughtsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                    <!--            <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.2%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 2.6% 0 0; width:14.7%;" href=""><img width="100%" src="papers/figs/proposalsNIPS15_small.jpg"></div>          
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:74.1%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">3D Object Proposals for Accurate Object Class Detection</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xiaozhi Chen*, Kaustav Kundu*, Yukun Zhu, Andrew Berneshawi, Huimin Ma, Sanja Fidler, Raquel Urtasun</p>

                        <p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:11.5px">* <em>Denotes equal contribution</em></p>
            
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>
	     
	      <p style="font-size:13.4px; color:#E62E00;" style="padding:0px 0px 0px 0px;"> Currently third in Car, and first in Pedestrian and Cyclist detection on KITTI's <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">Leaderboard</a></p>-->
	      
	      <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, Montreal, Canada, 2015</p>


<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/3dopNIPS15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#proposalsNIPS15abs" href="#proposalsNIPS15abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/objprop3d/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#proposalsNIPS15" href="#proposalsNIPS15-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="proposalsNIPS15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{XiaozhiNIPS15,<br/>
    title = {3D Object Proposals for Accurate Object Class Detection},<br/>
    author = {Xiaozhi Chen and Kaustav Kundu and Yukun Zhu and Andrew Berneshawi and Huimin Ma and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {NIPS},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="proposalsNIPS15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to  place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors,  ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground.  
Our experiments  show significant performance gains over existing RGB and RGB-D object proposal methods  on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on <em>Car</em> and <em>Cyclist</em>, and is competitive for the <em>Pedestrian</em> class.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                       <!--         <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:5.0%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> -->
                              </div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 3% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/arxiv2015generation_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Generating Multi-Sentence Lingual Descriptions of Indoor Scenes&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Dahua Lin, Sanja Fidler, Chen Kong, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>British Machine Vision Conference</em> (<strong>BMVC</strong>), Swansea, UK, 2015</p>
            <!--<p style="font-size:13.4px"><em>arXiv:1503.00064</em>, Feb 28, 2015</p>-->
            
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            
            
            <a href="papers/LinBMVC15.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#Lin15abs" href="#Lin15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Lin15" href="#Lin15-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Lin15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{LinBMVC15,<br/>
    title = {Generating Multi-Sentence Lingual Descriptions of Indoor Scenes},<br/>
    author = {Dahua Lin  and Sanja Fidler and Chen Kong and Raquel Urtasun},<br/>
    booktitle = {BMVC},<br/>
    year = {2015}}
</p>      
</div>
<div id="Lin15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
This paper proposes a novel framework for generating lingual descriptions of
indoor scenes. Whereas substantial efforts have been made to tackle this
problem, previous approaches focusing primarily on generating a single sentence
for each image, which is not sufficient for describing complex scenes. We
attempt to go beyond this, by generating coherent descriptions with multiple
sentences. Our approach is distinguished from conventional ones in several
aspects: (1) a 3D visual parsing system that jointly infers objects,
attributes, and relations; (2) a generative grammar learned automatically from
training text; and (3) a text generation algorithm that takes into account the
coherence among sentences. Experiments on the augmented NYU-v2 dataset show
that our framework can generate natural descriptions with substantially higher
ROGUE scores compared to those produced by the baseline.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.4% 1% 0% 0%; width:11%;">
<a href="http://arxiv.org/abs/1503.00064" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#Lin15abs" href="#Lin15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#Lin15" href="#Lin15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
  <p style="margin:-8.5px 0px 0px 0px;"></p>  
    
    
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 4.6% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/cvpr15aptlayout_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:67%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Rent3D: Floor-Plan  Priors for Monocular Layout Estimation &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Chenxi Liu*, Alex Schwing*, Kaustav Kundu, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
                        <p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:11.5px">* <em>Denotes equal contribution</em></p>
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;"> Rent an apartment in 3D: 3D apartment reconstruction from a rental ad</p>-->

<p style="margin:-9.0px 0px 0px 0px;"></p>
            
            <a href="papers/rent3DCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ApartmentsCVPR15abs" href="#ApartmentsCVPR15abs-list">Abstract</a>&nbsp
<a href="papers/rent3d_suppl.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a href="http://www.cs.toronto.edu/~fidler/projects/rent3D.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ApartmentsCVPR15" href="#ApartmentsCVPR15-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ApartmentsCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ApartmentsCVPR15,<br/>
    title = {Rent3D: Floor-Plan  Priors for Monocular Layout Estimation},<br/>
    author = {Chenxi Liu and Alex Schwing and Kaustav Kundu and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}}
</p>      
</div>
<div id="ApartmentsCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The goal of this paper is to enable a 3D "virtual-tour" of an apartment given a small set of monocular images of different rooms, 
as well as a 2D floor plan. We frame the problem as the one of inference in a Markov random field which  reasons about the layout 
of each room and its relative pose (3D rotation and translation) within the full apartment. This gives us information, for example, 
about in which room the picture was taken.  
What sets us apart from past work in layout estimation is the use of floor plans as a source of prior knowledge.  
In particular, we exploit the floor plan to impose aspect ratio constraints across the layouts of different rooms, as well as to 
extract  semantic information, e.g., the location of windows which are labeled in floor plans.  We show that this information 
can significantly help in resolving the challenging  room-apartment alignment problem. 
We also derive an efficient exact inference algorithm which takes only a few ms per apartment. This is due to the fact that we 
exploit integral geometry as well as our new bounds on the aspect ratio of rooms which allow us to carve the space, reducing 
significantly the number of physically possible configurations. 
We demonstrate the effectiveness of our approach in a new dataset which contains over 200 apartments. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:11.6%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="http://techtalks.tv/talks/rent3d-floor-plan-priors-for-monocular-layout-estimation/61611/"><img width="100%" src="papers/figs/rent3d-talk1.jpg"></a><br/><font style="font-size:12.0px;">[<a href="http://techtalks.tv/talks/rent3d-floor-plan-priors-for-monocular-layout-estimation/61611/">talk</a>]&nbsp&nbsp[<a href="slides/rent3d_talk.pdf">slides</a>]</font></div>
</div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/cvpr15osm_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:67%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Holistic 3D Scene Understanding from a Single Geo-tagged Image &nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Shenlong Wang, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Exploiting map priors for segmentation and monocular depth estimation</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>

 <a href="papers/wangCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#WangCVPR15abs" href="#WangCVPR15abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~slwang/kitti3d/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#WangCVPR15" href="#WangCVPR15-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="WangCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{WangCVPR15,<br/>
    title = {Holistic 3D Scene Understanding from a Single Geo-tagged Image},<br/>
    author = {Shenlong Wang and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}}
</p>      
</div>
<div id="WangCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we are interested in exploiting geographic
priors to help outdoor scene understanding. Towards this
goal we propose a holistic approach that reasons jointly
about 3D object detection, pose estimation, semantic segmentation
as well as depth reconstruction from a single image.
Our approach takes advantage of large-scale crowdsourced
maps to generate dense geographic, geometric and
semantic priors by rendering the 3D world. We demonstrate
the effectiveness of our holistic model on the challenging
KITTI dataset, and show significant improvements
over the baselines in all metrics and tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:11.6%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="http://techtalks.tv/talks/holistic-3d-scene-understanding-from-a-single-geo-tagged-image/61614/"><img width="100%" src="papers/figs/holistic3d-talk.jpg"></a><br/><font style="font-size:12.0px;">[<a href="http://techtalks.tv/talks/holistic-3d-scene-understanding-from-a-single-geo-tagged-image/61614/">talk</a>]&nbsp&nbsp[<a href="http://www.cs.toronto.edu/~slwang/kitti3d/cvpr15.pptx">slides</a>]</font></div>
</div>
      </li>
    </ul>

<p style="margin:-9.0px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:4px 4.2% 0 0; width:14.8%;" href="http://arxiv.org/abs/1502.04275"><img width="100%" src="papers/figs/segdeepm.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</h4>

<p style="margin:-9.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px" style="padding:0px 0px 0px 0px;">Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, Sanja Fidler</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px" style="padding:0px 0px 0px 0px;">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;" style="padding:0px 0px 0px 0px;"> Currently third in detection on PASCAL VOC <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=6&compid=4">Leaderboard</a></p>-->
   
   <p style="margin:-9.0px 0px 0px 0px;"></p>         
            <a href="http://arxiv.org/abs/1502.04275" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ZhuSegDeepM15abs" href="#ZhuSegDeepM15abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~yukun/segdeepm.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ZhuSegDeepM15" href="#ZhuSegDeepM15-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ZhuSegDeepM15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ZhuSegDeepM15,<br/>
    title = {segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection},<br/>
    author = {Yukun Zhu and Raquel Urtasun and Ruslan Salakhutdinov and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>
}
</p>      
</div>
<div id="ZhuSegDeepM15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we propose an approach that exploits object segmentation in order to improve the accuracy of object detection. 
We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as 
contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large 
pool of accurate object segmentation proposals. This enables the detector to incorporate additional evidence when it is available 
and thus results in more accurate detections. Our experiments show an improvement of 4.1% in mAP over the R-CNN baseline on PASCAL 
VOC 2010, and 3.4% over the current state-of-the-art, demonstrating the power of our approach.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://arxiv.org/abs/1502.04275" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#ZhuSegDeepM15abs" href="#ZhuSegDeepM15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.cs.toronto.edu/~yukun/segdeepm.html" class="buttonP" style="width:100%; padding:0 0 0px 0;">Project page</a>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#ZhuSegDeepM15" href="#ZhuSegDeepM15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 2.6% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/cvpr15fashion_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:66%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Neuroaesthetics in Fashion: Modeling the Perception of Beauty</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;"> How fashionable do you look in a photo? And how can you improve?</p>
            
            <p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/fashionCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SimoCVPR15abs" href="#SimoCVPR15abs-list">Abstract</a>&nbsp
<a href="http://hi.cs.waseda.ac.jp/~esimo/en/research/fashionability/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SimoCVPR15" href="#SimoCVPR15-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="SimoCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SimoCVPR15,<br/>
    title = {Neuroaesthetics in Fashion: Modeling the Perception of Beauty},<br/>
    author = {Edgar Simo-Serra and Sanja Fidler and Francesc Moreno-Noguer and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="SimoCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we analyze the fashion of clothing of a large social website. Our goal is to learn and predict how fashionable 
a person looks on a photograph and suggest subtle improvements the user could make to improve her/his appeal. We propose a 
Conditional Random Field model that jointly reasons about several fashionability factors such as the type of outfit and garments
 the user is wearing, the type of the user, the photograph's setting (e.g., the scenery behind the user), and the fashionability score. 
Importantly, our model is able to give rich feedback back to the user, conveying which garments or even scenery she/he should change 
in order to improve fashionability. We demonstrate that our joint approach significantly outperforms a variety of intelligent baselines. 
We additionally collected a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information 
which can be exploited for our task. We also provide a detailed analysis of the data, showing different outfit trends and fashionability 
scores across the globe and across a span of 6 years.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:11.6%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media"><img width="100%" src="news/torontostar-fashionlife.jpg"></a><br/><font style="font-size:12.0px;">[<a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media">Press</a>]</font></div>
</div>
<p style="margin:1px 0px 0px 0px;"></p>
<div id="Fashion15media" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>News and Tech websites</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td><a href="http://www.newscientist.com/article/dn27729-style-software-gives-fashion-tips-after-judging-what-you-wear.html#.VYCIbEaZIq9"><img style="height:29px;" src="news/ns_logo.jpg"></a></td>
<td><a href="http://qz.com/429399/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/"><img style="height:24px;" src="news/quartz.jpg"></a></td>
<td><a href="http://www.techtimes.com/articles/62288/20150620/look-better-on-instagram-algorithm.htm"><img src="news/techtimes.png" style="height:29px;"></td>
<td><a href="http://www.wired.co.uk/news/archive/2015-06/23/fashion-solving-algorithm-judges-your-instagram-photos"><img src="news/wired.png" style="height:29px;"></a></td>
<td><a href="http://mashable.com/2015/06/20/algorithm-how-to-take-fashionable-instagrams/"><img src="news/mashable.jpg" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.newscientist.com/article/dn27729-style-software-gives-fashion-tips-after-judging-what-you-wear.html#.VYCIbEaZIq9">New Scientist</a></td>
<td> <a href="http://qz.com/429399/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/">Quartz</a></td>
<td><a href="http://www.techtimes.com/articles/62288/20150620/look-better-on-instagram-algorithm.htm">Tech Times</a></td>
<td><a href="http://www.wired.co.uk/news/archive/2015-06/23/fashion-solving-algorithm-judges-your-instagram-photos">Wired</a>, UK</td>
<td><a href="http://mashable.com/2015/06/20/algorithm-how-to-take-fashionable-instagrams/">Mashable</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://on.aol.com/video/new-algorithm-for-instagram-will-help-you-dress-your-best-518908473"><img src="news/aol.png" style="height:29px;"></a></td>
<td><a href="http://www.huffingtonpost.co.uk/2015/06/22/instagram-update-algorithm-fashion-outfit_n_7634344.html?utm_hp_ref=uk&ir=UK"><img src="news/huff.png" style="height:25px;"></a></td>
<td><a href="http://www.huffingtonpost.ca/2015/06/23/new-study-instagram-outfits_n_7648476.html"><img src="news/huffca.png" style="height:25px;"></a></td>
<td><a href="http://www.msn.com/en-ca/news/other/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/ar-AAbH1Mf"><img src="news/msn.png" style="height:29px;"></a></td>
<td><a href="https://www.prote.in/en/feed/2015/06/this-fashion-bot-knows-what-s-in-vogue"><img src="news/protein.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://on.aol.com/video/new-algorithm-for-instagram-will-help-you-dress-your-best-518908473">AOL News</a> (video)</td>
<td><a href="http://www.huffingtonpost.co.uk/2015/06/22/instagram-update-algorithm-fashion-outfit_n_7634344.html?utm_hp_ref=uk&ir=UK">Huffington Post</a>, UK (video)</td>
<td><a href="http://www.huffingtonpost.ca/2015/06/23/new-study-instagram-outfits_n_7648476.html">Huffington Post</a>, Canada</td>
<td><a href="http://www.msn.com/en-ca/news/other/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/ar-AAbH1Mf">MSN</a>, Canada</td>
<td><a href="https://www.prote.in/en/feed/2015/06/this-fashion-bot-knows-what-s-in-vogue">Protein</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="https://ca.news.yahoo.com/blogs/dailybrew/the-algorithm-that-judges-your-fashion-sense-will-soon-be-an-app-195234022.html"><img src="news/yahooca.jpg" style="height:29px;"></a></td>
<td><a href="http://www.sciencedaily.com/releases/2015/07/150714083033.htm"><img src="news/sciencedaily.png" style="height:25px;"></a></td>
<td><a href="http://www.dailymail.co.uk/sciencetech/article-3163124/Would-fashion-advice-robot-Researchers-develop-mathematical-model-help-dressed.html" style="height:29px;"><img src="news/dailymail.jpg" style="height:25px;"></a></td>
<td><a href="http://www.psfk.com/2015/07/fashion-advice-algorithm-computer-vision-foundation-university-of-toronto.html"><img src="news/psfk.jpg" style="height:29px;"></a></td>
<td><a href="http://www.thestar.com/life/fashion_style/2015/07/20/u-of-t-scientists-create-software-to-analyze-outfits.html"><img src="news/torontostar.jpg" style="height:29px;"></a></td>

</tr><tr><td><a href="https://ca.news.yahoo.com/blogs/dailybrew/the-algorithm-that-judges-your-fashion-sense-will-soon-be-an-app-195234022.html">Yahoo</a>, Canada</td>
<td><a href="http://www.sciencedaily.com/releases/2015/07/150714083033.htm">Science Daily</a></td>
<td><a href="http://www.dailymail.co.uk/sciencetech/article-3163124/Would-fashion-advice-robot-Researchers-develop-mathematical-model-help-dressed.html">Daily Mail</a>,  UK</td>
<td><a href="http://www.psfk.com/2015/07/fashion-advice-algorithm-computer-vision-foundation-university-of-toronto.html">PSFK</a></td>
<td><a href="http://www.thestar.com/life/fashion_style/2015/07/20/u-of-t-scientists-create-software-to-analyze-outfits.html">Toronto Star</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.gizmag.com/computer-algorithm-fashion-advice/38470/"><img src="news/gizmag.jpg" style="height:29px;"></a></td>
<td><a href="http://www.therecord.com/living-story/5742841-can-a-software-program-make-you-stylish-/"><img src="news/therecord.png" style="height:29px;"></a></td>
<td><a href="http://www.idigitaltimes.com/researchers-create-robot-can-give-fashion-advice-458744" style="height:29px;"><img src="news/idigital.jpg" style="height:29px;"></a></td>
<td></td>
<td></td>

</tr><tr><td><a href="http://www.gizmag.com/computer-algorithm-fashion-advice/38470/">Gizmag</a></td>
<td><a href="http://www.therecord.com/living-story/5742841-can-a-software-program-make-you-stylish-/">TheRecord.com</a></td>
<td><a href="http://www.idigitaltimes.com/researchers-create-robot-can-give-fashion-advice-458744">iDigitalTimes</a></td>
<td></td>
<td></td>
</tr></tbody></table>
</div>
<hr>
</br>
<p style="margin:-20px 0px 0px 0px;"></p>

      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>Fashion magazines (online)</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td>
<a href="http://www.harpersbazaar.com/fashion/trends/a11271/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/bazaar.png" style="height:29px;"></a></td>
<td><a href="http://www.glamour.com/fashion/blogs/dressed/2015/06/outfit-making-computer-algorithm"><img src="news/glamour.png" style="height:29px;"></a></td>
<td><a href="http://www.elle.com/fashion/personal-style/a28974/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/elle.jpg" style="height:29px;"></a></td>
<td><a href="http://www.cosmopolitan.co.uk/fashion/style/news/a36721/algorithm-solves-fashion-problems/"><img src="news/cosmopolitan.jpg" style="height:29px;"></a></td>
<td><a href="http://www.marieclaire.com/fashion/street-style/a14806/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/marieclaire.jpg" style="height:26px;"></a></td>
</tr><tr><td><a href="http://www.harpersbazaar.com/fashion/trends/a11271/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Harper's Bazaar</a></td>
<td><a href="http://www.glamour.com/fashion/blogs/dressed/2015/06/outfit-making-computer-algorithm">Glamour</a></td>
<td><a href="http://www.elle.com/fashion/personal-style/a28974/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Elle</a></td>
<td><a href="http://www.cosmopolitan.co.uk/fashion/style/news/a36721/algorithm-solves-fashion-problems/">Cosmopolitan</a>, UK</td>
<td><a href="http://www.marieclaire.com/fashion/street-style/a14806/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Marie Claire</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.fashionmagazine.com/fashion/2015/06/24/best-instagram-items/"><img src="news/fashion.png" style="height:29px;"></a></td></td>
<td><a href="https://www.yahoo.com/style/thanks-to-science-ootd-posts-will-never-be-the-122177373293.html"><img src="news/yahoostyle.png" style="height:30px;"></a></td>
<td><a href="http://www.redonline.co.uk/fashion/fashion_news/instagram-algorithm-to-dress-better11"><img src="news/red.png" style="height:29px;"></a></td>
<td><a href="https://www.the-pool.com/news-views/fashion-news/2015/24/can-the-internet-help-you-get-dressed-"><img src="news/pool.png" style="height:26px;"></a></td>
<td><a href="http://www.fashionotes.com/content/2015/06/instagram-wants-to-help-you-dress-better/"><img src="news/fn.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.fashionmagazine.com/fashion/2015/06/24/best-instagram-items/">Fashion Magazine</a></td>
<td><a href="https://www.yahoo.com/style/thanks-to-science-ootd-posts-will-never-be-the-122177373293.html">Yahoo style</a></td>
<td><a href="http://www.redonline.co.uk/fashion/fashion_news/instagram-algorithm-to-dress-better11">Red Magazine</a>, UK</td>
<td><a href="https://www.the-pool.com/news-views/fashion-news/2015/24/can-the-internet-help-you-get-dressed-">The Pool</a>, UK</td>
<td><a href="http://www.fashionotes.com/content/2015/06/instagram-wants-to-help-you-dress-better/">FashionNotes</a></td>
</tr></tbody></table>
</div>
<hr>
</br>
<p style="margin:-20px 0px 0px 0px;"></p>

      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>International news</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td>
<a href="http://www.vogue.es/moda/news/articulos/programa-ordenador-calcula-nivel-estilo/22801"><img src="news/voguespain.png" style="height:29px;"></a></td>
<td><a href="http://www.stylebook.de/fashion/Outfit-Selfies-auf-Instagram-neuer-Algorithmus-zieht-uns-magisch-an-659850.html"><img src="news/stylebook.png" style="height:25px;"></a></td>
<td><a href="http://www.ansa.it/sito/notizie/tecnologia/software_app/2015/06/23/algoritmo-suggerira-outfit-piu-di-moda_7a973ee6-81c3-44e3-9c7a-55338107c627.html"><img src="news/ansa.png" style="height:29px;"></a></td>
<td><a href="http://www.cenariomt.com.br/noticia/455713/as-melhores-roupas-e-acessorios-para-ficar-incrivel-no-instagram-segundo-a-ciencia.html"><img src="news/cenario.png" style="height:29px;"></a></td>
<td><a href="http://amsterdam-ftv-blog.com/archives/37103"><img src="news/ams1.png" style="height:31px;"></a></td>
</tr><tr><td><a href="http://www.vogue.es/moda/news/articulos/programa-ordenador-calcula-nivel-estilo/22801">Vogue</a> (Spain)</td>
<td><a href="http://www.stylebook.de/fashion/Outfit-Selfies-auf-Instagram-neuer-Algorithmus-zieht-uns-magisch-an-659850.html">Stylebook</a> (Germany)</td>
<td><a href="http://www.ansa.it/sito/notizie/tecnologia/software_app/2015/06/23/algoritmo-suggerira-outfit-piu-di-moda_7a973ee6-81c3-44e3-9c7a-55338107c627.html">Ansa</a> (Italy)</td>
<td><a href="http://www.cenariomt.com.br/noticia/455713/as-melhores-roupas-e-acessorios-para-ficar-incrivel-no-instagram-segundo-a-ciencia.html">CenarioMT</a>  (Brazil)</td>
<td><a href="http://amsterdam-ftv-blog.com/archives/37103">Amsterdam Fashion</a> (NL)</td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.marieclaire.fr/,on-a-trouve-le-secret-pour-etre-mieux-habillees-que-vos-amies,738591.asp"><img src="news/marieclaire.jpg" style="height:29px;"></a></td></td>
<td><a href="http://fashionpoliceng.com/there-is-a-new-algorithm-for-instagram-to-help-you-imrove-your-dress-sense/"><img src="news/fpn.png" style="height:29px;"></a></td>
<td><a href="http://naukawpolsce.pap.pl/aktualnosci/news,405480,komputer-zamiast-stylisty.html"><img src="news/nauka.png" style="height:29px;"></a></td>
<td><a href="http://www.pluska.sk/spravy/zo-zahranicia/instagram-vie-co-si-mate-obliect-moda-podla-matematiky.html"><img src="news/pluska.png" style="height:29px;"></a></td></td>
<td><a href="http://www.pressetext.com/news/20150701001"><img src="news/pressetext.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.marieclaire.fr/,on-a-trouve-le-secret-pour-etre-mieux-habillees-que-vos-amies,738591.asp">Marie Claire</a> (France)</td>
<td><a href="http://fashionpoliceng.com/there-is-a-new-algorithm-for-instagram-to-help-you-imrove-your-dress-sense/">Fashion Police</a> (Nigeria)</td>
<td><a href="http://naukawpolsce.pap.pl/aktualnosci/news,405480,komputer-zamiast-stylisty.html">Nauka</a> (Poland)</td>
<td><a href="http://www.pluska.sk/spravy/zo-zahranicia/instagram-vie-co-si-mate-obliect-moda-podla-matematiky.html">Pluska</a> (Slovakia)</td>
<td><a href="http://www.pressetext.com/news/20150701001">Pressetext</a> (Austria)</td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="https://www.wired.de/collection/latest/diese-software-kennst-sich-aus-sachen-fashion"><img src="news/wired_de.png" style="height:29px;"></a></td>
<td><a href="http://jetzt.sueddeutsche.de/texte/anzeigen/593320/Was-zieh-ich-an"><img src="news/jetzt.png" style="height:29px;"></a></td>
<td><a href="http://www.gazzetta.it/Sportlife/Moda/01-07-2015/dammi-foto-ti-diro-come-vestirti-l-algoritmo-look-che-piace-120388048293.shtml?refresh_ce-cp"><img src="news/gazzetta.png" style="height:29px;"></a></td>
<td><a href="http://www.popsugar.com.au/fashion/Best-Outfits-Wear-Instagram-37781122"><img src="news/popsugar.png" style="height:23px;"></a></td>
<td><a href="http://www.sinembargo.mx/19-06-2015/1384780"><img src="news/sinembargo.png" style="height:26px;"></a></td>
</tr><tr>
<td><a href="https://www.wired.de/collection/latest/diese-software-kennst-sich-aus-sachen-fashion">Wired</a> (Germany)</td>
<td><a href="http://jetzt.sueddeutsche.de/texte/anzeigen/593320/Was-zieh-ich-an">Jetzt</a> (Germany)</td>
<td><a href="http://www.gazzetta.it/Sportlife/Moda/01-07-2015/dammi-foto-ti-diro-come-vestirti-l-algoritmo-look-che-piace-120388048293.shtml?refresh_ce-cp">La Gazzetta</a> (Italy)</td>
<td><a href="http://www.popsugar.com.au/fashion/Best-Outfits-Wear-Instagram-37781122">PopSugar</a> (Australia)</td>
<td><a href="http://www.sinembargo.mx/19-06-2015/1384780">SinEmbargo</a> (Mexico)</td>
</tr><tr>
</tr></tbody></table>
</div>
<hr>
<p style="margin:25px 0px 0px 0px;"></p>
<p><b>A more complete list is maintained on our <a href="http://www.iri.upc.edu/people/esimo/en/research/fashionability/">project webpage</a>.</b></p>
<a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media">close window</a>
</div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 2.5% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/cvpr15suppixels_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Real-Time Coarse-to-fine Topologically Preserving Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jian Yao, Marko Boben, Sanja Fidler, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Exploiting map priors for segmentation and monocular depth estimation</p>-->

<p style="margin:-8px 0px 0px 0px;">
 <a href="papers/yaoCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#YaoCVPR15abs" href="#YaoCVPR15abs-list">Abstract</a>&nbsp
<a href="https://bitbucket.org/mboben/spixel" target="_blank" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#YaoCVPR15" href="#YaoCVPR15-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="YaoCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{YaoCVPR15,<br/>
    title = {Real-Time Coarse-to-fine Topologically Preserving Segmentation},<br/>
    author = {Jian Yao and Marko Boben and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="YaoCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we tackle the problem of unsupervised segmentation
in the form of superpixels. Our main emphasis is
on speed and accuracy. We build on [Yamaguchi et al., ECCV'14] to define the problem
as a boundary and topology preserving Markov random
field. We propose a coarse to fine optimization technique
that speeds up inference in terms of the number of updates
by an order of magnitude. Our approach is shown to outperform
[Yamaguchi et al., ECCV'14] while employing a single iteration. We evaluate
and compare our approach to state-of-the-art superpixel algorithms
on the BSD and KITTI benchmarks. Our approach
significantly outperforms the baselines in the segmentation
metrics and achieves the lowest error on the stereo task.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/yaoCVPR15.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#YaoCVPR15abs" href="#YaoCVPR15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#YaoCVPR15" href="#YaoCVPR15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
      
 
<p style="margin:-8.5px 0px 0px 0px;"></p>
    
       <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 4.8% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/lee_symmetry15.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Framework for Symmetric Part Detection in Cluttered Scenes</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tom Lee, Sanja Fidler, Alex Levinshtein, Cristian Sminchisescu, Sven Dickinson</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Symmetry</em>, Vol. 7, 2015, pp 1333-1351</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>

    <a href="papers/Symmetry2015.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#LeeSymmetry15abs" href="#LeeSymmetry15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeSymmetry15" href="#LeeSymmetry15-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="LeeSymmetry15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@article{LeeSymmetry2015,<br/>
    title = {A Framework for Symmetric Part Detection in Cluttered Scenes},<br/>
    author = {Tom Lee and Sanja Fidler and Alex Levinshtein and Cristian Sminchisescu and Sven Dickinson},<br/>
    journal = {Symmetry},<br/>
    volume = {7},<br/>
    pages = {1333-1351},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="LeeSymmetry15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The role of symmetry in computer vision has waxed and waned in importance
during the evolution of the field from its earliest days. At first figuring prominently in support
of bottom-up indexing, it fell out of favour as shape gave way to appearance and recognition
gave way to detection. With a strong prior in the form of a target object, the role of the
weaker priors offered by perceptual grouping was greatly diminished. However, as the field
returns to the problem of recognition from a large database, the bottom-up recovery of the
parts that make up the objects in a cluttered scene is critical for their recognition. The medial
axis community has long exploited the ubiquitous regularity of symmetry as a basis for the
decomposition of a closed contour into medial parts. However, today's recognition systems
are faced with cluttered scenes and the assumption that a closed contour exists, i.e., that
figure-ground segmentation has been solved, rendering much of the medial axis community's
work inapplicable. In this article, we review a computational framework, previously reported
in [Lee et al., ICCV'13, Levinshtein et al., ICCV'09, Levinshtein et al., IJCV'13], that bridges the representation power of the medial axis and the need to recover and
group an object's parts in a cluttered scene. Our framework is rooted in the idea that a
maximally-inscribed disc, the building block of a medial axis, can be modelled as a compact
superpixel in the image. We evaluate the method on images of cluttered scenes.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>
<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>
<p style="margin:65px 0px 0px 0px;"></p>


<h2>Year 2014</h2>
<br/>
<p style="margin:-12.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 3.7% 0 0; width:14.8%;" href="papers/simo_et_al_accv14.pdf"><img width="100%" src="papers/figs/accv14clothing.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A High Performance CRF Model for Clothes Parsing</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, Raquel Urtasun</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Asian Conference on Computer Vision</em> (<strong>ACCV</strong>), Singapore, November, 2014</p>
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Significant performance gain over state-of-the-art. Code, features & models in Project page!</p>
            -->
<p style="margin:-9.0px 0px 0px 0px;"></p>

<a href="papers/simo_et_al_accv14.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#EdgarACCV14abs" href="#EdgarACCV14abs-list">Abstract</a>&nbsp
<a href="http://www.iri.upc.edu/people/esimo/research/fashion/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#EdgarACCV14" href="#EdgarACCV14-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="EdgarACCV14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
<p style="line-height:8px"> &nbsp</p>
            <p style="font-size:15px;">@inproceedings{SimoACCV14,<br/>
    title = {A High Performance CRF Model for Clothes Parsing},<br/>
    author = {Edgar Simo-Serra and Sanja Fidler and Francesc Moreno-Noguer and Raquel Urtasun},<br/>
    booktitle = {ACCV},<br/>
    year = {2014}}
</p>       
</div>
<div id="EdgarACCV14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we tackle the problem of clothing parsing: Our goal is to
segment and classify different garments a person is wearing. We frame the problem
as the one of inference in a pose-aware Conditional Random Field (CRF)
which exploits appearance, figure/ground segmentation, shape and location priors
for each garment as well as similarities between segments, and symmetries
between different human body parts. We demonstrate the effectiveness of our approach
on the Fashionista dataset [Yamaguchi et al., CVPR'12] and show that we can obtain a significant
improvement over the state-of-the-art.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/simo_et_al_accv14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.iri.upc.edu/people/esimo/research/fashion/" class="buttonP" style="width:100%; padding:0 0 0px 0;">Project page</a>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#EdgarACCV14" href="#EdgarACCV14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>


<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 4.2% 0 0; width:14.8%;" href="papers/lee_et_al_accv14.pdf"><img width="100%" src="papers/figs/accv14multicue.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Multi-cue Mid-level Grouping</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tom Lee, Sanja Fidler, Sven Dickinson</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Asian Conference on Computer Vision</em> (<strong>ACCV</strong>), Singapore, November, 2014</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            
 <a href="papers/lee_et_al_accv14.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LeeACCV14abs" href="#LeeACCV14abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeACCV14" href="#LeeACCV14-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="LeeACCV14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
<p style="line-height:8px"> &nbsp</p>
            <p style="font-size:15px;">@inproceedings{LeeACCV14,<br/>
    title = {Multi-cue mid-level grouping},<br/>
    author = {Tom Lee and Sanja Fidler and Sven Dickinson},<br/>
    booktitle = {ACCV},<br/>
    year = {2014}<br/>}
</p>    
</div>
<div id="LeeACCV14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Region proposal methods provide richer object hypotheses
than sliding windows with dramatically fewer proposals, yet they still
number in the thousands. This large quantity of proposals typically results
from a diversification step that propagates bottom-up ambiguity in
the form of proposals to the next processing stage. In this paper, we take
a complementary approach in which mid-level knowledge is used to resolve
bottom-up ambiguity at an earlier stage to allow a further reduction
in the number of proposals. We present a method for generating regions
using the mid-level grouping cues of closure and symmetry. In doing so,
we combine mid-level cues that are typically used only in isolation, and
leverage them to produce fewer but higher quality proposals. We emphasize
that our model is mid-level by learning it on a limited number of
objects while applying it to different objects, thus demonstrating that
it is transferable to other objects. In our quantitative evaluation, we 1)
establish the usefulness of each grouping cue by demonstrating incremental
improvement, and 2) demonstrate improvement on two leading
region proposal methods with a limited budget of proposals.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>

<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/lee_et_al_accv14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LeeACCV14" href="#LeeACCV14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.8%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 4.8% 0 0; width:14.3%;" href="http://arxiv.org/pdf/1408.5516v1.pdf"><img width="100%" src="papers/figs/hierarchy14a.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Marko Boben, Ales Leonardis</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>arXiv preprint arXiv:1408.5516</em>, 2014</p>

<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">Journal version of my PhD work on learning compositional hierarchies encoding spatial relations</p>
<p style="margin:-9.0px 0px 0px 0px;"></p> 

<a href="http://arxiv.org/pdf/1408.5516v1.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#FidlerArxiv14abs" href="#FidlerArxiv14abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerArxiv14" href="#FidlerArxiv14-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerArxiv14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{FidlerArxiv14,<br/>
    title = {Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation},<br/>
    author = {Sanja Fidler and Marko Boben and Ale\v{s} Leonardis},<br/>
    booktitle = {ArXiv:1408.5516},<br/>
    year = {2014}<br/>}
</p>       
</div>
<div id="FidlerArxiv14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Hierarchies allow feature sharing between objects at multiple levels of representation, can code exponential variability in
a very compact way and enable fast inference. This makes them potentially suitable for learning and recognizing a higher number of
object classes. However, the success of the hierarchical approaches so far has been hindered by the use of hand-crafted features or
predetermined grouping rules. This paper presents a novel framework for learning a hierarchical compositional shape vocabulary for
representing multiple object classes. The approach takes simple contour fragments and learns their frequent spatial configurations.
These are recursively combined into increasingly more complex and class-specific shape compositions, each exerting a high degree of
shape variability. At the top-level of the vocabulary, the compositions are sufficiently large and complex to represent the whole shapes
of the objects. We learn the vocabulary layer after layer, by gradually increasing the size of the window of analysis and reducing the
spatial resolution at which the shape configurations are learned. The lower layers are learned jointly on images of all classes, whereas
the higher layers of the vocabulary are learned incrementally, by presenting the algorithm with one object class after another. The
experimental results show that the learned multi-class object representation scales favorably with the number of object classes and
achieves a state-of-the-art detection performance at both, faster inference as well as shorter training times.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://arxiv.org/pdf/1408.5516v1.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerArxiv14" href="#FidlerArxiv14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:4px 3.5% 0 0; width:14.8%;" href="papers/kong_et_al_cvpr14.pdf"><img src="papers/figs/cvpr14coref.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">What are you talking about? Text-to-Image Coreference</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, Sanja Fidler</p>

	     <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), ColumbUSA, USA, June, 2014</p>
<!--<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Exploits text for visual parsing and aligns nouns to objects. Code and data out soon!</p>
            -->
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/kong_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#KongCVPR14abs" href="#KongCVPR14abs-list">Abstract</a>&nbsp
             <a href="projects/sentences3D.html" target="_blank" class="buttonPP">Project page</a>&nbsp        
<a class="buttonSS" data-toggle="collapse" data-parent="#KongCVPR14" href="#KongCVPR14-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="KongCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{KongCVPR14,<br/>
    title = {What are you talking about? Text-to-Image Coreference},<br/>
    author = {Chen Kong and Dahua Lin and Mohit Bansal and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}
</p>  
</div>
<div id="KongCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we exploit natural sentential descriptions
of RGB-D scenes in order to improve 3D semantic parsing.
Importantly, in doing so, we reason about which particular
object each noun/pronoun is referring to in the image. This
allows us to utilize visual information in order to disambiguate
the so-called coreference resolution problem that
arises in text. Towards this goal, we propose a structure
prediction model that exploits potentials computed from text
and RGB-D imagery to reason about the class of the 3D objects,
the scene type, as well as to align the nouns/pronouns
with the referred visual objects. We demonstrate the effectiveness
of our approach on the challenging NYU-RGBD v2
dataset, which we enrich with natural lingual descriptions.
We show that our approach significantly improves 3D detection
and scene classification accuracy, and is able to reliably
estimate the text-to-image alignment. Furthermore,
by using textual and visual information, we are also able to
successfully deal with coreference in text, improving upon
the state-of-the-art Stanford coreference system.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/kong_et_al_cvpr14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#KongCVPR14" href="#KongCVPR14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 2% 0 0; width:14.8%;" href="papers/lin_et_al_cvpr14.pdf"><img width="100%" src="papers/figs/cvpr14vsearch.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Visual Semantic Search: Retrieving Videos via Complex Textual Queries</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Dahua Lin, Sanja Fidler, Chen Kong, Raquel Urtasun</p>

	    <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), ColumbUSA, USA, June, 2014</p>
 <!--
<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">Video retrieval when a query is a longer sentence or a multi-sentence description</p>
            -->
<p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/lin_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#LinCVPR14abs" href="#LinCVPR14abs-list">Abstract</a>&nbsp
<a href="papers/cvpr14_retrieval_supplemental.pdf" class="buttonMM" >Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LinCVPR14" href="#LinCVPR14-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="LinCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{LinCVPR14,<br/>
    author = {Dahua Lin and  Sanja Fidler and Chen Kong and Raquel Urtasun},<br/>
    title = {Visual Semantic Search: Retrieving Videos via Complex Textual Queries},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="LinCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we tackle the problem of retrieving videos
using complex natural language queries. Towards this goal,
we first parse the sentential descriptions into a semantic
graph, which is then matched to visual concepts using a
generalized bipartite matching algorithm. Our approach
exploits object appearance, motion and spatial relations,
and learns the importance of each term using structure prediction.
We demonstrate the effectiveness of our approach
on a new dataset designed for semantic search in the context
of autonomous driving, which exhibits complex and highly
dynamic scenes with many objects. We show that our approach
is able to locate a major portion of the objects described
in the query with high accuracy, and improve the
relevance in video retrieval.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/lin_et_al_cvpr14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="papers/cvpr14_retrieval_supplemental.pdf" class="buttonM" style="width:100%; font-size:11.5px;">Suppl. Mat.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LinCVPR14" href="#LinCVPR14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 1.8% 0 0; width:14.8%;" href="papers/chen_et_al_cvpr14b.pdf"><img width="100%" src="papers/figs/cvpr14segkitti.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Beat the MTurkers: Automatic Image Labeling from Weak 3D Supervision</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Liang-Chieh Chen, Sanja Fidler, Alan Yuille, Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), ColumbUSA, USA, June, 2014</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;">Ground-truth segmentations provided for a subset of KITTI cars in Project page</p>
<p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/chen_et_al_cvpr14b.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ChenCVPR14abs" href="#ChenCVPR14abs-list">Abstract</a>&nbsp
<a href="http://liangchiehchen.com/projects/beat_the_MTurkers.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a href="projects/CAD.html" class="buttonPP">CAD models</a>&nbsp
<a href="papers/cvpr14_seg_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ChenCVPR14" href="#ChenCVPR14-list">Bibtex</a><br/>

 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="ChenCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{ChenCVPR14,<br/>
    author = {Liang-Chieh Chen and  Sanja Fidler and Alan Yuille and Raquel Urtasun},<br/>
    title = {Beat the MTurkers: Automatic Image Labeling from Weak 3D Supervision},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="ChenCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Labeling large-scale datasets with very accurate object
segmentations is an elaborate task that requires a high degree
of quality control and a budget of tens or hundreds of
thousands of dollars. ThUSA, developing solutions that can
automatically perform the labeling given only weak supervision
is key to reduce this cost. In this paper, we show how
to exploit 3D information to automatically generate very accurate
object segmentations given annotated 3D bounding
boxes. We formulate the problem as the one of inference in
a binary Markov random field which exploits appearance
models, stereo and/or noisy point clouds, a repository of 3D
CAD models as well as topological constraints. We demonstrate
the effectiveness of our approach in the context of autonomous
driving, and show that we can segment cars with
the accuracy of 86% intersection-over-union, performing as
well as highly recommended MTurkers!
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 3.2% 0 0; width:14.8%;" href="papers/Mottaghi14cvpr.pdf"><img width="100%" src="papers/figs/cvpr14contexta.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">The Role of Context for Object Detection and Semantic Segmentation in the Wild</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Roozbeh Mottaghi,  Xianjie Chen, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), ColumbUSA, USA, June, 2014</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;"> PASCAL VOC with dense segmentation labels for 400+ classes in Project page</p>
<p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/Mottaghi14cvpr.pdf" class="buttonTT">Paper</a>&nbsp
<a href="papers/errataMottaghi2014.pdf" class="buttonTT">Errata</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#MottaghiCVPR14abs" href="#MottaghiCVPR14abs-list">Abstract</a>&nbsp
<a href="http://www.cs.stanford.edu/~roozbeh/pascal-context/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a href="papers/cvpr14_context_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MottaghiCVPR14" href="#MottaghiCVPR14-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="MottaghiCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{MottaghiCVPR14,<br/>
    author = {Roozbeh Mottaghi and  Xianjie Chen and Xiaobai Liu and Sanja Fidler and Raquel Urtasun and Alan Yuille},<br/>
    title = {The Role of Context for Object Detection and Semantic Segmentation in the Wild},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="MottaghiCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we study the role of context in existing state-of-the-art
detection and segmentation approaches. Towards
this goal, we label every pixel of PASCAL VOC 2010 detection
challenge with a semantic category. We believe this
data will provide plenty of challenges to the community, as
it contains 520 additional classes for semantic segmentation
and object detection. Our analysis shows that nearest
neighbor based approaches perform poorly on semantic
segmentation of contextual classes, showing the variability
of PASCAL imagery. Furthermore, improvements of existing
contextual models for detection is rather modest. In
order to push forward the performance in this difficult scenario,
we propose a novel deformable part-based model,
which exploits both local context around each candidate detection
as well as global context at the level of the scene.
We show that this contextual reasoning significantly helps
in detecting objects at all scales.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.1% 1% 0% 0%; width:11%;">
<a href="papers/Mottaghi14cvpr.pdf" class="buttonT" style="width:100%; height:15.0px; font-size:11.0px;">Paper</a><br/>
<p style="margin:-1.9px 0px 0px 0px;"></p>
<a href="papers/errataMottaghi2014.pdf" class="buttonT" style="width:100%; height:15.0px; font-size:11px;">Errata</a><br/>
<p style="margin:-1.9px 0px 0px 0px;"></p>
<a href="http://www.cs.stanford.edu/~roozbeh/pascal-context/" class="buttonP" style="width:100%; height:15.0px; font-size:11.0px;">Project page</a><br/>
<p style="margin:-1.4px 0px 0px 0px;"></p>
<a href="papers/cvpr14_context_supplemental.pdf" class="buttonM" style="width:100%; height:15.0px; font-size:11.0px;">Suppl. Mat.</a><br/>
<p style="margin:-2.7px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#MottaghiCVPR14" href="#MottaghiCVPR14-list" style="width:100%; height:15.0px; font-size:11px; margin-top:4px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 3.4% 0 0; width:14.8%;" href="papers/chen_et_al_cvpr14.pdf"><img width="100%" src="papers/figs/cvpr14parts.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Nam-Gyu Cho, Sanja Fidler, Raquel Urtasun, Alan Yuille</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), ColumbUSA, USA, June, 2014</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px; color:#E62E00;"> PASCAL VOC with object parts segmentations available in Project page</p>
<p style="margin:-9.0px 0px 0px 0px;"></p>
 <a href="papers/chen_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#PartsCVPR14abs" href="#PartsCVPR14abs-list">Abstract</a>&nbsp
<a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#PartsCVPR14" href="#PartsCVPR14-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="PartsCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{PartsCVPR14,<br/>
    author = {Xianjie Chen and Roozbeh Mottaghi and  Xiaobai Liu and Nam-Gyu Cho and Sanja Fidler and Raquel Urtasun and Alan Yuille},<br/>
    title = {Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="PartsCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Detecting objects becomes difficult when we need to deal
with large shape deformation, occlusion and low resolution.
We propose a novel approach to i) handle large deformations
and partial occlusions in animals (as examples
of highly deformable objects), ii) describe them in terms of
body parts, and iii) detect them when their body parts are
hard to detect (e.g., animals depicted at low resolution). We
represent the holistic object and body parts separately and
use a fully connected model to arrange templates for the
holistic object and body parts. Our model automatically
decouples the holistic object or body parts from the model
when they are hard to detect. This enables us to represent a
large number of holistic object and body part combinations
to better deal with different "detectability" patterns caused
by deformations, occlusion and/or low resolution.

We apply our method to the six animal categories in the
PASCAL VOC dataset and show that our method significantly improves state-of-the-art (by 4.1% AP) and provides
a richer representation for objects. During training we use
annotations for body parts (e.g., head, torso, etc), making
use of a new dataset of fully annotated object parts for PASCAL
VOC 2010, which provides a mask for each part.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/chen_et_al_cvpr14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#PartsCVPR14" href="#PartsCVPR14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    <p style="margin:-8px 0px 0px 0px;"></p>
<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>

<p style="margin:65px 0px 0px 0px;"></p>


<h2>Year 2013</h2>
<br/>
<p style="margin:-12.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 1.7% 0 0; width:14.8%;" href="papers/lin_et_al_iccv13.pdf"><img width="100%" src="papers/figs/iccv13cuboids.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Holistic Scene Understanding for 3D Object Detection with RGBD cameras<em>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<b>oral presentation</b>)</em></h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Dahua Lin, Sanja Fidler, Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">Code, models and ground-truth cuboids for NYU-v2 in Project page</p>
<p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/lin_et_al_iccv13.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LinICCV13abs" href="#LinICCV13abs-list">Abstract</a>&nbsp
<a href="projects/scenes3D.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a href="data/scenes3d/iccv2013_talk.pdf" class="buttonPP">Talk slides</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LinICCV13" href="#LinICCV13-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="LinICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{LinICCV13,<br/>
    author = {Dahua Lin and Sanja Fidler and Raquel Urtasun},<br/>
    title = {Holistic Scene Understanding for 3D Object Detection with RGBD cameras},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="LinICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper, we tackle the problem of indoor scene understanding
using RGBD data. Towards this goal, we propose
a holistic approach that exploits 2D segmentation, 3D
geometry, as well as contextual relations between scenes
and objects. Specifically, we extend the CPMC framework
to 3D in order to generate candidate cuboids, and
develop a conditional random field to integrate information
from different sources to classify the cuboids. With this
formulation, scene classification and 3D object recognition
are coupled and can be jointly solved through probabilistic
inference. We test the effectiveness of our approach on
the challenging NYU v2 dataset. The experimental results
demonstrate that through effective evidence integration and
holistic reasoning, our approach achieves substantial improvement
over the state-of-the-art.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/lin_et_al_iccv13.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="projects/scenes3D.html" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="data/scenes3d/iccv2013_talk.pdf" class="buttonP" style="width:100%;">Talk slides</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LinICCV13" href="#LinICCV13-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>	


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 3% 0 0; width:14.8%;" href="papers/Schwing_etal2013.pdf"><img width="100%" src="papers/figs/iccv13box.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Box In the Box: Joint 3D Layout and Object Reasoning from Single Images</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Alex Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">Parallel and improved implementation of Structured SVMs released</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/Schwing_etal2013.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#SchwingICCV13abs" href="#SchwingICCV13abs-list">Abstract</a>&nbsp
<a href="http://www.alexander-schwing.de/projectsGeneralStructuredPredictionLatentVariables.php" class="buttonPP">Learning code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SchwingICCV13" href="#SchwingICCV13-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="SchwingICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{SchwingICCV13,<br/>
    author = {Alex Schwing and Sanja Fidler and Marc Pollefeys and Raquel Urtasun},<br/>
    title = {Box In the Box: Joint 3D Layout and Object Reasoning from Single Images},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="SchwingICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we propose an approach to jointly infer the
room layout as well as the objects present in the scene. Towards
this goal, we propose a branch and bound algorithm
which is guaranteed to retrieve the global optimum of the
joint problem. The main difficulty resides in taking into
account occlusion in order to not over-count the evidence.
We introduce a new decomposition method, which generalizes
integral geometry to triangular shapes, and allows us
to bound the different terms in constant time. We exploit
both geometric cues and object detectors as image features
and show large improvements in 2D and 3D object detection
over state-of-the-art deformable part-based models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/Schwing_etal2013.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.alexander-schwing.de/projectsGeneralStructuredPredictionLatentVariables.php" class="buttonP" style="width:100%;">Learning code</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#SchwingICCV13" href="#SchwingICCV13-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 3.5% 0 0; width:14.8%;" href="papers/lee_et_al_iccv13.pdf"><img width="100%" src="papers/figs/iccv13symmetrya.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Detecting Curved Symmetric Parts using a Deformable Disc Model</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tom Lee, Sanja Fidler, Sven Dickinson</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
 <p style="margin:-9.0px 0px 0px 0px;"></p>
 <a href="papers/lee_et_al_iccv13.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LeeICCV13abs" href="#LeeICCV13abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~tshlee/SymmetricParts/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeICCV13" href="#LeeICCV13-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="LeeICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{LeeICCV13,<br/>
    author = {Tom Lee and Sanja Fidler and Sven Dickinson},<br/>
    title = {Detecting Curved Symmetric Parts using a Deformable Disc Model},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="LeeICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Symmetry is a powerful shape regularity that's been exploited
by perceptual grouping researchers in both human
and computer vision to recover part structure from an image
without a priori knowledge of scene content. Drawing
on the concept of a medial axis, defined as the locus of
centers of maximal inscribed discs that sweep out a symmetric
part, we model part recovery as the search for a
sequence of deformable maximal inscribed disc hypotheses
generated from a multiscale superpixel segmentation,
a framework proposed by [Levinshtein et al., ICCV'09]. However, we learn affinities
between adjacent superpixels in a space that's invariant to
bending and tapering along the symmetry axis, enabling us
to capture a wider class of symmetric parts. Moreover, we
introduce a global cost that perceptually integrates the hypothesis
space by combining a pairwise and a higher-level
smoothing term, which we minimize globally using dynamic
programming. The new framework is demonstrated on two
datasets, and is shown to significantly outperform the baseline
[Levinshtein et al., ICCV'09].
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/lee_et_al_iccv13.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.3px 0px 0px 0px;"></p>
<a href="http://www.cs.toronto.edu/~tshlee/SymmetricParts/" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LeeICCV13" href="#LeeICCV13-list" style="width:100%; margin-top:5px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>	

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:2px 1.8% 0 0; width:14.8%;" href="papers/fidler_et_al_cvpr13a.pdf"><img width="100%" src="papers/figs/cvpr13segdpma.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Bottom-up Segmentation for Top-down Detection</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>

            <a href="papers/fidler_et_al_cvpr13a.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#segdpmCVPR13abs" href="#segdpmCVPR13abs-list">Abstract</a>&nbsp
<a href="projects/segDPM.html" target="_blank" class="buttonPP">Project page</a>&nbsp
<a href="papers/segdpm_supplemental.pdf" class="buttonPP">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#segdpmCVPR13" href="#segdpmCVPR13-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="segdpmCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{segdpmCVPR13,<br/>
    author = {Sanja Fidler and Roozbeh Mottaghi and Alan Yuille and Raquel Urtasun},<br/>
    title = {Bottom-up Segmentation for Top-down Detection},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="segdpmCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we are interested in how semantic segmentation
can help object detection. Towards this goal, we
propose a novel deformable part-based model which exploits
region-based segmentation algorithms that compute
candidate object regions by bottom-up clustering followed
by ranking of those regions. Our approach allows every
detection hypothesis to select a segment (including void),
and scores each box in the image using both the traditional
HOG filters as well as a set of novel segmentation features.
Thus our model "blends" between the detector and segmentation
models. Since our features can be computed very efficiently given the segments, we maintain the same complexity
as the original DPM. We demonstrate the effectiveness
of our approach in PASCAL VOC 2010, and show that
when employing only a root filter our approach outperforms
Dalal & Triggs detector on all classes, achieving 13%
higher average AP. When employing the parts, we outperform
the original DPM in 19 out of 20 classes, achieving
an improvement of 8% AP. Furthermore, we outperform
the previous state-of-the-art on VOC'10 test by 4%.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.4% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_cvpr13a.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="projects/segDPM.html" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="papers/segdpm_supplemental.pdf" class="buttonP" style="width:100%;">Suppl. Mat.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#segdpmCVPR13" href="#segdpmCVPR13-list" style="width:100%; margin-top:5px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>		

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:3px 2.8% 0 0; width:14.8%;" href="papers/fidler_et_al_cvpr13b.pdf"><img width="100%" src="papers/figs/cvpr13sent.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Sentence is Worth a Thousand Pixels</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Abhishek Sharma,  Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
 <p style="margin:-9.0px 0px 0px 0px;"></p>
 <a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR13abs" href="#FidlerCVPR13abs-list">Abstract</a>&nbsp
<a href="papers/cvpr13_sent_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{FidlerCVPR13,<br/>
    author = {Sanja Fidler and Abhishek Sharma and Raquel Urtasun},<br/>
    title = {A Sentence is Worth a Thousand Pixels},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="FidlerCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We are interested in holistic scene understanding where
images are accompanied with text in the form of complex
sentential descriptions. We propose a holistic conditional
random field model for semantic parsing which reasons
jointly about which objects are present in the scene, their
spatial extent as well as semantic segmentation, and employs
text as well as image information as input. We automatically
parse the sentences and extract objects and their
relationships, and incorporate them into the model, both via
potentials as well as by re-ranking candidate detections. We
demonstrate the effectiveness of our approach in the challenging
UIUC sentences dataset and show segmentation improvements
of 12.5% over the visual only model and detection
improvements of 5% AP over deformable part-based
models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.4px 0px 0px 0px;"></p>
<a href="cvpr13_sent_supplemental.pdf" class="buttonM" style="width:100%;">Suppl. Mat.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list" style="width:100%; margin-top:5px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
							
<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 5.1% 0 0; width:14.8%;" href="papers/mottaghi_et_al_cvpr13.pdf"><img width="100%" src="papers/figs/cvpr13debugcrf.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Analyzing Semantic Segmentation Using Human-Machine Hybrid CRFs</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p>

            <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
 <a href="papers/mottaghi_et_al_cvpr13.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#MottaghiCVPR13abs" href="#MottaghiCVPR13abs-list">Abstract</a>&nbsp
<a href="papers/cvpr13_debug_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MottaghiCVPR13" href="#MottaghiCVPR13-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="MottaghiCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{MottaghiCVPR13,<br/>
    author = {Roozbeh Mottaghi and Sanja Fidler and Jian Yao and Raquel Urtasun and Devi Parikh},<br/>
    title = {Analyzing Semantic Segmentation Using Human-Machine Hybrid CRFs},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="MottaghiCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Recent trends in semantic image segmentation have
pushed for holistic scene understanding models that jointly
reason about various tasks such as object detection, scene
recognition, shape analysis, contextual reasoning. In this
work, we are interested in understanding the roles of these
different tasks in aiding semantic segmentation. Towards
this goal, we "plug-in" human subjects for each of the
various components in a state-of-the-art conditional random
field model (CRF) on the MSRC dataset. Comparisons
among various hybrid human-machine CRFs give us indications
of how much "head room" there is to improve segmentation
by focusing research efforts on each of the tasks.
One of the interesting findings from our slew of studies was
that human classification of isolated super-pixels, while being
worse than current machine classifiers, provides a significant
boost in performance when plugged into the CRF!
Fascinated by this finding, we conducted in depth analysis
of the human generated potentials. This inspired a new machine
potential which significantly improves state-of-the-art
performance on the MRSC dataset.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/mottaghi_et_al_cvpr13.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.1px 0px 0px 0px;"></p>
<a href="cvpr13_debug_supplemental.pdf" class="buttonM" style="width:100%;">Suppl. Mat.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#MottaghiCVPR13" href="#MottaghiCVPR13-list" style="width:100%; margin-top:4px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    <p style="margin:-8px 0px 0px 0px;"></p>
<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>

<p style="margin:65px 0px 0px 0px;"></p>


<h2>Year 2012</h2>
<br/>
<p style="margin:-12.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 1.5% 0 0; width:14.8%;" href="papers/fidler_et_al_nips12.pdf"><img width="100%" src="papers/figs/nips12boxy2.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<em>(<b>spotlight</b>)</em></h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Sven Dickinson, Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, Lake Tahoe, USA, December 2012</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">800 CAD models registered to canonical viewpoint released</p>
<p style="margin:-9.0px 0px 0px 0px;"></p>
 <a href="papers/fidler_et_al_nips12.pdf" class="buttonTT">Paper</a>&nbsp
  <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerNIPS12abs" href="#FidlerNIPS12abs-list">Abstract</a>&nbsp
<a href="projects/CAD.html" class="buttonPP">CAD dataset</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerNIPS12" href="#FidlerNIPS12-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerNIPS12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerNIPS12,<br/>
    author = {Sanja Fidler and Sven Dickinson and Raquel Urtasun},<br/>
    title = {3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model},<br/>
    booktitle = {NIPS},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="FidlerNIPS12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
This paper addresses the problem of category-level 3D object detection. Given
a monocular image, our aim is to localize the objects in 3D by enclosing them
with tight oriented 3D bounding boxes. We propose a novel approach that extends
the well-acclaimed deformable part-based model [1] to reason in 3D. Our model
represents an object class as a deformable 3D cuboid composed of faces and parts,
which are both allowed to deform with respect to their anchors on the 3D box. We
model the appearance of each face in fronto-parallel coordinates, thus effectively
factoring out the appearance variation induced by viewpoint. Our model reasons
about face visibility patters called aspects. We train the cuboid model jointly and
discriminatively and share weights across all aspects to attain efficiency. Inference
then entails sliding and rotating the box in 3D and scoring object hypotheses.
While for inference we discretize the search space, the variables are continuous
in our model. We demonstrate the effectiveness of our approach in indoor and
outdoor scenarios, and show that our approach significantly outperforms the state of-the-art
in both 2D and 3D object detection.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_nips12.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="projects/CAD.html" class="buttonP" style="width:100%;">CAD dataset</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerNIPS12" href="#FidlerNIPS12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 2.8% 0 0; width:14.8%;" href="papers/cvpr12seg.pdf"><img width="100%" src="papers/figs/cvpr12holistic.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Jian Yao, Sanja Fidler, Raquel Urtasun</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Providence, USA, June 2012</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">Code, trained models and annotated bounding boxes for MSRC in Project page</p>
<p style="margin:-9.0px 0px 0px 0px;"></p>
<a href="papers/cvpr12seg.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#YaoCVPR12abs" href="#YaoCVPR12abs-list">Abstract</a>&nbsp
<a href="http://ttic.uchicago.edu/~yaojian/HolisticSceneUnderstanding.html" target="_blank" class="buttonPP">Project page.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#YaoCVPR12" href="#YaoCVPR12-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="YaoCVPR12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{YaoCVPR12,<br/>
    author = {Jian Yao and Sanja Fidler and Raquel Urtasun},<br/>
    title = {Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation},<br/>
    booktitle = {CVPR},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="YaoCVPR12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we propose an approach to holistic scene
understanding that reasons jointly about regions, location,
class and spatial extent of objects, presence of a class in the
image, as well as the scene type. Learning and inference in
our model are efficient as we reason at the segment level,
and introduce auxiliary variables that allow us to decompose
the inherent high-order potentials into pairwise potentials
between a few variables with small number of states (at
most the number of classes). Inference is done via a convergent
message-passing algorithm, which, unlike graph-cuts
inference, has no submodularity restrictions and does not
require potential specific moves. We believe this is very important,
as it allows us to encode our ideas and prior knowledge
about the problem without the need to change the inference
engine every time we introduce a new potential. Our
approach outperforms the state-of-the-art on the MSRC-21
benchmark, while being much faster. Importantly, our holistic
model is able to improve performance in all tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/cvpr12seg.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://ttic.uchicago.edu/~yaojian/HolisticSceneUnderstanding.html" class="buttonP" style="width:100%;">Project page.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#YaoCVPR12" href="#YaoCVPR12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:4px 4.3% 0 0; width:14.8%;" href="papers/cvpr12sup.pdf"><img width="100%" src="papers/figs/cvpr12sup.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Super-edge grouping for object localization by combining appearance and shape information</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Zhiqi Zhang, Sanja Fidler, Jarell W. Waggoner, Yu Cao, Jeff M. Siskind, Sven Dickinson, Song Wang</p>

	    <p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Providence, USA, June 2012</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
 <a href="papers/cvpr12sup.pdf" class="buttonTT">Paper</a>&nbsp
  <a class="buttonAA" data-toggle="collapse" data-parent="#ZhangCVPR12abs" href="#ZhangCVPR12abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ZhangCVPR12" href="#ZhangCVPR12-list">Bibtex</a><br/>

<p style="margin:1px 0px 0px 0px;"></p>
<div id="ZhangCVPR12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{ZhangCVPR12,<br/>
    author = {Zhiqi Zhang and Sanja Fidler and Jarell W. Waggoner and Yu Cao and Jeff M. Siskind and Sven Dickinson and Song Wang},<br/>
    title = {Super-edge grouping for object localization by combining appearance and shape information},<br/>
    booktitle = {CVPR},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="ZhangCVPR12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Both appearance and shape play important roles in object
localization and object detection. In this paper, we
propose a new superedge grouping method for object localization
by incorporating both boundary shape and appearance
information of objects. Compared with the previous
edge grouping methods, the proposed method does
not subdivide detected edges into short edgels before grouping.
Such long, unsubdivided superedges not only facilitate
the incorporation of object shape information into localization,
but also increase the robustness against image noise
and reduce computation. We identify and address several
important problems in achieving the proposed superedge
grouping, including gap filling for connecting superedges,
accurate encoding of region-based information into individual
edges, and the incorporation of object-shape information
into object localization. In this paper, we use the bag
of visual words technique to quantify the region-based appearance
features of the object of interest. We find that the
proposed method, by integrating both boundary and region
information, can produce better localization performance
than previous subwindow search and edge grouping methods
on most of the 20 object categories from the VOC 2007
database. Experiments also show that the proposed method
is roughly 50 times faster than the previous edge grouping
method.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/cvpr12sup.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#ZhangCVPR12" href="#ZhangCVPR12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 4.6% 0 0; width:14.8%;" href="http://www.cs.toronto.edu/~sven/Papers/UAI2012.pdf"><img width="100%" src="papers/figs/uai12sent.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:82%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Video In Sentences Out &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <em>(<b>oral presentation</b>)</em></h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Uncertainty in Artificial Intelligence</em> (<strong>UAI</strong>), Catalina Island, USA, 2012</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="http://www.cs.toronto.edu/~sven/Papers/UAI2012.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#BarbuUAI12abs" href="#BarbuUAI12abs-list">Abstract</a>&nbsp
<a href="https://engineering.purdue.edu/~qobi/mindseye/" target="_blank" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#BarbuUAI12" href="#BarbuUAI12-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="BarbuUAI12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{BarbuUAI12,<br/>
    author = {Andrei Barbu and Alexander Bridge and Zachary Burchill and Dan Coroian and Sven Dickinson and Sanja Fidler and Aaron Michaux and Sam Mussman and Siddharth Narayanaswamy and Dhaval Salvi and Lara Schmidt and Jiangnan Shangguan and Jeffrey Mark Siskind and Jarrell Waggoner and Song Wang and Jinlian Wei and Yifan Yin and Zhiqi Zhang},<br/>
    title = {Video In Sentences Out},<br/>
    booktitle = {UAI},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="BarbuUAI12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
We present a system that produces sentential
descriptions of video: who did what to whom,
and where and how they did it. Action class
is rendered as a verb, participant objects as
noun phrases, properties of those objects as
adjectival modifiers in those noun phrases,
spatial relations between those participants
as prepositional phrases, and characteristics
of the event as prepositional-phrase adjuncts
and adverbial modifiers. Extracting the information
needed to render these linguistic
entities requires an approach to event recognition
that recovers object tracks, the track-to-role
assignments, and changing body posture.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://www.cs.toronto.edu/~sven/Papers/UAI2012.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="https://engineering.purdue.edu/~qobi/mindseye/" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#BarbuUAI12" href="#BarbuUAI12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 5.2% 0 0; width:14.8%;" href="papers/SEM2012.pdf"><img width="100%" src="papers/figs/sem12wsd.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Unsupervised Disambiguation of Image Captions</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Wesley May, Sanja Fidler, Afsaneh Fazly, Suzanne Stevenson, Sven Dickinson</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>First Joint Conference on Lexical and Computational Semantics (*SEM)</em>, 2012</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/SEM2012.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#MaySEM12abs" href="#MaySEM12abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MaySEM12" href="#MaySEM12-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="MaySEM12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{MaySEM12,<br/>
    author = {Wesley May and Sanja Fidler and Afsaneh Fazly and Suzanne Stevenson and Sven Dickinson},<br/>
    title = {Unsupervised Disambiguation of Image Captions},<br/>
    booktitle = {First Joint Conference on Lexical and Computational Semantics (*SEM)},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="MaySEM12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Given a set of images with related captions,
our goal is to show how visual features can
improve the accuracy of unsupervised word
sense disambiguation when the textual context
is very small, as this sort of data is common
in news and social media. We extend
previous work in unsupervised text-only disambiguation
with methods that integrate text
and images. We construct a corpus by using
Amazon Mechanical Turk to caption sense-tagged
images gathered from ImageNet. Using
a Yarowsky-inspired algorithm, we show
that gains can be made over text-only disambiguation,
as well as multimodal approaches
such as Latent Dirichlet Allocation.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/SEM2012.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#MaySEM12" href="#MaySEM12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.9%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 4.9% 0 0; width:14.5%;" href="http://www.cs.toronto.edu/~tshlee/pub/crv12-shaperec.pdf"><img width="100%" src="papers/figs/crv12captions.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning Categorical Shape from Captioned Images</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Tom Lee, Sanja Fidler, Alex Levinshtein, Sven Dickinson</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Conference on Computer and Robot Vision</em> (CRV), Toronto, Canada, May 2012</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="http://www.cs.toronto.edu/~tshlee/pub/crv12-shaperec.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#LeeCRV12abs" href="#LeeCRV12abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeCRV12" href="#LeeCRV12-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="LeeCRV12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{LeeCRV12,<br/>
    author = {Tom Lee and Sanja Fidler and Alex Levinshtein and Sven Dickinson},<br/>
    title = {Learning Categorical Shape from Captioned Images},<br/>
    booktitle = {Canadian Conference on Computer and Robot Vision (CRV)},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="LeeCRV12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Given a set of captioned images of cluttered
scenes containing various objects in different positions and
scales, we learn named contour models of object categories
without relying on bounding box annotation. We extend a
recent language-vision integration framework that finds spatial
configurations of image features that co-occur with words in
image captions. By substituting appearance features with local
contour features, object categories are recognized by a contour
model that grows along the object's boundary. Experiments on
ETHZ are presented to show that 1) the extended framework
is better able to learn named visual categories whose within class
variation is better captured by a shape model than an
appearance model; and 2) typical object recognition methods
fail when manually annotated bounding boxes are unavailable.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://www.cs.toronto.edu/~tshlee/copy/crv12-shaperec.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LeeCRV12" href="#LeeCRV12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    <p style="margin:-8px 0px 0px 0px;"></p>
<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>
<p style="margin:65px 0px 0px 0px;"></p>

<h2>Years 2006-2011</h2>
<br/>
<p style="margin:-12.5px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 4% 0 0; width:14.8%;" href="papers/cvpr11rlda.pdf"><img width="100%" src="papers/figs/cvpr11rlda.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A Probabilistic Model for Recursive Factorized Image Features</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sergey Karayev, Mario Fritz, Sanja Fidler, Trevor Darrell</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2011</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/cvpr11rlda.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#KarayevCVPR11abs" href="#KarayevCVPR11abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#KarayevCVPR11" href="#KarayevCVPR11-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="KarayevCVPR11-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{KarayevCVPR11,<br/>
    author = {Sergey Karayev and Mario Fritz and Sanja Fidler and Trevor Darrell},<br/>
    title = {A Probabilistic Model for Recursive Factorized Image Features},<br/>
    booktitle = {CVPR},<br/>
    year = {2011}<br/>}</p>
</div>
<div id="KarayevCVPR11abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Layered representations for object recognition are important
due to their increased invariance, biological plausibility,
and computational benefits. However, most of existing
approaches to hierarchical representations are strictly
feedforward, and thus not well able to resolve local ambiguities.
We propose a probabilistic model that learns and infers
all layers of the hierarchy jointly. Specifically, we suggest
a process of recursive probabilistic factorization, and
present a novel generative model based on Latent Dirichlet
Allocation to this end. The approach is tested on a standard
recognition dataset, outperforming existing hierarchical
approaches and demonstrating performance on par with
current single-feature state-of-the-art models. We demonstrate
two important properties of our proposed model: 1)
adding an additional layer to the representation increases
performance over the flat model; 2) a full Bayesian approach
outperforms a feedforward implementation of the
model.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/cvpr11rlda.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#KarayevCVPR11" href="#KarayevCVPR11-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:5px 3.5% 0 0; width:14.8%;" href="papers/fidler_eccv10.pdf"><img width="100%" src="papers/figs/eccv10hier2.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">A coarse-to-fine Taxonomy of Constellations for Fast Multi-class Object Detection</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">In <em>European Conference in Computer Vision (<strong>ECCV</strong>)</em>, 2010</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/fidler_eccv10.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerECCV10abs" href="#FidlerECCV10abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerECCV10" href="#FidlerECCV10-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerECCV10-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerECCV10,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {A coarse-to-fine Taxonomy of Constellations for Fast Multi-class Object Detection},<br/>
    booktitle = {ECCV},<br/>
    year = {2010}<br/>}
</p>
</div>
<div id="FidlerECCV10abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In order for recognition systems to scale to a larger number of
object categories building visual class taxonomies is important to achieve
running times logarithmic in the number of classes [1, 2]. In this paper we
propose a novel approach for speeding up recognition times of multi-class
part-based object representations. The main idea is to construct a taxonomy
of constellation models cascaded from coarse-to-fine resolution and
use it in recognition with an efficient search strategy. The taxonomy is
built automatically in a way to minimize the number of expected computations
during recognition by optimizing the cost-to-power ratio [Blanchard and Geman, <em>Annals of Statistics</em>, 2005]. The
structure and the depth of the taxonomy is not pre-determined but is
inferred from the data. The approach is utilized on the hierarchy-of-parts
model achieving efficiency in both, the representation of the structure
of objects as well as in the number of modeled object classes. We achieve
speed-up even for a small number of object classes on the ETHZ and
TUD dataset. On a larger scale, our approach achieves detection time
that is logarithmic in the number of classes.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_eccv10.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerECCV101" href="#FidlerECCV101-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 1.6% 0 0; width:14.4%;" href="http://www.cognitivesystems.org/cosybook/cosy-book.pdf"><img width="100%" src="papers/figs/catperc.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Categorial Perception</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Mario Fritz, Mykhaylo Andriluka, Sanja Fidler, Michael Stark, Ales Leonardis, Bernt Schiele</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Cognitive Systems</em>, No. 8, 2010</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="http://www.cognitivesystems.org/cosybook/cosy-book.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FritzChapter09abs" href="#FritzChapter09abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FritzChapter09" href="#FritzChapter09-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FritzChapter09-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p>@InCollection{FritzChapter09,<br/>
    author = {Mario Fritz and Mykhaylo Andriluka and Sanja Fidler and  Michael Stark and Ales Leonardis and Bernt Schiele},<br/>
    title = {Categorical Perception},<br/>
    booktitle = {Cognitive Systems},<br/>
    series = {Cognitive Systems Monographs},<br/>
    volume = {8}, <br/>
    year = {2010},<br/>
    publisher = {Springer},<br/>
    organization = {Springer},<br/>
    chapter = {Categorical Perception}<br/>}
</p>
</div>
<div id="FritzChapter09abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
The ability to recognize and categorize entities in its environment is a vital
competence of any cognitive system. Reasoning about the current state of the
world, assessing consequences of possible actions, as well as planning future
episodes requires a concept of the roles that objects and places may possibly
play. For example, objects afford to be used in specific ways, and places are
usually devoted to certain activities. The ability to represent and infer these
roles, or, more generally, categories, from sensory observations of the world, is
an important constituent of a cognitive system's perceptual processing (Section
1.3 elaborates on this with a very visual example).

In the CoSy project, a substantial amount of work has been conducted on
the advancement of methods that recognize and categorize objects and places
by using different modalities, namely, vision, language, and laser range data.
Our progress contributes to our effort to build systems that evolve through
interaction with its environment in an ultimately live-long learning process.

While this chapter describes our contribution to modeling, learning and
representing of visual categories, Chapter 7 shows how to combine the visual
information with other modalities in a multi-modal learning process (e.g.
speech/language as detailed in Chapter 8). Finally, Chapter 9 and 10 shows
how we integrated these concepts in a autonomous systems to understand
the implications of our progress in categorization on an interactive evolving
system.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://www.cognitivesystems.org/cosybook/cosy-book.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FritzChapter09" href="#FritzChapter09-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 3.8% 0 0; width:14.8%;" href="papers/fidler_et_al_nips2009.pdf"><img width="100%" src="papers/figs/nips09hier.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, 2009</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/fidler_et_al_nips2009.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerNIPS09abs" href="#FidlerNIPS09abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerNIPS09" href="#FidlerNIPS09-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerNIPS09-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerNIPS09,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {Evaluating multi-class learning strategies in a generative hierarchical framework for object detection},<br/>
    booktitle = {NIPS},<br/>
    year = {2009}<br/>}</p>
</div>
<div id="FidlerNIPS09abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Multi-class object learning and detection is a challenging problem due to the
large number of object classes and their high visual variability. Specialized detectors
usually excel in performance, while joint representations optimize sharing
and reduce inference time -- but are complex to train. Conveniently, sequential
class learning cuts down training time by transferring existing knowledge to novel
classes, but cannot fully exploit the shareability of features among object classes
and might depend on ordering of classes during learning. In hierarchical frameworks
these issues have been little explored. In this paper, we provide a rigorous
experimental analysis of various multiple object class learning strategies within a
generative hierarchical framework. Specifically, we propose, evaluate and compare
three important types of multi-class learning: 1.) independent training of
individual categories, 2.) joint training of classes, and 3.) sequential learning of
classes. We explore and compare their computational behavior (space and time)
and detection performance as a function of the number of learned object classes
on several recognition datasets. We show that sequential training achieves the best
trade-off between inference and training times at a comparable detection performance
and could thus be used to learn the classes on a larger scale.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_nips2009.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerNIPS09" href="#FidlerNIPS09-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.8%;" href=""></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Optimization framework for learning a hierarchical shape vocabulary for object class detection</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2009</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerBMVC09" href="#FidlerBMVC09-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerBMVC09-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerBMVC09,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {Optimization framework for learning a hierarchical shape vocabulary for object class detection},<br/>
    booktitle = {BMVC},<br/>
    year = {2009}<br/>}</p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerBMVC09" href="#FidlerBMVC09-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.8%;" href=""></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning Hierarchical Compositional Representations of Object Structure</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Object Categorization: Computer and Human Vision Perspectives</em></br>
	     Editors: S. Dickinson, A. Leonardis, B. Schiele and M. J. Tarr<br />
            Cambridge university press, 2009</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a class="buttonSS" data-toggle="collapse" data-parent="#FidlerChapter09" href="#FidlerChapter09-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerChapter09-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p>@InCollection{FidlerChapter09,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {Learning Hierarchical Compositional Representations of Object Structure},<br/>
    booktitle = {Object Categorization: Computer and Human Vision Perspectives},<br/>
    editor    = {Sven Dickinson and Ale\v{s} Leonardis and Bernt Schiele and Michael J. Tarr},<br/>
    year      = {2009},<br/>
    publisher = {Cambridge University Press},<br/>
    pages = {}<br/>}
</p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerChapter09" href="#FidlerChapter09-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.8%;" href=""></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Similarity-based cross-layered hierarchical representation for object categorization</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2008</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR08" href="#FidlerCVPR08-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerCVPR08-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerCVPR08,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {Similarity-based cross-layered hierarchical representation for object categorization},<br/>
    booktitle = {CVPR},<br/>
    year = {2008}<br/>}</p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerCVPR08" href="#FidlerCVPR08-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:7px 4.1% 0 0; width:14.8%;" href="http://eprints.fri.uni-lj.si/245/1/FuerstFidlerLeonardis_PRL_2008.pdf"><img width="100%" src="papers/figs/prl08.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Selecting features for object detection using an AdaBoost-compatible evaluation function</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Luka Furst, Sanja Fidler, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Pattern Recognition Letters</em>, Vol. 29, No. 11, pp. 1603-1612, 2008</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="http://eprints.fri.uni-lj.si/245/1/FuerstFidlerLeonardis_PRL_2008.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FurstPRL08abs" href="#FurstPRL08abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FurstPRL08" href="#FurstPRL08-list">Bibtex</a><br/>
 
<p style="margin:0px 0px 0px 0px;"></p>
<div id="FurstPRL08-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@article{FurstPRL08,<br/>
    author = {Luka Furst and Sanja Fidler and Ales Leonardis},<br/>
    title = {Similarity-based cross-layered hierarchical representation for object categorization},<br/>
    journal = {Pattern Recognition Letters},<br/>
    volume = {29},<br/>
    number = {11},<br/>
    pages = {1603-1612},<br/>
    year = {2008}<br/>}</p>
</div>
<div id="FurstPRL08abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
This paper addresses the problem of selecting features in a visual object detection setup where a detection
algorithm is applied to an input image represented by a set of features. The set of features to be
employed in the test stage is prepared in two training-stage steps. In the first step, a feature extraction
algorithm produces a (possibly large) initial set of features. In the second step, on which this paper
focuses, the initial set is reduced using a selection procedure. The proposed selection procedure is based
on a novel evaluation function that measures the utility of individual features for a certain detection task.
Owing to its design, the evaluation function can be seamlessly embedded into an AdaBoost selection
framework. The developed selection procedure is integrated with state-of-the-art feature extraction
and object detection methods. The presented system was tested on five challenging detection setups.
In three of them, a fairly high detection accuracy was effected by as few as six features selected out of
several hundred initial candidates.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://eprints.fri.uni-lj.si/245/1/FuerstFidlerLeonardis_PRL_2008.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FurstPRL08" href="#FurstPRL08-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:3px 4.8% 0 0; width:14.8%;" href=""><img width="100%" src="papers/figs/issr07hier.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Learning hierarchical representations of object categories for robot vision &nbsp&nbsp&nbsp&nbsp&nbsp <b>(invited paper)</b></h4>
	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Ales Leonardis, Sanja Fidler</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>International Symposium of Robotics Research (<strong>ISRR</strong>)</em>, 2007</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="http://www.mobvis.org/publications/isrr07LeonardisFidler.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerISSR07abs" href="#FidlerISSR07abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerISSR07" href="#FidlerISSR07-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerISSR07-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerISSR07,<br/>
    author = {Ales Leonardis and Sanja Fidler},<br/>
    title = {Learning hierarchical representations of object categories for robot vision},<br/>
    booktitle = {International Symposium of Robotics Research (ISRR},<br/>
    year = {2007}<br/>}</p>
</div>
<div id="FidlerISSR07abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
This paper presents our recently developed approach to constructing a hierarchical
representation of visual input that aims to enable recognition and detection of a large
number of object categories. Inspired by the principles of efficient indexing, robust matching,
and ideas of compositionality, our approach learns a hierarchy of spatially flexible compositions,
i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent
features, we learn the statistically most significant compositions (parts composed of parts),
which consequently define the next layer. Parts are learned sequentially, layer after layer, optimally
adjusting to the visual data. Lower layers are learned in a category-independent way
to obtain complex, yet sharable visual building blocks, which is a crucial step towards a scalable
representation. Higher layers of the hierarchy, on the other hand, are constructed by using
specific categories, achieving a category representation with a small number of highly generalizable
parts that gained their structural flexibility through composition within the hierarchy.
Built in this way, new categories can be efficiently and continuously added to the system by
adding a small number of parts only in the higher layers. The approach is demonstrated on a
large collection of images and a variety of object categories.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://www.mobvis.org/publications/isrr07LeonardisFidler.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerISSR07" href="#FidlerISSR07-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 3% 0 0; width:14.8%;" href="papers/cvpr07fidler.pdf"><img width="100%" src="papers/figs/cvpr07hier2.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2007</p>
<p style="margin:-10.0px 0px 0px 0px;"></p>
            <p style="font-size:13.4px; color:#E62E00;">Learning a compositional hierarchy of interpretable features encoding spatial relations</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/cvpr07fidler.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR07abs" href="#FidlerCVPR07abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR07" href="#FidlerCVPR07-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerCVPR07-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerCVPR07,<br/>
    author = {Sanja Fidler and Ales Leonardis},<br/>
    title = {Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts},<br/>
    booktitle = {CVPR},<br/>
    year = {2007}<br/>}</p>
</div>
<div id="FidlerCVPR07abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
This paper proposes a novel approach to constructing
a hierarchical representation of visual input that aims to
enable recognition and detection of a large number of object
categories. Inspired by the principles of efficient indexing
(bottom-up), robust matching (top-down), and ideas of
compositionality, our approach learns a hierarchy of spatially
flexible compositions, i.e. parts, in an unsupervised,
statistics-driven manner. Starting with simple, frequent features,
we learn the statistically most significant compositions
(parts composed of parts), which consequently define
the next layer. Parts are learned sequentially, layer after
layer, optimally adjusting to the visual data. Lower layers
are learned in a category-independent way to obtain complex,
yet sharable visual building blocks, which is a crucial
step towards a scalable representation. Higher layers of the
hierarchy, on the other hand, are constructed by using specific
categories, achieving a category representation with a
small number of highly generalizable parts that gained their
structural flexibility through composition within the hierarchy.
Built in this way, new categories can be efficiently and
continuously added to the system by adding a small number
of parts only in the higher layers. The approach is demonstrated
on a large collection of images and a variety of object
categories. Detection results confirm the effectiveness
and robustness of the learned parts.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/cvpr07fidler.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerCVPR07" href="#FidlerCVPR07-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 3.5% 0 0; width:14.8%;" href="papers/Fidler2006Combining.pdf"><img width="100%" src="papers/figs/pami06.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Combining Reconstructive and Discriminative Subspace Methods for Robust Classification and Regression by Subsampling</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Danijel Skocaj, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>IEEE Trans. on Pattern Anal. and Machine Intell. (<strong>PAMI</strong>)</em>, vol. 28, no. 3, pp. 337-350, 2006</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/Fidler2006Combining.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerPAMI06abs" href="#FidlerPAMI06abs-list">Abstract</a>&nbsp
<a href="code/robust-lda.zip" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerPAMI06" href="#FidlerPAMI06-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerPAMI06-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
    <p style="font-size:15px;">@article{FidlerPAMI06,<br/>
    author = {Sanja Fidler and Danijel Skocaj and Ales Leonardis},<br/>
    title = {Combining Reconstructive and Discriminative Subspace Methods for Robust Classification and Regression by Subsampling},<br/>
    journal = {IEEE Trans. on Pattern Analysis and Machine Intelligence},<br/>
    volume = {28},<br/>
    number = {3},<br/>
    pages = {337-350},<br/>
    year = {2006}<br/>}</p>
</div>
<div id="FidlerPAMI06abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Linear subspace methods that provide sufficient reconstruction of the data, such as PCA, offer an efficient way of dealing
with missing pixels, outliers, and occlusions that often appear in the visual data. Discriminative methods, such as LDA, which, on the
other hand, are better suited for classification tasks, are highly sensitive to corrupted data. We present a theoretical framework for
achieving the best of both types of methods: An approach that combines the discrimination power of discriminative methods with the
reconstruction property of reconstructive methods which enables one to work on subsets of pixels in images to efficiently detect and
reject the outliers. The proposed approach is therefore capable of robust classification with a high-breakdown point. We also show that
subspace methods, such as CCA, which are used for solving regression tasks, can be treated in a similar manner. The theoretical
results are demonstrated on several computer vision tasks showing that the proposed approach significantly outperforms the standard
discriminative methods in the case of missing pixels and images containing occlusions and outliers.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.1%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 4.4% 0 0; width:14.8%;" href="papers/fidlercvpr06.pdf"><img width="100%" src="papers/figs/cvpr06hier.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Hierarchical Statistical Learning of Generic Parts of Object Structure</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Gregor Berginc, Ales Leonardis</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2006</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/fidlercvpr06.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR06abs" href="#FidlerCVPR06abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR06" href="#FidlerCVPR06-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerCVPR06-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerCVPR06,<br/>
    author = {Sanja Fidler and Gregor Berginc and Ales Leonardis},<br/>
    title = {Hierarchical Statistical Learning of Generic Parts of Object Structure},<br/>
    booktitle = {CVPR},<br/>
    year = {2006}<br/>}</p>
</div>
<div id="FidlerCVPR06abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
With the growing interest in object categorization various
methods have emerged that perform well in this challenging
task, yet are inherently limited to only a moderate
number of object classes. In pursuit of a more general categorization
system this paper proposes a way to overcome
the computational complexity encompassing the enormous
number of different object categories by exploiting the statistical
properties of the highly structured visual world. Our
approach proposes a hierarchical acquisition of generic
parts of object structure, varying from simple to more complex
ones, which stem from the favorable statistics of natural
images. The parts recovered in the individual layers of
the hierarchy can be used in a top-down manner resulting
in a robust statistical engine that could be efficiently used
within many of the current categorization systems. The proposed
approach has been applied to large image datasets
yielding important statistical insights into the generic parts
of object structure.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidlercvpr06.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerCVPR06" href="#FidlerCVPR06-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    <p style="margin:-8px 0px 0px 0px;"></p>
<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>
<p style="margin:65px 0px 0px 0px;"></p>


<h2>Earlier work</h2>
<br/>
<p style="margin:-12.5px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:1.0% 2.1% 0 0; width:0.0%;" href="papers/Skocaj2004Robust.pdf"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Robust estimation of canonical correlation coefficients</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Danijel Skocaj, Ales Leonardis, Sanja Fidler</p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>28th wrk. of the Austrian Association for Pattern Recognition (OAGM/AAPR)</em>, 2004</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="papers/Skocaj2004Robust.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#SkocajOAGM04abs" href="#SkocajOAGM04abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SkocajOAGM04" href="#SkocajOAGM04-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="SkocajOAGM04-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{SkocajOAGM04,<br/>
    author = {Danijel Skocaj and Ales Leonardis and Sanja Fidler},<br/>
    title = {Robust estimation of canonical correlation coefficients},<br/>
    booktitle = {28th workshop of the Austrian Association for Pattern Recognition (OAGM/AAPR)},<br/>
    year = {2004}<br/>}
</p>
</div>
<div id="SkocajOAGM04abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
Canonical Correlation Analysis is well suited for regression tasks in appearance-based approach to modeling of objects and scenes. However, since it relies on the standard projection it is inherently non-robust. In this paper, we propose to embed the estimation of the CCA coefficients in an augmented PCA space, which enables detection of outliers and preserves regression-relevant information enabling robust estimation of canonical correlation coefficients.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/Skocaj2004Robust.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#SkocajOAGM04" href="#SkocajOAGM04-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:1.0% 2.1% 0 0; width:0.0%;" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4624360&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4624360"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Robust LDA classification by subsampling</h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Ales Leonardis</strong></p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>In Workshop in Statistical Analysis in Computer Vision</em> (in conjunction with CVPR), 2003</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
            <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4624360&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4624360" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerSACV03abs" href="#FidlerSACV03abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerSACV03" href="#FidlerSACV03-list">Bibtex</a><br/>
 
<p style="margin:1px 0px 0px 0px;"></p>
<div id="FidlerSACV03-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerSACV03,<br/>
    author = {Sanja Fidler and Ales Leonardis},<br/>
    title = {Robust LDA classification by subsampling},<br/>
    booktitle = {Workshop in Statistical Analysis in Computer Vision in conjunction with CVPR},<br/>
    year = {2003}<br/>}</p>
</div>
<div id="FidlerSACV03abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.1px;">
In this paper we present a new method which enables a robust calculation of the LDA classification rule, thus making the recognition of objects under non-ideal conditions possible, i.e., in situations when objects are occluded or they appear on a varying background, or when their images are corrupted by outliers. The main idea behind the method is to translate the task of calculating the LDA classification rule into the problem of determining the coefficients of an augmented generative model (PCA). Specifically, we construct an augmented PCA basis which, on the one hand, contains information necessary for the classification (in the LDA sense), and, on the other hand, enables us to calculate the necessary coefficients by means of a subsampling approach resulting in a high breakdown point classification. The theoretical results are evaluated on the ORL face database showing that the proposed method significantly outperforms the standard LDA.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_eccv10.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerSACV03" href="#FidlerSACV03-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8.5px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 0.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:1.0% 2.1% 0 0; width:0.0%;" href=""></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
<p style="margin:-3px 0px 0px 0px;"></p>
            <h4 style="font-size:15.1px; line-height:155%">Robust LDA classification &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<em><b>(best paper award)</b></em></h4>

	     <p style="margin:-9.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px">Sanja Fidler, Ales Leonardis</strong></p>

		<p style="margin:-10.0px 0px 0px 0px;"></p>

            <p style="font-size:13.4px"><em>27th wrk. of the Austrian Association for Pattern Recognition (OAGM/AAPR)</em>, 2003</p>
            <p style="margin:-9.0px 0px 0px 0px;"></p>
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerOAGM03" href="#FidlerOAGM03-list">Bibtex</a><br/>
 
<p style="margin:6px 0px 0px 0px;"></p>
<div id="FidlerOAGM03-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerOAGM03,<br/>
    author = {Sanja Fidler and Ales Leonardis},<br/>
    title = {Robust LDA classification},<br/>
    booktitle = {27th workshop of the Austrian Association for Pattern Recognition (OAGM/AAPR)},<br/>
    year = {2003}<br/>}
</p>
</div>
</div>
      </li>
    </ul>
    <p style="margin:-8px 0px 0px 0px;"></p>
<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>
<p style="margin:65px 0px 0px 0px;"></p>




<h2>Extended abstracts</h2>
<br/>
<font style="font-size:14.8px;">
		<li>Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, Sanja Fidler<br />
			  <span class="style2"><a href="papers/kong_coref_sunw14.pdf" class="style2">
What are you talking about? Text-to-Image Coreference </span><a href="" class="style2"></a><span class="style5"></a></span> <br />
<em>In Scene Understanding Workshop (<strong>SUNw</strong>), ColumbUSA, USA, June 2014</em><br/>
<br />	

		<li>Roozbeh Mottaghi,  Xianjie Chen, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille<br />
			  <span class="style2"><a href="papers/Mottaghi_SUNw14.pdf" class="style2">
The Role of Context for Object Detection and Semantic Segmentation in the Wild </span><a href="" class="style2"></a><span class="style5"></a></span> <br />
<em>In Scene Understanding Workshop (<strong>SUNw</strong>), ColumbUSA, USA, June 2014</em><br/>	
						<br />	

		<li>Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun<br />
			  <span class="style2"><a href="papers/fidler_segdpm_sunw13.pdf" class="style2">Bottom-up Segmentation for Top-down Detection </span><a href="fidler_segdpm_sunw13.pdf" class="style2"></a><span class="style5"></a></span> <br />
                            <em>In Scene Understanding Workshop (<strong>SUNw</strong>), Portland, USA, June 2013</em><br/>
							<br />	
							
							<li>Sanja Fidler, Abhishek Sharma,  Raquel Urtasun<br />
			  <span class="style2"><a href="papers/fidler_sent_sunw13.pdf" class="style2">A Sentence is Worth a Thousand Pixels </span><a href="fidler_sent_sunw13.pdf" class="style2"></a><span class="style5"></a></span> <br />
				<em>In Scene Understanding Workshop (<strong>SUNw</strong>), Portland, USA, June 2013</em><br/>							<br />

			  <li>Sanja Fidler, Sven Dickinson, Raquel Urtasun<br />
			  <span class="style2"><a href="papers/fidler_3d_sunw13.pdf" class="style2">3D Object Detection with a Deformable 3D Cuboid Model</span><a href="fidler_3d_sunw13.pdf" class="style2"></a><span class="style5"></a></span> <br />
			 shop (<strong>SUNw</strong>), Portland, USA, June 2013</em><br/>
							<br />
							</font>
</ul>

<p><a href="#"><span class="glyphicon glyphicon-arrow-up"></span> back to top</a></p>
</div>

</body></html>