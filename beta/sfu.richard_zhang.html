<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>Publications</title>
  <link rel="stylesheet" href="mystyle.css">
</head>

<body>

<!-- Site navigation menu -->
<ul class="navbar">
  <li><a href="index.html">Home</a>
  <li><a href="bio.html">Bio+Awards+CV</a>
  <li><a href="news.html">News</a>
  <li><a href="papers.html">Publications</a>
  <li><a href="people.html">People</a>
  <li><a href="talks.html">Talks</a>
  <li><a href="service.html">Services</a>
  <li><a href="teaching.html">Teaching</a>
  <li><a href="personal.html">Personal &nbsp; <img border=s src="photos/haoscript.jpg" width="40"></a>
</ul>

<!-- Main content -->

<hr>

<i>There are many highly respectable motives which may lead men to
prosecute research, but three which are much more important than the
rest: <b>intellectual curiosity</b>, <b>professional pride</b>, and
finally, <b>ambition</b>,
desire for reputation, and the position, even the power or the money,
which it brings ... if (anyone) were to tell me that the driving force
in his work had been the desire to benefit humanity, then I should not
believe him (nor should I think the better of him if I did).</i>
- G. H. Hardy (A Mathematician's Apology) </i>

<hr>

<p>

<h2>Selected publications by year (<a href="papers_by_cat.html">by category</a>)</h2>

<a href="#papers2022">2022</a> | <a href="#papers2021">2021</a> | <a href="#papers2020">2020</a> |
<a href="#papers2019">2019</a> | <a href="#papers2018">2018</a> |
<a href="#papers2017">2017</a> | <a href="#papers2016">2016</a> | <a href="#papers2015">2015</a> |
<a href="#papers2014">2014</a> | <a href="#papers2013">2013</a> | <a href="#papers2012">2012</a> |
<a href="#papers2011">2011</a> | <a href="#papers2010">2010</a> | <a href="#papers2009">2009</a> |
<a href="#papers2008p">2008 and before</a>

<p>

<strong>H-index: 59</strong> | <a href="http://scholar.google.com/citations?user=osTl-5IAAAAJ&hl=en">Google citation</a> | <a href="http://www.informatik.uni-trier.de/~ley/pers/hd/z/Zhang_0002:Hao.html">DBLP</a> | <a href="http://csrankings.org/#/index?all&ca">CS Rankings</a>

<a name="papers2022">

<h3>2022</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/Drone_SIGA_long.png">
<IMG width=200 src="pubs/images/Drone_SIGA.png"></a>
</TD>
<TD vAlign=center align=left>
10. <a href="https://yilinliu77.github.io/">Yilin Liu</a>, Liqiang Lin, Ke Xie,
   <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>,
   <strong>Hao Zhang</strong>, and <a href="https://vcc.tech/~huihuang">Hui Huang</a>,
   "<strong>Learning Reconstructability for Drone Aerial Path Planning</strong>", <i>SIGGRAPH Asia</i> (journal), 2022.
   [<a href="https://vcc.tech/research/2022/DroneRecon">Project</a> | <a href="https://arxiv.org/abs/2209.10174">arXiv</a> | <a href="pubs/haoz_paper_bib.html#drone_siga22">bibtex</a>]
<p>
We introduce the first learning-based reconstructability predictor to improve view and path planning for
large-scale 3D urban scene acquisition using unmanned drones. In contrast to previous heuristic approaches, our method learns a model that explicitly predicts how well a 3D urban scene will be reconstructed from a set of viewpoints. To make such a model trainable and simultaneously applicable to drone path planning, we simulate the proxy-based 3D scene reconstruction during training to set up the prediction. Specifically, the neural network we design is trained to predict the scene reconstructability as a function of the proxy geometry, a set of viewpoints, and optionally a series of scene images acquired in flight ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/NIFT_ICRA_long.png">
<IMG width=200 src="pubs/images/NIFT_ICRA.png"></a>
</TD>
<TD vAlign=center align=left>
9. Zeyu Huang, Juzhan Xu, Sisi Dai, Kai Xu, <strong>Hao Zhang</strong>, 
   <a href="https://vcc.tech/~huihuang">Hui Huang</a>,
   and <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>,
   [<a href="https://arxiv.org/abs/2210.10992">arXiv</a> | <a href="pubs/haoz_paper_bib.html#nift_icra23">bibtex</a>]
<p>
We introduce NIFT, Neural Interaction Field and Template, a descriptive and robust interaction representation of object manipulations to facilitate imitation learning. Given a few object manipulation demos, NIFT guides the generation of the interaction imitation for a new object instance by matching the Neural Interaction Template (NIT) extracted from the demos to the Neural Interaction Field (NIF) defined for the new object. Specifically, the NIF is a neural field which encodes the relationship between each spatial point and a given object, where the relative position is defined by a spherical distance function rather than occupancies or signed distances, which are commonly adopted by conventional neural fields but less informative ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/NDC_long.png">
<IMG width=200 src="pubs/images/NDC.png"></a>
</TD>
<TD vAlign=center align=left>
8. <a href="https://czq142857.github.io/">Zhiqin Chen</a>, 
   <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
   <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>Neural Dual Contouring</strong>", <i>SIGGRAPH</i> (journal), 2022.
   [<a href="https://arxiv.org/abs/2202.01999">arXiv</a> | <a href="pubs/haoz_paper_bib.html#ndc_sig22">bibtex</a>]
<p>
We introduce <i>neural dual contouring</i> (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC). Like traditional DC, it produces exactly one vertex per grid cell and one quad for each grid edge intersection, a natural and efficient structure for reproducing sharp features. However, rather than computing vertex locations and edge crossings with hand-crafted functions that depend directly on difficult-to-obtain surface gradients, NDC uses a neural network to predict them. As a result, NDC can be trained to produce meshes from signed or unsigned distance fields, binary voxel grids, or point clouds (with or without normals); and it can produce open surfaces in cases where the input represents a sheet or partial surface.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/SACGAN_long.png">
<IMG width=200 src="pubs/images/SACGAN.png"></a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://www.sfu.ca/~hza162/">Hang Zhou</a>,
   <a href="https://ruim-jlu.github.io/">Rui Ma</a>, Lingxiao Zhang,
   <a href="http://geometrylearning.com/">Lin Gao</a>,
   <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>SAC-GAN: Structure-Aware Image Composition</strong>", 
   <i>IEEE Trans. on Visualization and Computer Graphics (TVCG)</i>, 2023.
   [<a href="https://arxiv.org/abs/2112.06596">arXiv</a> | <a href="pubs/haoz_paper_bib.html#sacgan_tvcg23">bibtex</a>]
<p>
We introduce an end-to-end learning framework for image-to-image composition, aiming to seamlessly compose an object represented as a cropped patch from an object image into a background scene image. As our approach emphasizes more on semantic and structural coherence of the composed images, rather than their pixel-level RGB accuracies, we tailor the input and output of our network with structure-aware features and design our network losses accordingly, with ground truth established in a self-supervised setting through the object cropping. Specifically, our network takes the semantic layout features from the input scene image, features encoded from the edges and silhouette in the input object patch, as well as a latent code as inputs, and generates a 2D spatial affine transform defining the translation and scaling of the object patch.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/CAPRINet_long.png">
<IMG width=200 src="pubs/images/CAPRINet.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="https://fenggenyu.github.io/">Fenggen Yu</a>,
   <a href="https://czq142857.github.io/">Zhiqin Chen</a>,
   <a href="https://manyili12345.github.io/">Manyi Li</a>, Aditya Sanghi, Hooman Shayani,
   <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly</strong>", <i>CVPR</i>, 2022.
   [<a href="https://arxiv.org/abs/2104.05652">arXiv</a> | <a href="pubs/haoz_paper_bib.html#caprinet_cvpr22">bibtex</a>]
<p>
We introduce CAPRI-Net, a neural network for learning compact and interpretable implicit representations of 3D
computer-aided design (CAD) models, in the form of adaptive primitive assemblies. Our network takes an input 3D
shape that can be provided as a point cloud or voxel grids, and reconstructs it by a compact assembly of quadric
surface primitives via constructive solid geometry (CSG) operations. The network is self-supervised with a reconstruction
loss, leading to faithful 3D reconstructions with sharp edges and plausible CSG trees, without any ground-truth shape
assemblies.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/UNIST_long.png">
<IMG width=200 src="pubs/images/UNIST.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="https://qiminchen.github.io/">Qimin Chen</a>, Johannes Merz,
   Aditya Sanghi, Hooman Shayani,
   <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>UNIST: Unpaired Neural Implicit Shape Translation Network</strong>", <i>CVPR</i>, 2022.
   [<a href="https://arxiv.org/abs/2112.05381">arXiv</a> | <a href="pubs/haoz_paper_bib.html#unist_cvpr22">bibtex</a>]
<p>
We introduce UNIST, the first deep neural implicit model for general-purpose, unpaired shape-to-shape translation, in both 2D and 3D domains. Our model is built on autoencoding implicit fields, rather than point clouds which represents the state of the art. Furthermore, our translation network is trained to perform the task over a latent grid representation which combines the merits of both latent-space processing and position awareness, to not only enable drastic shape transforms but also well preserve spatial features and fine local details for natural shape translations.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/RIMNet_long.png">
<IMG width=200 src="pubs/images/RIMNet.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="https://chengjieniu.github.io/">Chengjie Niu</a>,
   <a href="https://manyili12345.github.io/">Manyi Li</a>, 
   <a href="https://kevinkaixu.net/">Kai Xu</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures</strong>", <i>CVPR</i>, 2022.
   [<a href="https://arxiv.org/abs/2201.12763">arXiv</a> | <a href="pubs/haoz_paper_bib.html#rimnet_cvpr22">bibtex</a>]
<p>
We introduce RIM-Net, a neural network which learns recursive implicit fields for unsupervised inference of hierarchical 
shape structures. Our network recursively decomposes an input 3D shape into two parts, resulting in a binary tree hierarchy. 
Each level of the tree corresponds to an assembly of shape parts, represented as implicit functions, to reconstruct the 
input shape. At each node of the tree, simultaneous feature decoding and shape decomposition are carried out by their 
respective feature and part decoders, with weight sharing across the same hierarchy level ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/chen_BSPNET_long.png">
<IMG width=200 src="pubs/images/chen_BSPNET.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="https://czq142857.github.io/">Zhiqin Chen</a>, <a href="https://ai.google/research/people/106392/">Andrea Tagliasacchi</a>, and <strong>Hao Zhang</strong>,
   "<strong>Learning Mesh Representations via Binary Space Partitioning Tree Networks</strong>", <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</i> (invited and extended article from CVPR 2020 as Best Student Paper Award winner), 2022.
[<a href="https://arxiv.org/abs/2106.14274">arXiv</a> | <a href="https://bsp-net.github.io/">Project page (code+video)</a> | <a href="pubs/haoz_paper_bib.html#bspnet_pami21">bibtex</a>]
<p>
Polygonal meshes are ubiquitous, but have only played a relatively minor role in the deep learning revolution. State-of-the-art neural generative models for 3D shapes learn implicit functions and generate meshes via expensive iso-surfacing. We overcome these challenges by employing a classical spatial data structure from graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core operation of BSP involves recursive subdivision of 3D space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition without supervision. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built over a set of planes, where the planes and convexes are both defined by learned network weights.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/lin_DAP_long.png">
<IMG width=200 src="pubs/images/lin_DAP.png"></a>
</TD>
<TD vAlign=center align=left>
2. Liqiang Lin, Pengdi Huang, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>,
    <a href="https://kevinkaixu.net/">Kai Xu</a>, <strong>Hao Zhang</strong>, and <a href="https://vcc.tech/~huihuang">Hui Huang</a>,
   "<strong>One Point is All You Need: Directional Attention Point for Feature Learning</strong>", 
   <i>Science China Information Sciences (SCIS)</i>, 2022.
   [<a href="pubs/lin_LAP.pdf">PDF</a> | <a href="https://arxiv.org/abs/2012.06257">arXiv</a> | <a href="">bibtex</a>]
<p>
We present a novel attention-based mechanism for learning enhanced point features for tasks such as point cloud classification and segmentation. Our key message is that
if the right attention point is selected, then “one point is all you need” — not a sequence as in a recurrent model and not
a pre-selected set as in all prior works. Also, where the attention point is should be learned, from data and specific to
the task at hand. Our mechanism is characterized by a new and simple convolution, which combines the feature at an input point with the feature at its associated attention point. We call such a point adirectional attention point (DAP) ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/guan_tvcg20_fame.png">
<IMG width=200 src="pubs/images/guan_tvcg20_fame_small.png"></a>
</TD>
<TD vAlign=center align=left>
1. Yanran Guan, Han Liu, Kun Liu, <a href="http://kangxue.org/">Kangxue Yin</a>,
   <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>,
   <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>,
   Yan Zhang, Ersin Yumer, Nathan Carr, Radomir Mech, and
   <strong>Hao Zhang</strong>,
   "<strong>FAME: 3D Shape Generation via Functionality-Aware Model Evolution</strong>",
   <i>IEEE Trans. on Visualization and Computer Graphics (TVCG)</i>, Vol. 28, No. 4, pp. 1758-1772, 2022.
   [<a href="https://arxiv.org/abs/2005.04464">arXiv</a> |
    <a href="pubs/haoz_paper_bib.html#fame_tvcg22">bibtex</a>]
<p>
We introduce a modeling tool which can evolve a set of 3D objects in a functionality-aware manner. Our goal is for the evolution to generate large and diverse sets of plausible 3D objects for data augmentation, constrained modeling, as well as open-ended exploration to possibly inspire new designs. Starting with an initial population of 3D objects belonging to one or more functional categories, we evolve the shapes through part re-combination to produce generations of hybrids or crossbreeds between parents from the heterogeneous shape collection  ...
</TD>
</TR>

</table>

<a name="papers2021">

<h3>2021</h3>

<table cellPadding=5 width="99%" border=0>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/nmc_long.png">
<IMG width=200 src="pubs/images/nmc.png"></a>
</TD>
<TD vAlign=center align=left>
11. <a href="https://czq142857.github.io/">Zhiqin Chen</a> and 
   <strong>Hao Zhang</strong>,
   "<strong>Neural Marching Cubes</strong>", ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 40, No. 6, 2021.
   [<a href="https://arxiv.org/abs/2106.11272">arXiv</a> | <a href="https://github.com/czq142857/NMC">code</a> | <a href="pubs/haoz_paper_bib.html#NMC_siga21">bibtex</a>]
<p>
We introduce Neural Marching Cubes (NMC), a data-driven approach for extracting a triangle mesh from a discretized implicit field. We re-cast MC from a deep learning perspective, by designing tessellation templates more apt at preserving geometric features, and learning the vertex positions and mesh topologies from training meshes, to account for contextual information from nearby cubes. We develop a compact per-cube parameterization to represent the output triangle mesh, while being compatible with neural processing, so that a simple 3D convolutional network can be employed for the training. We evaluate our neural MC approach by quantitative and qualitative comparisons to all well-known MC variants, demonstrating its superiority in faithful reconstruction of sharp features and mesh topology.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/RaidaR_long.png">
<IMG width=200 src="pubs/images/RaidaR.png"></a>
</TD>
<TD vAlign=center align=left>
10. Jiongchao Jin, Arezou Fatemi (equal contribution),
   <a href="http://www.sfu.ca/~wpintoli/">Wallace Lira</a>,
   <a href="https://fenggenyu.github.io/">Fenggen Yu</a>,
   Biao Leng, <a href="https://ruim-jlu.github.io/">Rui Ma</a>,
   <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes</strong>", 
   <a href="https://avvision.xyz/iccv21/">Second ICCV Workshop on Autonomous Vehicle Vision (AVVision)</a>, 2021.
   [<a href="https://raidar-dataset.com/">dataset</a> | <a href="https://arxiv.org/abs/2104.04606">arXiv</a> | <a href="pubs/haoz_paper_bib.html#raidar_iccvw21">bibtex</a>]
<p>
We introduce RaidaR, a rich annotated image dataset of rainy street scenes, to support autonomous driving research. The new dataset contains the largest number of rainy images (58,542) to date, 5,000 of which provide semantic segmentations and 3,658 provide object instance segmentations. The RaidaR images cover a wide range of realistic rain-induced artifacts, including fog, droplets, and road reflections, which can effectively augment existing street scene datasets to improve data-driven machine perception during rainy weather.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/gao_TMNET_long.png">
<IMG width=200 src="pubs/images/gao_TMNET.png"></a>
</TD>
<TD vAlign=center align=left>
9. <a href="http://geometrylearning.com/">Lin Gao</a>,
    Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, <a href="http://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>, and <strong>Hao Zhang</strong>,
   "<strong>TM-NET: Deep Generative Networks for Textured Meshes</strong>", ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 40, No. 6, 2021.
   [<a href="https://arxiv.org/abs/2010.06217">arXiv</a> | <a href="http://geometrylearning.com/TM-NET/">project page</a> | <a href="pubs/haoz_paper_bib.html#TMNet_siga21">bibtex</a>]
<p>
We introduce TM-NET, a novel deep generative model for synthesizing textured meshes in a part-aware manner. Once trained, the network can generate novel textured meshes from scratch or predict textures for a given 3D mesh, without image guidance. Plausible and diverse textures can be generated for the same mesh part, while texture compatibility between parts in the same shape is achieved via conditional generation. Specifically, our method produces texture maps for individual shape parts, each as a deformable box, leading to a natural UV map with minimal distortion. The network separately embeds part geometry (via a PartVAE) and part texture (via a TextureVAE) into their respective latent spaces ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zhang_drone_long.png">
<IMG width=200 src="pubs/images/zhang_drone.png"></a>
</TD>
<TD vAlign=center align=left>
8. Han Zhang, Yusong Yao, Ke Xie, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>,
   <strong>Hao Zhang</strong>, and <a href="https://vcc.tech/~huihuang">Hui Huang</a>,
   "<strong>Continuous Aerial Path Planning for 3D Urban Scene Reconstruction</strong>", ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 40, No. 6, 2021.
   [<a href="https://vcc.tech/research/2021/DronePath">Project page</a> | <a href="pubs/haoz_paper_bib.html#DronePath_siga21">bibtex</a>]
<p>We introduce a path-oriented drone trajectory planning algorithm, which performs continuous image acquisition along an aerial path, aiming to optimize both the scene reconstruction quality and path quality. Specifically, our method takes as input a rough 3D scene proxy and produces a drone trajectory and image capturing setup, which efficiently yields a high-quality
reconstruction of the 3D scene based on three optimization objectives: one
maximize the amount of 3D scene information that can be acquired along
the entirety of the trajectory, another one to optimize the scene capturing
efficiency by maximizing the scene information that can be acquired per
unit length along the aerial path, and the last one to minimize the total
turning angles along the aerial path, so as to reduce the number of sharp turns.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/3DFRONT_long.png">
<IMG width=200 src="pubs/images/3DFRONT.png"></a>
</TD>
<TD vAlign=center align=left>
7. Huan Fu, Bowen Cai, <a href="http://geometrylearning.com/">Lin Gao</a>,
    Lingxiao Zhang, Cao Li, Zengqi Xun, Chengyue Sun, Yiyun Fei, Yu Zheng, Ying Li, Yi Liu, Peng Liu, Lin Ma, Le Weng,
    Xiaohang Hu, Xin Ma, Qian Qian, Rongfei Jia, Binqiang Zhao, <!-- <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, -->
    and <strong>Hao Zhang</strong>,
   "<strong>3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics</strong>", ICCV, 2021.
   [<a href="https://arxiv.org/abs/2011.09127">arXiv</a> | <a href="pubs/haoz_paper_bib.html#3DFRONT_iccv21">bibtex</a>]
<p>
We introduce 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics), a new, large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility. From layout semantics down to texture details of individual objects, our dataset is freely available to the academic community and beyond. Currently, 3D-FRONT contains 18,797 rooms diversely furnished by 3D objects, far surpassing all publicly available scene datasets. In addition, the 7,302 furniture objects all come with high-quality textures ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/gal_MRGAN_long.png">
<IMG width=200 src="pubs/images/gal_MRGAN.png"></a>
</TD>
<TD vAlign=center align=left>
6. Rinon Gal, Amit Bermano, <strong>Hao Zhang</strong>, and <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   "<strong>MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part Disentanglement</strong>", 
   <a href="https://geometry.stanford.edu/struco3d/">ICCV Workshop on Structural and Compositional Learning on 3D Data (StruCo3D)</a>, 2021.
   [<a href="https://arxiv.org/abs/2007.12944">arXiv</a> | <a href="pubs/haoz_paper_bib.html#mrgan_iccvw21">bibtex</a>]
<p>
We present MRGAN, a multi-rooted adversarial network which generates part-disentangled 3D point-cloud shapes without part-based shape supervision. The network fuses multiple branches of tree-structured graph convolution layers which produce point clouds, with learnable constant inputs at the tree roots. Each branch learns to grow a different shape part, offering control over the shape generation at the part level. Our network encourages disentangled generation of semantic parts via two key ingredients: a root-mixing training strategy which helps decorrelate the different branches to facilitate disentanglement, and a set of loss terms designed with part disentanglement and shape semantics in mind.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_D2IM_long.png">
<IMG width=200 src="pubs/images/li_D2IM.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="https://manyili12345.github.io/">Manyi Li</a> and <strong>Hao Zhang</strong>,
   "<strong>D^2IM-Net: Learning Detail Disentangled Implicit Fields from Single Images</strong>", CVPR, 2021.
   [<a href="https://arxiv.org/abs/2012.06650">arXiv</a> | <a href="pubs/haoz_paper_bib.html#D2IMNET_cvpr21">bibtex</a>]
<p>
We present the first single-view 3D reconstruction network aimed at recovering geometric details from an input image which encompass both topological shape structures and surface features. Our key idea is to train the network to learn a detail disentangled reconstruction consisting of two functions, one implicit field representing the coarse 3D shape and the other capturing the details. Given an input image, our network, coined D2IM-Net, encodes it into global and local features which are respectively fed into two decoders. The base decoder uses the global features to reconstruct a coarse implicit field, while the detail decoder reconstructs, from the local features, two displacement maps, defined over the front and back sides of the captured object. The final 3D reconstruction is a fusion between the base shape and the displacement maps, with three losses enforcing the recovery of coarse shape, overall structure, and surface details via a novel Laplacian term.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/chen_DECOR_long.png">
<IMG width=200 src="pubs/images/chen_DECOR.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="https://czq142857.github.io/">Zhiqin Chen</a>,
    <a href="http://www.vovakim.com/">Vladimir Kim</a>,
    <a href="https://techmatt.github.io/">Matthew Fisher</a>,
    <a href="https://noamaig.github.io/">Noam Aigerman</a>, <strong>Hao Zhang</strong>, and
    <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
   "<strong>DECOR-GAN: 3D Shape Detailization by Conditional Refinement</strong>", CVPR <i>oral</i>, 2021.
   [<a href="https://arxiv.org/abs/2012.09159">arXiv</a> | <a href="pubs/haoz_paper_bib.html#DecorGAN_cvpr21">bibtex</a>]
<p>
We introduce a deep generative network for 3D shape detailization, akin to stylization with the style being geometric details. We address the challenge of creating large varieties of high-resolution and detailed 3D geometry from a small set of exemplars by treating the problem as that of geometric detail transfer. Given a low-resolution coarse voxel shape, our network refines it, via voxel upsampling, into a higher-resolution shape enriched with geometric details. The output shape preserves the overall structure (or content) of the input, while its detail generation is conditioned on an input "style code" corresponding to a detailed exemplar.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/patil_LayoutGMN_long.png">
<IMG width=200 src="pubs/images/patil_LayoutGMN.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, <a href="https://manyili12345.github.io/">Manyi Li</a>,
    <a href="https://research.adobe.com/person/matt-fisher/">Matthew Fisher</a>,
    <a href="http://msavva.github.io/">Manolis Savva</a>, and <strong>Hao Zhang</strong>,
   "<strong>LayoutGMN: Neural Graph Matching for Structural Layout Similarity</strong>", CVPR, 2021.
   [<a href="https://arxiv.org/abs/2012.06547">arXiv</a> | <a href="pubs/haoz_paper_bib.html#LayoutGMN_cvpr21">bibtex</a>]
<p>
We present a deep neural network to predict structural similarity between 2D layouts by leveraging Graph Matching Networks (GMN). Our network, coined LayoutGMN, learns the layout metric via neural graph matching, using an attention-based GMN designed under a triplet network setting. To train our network, we utilize weak labels obtained by pixel-wise Intersection-over-Union (IoUs) to define the triplet loss. Importantly, LayoutGMN is built with a structural bias which can effectively compensate for the lack of structure awareness in IoUs.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/qian_RoofGAN_long.png">
<IMG width=200 src="pubs/images/qian_RoofGAN.png"></a>
</TD>
<TD vAlign=center align=left>
2. <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>, <strong>Hao Zhang</strong>, and <a href="https://www2.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a>, 
   "<strong>Roof-GAN: Learning to Generate Roof Geometry and Relations for Residential Houses</strong>", CVPR, 2021.
   [<a href="https://arxiv.org/abs/2012.09340">arXiv</a> | <a href="pubs/haoz_paper_bib.html#RoofGAN_cvpr21">bibtex</a>]
<p>
This paper presents Roof-GAN, a novel generative adversarial network that generates structured geometry of residential roof structures as a set of roof primitives and their relationships. Given the number of primitives, the generator produces a structured roof model as a graph, which consists of 1) primitive geometry as raster images at each node, encoding facet segmentation and angles; 2) inter-primitive colinear/coplanar relationships at each edge; and 3) primitive geometry in a vector format at each node, generated by a novel differentiable vectorizer while enforcing the relationships.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/or_BalaGAN_long.png">
<IMG width=200 src="pubs/images/or_BalaGAN.png"></a>
</TD>
<TD vAlign=center align=left>
1. Or Patashnik, Dov Danon, <strong>Hao Zhang</strong>, and <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   "<strong>BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer</strong>", CVPR Workshop on 
   Learning from Limited and Imperfect Data (L2ID), 2021.
   [<a href="https://orpatashnik.github.io/BalaGAN/">Project page</a> | <a href="https://arxiv.org/abs/2010.02036">arXiv</a> | <a href="pubs/haoz_paper_bib.html#BalaGAN_cvprw21">bibtex</a>]
<p>
State-of-the-art image-to-image translation methods tend to struggle in an imbalanced domain setting, where one image domain lacks richness and diversity. We introduce a new unsupervised translation network, BalaGAN, specifically designed to tackle the domain imbalance problem. We leverage the latent modalities of the richer domain to turn the image-to-image translation problem, between two imbalanced domains, into a balanced, multi-class, and conditional translation problem, more resembling the style transfer setting. Specifically, we analyze the source domain and learn a decomposition of it into a set of latent modes or classes, without any supervision. This leaves us with a multitude of balanced cross-domain translation tasks, between all pairs of classes, including the target domain. During inference, the trained network takes as input a source image, as well as a reference or style image from one of the modes as a condition, and produces an image which resembles the source on the pixel-wise level, but shares the same mode as the reference.
</TD></TR>

</table>

<a name="papers2020">

<h3>2020</h3>

<table cellPadding=5 width="99%" border=0>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/wang_PIE_long.png">
<IMG width=200 src="pubs/images/wang_PIE.png"></a>
</TD>
<TD vAlign=center align=left>
13. Xiaogang Wang, Yuelang Xu, <a href="https://kevinkaixu.net/">Kai Xu</a>, <a href="https://research.google/people/106392/">Andrea Tagliasacchi</a>,
   Bin Zhou, <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>, and <strong>Hao Zhang</strong>,
   "<strong>PIE-NET: Parametric Inference of Point Cloud Edges</strong>", <i>NeurIPS</i>, 2020.
   [<a href="https://arxiv.org/abs/2007.04883">arXiv</a> | <a href="pubs/haoz_paper_bib.html#PIE_nips20">bibtex</a>]
<p>
We introduce an end-to-end learnable technique to robustly identify feature edges in 3D point cloud data. We represent these edges as a collection 
of parametric curves (i.e., lines, circles, and B-splines). Accordingly, our deep neural network, coined PIE-NET, is trained for parametric inference 
of edges. The network is trained on the ABC dataset and relies on a "region proposal" architecture, where a first module proposes an over-complete 
collection of edge and corner points, and a second module ranks each proposal to decide whether it should be considered.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/yin_COALESCE_long.png">
<IMG width=200 src="pubs/images/yin_COALESCE.png"></a>
</TD>
<TD vAlign=center align=left>
12. <a href="http://kangxue.org/">Kangxue Yin</a>, <a href="https://czq142857.github.io/">Zhiqin Chen</a>,
   <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
   <a href="https://techmatt.github.io/">Matt Fisher</a>, <a href="http://www.vovakim.com/">Vladimir Kim</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>COALESCE: Component Assembly by Learning to Synthesize Connections</strong>", 3D Vision (3DV) <i>oral</i>, 2020.
   [<a href="https://arxiv.org/abs/2008.01936">arXiv</a> | <a href="pubs/haoz_paper_bib.html#COAL_3dv20">bibtex</a>]
<p>
We introduce COALESCE, the first data-driven framework for component-based shape assembly which employs deep learning to synthesize part connections. To handle geometric and topological mismatches between parts, we remove the mismatched portions via erosion, and rely on a joint synthesis step, which is learned from data, to fill the gap and arrive at a natural and plausible part joint. Given a set of input parts extracted from different objects, COALESCE automatically aligns them and synthesizes plausible joints to connect the parts into a coherent 3D object represented by a mesh. The joint synthesis network, designed to focus on joint regions, reconstructs the surface between the parts by predicting an implicit shape representation that agrees with existing parts, while generating a smooth and topologically meaningful connection.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/ali_vdac_long.png">
<IMG width=200 src="pubs/images/ali_vdac.png"></a>
</TD>
<TD vAlign=center align=left>
11. <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>, 
    <a href="https://fenggenyu.github.io/">Fenggen Yu</a>,
    <a href="https://haisenzhao.github.io/">Haisen Zhao</a>, 
    <a href="https://homes.cs.washington.edu/~adriana/">Adriana Schulz</a>,
    and <strong>Hao Zhang</strong>,
   "<strong>VDAC: Volume Decompose-and-Carve for Subtractive Manufacturing</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 39, No. 6, 2020.
   [<a href="pubs/mahdavi_siga21_VDAC.pdf">PDF</a> | <a href="https://sites.google.com/site/alimahdaviamiri/projects/vdac?authuser=0">Project page</a> | <a href="pubs/haoz_paper_bib.html#vdac_siga20">bibtex</a>]
<p>
We introduce <i>carvable volume decomposition</i> for efficient 3-axis CNC machining of 3D freeform objects,
where our goal is to develop a fully automatic method to jointly optimize setup and path planning. 
We formulate our joint optimization as a volume decomposition problem which prioritizes minimizing the number 
of setup directions while striving for a minimum number of continuously carvable volumes, where a 3D volume
is continuously carvable, or simply carvable, if it can be carved with the machine cutter traversing a single 
continuous path. Geometrically, carvability combines visibility and monotonicity and presents a new shape property 
which had not been studied before.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/hu_tap_long.png">
<IMG width=200 src="pubs/images/hu_tap.png"></a>
</TD>
<TD vAlign=center align=left>
10. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Juzhan Xu, Bin Chen,
   <a href="http://socs.uoguelph.ca/~minglun/">Minglun Gong</a>, <strong>Hao Zhang</strong>,
   and <a href="http://vcc.szu.edu.cn/_huihuang.html">Hui Huang</a>,
   "<strong>TAP-Net: Transport-and-Pack using Reinforcement Learning</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 39, No. 6, 2020.
   [<a href="https://vcc.tech/research/2020/TAP">Project page</a> | 
    <a href="https://arxiv.org/abs/2009.01469">arXiv</a> | 
    <a href="pubs/haoz_paper_bib.html#tap_siga20">bibtex</a>]
<p>
We introduce the transport-and-pack (TAP) problem, a frequently encountered instance of real-world packing, 
and develop a neural optimization solution based on reinforcement learning. Given an initial spatial 
configuration of boxes, we seek an efficient method to iteratively transport and pack the boxes compactly 
into a target container. Due to obstruction and accessibility constraints, our problem has to add a new 
search dimension, i.e., finding an optimal transport sequence, to the already immense search space for 
packing alone. Using a learning-based approach, a trained network can learn and encode solution patterns 
to guide the solution of new problem instances instead of executing an expensive online search.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/yi_branchGAN_long.png">
<IMG width=200 src="pubs/images/yi_branchGAN.png"></a>
</TD>
<TD vAlign=center align=left>
9. Zili Yi, <a href="http://www.sfu.ca/~zhiqinc/">Zhiqin Chen</a>, Hao Cai, Wendong Mao,
   <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>, and <strong>Hao Zhang</strong>,
   "<strong>BSD-GAN: Branched Generative Adversarial Networks for Scale-Disentangled Learning and Synthesis of Images</strong>",
   <i>IEEE Trans. on Image Processing</i>, Vol. 29, pp. 9073-9083, 2020.
   [<a href="https://arxiv.org/abs/1803.08467">arXiv</a> | <a href="https://github.com/duxingren14/BranchGAN">code</a> |
    <a href="pubs/haoz_paper_bib.html#bsdgan_tip20">bibtex</a>]
<p>
We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively "de-freeze" the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are 
added.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/lira_GANH_long.png">
<IMG width=200 src="pubs/images/lira_GANH.png"></a>
</TD>
<TD vAlign=center align=left>
8. <a href="http://www.sfu.ca/~wpintoli/">Wallace Lira</a>, Johannes Merz, <a href="https://dritchie.github.io/">Daniel Ritchie</a>,
   <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and <strong>Hao Zhang</strong>,
   "<strong>GANHopper: Multi-Hop GAN for Unsupervised Image-to-Image Translation</strong>", ECCV, 2020.
   [<a href="https://arxiv.org/abs/2002.10102">arXiv</a> | <a href="https://www.youtube.com/watch?v=jQ-dxwgBm3Q">Youtube video</a> | <a href="pubs/haoz_paper_bib.html#GANH_eccv20">bibtex</a>]
<p>
We introduce GANhopper, an unsupervised image-to-image translation network that transforms images gradually between two domains, through multiple hops. Instead of executing translation directly, we steer the translation by requiring the network to produce in-between images which resemble weighted hybrids between images from the two input domains. Our network is trained on unpaired images from the two domains only, without any in-between images. All hops are produced using a single generator along each direction. In addition to the standard cycle-consistency and adversarial losses, we introduce a new hybrid discriminator, which is trained to classify the intermediate images produced by the generator as weighted hybrids, with weights based on a predetermined hop count.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/jin_DRKFD_long.png">
<IMG width=200 src="pubs/images/jin_DRKFD.png"></a>
</TD>
<TD vAlign=center align=left>
7. Jiongchao Jin, <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, Zhang Xiong, and <strong>Hao Zhang</strong>,
   "<strong>DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape Reconstruction</strong>", ECCV, 2020.
   [<a href="https://arxiv.org/abs/1911.09204">arXiv</a> | <a href="pubs/haoz_paper_bib.html#DRKFS_eccv20">bibtex</a>]
<p>
We introduce a differential visual similarity metric to train deep neural networks for 3D reconstruction, aimed at improving reconstruction quality. The metric compares two 3D shapes by measuring distances between multi-view images differentiably rendered from the shapes. Importantly, the image-space distance is also differentiable and measures visual similarity, rather than pixel-wise distortion. Specifically, the similarity is defined by mean-squared errors over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape similarity metric can be easily plugged into various 3D reconstruction networks, replacing their distortion-based losses, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstructions with better structural fidelity and visual quality.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/xu_tiling_long.png">
<IMG width=200 src="pubs/images/xu_tiling.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="https://appsrv.cse.cuhk.edu.hk/~haoxu/index.html">Hao Xu</a>, Ka Hei Hui,
   <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, and <strong>Hao Zhang</strong>,
   "<strong>TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 39, No. 4, 2020.
   [<a href="https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/">Project page</a> | <a href="https://arxiv.org/abs/2007.02278">arXiv</a> | 
    <a href="https://github.com/xuhaocuhk/TilinGNN/">Code</a> | <a href="pubs/haoz_paper_bib.html#tiling_sig20">bibtex</a>]
<p>
We introduce the first neural optimization framework to solve a classical instance of 
the tiling problem. Namely, we seek a non-periodic tiling of an arbitrary 2D shape 
using one or more types of tiles: the tiles maximally fill the shape’s interior without 
overlaps or holes. To start, we reformulate tiling as a graph problem by modeling 
candidate tile locations in the target shape as graph nodes and connectivity between 
tile locations as edges. We build a graph convolutional neural network, coined TilinGNN, 
to progressively propagate and aggregate features over graph edges and predict tile 
placements. Our network is self-supervised and trained by maximizing the tiling coverage 
on target shapes, while avoiding overlaps and holes between the tiles. After training, 
TilinGNN has a running time that is roughly linear to the number of candidate tile 
locations, significantly outperforming traditional combinatorial search.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/hu_gtop_long.png">
<IMG width=200 src="pubs/images/hu_gtop.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Zeyu Huang, Yuhan Tang,
   <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>, <strong>Hao Zhang</strong>,
   and <a href="http://vcc.szu.edu.cn/_huihuang.html">Hui Huang</a>,
   "<strong>Graph2Plan: Learning Floorplan Generation from Layout Graphs</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 39, No. 4, 2020.
   [<a href="https://vcc.tech/research/2020/Graph2Plan">Project page</a> | <a href="https://arxiv.org/abs/2004.13204">arXiv</a> | <a href="pubs/haoz_paper_bib.html#gtop_sig20">bibtex</a>]
<p>
We introduce a learning framework for automated floorplan generation which combines generative modeling using deep 
neural networks and user-in-the-loop designs to enable human users to provide sparse design constraints. Such
constraints are represented by a layout graph. The core component of our learning framework is a deep neural 
network, Graph2Plan, which is trained on RPLAN, a large-scale dataset consisting of 80K annotated, human-designed
floorplans. The network converts a layout graph, along with a building boundary, into a floorplan that 
fulfills both the layout and boundary constraints.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/chen_BSPNET_long.png">
<IMG width=200 src="pubs/images/chen_BSPNET.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="https://czq142857.github.io/">Zhiqin Chen</a>, <a href="https://ai.google/research/people/106392/">Andrea Tagliasacchi</a>, and <strong>Hao Zhang</strong>,
   "<strong>BSP-Net: Generating Compact Meshes via Binary Space Partitioning</strong>", CVPR <i>oral</i>, 2020. <b>Best Student Paper Award.</b>
[<a href="https://bsp-net.github.io/">Project page (code+video)</a> | <a href="https://arxiv.org/abs/1911.06971">arXiv</a> | <a href="pubs/haoz_paper_bib.html#bspnet_cvpr20">bibtex</a>]
<p>
Polygonal meshes are ubiquitous in the digital 3D domain. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zhu_adacoseg_long.png">
<IMG width=200 src="pubs/images/zhu_adacoseg.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.sfu.ca/~cza68/">Chenyang Zhu</a>, <a href="https://kevinkaixu.net/">Kai Xu</a>,
   <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>, <a href="https://cs.stanford.edu/~ericyi/">Li Yi</a>,
    <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>, and <strong>Hao Zhang</strong>,
   "<strong>AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</strong>", CVPR <i>oral</i>, 2020.
   [<a href="">Project page</a> | <a href="https://arxiv.org/abs/1903.10297">arXiv</a> | <a href="pubs/haoz_paper_bib.html#adacoseg_cvpr20">bibtex</a>]
<p>
We introduce AdaSeg, a deep neural network architecture for adaptive co-segmentation of a set of 3D shapes represented as point clouds. Differently from the familiar
 single-instance segmentation problem, co-segmentation is
intrinsically contextual: how a shape is segmented can vary
depending on the set it is in. Hence, our network features
an adaptive learning module to produce a consistent shape
segmentation which adapts to a set.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/wu_PQNET_long.png">
<IMG width=200 src="pubs/images/wu_PQNET.png"></a>
</TD>
<TD vAlign=center align=left>
2. <a href="https://chriswu1997.github.io/">Rundi Wu</a>, Yixin Zhuang, <a href="https://kevinkaixu.net/">Kai Xu</a>, <strong>Hao Zhang</strong>, and <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
   "<strong>PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes</strong>", CVPR, 2020.
   [<a href="https://arxiv.org/abs/1911.10949">arXiv</a> | <a href="pubs/haoz_paper_bib.html#pqnet_cvpr20">bibtex</a>]
<p>
We introduce PQ-NET, a deep neural network which represents and generates 3D shapes via sequential part assembly. The input to our network is a 3D shape segmented into parts, where each part is first encoded into a feature representation using a part autoencoder. The core component of PQ-NET is a sequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part features into a latent vector of fixed size, and the decoder reconstructs the 3D shape, one part at a time, resulting in a sequential assembly. The latent space formed by the Seq2Seq encoder encodes both part structure and fine part geometry. The decoder can be adapted to perform several generative tasks including shape autoencoding, interpolation, novel shape generation, and single-view 3D reconstruction, where the generated shapes are all composed of meaningful parts.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/egstar_2020_long.png">
<IMG width=200 src="pubs/images/egstar_2020.png"></a>
</TD>
<TD vAlign=center align=left>
1. <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>, 
   <a href="https://dritchie.github.io/">Daniel Ritchie</a>, 
   <a href="https://jiajunwu.com/">Jiajun Wu</a>,
   <a href="https://kevinkaixu.net/">Kai Xu</a>, and <strong>Hao Zhang</strong>,
   "<strong>Learning Generative Models of 3D Structures</strong>", 
   <i>Computer Graphics Forum (Eurographics STAR)</i>, 2020.
   [<a href="">Project page</a> | <a href="pubs/egstar2020.pdf">PDF</a> | <a href="pubs/haoz_paper_bib.html#egstar_2020">bibtex</a>]
<p>
3D models of objects and scenes are critical to many academic disciplines and industrial applications. Of particular interest is the emerging opportunity for 3D graphics to serve artificial intelligence: computer vision systems can benefit from synthetically- generated training data rendered from virtual 3D scenes, and robots can be trained to navigate in and interact with real-world environments by first acquiring skills in simulated ones. One of the most promising ways to achieve this is by learning and applying generative models of 3D content: computer programs that can synthesize new 3D shapes and scenes. To allow users to edit and manipulate the synthesized 3D content to achieve their goals, the generative model should also be structure-aware: it should express 3D shapes and scenes using abstractions that allow manipulation of their high-level structure. This state-of-the- art report surveys historical work and recent progress on learning structure-aware generative models of 3D shapes and scenes.
</TD></TR>

</table>

<a name="papers2019">

<h3>2019</h3>

<table cellPadding=5 width="99%" border=0>

<!--

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/jin_DRKFD_long.png">
<IMG width=200 src="pubs/images/jin_DRKFD.png"></a>
</TD>
<TD vAlign=center align=left>
13. Jiongchao Jin, <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, and <strong>Hao Zhang</strong>,
   "<strong>DR-KFD: A Differentiable Visual Metric for 3D Shape Reconstruction</strong>", 2019.
   [<a href="">Project page</a> | <a href="https://arxiv.org/abs/1911.09204">arXiv</a> | <a href="">bibtex</a>]
<p>
We advocate the use of differential visual shape metrics to train deep neural networks for 3D reconstruction. We introduce such a metric which compares two 3D shapes by measuring visual, image-space differences between multiview images differentiably rendered from the shapes. Furthermore, we develop a differentiable image-space distance based on mean-squared errors defined over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape metric can be easily plugged into various reconstruction networks, replacing the object-space distortion measures, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstruction results with better structural fidelity and visual quality.
</TD></TR>

<p>

-->

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/yin_LOGAN_long.png">
<IMG width=200 src="pubs/images/yin_LOGAN.png"></a>
</TD>
<TD vAlign=center align=left>
11. <a href="http://kangxue.org/">Kangxue Yin</a>, 
    <a href="http://www.sfu.ca/~zhiqinc/">Zhiqin Chen</a>, 
    <a href="http://vcc.szu.edu.cn/_huihuang.html">Hui Huang</a>,
   <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and <strong>Hao Zhang</strong>,
   "<strong>LOGAN: Unpaired Shape Transform in Latent Overcomplete Space</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 38, No. 6, Article 198, 2019. <b>One of six papers selected for press release at SIGGRAPH Asia.</b>
   [<a href="https://arxiv.org/abs/1903.10170">arXiv</a> | <a href="https://github.com/kangxue/LOGAN">code</a> | <a href="pubs/haoz_paper_bib.html#logan_siga19">bibtex</a>]
<p>
We introduce LOGAN, a deep neural network aimed at learning general-purpose shape transforms 
from unpaired domains. The network is trained on two sets of shapes, e.g., tables and chairs, while 
there is neither a pairing between shapes from the domains as supervision nor any point-wise correspondence 
between any shapes. Once trained, LOGAN takes a shape from one domain and transforms it into the 
other. Our network consists of an autoencoder to encode shapes from the two input domains into a 
common latent space, where the latent codes concatenate multi-scale shape features, resulting in
an overcomplete representation. The translator is based on a latent generative adversarial network (GAN), 
where an adversarial loss enforces cross-domain translation while a feature
preservation loss ensures that the right shape features are preserved for a natural shape transform.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/gao_SDM_long.png">
<IMG width=200 src="pubs/images/gao_SDM.png"></a>
</TD>
<TD vAlign=center align=left>
10. <a href="http://geometrylearning.com">Lin Gao</a>, Jie Yang, Tong Wu, Yu-Jie Yuan,
   <a href="http://sweb.cityu.edu.hk/hongbofu/">Hongbo Fu</a>, <a href="http://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>SDM-NET: Deep Generative Network for Structured Deformable Mesh</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 38, No. 6, Article 243, 2019.
   [<a href="http://geometrylearning.com/sdm-net/">Project page</a> | <a href="https://arxiv.org/abs/1908.04520">arXiv</a> | <a href="pubs/haoz_paper_bib.html#sdmnet_siga19">bibtex</a>]
<p>
We introduce SDM-NET, a deep generative neural network which produces structured deformable meshes. Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respect the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by deforming the box. The architecture of SDM-NET is that of a two-level variational autoencoder (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which jointly learns the part structure of a shape collection and the part geometries, ensuring a coherence between global shape structure and surface details.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/xu_LEGO_long.png">
<IMG width=200 src="pubs/images/xu_LEGO.png"></a>
</TD>
<TD vAlign=center align=left>
9. <a href="https://appsrv.cse.cuhk.edu.hk/~haoxu/index.html">Hao Xu</a>, Ka Hei Hui, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, and <strong>Hao Zhang</strong>,
   "<strong>Computational LEGO Technic Design</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 38, No. 6, Article 196, 2019. <b>Included in the SIGGRAPH Asia 2019 technical paper trailer.</b>
   [<a href="http://appsrv.cse.cuhk.edu.hk/~haoxu/projects/compute_technic/">Project page</a> | <a href="pubs/haoz_paper_bib.html#LEGO_siga19">bibtex</a>]
<p>
We introduce a method to automatically compute LEGO Technic models from user input sketches, optionally with motion annotations. The generated models resemble the input sketches with coherently-connected bricks and simple layouts, while respecting the intended symmetry and mechanical properties expressed in the inputs. This complex computational assembly problem involves an immense search space, and a much richer brick set and connection mechanisms than regular LEGO. To address it, we first comprehensively model the brick properties and connection mechanisms, then formulate the construction requirements into an objective function, accounting for faithfulness to input sketch, model simplicity, and structural integrity. Next, we model the problem as a sketch cover, where we iteratively refine a random initial layout to cover the input sketch, while guided by the objective. At last, we provide a working system to analyze the balance, stress, and assemblability of the generated model.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/yan_RPM_long.png">
<IMG width=200 src="pubs/images/yan_RPM.png"></a>
</TD>
<TD vAlign=center align=left>
8. Zhihao Yan, <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Xingguang Yan, Luanmin Chen, 
   <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>, 
   <strong>Hao Zhang</strong>, and <a href="http://vcc.szu.edu.cn/_huihuang.html">Hui Huang</a>,
   "<strong>RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 38, No. 6, Article 240, 2019.
   [<a href="https://vcc.tech/research/2019/RPMNet">Project page</a> | <a href="pubs/haoz_paper_bib.html#rpmnet_siga19">bibtex</a>]
<p>
We introduce RPM-Net, a deep learning-based approach which simultaneously infers movable parts and hallucinates their motions from a single, un-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel Recurrent Neural Network (RNN), composed of an encoder-decoder pair with interleaved Long Short-Term Memory (LSTM) components, which together predict a temporal sequence of point-wise displacements for the input shape. At the same time, the displacements allow the network to learn moveable parts, resulting in a motion-based shape segmentation. Recursive applications of RPM-Net on the obtained parts can predict finer-level part motions, resulting in a hierarchical object segmentation. Furthermore, we develop a separate network to estimate part mobilities, e.g., per part motion parameters, from the segmented motion sequence.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/chen_BAENET_long.png">
<IMG width=200 src="pubs/images/chen_BAENET.png"></a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://www.sfu.ca/~zhiqinc/">Zhiqin Chen</a>, <a href="http://kangxue.org/">Kangxue Yin</a>,
   <a href="https://techmatt.github.io/">Matt Fisher</a>, <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>BAE-NET: Branched Autoencoder for Shape Co-Segmentation</strong>", ICCV, 2019.
   [<a href="https://arxiv.org/abs/1903.11228">arXiv</a> | <a href="https://github.com/czq142857/BAE-NET">code</a> | <a href="pubs/haoz_paper_bib.html#baenet_iccv19">bibtex</a>]
<p>
We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with all shapes in an input collection using a shape reconstruction loss, without ground-truth segmentations. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/schor_partAE_long.png">
<IMG width=200 src="pubs/images/schor_partAE.png"></a>
</TD>
<TD vAlign=center align=left>
6. Nadav Schor, Oren Katzier, <strong>Hao Zhang</strong>, and <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   "<strong>CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition</strong>", ICCV, 2019.
   [<a href="https://arxiv.org/abs/1811.07441">arXiv</a> | <a href="https://github.com/nschor/CompoNet">code</a> | <a href="pubs/haoz_paper_bib.html#partgen_iccv19">bibtex</a>]
<p>
Data-driven generative modeling has made remarkable
progress by leveraging the power of deep neural networks.
A reoccurring challenge is how to sample a rich variety
of data from the entire target distribution, rather than only
from the distribution of the training data. In other words, we
would like the generative model to go beyond the observed
training samples and learn to also generate "unseen" data.
In our work, we present a generative neural network for
shapes that is based on a part-based prior, where the key
idea is for the network to synthesize shapes by varying both
the shape parts and their compositions.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/chen_cvpr19_IMGAN_long.png">
<IMG width=200 src="pubs/images/chen_cvpr19_IMGAN.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://www.sfu.ca/~zhiqinc/">Zhiqin Chen</a> and <strong>Hao Zhang</strong>,
   "<strong>Learning Implicit Fields for Generative Shape Modeling</strong>", <i>CVPR</i>,
   <a href="https://arxiv.org/abs/1812.02822"><i>arXiv:1812.02822</i></a>, 2019.
   [<a href="pubs/chen_cvpr19_imgan.pdf">PDF</a> | 
    <a href="https://github.com/czq142857/implicit-decoder">code</a> | 
    <a href="pubs/haoz_paper_bib.html#imgan_cvpr19">bibtex</a>]
<p>
We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder for 
shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each 
point in 3D space, so that a shape can be extracted as an iso-surface. Our implicit field decoder is trained to perform this 
assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding 
a shape, and outputs a value which indicates whether the point is outside the shape or not ...
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_tog19_grains_long.png">
<IMG width=200 src="pubs/images/li_tog19_grains.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="https://manyili12345.github.io/">Manyi Li</a>, <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>,
   <a href="https://kevinkaixu.net/">Kai Xu</a>,
   <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>, Owais Khan,
   <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>, Changhe Tu,
   <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
   <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and <strong>Hao Zhang</strong>,
   "<strong>GRAINS: Generative Recursive Autoencoders for INdoor Scenes</strong>",
   <i>ACM Transactions on Graphics</i>, Vol. 38, No. 2, Article 12, presented at SIGGRAPH, 2019.
   [<a href="https://arxiv.org/abs/1807.09193">arXiv</a> |
    <a href="pubs/haoz_paper_bib.html#grains_tog19">bibtex</a>]
<p>
We present a generative neural network which enables us to generate plausible 3D indoor scenes in large quantities and varieties, easily and highly efficiently. Our key observation is that indoor scene structures are inherently hierarchical. Hence, our network is not convolutional; it is a recursive neural network or RvNN. Using a dataset of annotated scene hierarchies, we train a variational recursive autoencoder, or RvNN-VAE, which performs scene object grouping during its encoding phase and scene generation during decoding.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<IMG width=200 src="pubs/images/gan_gi19_album.png"
</TD>
<TD vAlign=center align=left>
3. Yuan Gan, Yan Zhang, and <strong>Hao Zhang</strong>,
   "<strong>Qualitative Organization of Photo Collections via Quartet Analysis and Active Learning</strong>", <i>Proc. of Graphics Interface</i>, 2019.
   [<a href="">PDF</a>]
<p>
We introduce the use of qualitative analysis and active learning to photo album construction. Given a heterogeneous collection of pho- tos, we organize them into a hierarchical categorization tree (C-tree) based on qualitative analysis using quartets instead of relying on conventional, quantitative image similarity metrics. The main moti- vation is that in a heterogeneous collection, quantitative distances may become unreliable between dissimilar data and there is unlikely a single metric that is well applicable to all data.
</TD></TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/xu_cvm19_mosaic_long.png">
<IMG width=200 src="pubs/images/xu_cvm19_mosaic.png"></a>
</TD>
<TD vAlign=center align=left>
2. <a href="http://pengfeixu.com/">Pengfei Xu</a>, Jiangqiang Ding,
   <strong>Hao Zhang</strong>, and <a href="http://vcc.szu.edu.cn/_huihuang.html">Hui Huang</a>,
   "<strong>Discernible Image Mosaic with Edge-Aware Adaptive Tiles</strong>",
   <i>Computational Visual Media (CVM)</i>, 2019.
   [<a href="">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#mosaic_cvm19">bibtex</a>]
<p>
We present a novel method to produce discernible image mosaics, with relatively large image tiles replaced by images drawn from a database, to resemble a target image. Since visual edges strongly support content perception, we compose our mosaic via edge-aware photo retrieval to best preserve visual edges in the target image. Moreover, unlike most previous works which apply a pre-determined partition to an input image, our image mosaics are composed by adaptive tiles, whose sizes are determined based on the available images and an objective of maximizing resemblance to the target.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/yin_tvcg18_intpair.png">
<IMG width=200 src="pubs/images/yin_tvcg18_intpair_small.png"></a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://kangxue.org/">Kangxue Yin</a>,
   <a href="http://vcc.szu.edu.cn/_huihuang.html">Hui Huang</a>,
   Edmond S. L. Ho, Hao Wang, Taku Komura,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>A Sampling Approach to Generating Closely Interacting 3D Pose-pairs from 2D
   Annotations</strong>",
   <i>IEEE Trans. on Visualization and Computer Graphics (TVCG)</i>, Vol. 25, No. 6, pages 2217-2227, 2019.
   [<a href="">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#intpair_tvcg18">bibtex</a>]
<p>
We introduce a data-driven method to generate a large number of plausible, closely interacting 3D human pose-pairs, for a given motion category, e.g., wrestling or salsa dance. With much difficulty in acquiring close interactions using 3D sensors, our approach utilizes abundant existing video data which cover many human activities. Instead of treating the data generation problem as one of reconstruction, we present a solution based on Markov Chain Monte Carlo (MCMC) sampling. Given a motion category and a set of video frames depicting the motion with the 2D pose-pair in each frame annotated, we start the sampling with one or few seed 3D pose-pairs which are manually created based on the target motion category. The initial set is then augmented by MCMC sampling around the seeds, via the Metropolis-Hastings algorithm and guided by a probability density function (PDF) that is defined by two terms to bias the sampling towards 3D pose-pairs that are physically valid and plausible for the motion category.
</TD>
</TR>

</table>

<a name="papers">

<a name="papers2018">

<h3>2018</h3>

<table cellPadding=5 width="99%" border=0>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zhu_siga18_scores_long.png">
<IMG width=200 src="pubs/images/zhu_siga18_scores.png"></a>
</TD>
<TD vAlign=center align=left>
11. <a href="http://www.sfu.ca/~cza68/">Chenyang Zhu</a>,
   <a href="https://kevinkaixu.net/">Kai Xu</a>,
   <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
   Renjiao Yi, and <strong>Hao Zhang</strong>,
   "<strong>SCORES: Shape Composition with Recursive Substructure Priors</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 37, No. 6, Article 211, 2018.
   [<a href="https://arxiv.org/abs/1809.05398">arXiv</a> | <a href="https://kevinkaixu.net/projects/scores.html">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#scores_siga18">bibtex</a>]
<p>
We introduce SCORES, a recursive neural network for shape composition. Our network takes as input sets of parts from two or more source 3D shapes and a rough initial placement of the parts. It outputs an optimized part structure for the composed shape, leading to high-quality geometry construction. A unique feature of our composition network is that it is not merely learning how to connect parts. Our goal is to produce a coherent and plausible 3D shape, despite large incompatibilities among the input parts. The network may significantly alter the geometry and structure of the input parts and synthesize a novel shape structure based on the inputs, while adding or removing parts to minimize a structure plausibility loss.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/lira_siga18_wire_long.png">
<IMG width=200 src="pubs/images/lira_siga18_wire.png"></a>
</TD>
<TD vAlign=center align=left>
10. <a href="http://www.sfu.ca/~wpintoli/">Wallace Lira</a>, 
    <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, and <strong>Hao Zhang</strong>,
   "<strong>Fabricable Eulerian Wires for 3D Shape Abstraction</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 37, No. 6, Article 240, 2018.
   [<a href="http://gruvi.cs.sfu.ca/project/eulerianwires/">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#wire_siga18">bibtex</a>]
<p>
We present a fully automatic method that finds a small number of machine fabricable wires with minimal overlap to reproduce a wire sculpture design as a 3D shape abstraction. Importantly, we consider non-planar wires, which can be fabricated by a wire bending machine, to enable efficient construction of complex 3D sculptures that cannot be achieved by previous works. We call our wires Eulerian wires, since they are as Eulerian as possible with small overlap to form the target design together.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_siga18_riot_long.png">
<IMG width=200 src="pubs/images/li_siga18_riot.png"></a>
</TD>
<TD vAlign=center align=left>
9. Shuhua Li, <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>, 
   <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Han Liu,
   <a href="https://changqingzou.weebly.com/">Changqing Zou</a>, <a href="http://people.scs.carleton.ca/~olivervankaick/">Oliver van Kaick</a>,
   Xiuping Liu, <a href="http://vcc.szu.edu.cn/~huihuang">Hui Huang</a>, and <strong>Hao Zhang</strong>,
   "<strong>Construction and Fabrication of Reversible Shape Transforms</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 37, No. 6, Article 190, 2018.
   [<a href="https://sites.google.com/site/alimahdaviamiri/projects/reversible-shapes">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#riot_siga18">bibtex</a>]
<p>
We study a new and elegant instance of geometric dissection of 2D shapes: reversible hinged dissection, which corresponds to a dual transform between two shapes where one of them can be dissected in its interior and then inverted inside-out, with hinges on the shape boundary, to reproduce the other shape, and vice versa. We call such a transform reversible inside-out transform or RIOT. Since it is rare for two shapes to possess even a rough RIOT, let alone an exact one, we develop both a RIOT construction algorithm and a quick filtering mechanism to pick, from a shape collection, potential shape pairs that are likely to possess the transform. Our construction algorithm is fully automatic. It computes an approximate RIOT between two given input 2D shapes, whose boundaries can undergo slight deformations, while the filtering scheme picks good inputs for the construction.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/ma_siga18_t2s_long.png">
<IMG width=200 src="pubs/images/ma_siga18_t2s.png"></a>
</TD>
<TD vAlign=center align=left>
8. <a href="https://ruim-jlu.github.io/">Rui Ma</a>, <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a> (co-first author),
    <a href="https://techmatt.github.io/">Matt Fisher</a>, <a href="https://manyili12345.github.io/">Manyi Li</a>,
    <a href="http://www.pirk.info/">Soren Pirk</a>, <a href="http://sonhua.github.io/">Binh-Son Hua</a>,
    <a href="http://www.saikit.org/">Sai-Kit Yeung</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>,
    <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>, and <strong>Hao Zhang</strong>,
   "<strong>Language-Driven Synthesis of 3D Scenes Using Scene Databases</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 37, No. 6, Article 212, 2018.
   [<a href="http://www.sfu.ca/~agadipat/publications/2018/T2S/project_page.html">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#t2s_siga18">bibtex</a>]
<p>
We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene databases. The advantage of natural language editing interfaces is strongest when performing semantic operations at the sub-scene level, acting on groups of objects. We learn how to manipulate these sub-scenes by analyzing existing 3D scenes. We perform edits by first parsing a natural language command from the user and trans- forming it into a semantic scene graph that is used to retrieve corresponding sub-scenes from the databases that match the command. We then augment this retrieved sub-scene by incorporating other objects that may be implied by the scene context. Finally, a new 3D scene is synthesized by aligning the augmented sub-scene with the user’s current scene, where new objects are spliced into the environment, possibly triggering appropriate adjustments to the existing scene arrangement.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/chen_siga18_pym_long.png">
<IMG width=200 src="pubs/images/chen_siga18_pym.png"></a>
</TD>
<TD vAlign=center align=left>
7. Xuelin Chen, <a href="http://honghuali.github.io/">Honghua Li</a>,
   <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, <strong>Hao Zhang</strong>, 
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
   "<strong>3D Fabrication with Universal Building Blocks and Pyramidal Shells</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 37, No. 6, Article 189, 2018.
   [<a href="http://irc.cs.sdu.edu.cn/~xuelin/prefab/index.html">Project page</a> | <a href="pubs/haoz_paper_bib.html#pym_siga18">bibtex</a>]
<p>
We introduce a computational solution for cost-efficient 3D fabrication using universal building blocks. Our key idea is to employ a set of universal blocks, which can be massively prefabricated at a low cost, to quickly assemble and constitute a significant internal core of the target object, so that only the residual volume need to be 3D printed online. We further improve the fabrication efficiency by decomposing the residual volume into a small number of printing-friendly pyramidal pieces.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zou_eccv18_sketch_long.png">
<IMG width=200 src="pubs/images/zou_eccv18_sketch.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="https://changqingzou.weebly.com/">Changqing Zou</a>, 
   <a href="https://www.eecs.qmul.ac.uk/~qian/">Qian Yu</a>, Ruofei Du, Haoran Mo, 
   Yi-Zhe Song, Tao Xiang, Chengyi Gao, Baoquan Chen, and <strong>Hao Zhang</strong>, 
   "<strong>SketchyScene: Richly-Annotated Scene Sketches</strong>",
   <i>ECCV</i>, 2018.
   [<a href="pubs/zou_eccv18_sketch.pdf">PDF</a> | 
    <a href="pubs/haoz_paper_bib.html#sketch_eccv18">bibtex</a>]
<p>
We contribute the first large-scale dataset of scene sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks.  The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. We demonstrate the potential impact of SketchyScene by training new computational models for semantic segmentation of scene sketches and showing how the new dataset enables several applications including image retrieval, sketch colorization, editing, and captioning, etc. We will release the complete crowdsourced dataset to the community.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/yin_sig18_p2p_long.png">
<IMG width=200 src="pubs/images/yin_sig18_p2p.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://kangxue.org/">Kangxue Yin</a>, <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and <strong>Hao Zhang</strong>,
   "<strong>P2P-NET: Bidirectional Point Displacement Net for Shape Transform</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 37, No. 4, Article 152, 2018.
   [<a href="pubs/yin_sig18_p2p.pdf">PDF</a> | <a href="https://arxiv.org/abs/1803.09263">arXiv</a> | <a href="pubs/haoz_paper_bib.html#p2p_sig18">bibtex</a>]
<p>
We introduce P2P-NET, a general-purpose deep neural network which learns geometric transformations between point-based shape representations from two domains, e.g., meso-skeletons and surfaces, partial and complete scans, etc. The architecture of the P2P-NET is that of a bi-directional point dis- placement network, which transforms a source point set to a prediction of the target point set with the same cardinality, and vice versa, by applying point-wise displacement vectors learned from data. P2P-NET is trained on paired shapes from the source and target domains, but without relying on point-to-point correspondences between the source and target point sets. The training loss combines two uni-directional geometric losses, each enforc- ing a shape-wise similarity between the predicted and the target point sets, and a cross-regularization term to encourage consistency between displace- ment vectors going in opposite directions.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/hu_sig18_icon4_long.png">
<IMG width=200 src="pubs/images/hu_sig18_icon4.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Zhihao Yan, Jingwen Zhang,
<a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>,
   <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
   <strong>Hao Zhang</strong>, and <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   "<strong>Predictive and Generative Neural Networks for Object Functionality</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 37, No. 4, Article 151, 2018.
   [<a href="https://arxiv.org/abs/2006.15520">arXiv</a> | <a href="https://vcc.tech/research/2018/ICON4">Project Page</a> |
    <a href="pubs/haoz_paper_bib.html#icon4_siga18">bibtex</a>]
<p>
Humans can predict the functionality of an object even without any surroundings, since their knowledge and experience would allow them to "hallucinate" the interaction or usage scenarios involving the object. We develop predictive and generative deep convolutional neural networks to replicate this feat. Our networks are trained on a database of scene contexts, called interaction contexts, each consisting of a central object and one or more surrounding objects, that represent object functionalities. Given a 3D object in isolation, our functional similarity network (fSIM-NET), a variation of the triplet network, is trained to predict the functionality of the object by inferring functionality-revealing interaction contexts involving the object. fSIM-NET is complemented by a generative network (iGEN-NET) and a segmentation network (iSEG-NET). iGEN-NET takes a single voxelized 3D object and synthesizes a voxelized surround, i.e., the interaction context which visually demonstrates the object's functionalities. iSEG-NET separates the interacting objects into different groups according to their interaction types.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zhao_sig18_cnc_long.png">
<IMG width=200 src="pubs/images/zhao_sig18_cnc.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="https://haisenzhao.github.io/">Haisen Zhao</a>, <strong>Hao Zhang</strong>, Shiqing Xin, Yuanmin Deng, Changhe Tu, 
   <a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>, 
   <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
   "<strong>DSCarver: Decompose-and-Spiral-Carve for Subtractive Manufacturing</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 37, No. 4, Article 137, 2018.
   [<a href="pubs/zhao_sig18_cnc.pdf">PDF</a> | <a href="pubs/haoz_paper_bib.html#cnc_sig18">bibtex</a>]
<p>
We present an automatic algorithm for subtractive manufacturing of freeform 3D objects
using high-speed CNC machining. Our method decomposes the input object's surface into a small number 
of patches each of which is fully accessible and machinable by the CNC machine, in continuous fashion, 
under a fixed drill-object setup configuration. This is achieved by covering the input surface using a 
minimum number of accessible regions and then extracting a set of machinable patches from each accessible 
region. For each patch obtained, we compute a continuous, space-filling, and iso-scallop tool path, 
in the form of connected Fermat spirals, which conforms to the patch boundary. Furthermore, we develop a 
novel method to control the spacing of Fermat spirals based on directional surface curvature and adapt the 
heat method to obtain iso-scallop carving.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/yu_tog18_style_long.png">
<IMG width=200 src="pubs/images/yu_tog18_style.png"></a>
</TD>
<TD vAlign=center align=left>
2. Fenggen Yu, Yan Zhang,
   <a href="http://kevinkaixu.net/">Kai Xu</a>,
   <a href="https://www.sfu.ca/~amahdavi/Home.html">Ali Mahdavi-Amiri</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines</strong>",
   <i>ACM Transactions on Graphics</i>, Vol. 37, No. 2, Article 21, 2018. (<strong>Awarded the <a href="http://www.replicabilitystamp.org/">Replicability Stamp</a>!</strong>)
   [<a href="pubs/yu_tog18_style.pdf">PDF</a> | <a href="https://arxiv.org/abs/1804.06579">arXiv</a> | <a href="http://kevinkaixu.net/projects/projstyle.html">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#style_tog18">bibtex</a>]
<p>
We present a semi-supervised co-analysis method for learning 3D shape styles from projected feature lines, achieving style patch localization with only weak supervision. Given a collection of 3D shapes spanning multiple object categories and styles, we perform style co-analysis over projected feature lines of each 3D shape and then backproject the learned style features onto the 3D shapes.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_gm18_csmetric_long.png">
<IMG width=200 src="pubs/images/li_gm18_csmetric.png"></a>
</TD>
<TD vAlign=center align=left>
1. Manyi Li, Noa Fish, Lili Cheng, Changhe Tu,
   <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   <strong>Hao Zhang</strong>, and <a href="http://www.cs.sdu.edu.cn/~baoquan/">
Baoquan Chen</a>,
   "<strong>Class-Sensitive Shape Dissimilarity Metric</strong>",
   <i>Graphical Models</i>, , 2018.
   [<a href="pubs/li_gm18_csmetric.pdf">PDF</a> | <a href="pubs/haoz_paper_bib.html#csmetric_gm18">bibtex</a>]
<p>
Shape dissimilarity is a fundamental problem with many applications such as shape exploration, retrieval, and classification. Given a collection of shapes, all existing methods develop a consistent global metric to compareand organize shapes. The global nature of the involved shape descriptors implies that overall shape appearanceis compared. These methods work well to distinguishshapes from different categories, but often fail for fine-grained classes within the same category. In this paper, we develop a dissimilarity metric for fine-grained classes by fusing together multiple distinctive metrics for different classes. The fused metric measures the dissimilarities among inter-class shapes by observing their unique traits.
</TD>
</TR>

</table>

<p>

<a name="papers2017">

<h3>2017</h3>

<table cellPadding=5 width="99%" border=0>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/hu_siga17_icon3_long.png">
<IMG width=200 src="pubs/images/hu_siga17_icon3.jpg"></a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Wenchao Li,
<a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>,
   <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
   <strong>Hao Zhang</strong>, and <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   "<strong>Learning to Predict Part Mobility from a Single Static Snapshot</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 36, No. 6, Article 227, 2017.
   [<a href="pubs/hu_siga17_icon3.pdf">PDF</a> | <a href="https://vcc.tech/research/2017/ICON3">Project Page</a> |
    <a href="pubs/haoz_paper_bib.html#icon3_siga17">bibtex</a>]
<p>
We introduce a method for learning a model for the mobility of parts in 3D objects. Our method allows not only to understand the dynamic function- alities of one or more parts in a 3D object, but also to apply the mobility functions to static 3D models. Specifically, the learned part mobility model can predict mobilities for parts of a 3D object given in the form of a single static snapshot reflecting the spatial configuration of the object parts in 3D space, and transfer the mobility from relevant units in the training data ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zou_siga17_group_long.png">
<IMG width=200 src="pubs/images/zou_siga17_group.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://people.cs.umass.edu/~zlun/">Zhaoliang Lun</a>,
   <a href="http://changqingzou.weebly.com/">Changqing Zou</a> (joint first author),
   <a href="http://people.cs.umass.edu/~hbhuang/">Haibin Huang</a>,
   <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>,
   <a href="http://www.cs.sfu.ca/~pingtan/">Ping Tan</a>,
   <a href="https://team.inria.fr/imagine/marie-paule-cani/">Marie-Paule Cani</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>Learning to Group Discrete Graphical Patterns</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 36, No. 6, Article 225, 2017.
   [<a href="pubs/zou_siga17_group.pdf">PDF</a> | <a href="http://people.cs.umass.edu/~zlun/papers/PatternGrouping/">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#group_siga17">bibtex</a>]
<p>
We introduce a deep learning approach for grouping discrete patterns common in graphical designs. Our approach is based on a convolutional neural network architecture that learns a grouping measure defined over a pair of pattern elements. Motivated by perceptual grouping principles, the key feature of our network is the encoding of element shape, context, symmetries, and structural arrangements. These element properties are all jointly considered and appropriately weighted in our grouping measure ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/yi_arx17_dualgan.png">
<IMG width=200 src="pubs/images/yi_arx17_dualgan.png"></a>
</TD>
<TD vAlign=center align=left>
5. Zili Yi, <strong>Hao Zhang</strong>, <a href="http://www.cs.sfu.ca/~pingtan/">Ping Tan</a>, and
   <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>,
   "<strong>DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</strong>", 
   <i>Proc. of <a href="http://iccv2017.thecvf.com/">ICCV</a></i>, also available at
   <a href="https://arxiv.org/abs/1704.02510?from=groupmessage"><i>arXiv:1704.02510</i></a>, 2017.
   [<a href="pubs/yi_iccv17_dualGAN.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#yi_iccv17">bibtex</a>]
<p>
Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/wr_cadg17_exq.png">
<IMG width=200 src="pubs/images/wr_cadg17_exq.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="http://warunika.weebly.com/">Warunika Ranaweera</a>,
   <a href="http://hci.cs.sfu.ca/">Parmit Chilana</a>,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>ExquiMo: An Exquisite Corpse Tool for Co-creative 3D Shape Modeling</strong>",
   <i>International Conference on Computer-Aided Design and Computer Graphics (CAD/Graphics)</i>, Zhangjiajie, China, August 25-27, 2017. <b>One of
   three Best Paper Awards at the conference.</b>
   [<a href="pubs/rana_cg17_exquimo.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#exq_cadg17">bibtex</a>]
<p>
We introduce a shape modeling tool, ExquiMo, which is guided by the idea of improving the creativity of 3D shape designs through collaboration. Inspired by the game of Exquisite Corpse, our tool allocates distinct parts of a shape to multiple players who model the assigned parts in a sequence. Our approach is motivated by the understanding that effective surprise leads to creative outcomes. Hence, to maintain the surprise factor of the output, we conceal the previously modeled parts from the most recent player. Part designs from individual players are fused together to produce an often unexpected, hence creative, end result ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zhu_sig17_scsr.png">
<IMG width=200 src="pubs/images/zhu_sig17_scsr_small.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.sfu.ca/~cza68/">Chenyang Zhu</a>, Renjiao Yi,
   <a href="http://www.sfu.ca/~wpintoli/">Wallace Lira</a>,
   <a href="http://ialhashim.github.io/index.html">Ibraheem Alhashim</a>,
   <a href="http://kevinkaixu.net/">Kai Xu</a>,
   and <strong>Hao Zhang</strong>,
   "<strong>Deformation-Driven Shape Correspondence via Shape Recognition</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 36, No. 4, Article 51, 2017.
   [<a href="https://kevinkaixu.net/projects/scsc.html">Project page</a> |
    <a href="pubs/zhu_sig17_scsr_small.pdf">PDF reduced</a> (1.7MB) |
    <a href="pubs/haoz_paper_bib.html#scsr_sig17">bibtex</a>]
<p>
Many approaches to shape comparison and recognition start by establishing a shape correspondence.
We ``turn the table'' and show that quality shape correspondences can be obtained by performing
many shape recognition tasks. What is more, the method we develop computes a 
<i>fine-grained</i>, <i>topology-varying</i> part correspondence between two 3D shapes
where the core evaluation mechanism only recognizes shapes <i>globally</i>. This is made
possible by casting the part correspondence problem in a deformation-driven framework
and relying on a <i>data-driven</i> ``deformation energy'' which rates visual similarity 
between deformed shapes and models from a shape repository. Our basic premise is that 
if a correspondence between two chairs (or airplanes, bicycles, etc.) is correct, then a 
reasonable deformation between the two chairs anchored on the correspondence ought 
to produce <i>plausible</i>, ``chair-like'' in-between shapes.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_sig17_grass.png">
<IMG width=200 src="pubs/images/li_sig17_grass_small.png"></a>
</TD>
<TD vAlign=center align=left>
2. Jun Li, <a href="http://kevinkaixu.net/">Kai Xu</a>,
   <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
   <a href="http://www.meyumer.com/">Ersin Yumer</a>,
   <strong>Hao Zhang</strong>, <a href="https://profiles.stanford.edu/leonidas-guibas">Leonidas Guibas</a>,
   "<strong>GRASS: Generative Recursive Autoencoders for Shape Structures</strong>",
   <i>ACM Transactions on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 36, No. 4, Article 52, 2017. <b>One of six papers selected for press release at SIGGRAPH.</b>
   [<a href="pubs/li_sig17_grass.pdf">PDF</a> | <a href="http://arxiv.org/abs/1705.02090">arXiv</a> | <a href="http://kevinkaixu.net/projects/grass.html">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#grass_sig17">bibtex</a>]
<p>
We introduce a novel neural network architecture for <i>encoding</i> and <i>synthesis</i> of 3D shapes, particularly their <i>structures</i>. Our key insight is that 3D shapes are effectively characterized by their <i>hierarchical</i> organization of parts, which reflects fundamental intra-shape relationships such as adjacency and symmetry. We develop a <i>recursive</i> neural net (RvNN) based autoencoder to map a flat, unlabeled, arbitrary part layout to a compact code. The code effectively captures the hierarchical structures of varying complexity despite being fixed-dimensional: an associated decoder maps a code back to a full hierarchy. The learned bidirectional mapping is further tuned using an adversarial setup to yield a generative model of plausible structures, from which novel structures can be sampled. Finally, our structure synthesis framework is augmented by a second trained module that produces fine-grained part geometry, conditioned on global and local structural context, leading to a full generative pipeline for 3D shapes.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/hu_tog17_style.png">
<IMG width=200 src="pubs/images/hu_tog17_style_small.png"></a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Wenchao Li, 
   <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>,
   <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   <a href="http://geometry.cs.ucl.ac.uk/averkiou/">Melinos Averkiou</a>,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and <strong>Hao Zhang</strong>, 
   "<strong>Co-Locating Style-Defining Elements on 3D Shapes</strong>",
   <i>ACM Transactions on Graphics (to be presented at SIGGRAPH)</i>, Vol. 36, No. 3, pp. 33:1-33:15, 2017.
   [<a href="pubs/hu_tog17_style.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#style_tog17">bibtex</a>]
<p>
We introduce a method for co-locating style-defining elements over a set of 3D shapes. Our goal is to translate high-level style descriptions, such as "Ming" or "European" for furniture models, into explicit and localized regions over the geometric models that characterize each style. For each style, the set of style-defining elements is defined as the union of all the elements that are able to discriminate the style. Another property of the style-defining elements is that they are frequently-occurring, reflecting shape characteristics that appear across multiple shapes of the same style ...
</TD>
</TR>

</table>

<p>

<a name="papers">

<a name="papers2016">

<h3>2016</h3>

<table cellPadding=5 width="99%" border=0>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/ma_siga16_action.png">
<IMG width=200 src="pubs/images/ma_siga16_action_small.png"></a>
</TD>
<TD vAlign=center align=left>
8. <a href="https://ruim-jlu.github.io/">Rui Ma</a>,
   <a href="http://honghuali.github.io/">Honghua Li</a>,
   <a href="http://changqingzou.weebly.com/">Changqing Zou</a>,
   <a href="http://web.engr.illinois.edu/~liao17/">Zicheng Liao</a>,
   <a href="http://research.microsoft.com/en-us/um/people/xtong/xtong.html">Xin Tong</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Action-Driven 3D Indoor Scene Evolution</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 35, No. 6, Article 173, 2016.
   [<a href="https://maruitx.github.io/project/adise/">Projet page</a> |
    <a href="pubs/haoz_paper_bib.html#action_siga16">bibtex</a>]
<p>
We introduce a framework for action-driven evolution of 3D indoor scenes, where the goal is to simulate how scenes are altered by human actions, and specifically, by object placements necessitated by the actions. To this end, we develop an action model with each type of action combining information about one or more human poses, one or more object categories, and spatial configurations of object-object and object-human relations for the action. Importantly, all these pieces of information are learned from annotated photos.
</TD>
</TR>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_siga16tb.png">
<IMG width=200 src="pubs/images/li_siga16tb_small.png"></a>
</TD>
<TD vAlign=center align=left>
7. Lei Li, Zhe Huang,
   <a href="http://changqingzou.weebly.com/">Changqing Zou</a>,
   <a href="https://www.cse.ust.hk/~taicl/">Chiew-Lan Tai</a>, Rynson W.H. Lau,
   <strong>Hao Zhang</strong>, <a href="http://www.cs.sfu.ca/~pingtan/">Ping Tan</a>, and <a href="http://sweb.cityu.edu.hk/hongbofu/">Hongbo Fu</a>,
   "<strong>Model-driven Sketch Reconstruction with Structure-oriented Retrieval</strong>",
   <i>SIGGRAPH Asia Technical Brief</i>, 2016.
   [<a href="pubs/li_siga16tb_sketch.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#sketch_siga16tb">bibtex</a>]
<p>
We propose an interactive system that aims at lifting a 2D sketch
into a 3D sketch with the help of existing models in shape collections.
The key idea is to exploit part structure for shape retrieval and
sketch reconstruction. We adopt sketch-based shape retrieval and
develop a novel matching algorithm which considers structure in
addition to traditional shape features.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zeinab_sgp16_scene.png">
<IMG width=200 src="pubs/images/zeinab_sgp16_scene_small.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://www.sfu.ca/~zsadeghi/">Zeinab Sadeghipour</a>,
   <a href="http://web.engr.illinois.edu/~liao17/">Zicheng Liao</a>,
   <a href="http://www.cs.sfu.ca/~pingtan/">Ping Tan</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Learning 3D Scene Synthesis from Annotated RGB-D Images</strong>",
   <i>Computer Graphics Forum (Special Issue of SGP)</i>, Vol. 35, No. 5, pp. 197-206, 2016.
   [<a href="pubs/zeinab_sgp16_scene.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#scene_sgp16">bibtex</a>]
<p>
We present a data-driven method for synthesizing 3D indoor scenes by inserting objects 
progressively into an initial, possibly, empty scene. Instead of relying on few hundreds of 
hand-crafted 3D scenes, we take advantage of existing large-scale annotated RGB-D datasets, 
in particular, the SUN RGB-D database consisting of 10,000+ depth images of real scenes, to 
form the prior knowledge for our synthesis task. Our object insertion scheme follows a 
co-occurrence model and an arrangement model, both learned from the SUN dataset.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/hu_sig16_icon2.png">
<IMG width=200 src="pubs/images/hu_sig16_icon2_small.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>,
<a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>, Bojian Wu,
   <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Learning How Objects Function via Co-Analysis of Interactions</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 35, No. 4, Article 47, 2016.
   [<a href="pubs/hu_sig16_icon2.pdf">PDF</a> |
    <a href="https://vcc.tech/research/2016/ICON2">Projet page</a> | 
    <a href="pubs/haoz_paper_bib.html#icon2_sig16">bibtex</a>]
<p>
We introduce a co-analysis method which learns a functionality model for an object category, e.g., 
strollers or backpacks. Like previous works on functionality, we analyze object-to-object interactions 
and intra-object properties and relations. Differently from previous works, our model goes beyond 
providing a functionalityoriented descriptor for a single object; it prototypes the functionality of 
a category of 3D objects by co-analyzing typical interactions involving objects from the category.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zou_sig16_calli.png">
<IMG width=200 src="pubs/images/zou_sig16_calli_small.png"></a>
</TD>
<TD vAlign=center align=left>
4. <a href="http://changqingzou.weebly.com/">Changqing Zou</a>, Junjie Cao,
   <a href="http://warunika.weebly.com/">Warunika Ranaweera</a>,
   <a href="http://ialhashim.github.io/index.html">Ibraheem Alhashim</a>,
   <a href="http://www.cs.sfu.ca/~pingtan/">Ping Tan</a>,
   <a href="https://www.cs.ubc.ca/~sheffa/">Alla Sheffer</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Legible Compact Calligrams</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 35, No. 4, Article 122, 2016.
   [<a href="pubs/zou_sig16_calli.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#calli_sig16">bibtex</a>]
<p>
A calligram is an arrangement of words or letters that creates a visual image, and a 
compact calligram fits one word into a 2D shape. We introduce a fully automatic method 
for the generation of legible compact calligrams which provides a balance between 
conveying the input shape, legibility, and aesthetics.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zhao_sig16_fermat.png">
<IMG width=200 src="pubs/images/zhao_sig16_fermat_small.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="https://haisenzhao.github.io/">Haisen Zhao</a>, Fanglin Gu, <a href="http://ttic.uchicago.edu/~huangqx/">Qi-Xing Huang</a>, Jorge Garcia,
   <a href="http://www-bcf.usc.edu/~yongchen/">Yong Chen</a>, Changhe Tu,
   <a href="https://polytechnic.purdue.edu/profile/bbenes">Bedrich Benes</a>,
   <strong>Hao Zhang</strong>, <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
   "<strong>Connected Fermat Spirals for Layered Fabrication</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 35, No. 4, Article 100, 2016.
   [<a href="pubs/zhao_sig16_fermat.pdf">PDF</a> | <a href="https://haisenzhao.github.io/CFS/index.html">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#fermat_sig16">bibtex</a>]
<p>
We develop a new kind of "space-filling" curves, connected Fermat spirals, and show their 
compelling properties as a tool path fill pattern for layered fabrication. Unlike classical 
space-filling curves such as the Peano or Hilbert curves, which constantly wind and bind to 
preserve locality, connected Fermat spirals are formed mostly by long, low-curvature paths. 
This geometric property, along with continuity, influences the quality and efficiency of 
layered fabrication. 
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/wan_tvc16_sparse.png">
<IMG width=200 src="pubs/images/wan_tvc16_sparse.png"></a>
</TD>
<TD vAlign=center align=left>
2. Lili Wan, <a href="http://changqingzou.weebly.com/">Changqing Zou</a>, and
   <strong>Hao Zhang</strong>, 
   "<strong>Full and Partial Shape Similarity through Sparse Descriptor Reconstruction</strong>",
   <i>The Visual Computer</i>, to appear, 2016.
   [<a href="pubs/wan_tvc16_sparse.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#sparse_tvc16">bibtex</a>]
<p>
We introduce a novel approach to measure similarity between two 3D shapes based on sparse 
reconstruction of shape descriptors. The main feature of our approach is its applicability
to handle incomplete shapes. We characterize the shapes by learning a sparse dictionary 
from their local descriptors. The similarity between two shapes <i>A</i> and <i>B</i> is 
defined by the error incurred when reconstructing <i>B</i>'s descriptor set using the 
basis signals from <i>A</i>’s dictionary.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zhang_tvc16_creative.png">
<IMG width=200 src="pubs/images/zhang_tvc16_creative_small.png"></a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a> and
   <strong>Hao Zhang</strong>,
   "<strong>From inspired modeling to creative modeling</strong>",
   <i>Visual Computer (invited paper)</i>, Vol. 32, No. 1, 2016.
   [<a href="pubs/zhang_tvc16_creative.pdf">PDF</a> |
    <a href="pubs/haoz_paper_bib.html#creative_tvc16">bibtex</a>]
<p>
An intriguing and reoccurring question in many branches of computer science is whether machines can be creative, like humans.
In this exploratory paper, we examine the problem from a computer graphics, and more specifically, geometric modeling, 
perspective. We focus our discussions on the weaker but still intriguing question: "Can machines assist or inspire humans in 
a creative endeavor for the generation of geometric forms?"
</TD>
</TR>

</table>

<p>

<a name="papers2015">

<h3>2015</h3>

<table cellPadding=5 width="99%" border=0>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/ib_siga15_geotopo.png">
<IMG width=200 src="pubs/images/ib_siga15_geotopo_small.png"></a>
</TD>
<TD vAlign=center align=left>
11. <a href="http://ialhashim.github.io/index.html">Ibraheem Alhashim</a>,
   <a href="http://kevinkaixu.net/">Kai Xu</a>, Yixin Zhuang, Junjie Cao,
   <a href="http://faculty.cua.edu/simari/">Patricio Simari</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Deformation-Driven Topology-Varying 3D Shape Correspondence</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 34, No. 6, Article 236, 2015.
   [<a href="pubs/ib_siga15_geotopo.pdf">PDF</a> |
    <a href="http://gruvi.cs.sfu.ca/project/geotopo/">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#geotopo_siga15">bibtex</a>]
<p>
We present a deformation-driven approach to topology-varying 3D shape correspondence. In this paradigm, 
the best correspondence between two shapes is the one that results in a minimal-energy, possibly 
topology-varying, deformation that transforms one shape to conform to the other while respecting the 
correspondence. Our deformation model allows both geometric and topological operations such as part split, 
duplication, and merging ...
</TD>
</TR>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/chen_siga15_dap.png">
<IMG width=200 src="pubs/images/chen_siga15_dap_small.png"></a>
</TD>
<TD vAlign=center align=left>
10. Xuelin Chen, <strong>Hao Zhang</strong>, Jinjie Lin, <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Lin Lu,
   Qixing Huang, <a href="http://hpcg.purdue.edu/bbenes/">Bedrich Benes</a>,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
   "<strong>Dapper: Decompose-and-Pack for 3D Printing</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 34, No. 6, Article 213, 2015.
   [<a href="pubs/chen_siga15_dap.pdf">PDF</a> |
    <a href="http://irc.cs.sdu.edu.cn/DAP/Dapper.html">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#dap_siga15">bibtex</a>]
<p>
We pose the decompose-and-pack or DAP problem, which tightly combines shape decomposition and packing. 
While in general, DAP seeks to decompose an input shape into a small number of parts which can be efficiently 
packed, our focus is geared towards 3D printing. The goal is to optimally decompose-and-pack a 3D object 
into a printing volume to minimize support material, build time, and assembly cost. We present Dapper, 
a global optimization algo- rithm for the DAP problem which can be applied to both powder- and FDM-based 3D printing. 
</TD>
</TR>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zhou_siga15_gcd.png">
<IMG width=200 src="pubs/images/zhou_siga15_gcd_small.png"></a>
</TD>
<TD vAlign=center align=left>
9. Yang Zhou, <a href="http://kangxue.org/">Kangxue Yin</a>,
   <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   <strong>Hao Zhang</strong>, Minglun Gong, and
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   "<strong>Generalized Cylinder Decomposition</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 34, No. 6, Article 171, 2015.
   [<a href="pubs/zhou_siga15_gcd.pdf">PDF</a> |
    <a href="https://vcc.tech/research/2015/GCD">Project page</a> |
    <a href="pubs/haoz_paper_bib.html#gcd_siga15">bibtex</a>]
<p>
Decomposing a complex shape into geometrically simple primitives is a fundamental problem in geometry processing. 
We are interested in a shape decomposition problem where the simple primitives sought are generalized cylinders.
We introduce a quantitative measure of cylindricity for a shape part and develop a cylindricity-driven optimization 
algorithm, with a global objective function, for generalized cylinder decomposition.
</TD>
</TR>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/hu_sig15_icon.png">
<IMG width=200 src="pubs/images/hu_sig15_icon_small.png"></a>
</TD>
<TD vAlign=center align=left>
8. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Chenyang Zhu,
   <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>, Ligang Liu, 
   <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Interaction Context (ICON): Towards a Geometric Functionality Descriptor</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 34, No. 4, Article 83, 2015.
   [<a href="pubs/hu_sig15_icon.pdf">PDF</a> |
    <a href="https://vcc.tech/research/2015/ICON">Project page</a> | 
    <a href="pubs/haoz_paper_bib.html#icon_sig15">bibtex</a>]
<p>
We introduce a contextual descriptor which aims to provide a geometric description of the functionality of a 3D 
object in the context of a given scene. Differently from previous works, we do not regard functionality as an 
abstract label or represent it implicitly through an agent. Our descriptor, called interaction context or ICON 
for short, explicitly represents the geometry of object-to-object interactions. Our approach to object functionality 
analysis is based on the key premise that functionality should mainly be derived from interactions between objects and not objects in isolation.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/li_sig15_foldem.png">
<IMG width=200 src="pubs/images/li_sig15_foldem_small.png"></a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://honghuali.github.io/">Honghua Li</a>, 
   <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a> (co-first author),
   <a href="http://ialhashim.github.io/index.html">Ibraheem Alhashim</a>, and
   <strong>Hao Zhang</strong>,
   "<strong>Foldabilizing Furniture</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 34, No. 4, Article 90, 2015.
   [<a href="pubs/li_sig15_fold.pdf">PDF</a> |
   <a href="pubs/haoz_paper_bib.html#foldem_sig15">bibtex</a>]
<p>
We introduce the foldabilization problem for space-saving furniture design. Namely, given a 3D object representing 
a piece of furniture, the goal is to apply a minimum amount of modification to the object so that it can be folded 
to save space —-- the object is thus foldabilized. We focus on one instance of the problem where folding is with 
respect to a prescribed folding direction and allowed object modifications include hinge insertion and part shrinking.
We develop an automatic algorithm for foldabilization by formulating and solving a nested optimization problem ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/wan_pg15_sparse.png">
<IMG width=200 src="pubs/images/wan_pg15_sparse_small.png"></a>
</TD>
<TD vAlign=center align=left>
6. Lili Wan, Jingyu Jiang, and 
   <strong>Hao Zhang</strong>,
   "<strong>Incomplete 3D Shape Retrieval via Sparse Dictionary Learning</strong>",
   <i>Pacific Graphics (short paper)</i>, 2015.
   [<a href="pubs/wan_pg15.pdf">PDF</a> | <a href="pubs/haoz_paper_bib.html#sparse_pg15">bibtex</a>]
<p>
In this paper, we are interested in the problem of 3D shape retrieval where the query
shape is incomplete with moderate to significant portions of the original shape missing. The key idea of our method
is to grasp the basis local descriptors for each shape in the retrieved database by sparse dictionary learning
and apply them in sparsely coding the local descriptors of an incomplete query
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/li_dagstuhl_compaction.png">
<IMG width=200 src="pubs/images/li_dagstuhl_compaction.png"></a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://honghuali.github.io/">Honghua Li</a> and
   <strong>Hao Zhang</strong>,
   "<strong>Shape Compaction</strong>", in
   <i>Perspectives in Shape Analysis, Dagstuhl Seminar, editors: M. Breu, A. Bruckstein, P. Maragos, and S. Wuhrer</i>, 
   to appear, 2015.
   [<a href="">PDF</a> | <a href="pubs/haoz_paper_bib.html#compact_dag15">bibtex</a>]
<p>
We cover techniques designed for compaction of
shape representations or shape configurations. The goal of compaction is to reduce
storage space, a fundamental problem in many application domains.
Compaction of shape representations focuses on reducing
the memory space allocated for storing the shape geometry data digitally, whilst shape compaction
techniques in the physical domain reduce the physical space occupied by
shape configurations ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/cohenor_book.png">
<IMG width=200 src="pubs/images/cohenor_book_small.png"></a>
</TD>
<TD vAlign=center align=left>
4. Daniel Cohen-Or, Chen Greif, Tao Ju, Niloy J. Mitra, Ariel Shamir, Olga Sorkine-Hornung,
   and <strong>Hao Zhang</strong>,
   <i>A Sampler of Useful Computational Tools for Applied Geometry, Computer Graphics, and Image
   Processing</i>, CRC Press, 2015.
<p>
A Sampler of Useful Computational Tools for Applied Geometry, Computer Graphics, and Image Processing shows how to use a collection of mathematical techniques to solve important problems in applied mathematics and computer science areas. The book discusses fundamental tools in analytical geometry ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zheng_eg15_intsym.png">
<IMG width=200 src="pubs/images/zheng_eg15_intsym.png"></a>
</TD>
<TD vAlign=center align=left>
3. Qian Zheng, Zhuming Hao, 
   <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   <a href="http://www.kevinkaixu.net/">Kai Xu</a>,
   <strong>Hao Zhang</strong>,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
   "<strong>Skeleton-Intrinsic Symmetrization of Shapes</strong>",
   <i>Computer Graphics Forum (Special Issue of Eurographics)</i>, Vol. 34, No. 2, pp. 275-286, 2015.
   [<a href="pubs/zheng_eg15_intsym.pdf">PDF</a> | <a href="pubs/haoz_paper_bib.html#intsym_eg15">bibtex</a>]
<p>
Enhancing the self-symmetry of a shape is of fundamental aesthetic virtue. In this paper, we are interested in recov- ering the aesthetics of intrinsic reflection symmetries, where an asymmetric shape is symmetrized while keeping its general pose and perceived dynamics. The key challenge to intrinsic symmetrization is that the input shape has only approximate reflection symmetries, possibly far from perfect. The main premise of our work is that curve skeletons provide a concise and effective shape abstraction for analyzing approximate intrinsic symmetries as well as symmetrization. By measuring intrinsic distances over a curve skeleton for symmetry analysis, symmetrizing the skeleton, and then propagating the symmetrization from skeleton to shape, our approach to shape symmetrization is skeleton-intrinsic ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/elor_eg15_distill.png">
<IMG width=200 src="pubs/images/elor_eg15_distill_small.png"></a>
</TD>
<TD vAlign=center align=left>
2. Hadar Averbuch-Elor, Yunhai Wang, Yiming Qian,
   <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>, 
   <a href="http://johanneskopf.de/">Johannes Kopf</a>, <strong>Hao Zhang</strong>,
   and <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
   "<strong>Distilled Collections from Textual Image Queries</strong>",
   <i>Computer Graphics Forum (Special Issue of Eurographics)</i>, Vol. 34, No. 2, pp. 131-142, 2015.
   [<a href="pubs/elor_eg15_distill.pdf">PDF</a> | <a href="pubs/haoz_paper_bib.html#distill_eg15">bibtex</a>]
<p>
We present a distillation algorithm which operates on a large, unstructured, and noisy collection of internet images
returned from an online object query. We introduce the notion of a distilled set, which is a clean, coherent, and
structured subset of inlier images. In addition, the object of interest is properly segmented out throughout the
distilled set. Our approach is unsupervised, built on a novel clustering scheme, and solves the distillation and
object segmentation problems simultaneously. In essence, instead of distilling the collection of images, we distill
a collection of loosely cutout foreground “shapes”, which may or may not contain the queried object. Our key
observation, which motivated our clustering scheme, is that outlier shapes are expected to be random in nature,
whereas, inlier shapes, which do tightly enclose the object of interest, tend to be well supported by similar shapes
captured in similar views ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=EEEEEE>
<TD vAlign=center align=left>
<IMG width=200 src="pubs/images/liu_smi14.png">
</TD>
<TD vAlign=center align=left>
1. Zhenbao Liu, Caili Xie, Shuhui Bu, Xiao Wang, and 
   <strong>Hao Zhang</strong>,
   "<strong>Indirect Shape Analysis for 3D Shape Retrieval</strong>",
   <i>Computer & Graphics (Special Issue of SMI 2014)</i>, Vol. 46, pp. 110-116, 2015.
   [<a href="">PDF</a> | <a href="pubs/haoz_paper_bib.html#isa_smi14">bibtex</a>]
<p>
We introduce indirect shape analysis, or ISA, where a given shape is analyzed not based on geometric or topological features
computed directly from the shape itself, but by studying how external agents interact with the shape. The potential benefits
of ISA are two-fold. First, agent-object interactions often reveal an object’s function, which plays a key role in shape
understanding. Second, compared to direct shape analysis, ISA, which utilizes pre-selected agents, is less affected by
imperfections of, or inconsistencies between, the geometry or topology of the analyzed shapes. We employ digital human models
as the external agents and develop a prototype ISA scheme for 3D shape classification and retrieval ...
</TD>
</TR>

</table>

<a name="papers2014">

<h3>2014</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/hu_siga14_pym_long.png">
<IMG width=200 src="pubs/images/hu_siga14_pym.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>,
   <a href="http://www.computer-graphics.cn/~hh/">Honghua Li</a>,
   <strong>Hao Zhang</strong>, and
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>
   "<strong>Approximate Pyramidal Shape Decomposition</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 33, No. 6, Article 213, 2014.
   [<a href="http://csse.szu.edu.cn/staff/ruizhenhu/projects/pym/">Project page</a> | 
    <a href="pubs/hu_siga14_pym.pdf">PDF</a> | 
    <a href="pubs/haoz_paper_bib.html#pym_siga14">bibtex</a>]
<p>
A shape is pyramidal if it has a flat base with the remaining boundary forming a height function over 
the base. Pyramidal shapes are optimal for molding, casting, and layered 3D printing. We introduce an 
algorithm for approximate pyramidal shape decomposition. The general exact pyramidal decomposition 
problem is NP-hard. We turn this problem into an NP-complete Exact Cover Problem which admits a practical solution
... Our solution is equally applicable to 2D or 3D shapes, to shapes with polygonal or smooth boundaries, with or without holes ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/yin_siga14_morfit.png">
<IMG width=200 src="pubs/images/yin_siga14_morfit_small.png"></a>
</TD>
<TD vAlign=center align=left>
5. Kangxue Yin, <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
   <strong>Hao Zhang</strong>,
   <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
   "<strong>Morfit: Interactive Surface Reconstruction from Incomplete Point Clouds with Curve-Driven Topology and Geometry Control</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 33, No. 6, Article 202, 2014.
   [<a href="https://vcc.tech/research/2014/Morfit">Project page</a> |                                         <a href="pubs/yin_siga14_morfit_r.pdf">PDF (lowres 2MB)</a> |
    <a href="https://github.com/kangxue/Morfit">Code</a> |
    <a href="pubs/haoz_paper_bib.html#morfit_siga14">bibtex</a>]
<p>
We present an interactive technique for surface reconstruction from incomplete and sparse 
scans of 3D objects possessing sharp features ... 
We factor 3D editing by the user into two "orthogonal" interactions acting on skeletal and 
profile curves of the underlying shape, controlling its topology and geometric features, 
respectively. For surface completion, we introduce a novel skeleton-driven morph-to-fit, 
or morfit, scheme which reconstructs the shape as an ensemble of generalized cylinders. 
Morfit is a hybrid operator which optimally interpolates between adjacent curve profiles 
(the "morph") and snaps the surface to input points (the "fit") ... 
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/alhashim_sig14_long.png">
<IMG width=200 src="pubs/images/alhashim_sig14.png"></a>
</ TD>
<TD vAlign=center align=left>
4. <a href="http://ialhashim.github.io/">Ibraheem Alhashim</a>,
   <a href="http://www.computer-graphics.cn/~hh/">Honghua Li</a>,
   <a href="http://www.kevinkaixu.net/">Kai Xu</a>,
   Junjie Cao, <a href="https://ruim-jlu.github.io/">Rui Ma</a>,
   and <strong>Hao Zhang</strong>, 
   "<strong>Topology-Varying 3D Shape Creation via Structural Blending</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 33, No. 4, Article 158, 2014.
   [<a href="http://gruvi.cs.sfu.ca/project/topo/">Project page</a> | 
    <a href="https://code.google.com/archive/p/topo-blend/">Code</a> | 
    <a href="pubs/haoz_paper_bib.html#topo_sig14">bibtex</a>]
<p>
We introduce an algorithm for generating novel 3D models via <i>topology-varying</i>
shape blending. Given a source and a target shape, our method blends them topologically
and geometrically, producing continuous series of in-betweens as new shape creations. The blending 
operations are defined on a spatio-structural graph composed of medial curves and sheets. Such a shape abstraction is 
structure-oriented, part-aware, and facilitates topology manipulations. Fundamental topological operations 
including split and merge are realized by allowing one-to-many correspondences between 
the source and the target ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/xu_sig14_long.png">
<IMG width=200 src="pubs/images/xu_sig14.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.kevinkaixu.net/">Kai Xu</a>,
   <a href="https://ruim-jlu.github.io/">Rui Ma</a>,
   <strong>Hao Zhang</strong>, Chenyang Zhu,
   <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
   <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
   <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,    
   "<strong>Organizing Heterogeneous Scene Collections through Contextual Focal Points</strong>",
   <i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 33, No. 4, Article 35, 2014.
   [<a href="http://www.kevinkaixu.net/k/projects/focal.html">Project page</a> | <a 
href="pubs/haoz_paper_bib.html#focal_sig14">bibtex</a>]
<p>
We introduce focal points for characterizing, comparing, and organizing collections of complex and
heterogeneous data and apply the concepts and algorithms developed to collections of 3D indoor scenes. 
We represent each scene by a graph of its constituent objects and define focal points as representative 
substructures in a scene collection. To organize a heterogenous scene collection, we cluster the scenes 
based on a set of extracted focal points: scenes in a cluster are closely connected when viewed from the 
perspective of the representative focal points of that cluster ... The problem of focal point extraction is 
intermixed with the problem of clustering groups of scenes based on their representative focal points. 
We present a co-analysis algorithm ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zou_cvpr14.jpg">
<IMG width=200 src="pubs/images/zou_cvpr14.jpg"></a>
</TD>
<TD vAlign=center align=left>
2. Xiaowu Chen, Dongqing Zou, Jianwei Li, Xiaochun Cao, Qinping Zhao, and <strong>Hao Zhang</strong>,
"<strong>Sparse Dictionary Learning for Edit Propagation of High-resolution Images</strong>",
<i>Proc. of IEEE CVPR</i>, pp. 2854-2861, 2014.
[<a href="pubs/zou_cvpr14_sparse.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#sparse_cvpr14">bibtex</a>]
<p>
We introduce the use of sparse representation for edit propagation of
high-resolution images or video. Previous approaches for edit propagation typically
employ a global optimization over the whole set of image pixels, incurring a
prohibitively high memory and time consumption for high-resolution
images. Rather than propagating an edit pixel by pixel, we follow the principle of 
sparse representation to obtain a compact set of representative samples (or features)
and perform edit propagation on the samples instead ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/wang_gi14.png">
<IMG width=200 src="pubs/images/wang_gi14.png"></a>
</TD>
<TD vAlign=center align=left>
1. Hui Wang, <a href="http://http://faculty.cua.edu/simari/">Patricio Simari</a>,
Zhixun Su, and <strong>Hao Zhang</strong>,
"<strong>Spectral Global Intrinsic Symmetry Invariant Functions</strong>",
<i>Proc. of Graphics Interface</i>, pp. 209-215, 2014.
[<a href="http://huiw.weebly.com/spectralgisif.html">Project page</a> |
<a href="pubs/haoz_paper_bib.html#gisif_gi14">bibtex</a>]
<p>
We introduce spectral Global Intrinsic Symmetry Invariant Functions (GISIFs), a class 
of GISIFs obtained via eigendecomposition of the Laplace-Beltrami operator on compact 
Riemannian manifolds. We discretize the spectral GISIFs for 2D manifolds approximated 
either by triangle meshes or point clouds. In contrast to GISIFs obtained from 
geodesic distances, our spectral GISIFs are robust to local topological changes. 
Additionally, for symmetry analysis our spectral GISIFs can be viewed as generalizations 
of the classical Heat (HKSs) and Wave Kernel Signatures (WKSs), and, as such, represent 
a more expressive and versatile class of functions ...
</TD>
</TR>

</table>

<a name="papers2013">

<h3>2013</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/wang_siga13.png">
<IMG width=200 src="pubs/images/wang_siga13_small.png"></a>
</TD>
<TD vAlign=center align=left>
11. Yunhai Wang, <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>, 
Tianhua Wang,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>
<strong>Hao Zhang</strong>, and
<a href="">Baoquan Chen</a>,
"<strong>Projective Analysis for 3D Shape Segmentation</strong>",
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>,
Vol. 32, No. 6, Article 192, 2013.
[<a href="pubs/wang_siga13_psa.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#pas_siga13">bibtex</a>]
<p>
We introduce projective analysis for semantic segmentation and labeling of 3D shapes. 
The analysis treats an input 3D shape as a collection of 2D projections, label each 
projection by transferring knowledge from existing labeled images, and back-projects 
and fuses the labelings on the 3D shape ... 
Projective analysis simplifies the processing task by working in a lower-dimensional 
space, circumvents the requirement of having complete and well-modeled 3D shapes, 
and addresses the data challenge for 3D shape analysis by leveraging the massive 
image data. 
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/liu_smi13.png">
<IMG width=200 src="pubs/images/liu_smi13.png"></a>
</TD>
<TD vAlign=center align=left>
10. Zhenbao Liu, Sicong Tang, Shuhui Bu, and <strong>Hao Zhang</strong>,
"<strong>New Evaluation Metrics for Mesh Segmentation</strong>",
<i>Computer & Graphics (Special Issue of SMI)</i>,
Vol. 37, No. 6, pp. 553-564, 2013.
[<a href="pubs/liu_smi13_metric.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#seg_metrics_smi13">bibtex</a>]
<p>
The four metrics adopted by the well-known Princeton Segmentation 
Benchmark have been extensively applied to evaluate mesh segmentation 
algorithms. However, comparison to only a single ground-truth is problematic 
since one object may have multiple semantic segmentations. We propose two novel 
metrics to support comparison with multiple ground-truth mesh segmentations,
which we call Similarity Hamming Distance (SHD) and Adaptive Entropy Increment (AEI) ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zhang_sig13.png">
<IMG width=200 src="pubs/images/zhang_sig13_small.png"></a>
</TD>
<TD vAlign=center align=left>
9. <strong>Hao Zhang</strong>,
<a href="http://www.kevinkaixu.net/">Kai Xu</a>, Wei Jiang, Jinjie Lin,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>,
"<strong>Layered Analysis of Irregular Facades via Symmetry Maximization</strong>",
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>,
Vol. 32, No. 4, pp. 121:1-121:10, 2013.
[<a href="pubs/zhang_sig13_symax.pdf">PDF</a> |
<a href="http://www.kevinkaixu.net/k/projects/symbr.html">Project page</a> |
<a href="pubs/haoz_paper_bib.html#symax_sig13">bibtex</a>]
<p>
We present an algorithm for hierarchical and layered analysis of irregular facades, 
seeking a high-level understanding of facade structures. By introducing layering into 
the analysis, we no longer view a facade as a flat structure, but allow it to be 
structurally separated into depth layers, enabling more compact and natural 
interpretations of building facades. Computationally, we perform a symmetry-driven 
search for an optimal hierarchical decomposition defined by split and layering 
operations applied to an input facade. The objective is symmetry maximization ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_sig13.png">
<IMG width=200 src="pubs/images/vanKaick_sig13_small.png"></a>
</TD>
<TD vAlign=center align=left>
8. <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>,
<a href="http://www.kevinkaixu.net/">Kai Xu</a>, <strong>Hao Zhang</strong>,
Yanzhen Wang, Shuyang Sun, 
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Co-Hierarchical Analysis of Shape Structures</strong>",
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>,
Vol. 32, No. 4, Article 69, 2013.
[<a href="pubs/vanKaick_sig13_conshier.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#conshier_sig13">bibtex</a>]
<p>
We introduce an unsupervised co-hierarchical analysis of a set of shapes, 
aimed at discovering their hierarchical part structures and revealing relations 
between geometrically dissimilar yet functionally equivalent shape parts across 
the set. The core problem is that of representative co-selection. For each shape 
in the set, one representative hierarchy (tree) is selected from among many possible 
interpretations of the hierarchical structure of the shape. Collectively, the 
selected tree representatives maximize the within-cluster structural similarity among them. 
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/huang_ss_sig13.png">
<IMG width=200 src="pubs/images/huang_ss_sig13_small.png"></a>
</TD>
<TD vAlign=center align=left>
7. Shi-Sheng Huang,
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
Chao-Hui Shen, <strong>Hao Zhang</strong>, 
<a href="http://www.cs.ubc.ca/~sheffa/">Alla Sheffer</a>,
<a href="http://cg.cs.tsinghua.edu.cn/prof_hu.htm">Shi-Min Hu</a>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Qualitative Organization of Collections of Shapes via Quartet Analysis</strong>",
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>,
Vol. 32, No. 4, pp. 71:1-10, 2013.
[<a href="http://cg.cs.tsinghua.edu.cn/quartet/">Project page</a> |
<a href="pubs/haoz_paper_bib.html#quartet_sig13">bibtex</a>]
<p>
We present a method for organizing a heterogeneous collection of 3D shapes 
for overview and exploration. Instead of relying on quantitative distances, 
which may become unreliable between dissimilar shapes, we introduce a qualitative 
analysis which utilizes multiple distance measures but only in cases where the 
measures can be reliably compared. Our analysis is based on the notion of quartets, 
each defined by two pairs of shapes, where the shapes in each pair are close to 
each other, but far apart from the shapes in the other pair.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/huang_sig13.png">
<IMG width=200 src="pubs/images/huang_sig13_small.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
Shihao Wu,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>,
<strong>Hao Zhang</strong>, Guiqing Li, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>,
"<strong><i>L1</i>-Medial Skeleton of Point Cloud</strong>",
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, 
Vol. 32, No. 4, Article 65, 2013.
[<a href="pubs/huang_sig13_l1skel.pdf">PDF</a> |
<a href="https://vcc.tech/research/2013/L1">Project page</a> |
<a href="pubs/haoz_paper_bib.html#l1skel_sig13">bibtex</a>]
<p>
We introduce <i>L1</i>-medial skeleton as a curve skeleton representation for 3D point 
cloud data. The <i>L1</i>-median is well-known as a robust global center of an arbitrary 
set of points. We make the key observation that adapting <i>L1</i>-medians locally to 
a point set representing a 3D shape gives rise to a one-dimensional structure, which can 
be seen as a localized center of the shape ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/mitra_star13.png">
<IMG width=200 src="pubs/images/mitra_star13_small.png"> </a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://www0.cs.ucl.ac.uk/staff/N.Mitra/">Niloy Mitra</a>,
<a href="http://www.mpi-inf.mpg.de/~mwand/">Michael Wand</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
<a href="http://www.stanford.edu/~bokeloh/">Martin Bokeloh</a>,
"<strong>Structure-Aware Shape Processing</strong>,"
<i>Eurographics State-of-the-Art Report (STAR)</i>, 2013.
[<a href="pubs/mitra_star13.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#mitra_star13">bibtex</a>]
<p>
In this survey paper, we organize, summarize, and present the key concepts and methodological approaches towards efficient structure-aware shape processing. We discuss common models of structure, their implementation in terms of mathematical formalism and algorithms, and explain the key principles in the context of a number of state-of- the-art approaches. Further, we attempt to list the key open problems and challenges, both at the technical and at the conceptual level, to make it easier for new researchers to better explore and contribute to this topic.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<IMG width=200 src="pubs/images/jiang_gm13.png">
</TD>
<TD vAlign=center align=left>
4. Wei Jiang,
<a href="http://www.kevinkaixu.net/">Kai Xu</a>,
Zhiquan Cheng, and <strong>Hao Zhang</strong>,
"<strong>Skeleton-Based Intrinsic Symmetry Detection on Point Clouds</strong>,"
<i>Graphical Models</i>, Vol. 75, No. 4, pp. 177-188, 2013.
[<a href="pubs/">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#skelsym_gm13">bibtex</a>]
<p>
We present a skeleton-based algorithm for intrinsic symmetry detection
on imperfect 3D point cloud data. The data imperfections such as noise
and incompleteness make it difficult to reliably compute geodesic distances,
<p>
We introduce <i>L1</i>-medial skeleton as a curve skeleton representation for 3D point 
cloud data. The <i>L1</i>-median is well-known as a robust global center of an arbitrary 
set of points. We make the key observation that adapting <i>L1</i>-medians locally to 
a point set representing a 3D shape gives rise to a one-dimensional structure, which can 
be seen as a localized center of the shape ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_cgf13.png">
<IMG width=200 src="pubs/images/vanKaick_cgf13_small.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://people.scs.carleton.ca/~olivervankaick/index.html">Oliver van Kaick</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
"<strong>Bilateral Maps for Partial Matching</strong>"
<i>Computer Graphics Forum</i>, Vol. 32, No. 6, pp. 189-200, 2013.
[<a href="pubs/vanKaick_cgf13_bimap.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#vanKaick_cgf13">bibtex</a>]
<p>
We introduce <i>bilateral map</i>, a local shape descriptor whose
region of interest is defined by <i>two</i> feature points.  Compared
to the classical descriptor definition using single points, the
bilateral approach exploits the use of a second point to place more
constraints on the selection of the spatial context for feature
analysis. This leads to a descriptor where the shape of the region of
interest is anisotropic and adapts to the context of the two points, 
making it more refined for shape analysis, in particular, partial matching.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_cgf13_style.png">
<IMG width=200 src="pubs/images/li_cgf13_style_small.png"> </a>
</TD>
<TD vAlign=center align=left>
2. <a href="http://www.computer-graphics.cn/~hh/">Honghua Li</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.computer-graphics.cn/~eric/">Yanzhen Wang</a>,
<a href="http://sites.google.com/site/jjcaoshomepage/">Junjie Cao</a>,
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Curve Style Analysis in a Set of Shapes</strong>,"
<i>Computer Graphics Forum</i>, Vol. 32, No. 6, pp. 77-88, 2013.
[<a href="pubs/li_cgf13_style.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#style_cgf13">bibtex</a>]
<p>
We pose the open question "how to extract
styles from geometric shapes?" and address one instance of the problem. Speciﬁcally, we present an unsupervised
algorithm for identifying curve styles in a set of shapes ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/huang_tog13_ear.png">
<IMG width=200 src="pubs/images/huang_tog13_ear_small.png"> </a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
Shihao Wu,
<a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<a href="http://www.cs.ubc.ca/~ascher/">Uri Ascher</a>, and
<strong>Hao Zhang</strong>,
"<strong>Edge-Aware Point Set Resampling</strong>,"
<i>ACM Trans. on Graphics</i> (presented at SIGGRAPH 2013), Volume 32, Number 1, Article 9, 2013. 
[<a href="https://vcc.tech/research/2013/EAR">Project page with source code</a> |
 <a href="pubs/haoz_paper_bib.html#ear_tog13">bibtex</a>]
<p>
We propose a resampling approach to
process a noisy and possibly outlier-ridden point set in an edge-aware manner.
Our key idea is to first resample away from the edges so that reliable normals can be
computed at the samples, and then based on reliable data, we
progressively resample the point set while approaching the edge singularities ...
<p>
</TD>
</TR>

</table>

<a name="papers2012">

<h3>2012</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/xu_siga12.png">
<IMG width=200 src="pubs/images/xu_siga12_small.png"> </a>
</TD>
<TD vAlign=center align=left>
10. <a href="http://www.kevinkaixu.net/">Kai Xu</a>,
<strong>Hao Zhang</strong>, Wei Jiang,
<a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
<a href="http://www.computer-graphics.cn/~zhiquan/index.html">Zhiquan Cheng</a>,
<a href="http://staff.ustc.edu.cn/~lgliu/">Ligang Liu</a>, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>,
"<strong>Multi-Scale Partial Intrinsic Symmetry Detection,</strong>"
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 31, No. 6, Article 181, 2012.
[<a href="pubs/xu_siga12_msym.pdf">PDF</a> | <a href="http://www.kevinkaixu.net/k/projects/msym.html">Project page (with
data)</a> |
 <a href="pubs/haoz_paper_bib.html#msym_siga12">bibtex</a>]
<p>
We present an algorithm for multi-scale partial intrinsic symmetry detection over 2D 
and 3D shapes, where the scale of a symmetric region is defined by 
intrinsic distances between symmetric points over the region. To identify prominent 
symmetric regions which overlap and vary in form and scale, we decouple scale extraction 
and symmetry extraction by performing two levels of clustering. First, significant 
symmetry scales are identified by clustering sample point <i>pairs</i> from an input shape ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_siga12.png">
<IMG width=200 src="pubs/images/li_siga12.png"> </a>
</TD>
<TD vAlign=center align=left>
9. <a href="http://www.computer-graphics.cn/~hh/">Honghua Li</a>,
<a href="http://ialhashim.github.io/">Ibraheem Alhashim</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Stackabilization,</strong>"
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 31, No. 6, Article 158, 2012.
[<a href="pubs/li_siga12_stack.pdf">PDF</a> |
 <a href="http://code.google.com/p/stacker/">Code</a> |
 <a href="pubs/haoz_paper_bib.html#stack_siga12">bibtex</a>]
<p>
We introduce the geometric problem of <i>stackabilization</i>: how to geometrically modify a
3D object so that it is more amenable to stacking. Given a 3D object and a stacking direction,
we define a measure of stackability, which is derived from the gap between the lower
and upper <i>envelopes</i> of the object in a stacking configuration along the stacking 
direction. The main challenge in stackabilization lies in the desire to modify the object's 
geometry only subtly so that the intended functionality and aesthetic appearance of the 
original object are not significantly affected ... 
<p> 
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/huang_siga12.png">
<IMG width=200 src="pubs/images/huang_siga12_small.png"> </a>
</TD>
<TD vAlign=center align=left>
8. <a href="http://vcc.szu.edu.cn/~huihuang/">Hui Huang</a>,
<a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
Yaobin Ouyang, Fuwen Tao, and
<strong>Hao Zhang</strong>,
"<strong>Field-Guided Registration for Feature-Conforming Shape Composition,</strong>"
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 31, No. 6, Article 179, 2012.
[<a href="pubs/huang_siga12_stitch.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#stitch_siga12">bibtex</a>]
<p>
We present an automatic shape composition method to fuse two shape parts which may 
not overlap and possibly contain sharp features, a scenario often encountered when 
modeling man-made objects. At the core of our method is a novel <i>field-guided</i> 
approach to automatically align two input parts in a <i>feature-conforming</i> manner. 
The key to our field-guided shape registration is a <i>natural continuation</i> of 
one part into the ambient field as a means to introduce an overlap with 
the distant part, which then allows a surface-to-field registration ...
<p> 
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/wang_siga12.png">
<IMG width=200 src="pubs/images/wang_siga12_small.png"> </a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://web.siat.ac.cn/~yunhai/">Yunhai Wang</a>,
Shmulik Asafi,
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>,
"<strong>Active Co-Analysis of a Set of Shapes,</strong>"
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia)</i>, Vol. 31, No. 6, Article 165, 2012.
[<a href="pubs/wang_siga12_ssl.pdf">PDF</a> |
 <a href="http://www.yunhaiwang.org/public_html/ssl/ssl.htm">Project page</a> |
 <a href="http://www.yunhaiwang.org/public_html/ssl/ssd.htm">The Shape COSEG Dataset</a> |
 <a href="pubs/haoz_paper_bib.html#ssl_siga12">bibtex</a>]
<p>
We consider the use of a semi-supervised learning method where the user actively 
assists in the co-analysis by iteratively providing input that progressively 
constrains the system. We introduce a novel constrained clustering method based 
on a spring system which embeds elements to better respect their inter-distances 
in feature space together with the user given set of constraints. We also present an 
active learning method that suggests to the user where his input is 
likely to be the most effective in refining the results.
<p> </TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/aghdaii_cag12.png">
<IMG width=200 src="pubs/images/aghdaii_cag12_small.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://www.cs.sfu.ca/~naghdaii/personal/">Nima Aghdaii</a>,
Hamid Younesy, and
<strong>Hao Zhang</strong>,
"<strong>5-6-7 Meshes: Remeshing and Analysis</strong>"
<i>Computer & Graphics</i>, extended version of GI'12 paper,
Vol. 36, No. 8, pp. 1072-1083, 2012.
[<a href="pubs/aghdaii_cag12_567.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#aghdaii_cag12">bibtex</a>]
<p>
We introduce a new type of meshes called 5-6-7 meshes, analyze their properties, and
present a 5-6-7 remeshing algorithm. A 5-6-7 mesh is a closed triangle mesh where
each vertex has valence 5, 6, or 7. We prove that it is always possible to convert
an arbitrary mesh into a 5-6-7 mesh. We present a remeshing algorithm which converts
a closed triangle mesh with arbitrary genus into a 5-6-7 mesh which a) closely
approximates the original mesh geometrically, e.g., in terms of feature preservation,
and b) has a comparable vertex count as the original mesh.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/tag_sgp12.png">
<IMG width=200 src="pubs/images/tag_sgp12_small.png"> </a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://sites.google.com/site/andreatagliasacchi/">Andrea Tagliassachi</a>,
<a href="http://ialhashim.github.io/">Ibraheem Alhashim</a>,
<a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a>, and
<strong>Hao Zhang</strong>,
"<strong>Mean Curvature Skeletons,</strong>"
<i>Computer Graphics Forum</i> (Special Issue of SGP),
Volume 31, Number 5, pp. 1735-1744, 2012.
[<a href="pubs/tag_sgp12.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#tag_sgp12">bibtex</a>]
<p>
We formulate the skeletonization problem via mean curvature flow (MCF). While the classical application 
of MCF is surface fairing, we take advantage of its area-minimizing characteristic to drive the curvature 
flow towards the extreme so as to collapse the input mesh geometry and obtain a skeletal structure. By 
analyzing the differential characteristics of the flow, we reveal that MCF locally increases shape 
anisotropy. This justifies the use of curvature motion for skeleton computation, and leads to the 
generation of what we call "mean curvature skeletons" ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/sig12_civil.png">
<IMG width=200 src="pubs/images/sig12_civil_small.png"> </a>
</TD>
<TD vAlign=center align=left>
4. <a href="http://www.kevinkaixu.net/">Kai Xu</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>
"<strong>Fit and Diverse: Set Evolution for Inspiring 3D Shape Galleries</strong>,"
<i>ACM Trans. on Graphics (Special Issue of SIGGRAPH)</i>, Vol. 31, No. 4, pp. 57:1-57:10, 2012.
[<a href="pubs/xu_sig12_civil.pdf">PDF (15 MB)</a> |
 <a href="pubs/haoz_paper_bib.html#civil_sig12">bibtex</a>]
<p>
We introduce set evolution as a means for creative 3D shape modeling, where an
initial population of 3D models is evolved to produce generations of novel shapes.
Part of the evolving set is presented to a user as a shape gallery to offer modeling suggestions.
User preferences define the fitness for the evolution so
that over time, the shape population will mainly consist of individuals with good
fitness. However, to inspire the user's creativity, we must also keep the evolving
set diverse. Hence the evolution is ``fit and diverse'' ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/aghdaii_gi12.png">
<IMG width=200 src="pubs/images/aghdaii_gi12.png"></a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.cs.sfu.ca/~naghdaii/personal/">Nima Aghdaii</a>,
Hamid Younesy, and
<strong>Hao Zhang</strong>, 
"<strong>5-6-7 Meshes,</strong>"
<i>Proc. of Graphics Interface</i>, pp. 27-34, 2012.
[<a href="pubs/aghdaii_gi12_567.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#aghdaii_gi12">bibtex</a>]
<p>
A 5-6-7 mesh is a closed triangle mesh where each vertex has valence 5, 
6, or 7. An intriguing question is whether it is always possible to convert an arbitrary mesh into a 5-6-7 mesh. In this
paper, we answer the question in the positive. We present a 5-6-7 remeshing algorithm which converts any
closed triangle mesh with arbitrary genus into a 5-6-7 mesh which a) closely approximates the original mesh 
geometrically, e.g., in terms of feature preservation, and b) has a comparable vertex count as the original
mesh.
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/wang_gmp12.png">
<IMG width=200 src="pubs/images/wang_gmp12_small.png"> </a>
</TD>
<TD vAlign=center align=left>
2. <a href="http://huiwang.jimdo.com/">Hui Wang</a>, 
Zhixun Su, Jinjie Cao, Ye Wang, and
<strong>Hao Zhang</strong>,
"<strong>Empirical Mode Decomposition on Surfaces,</strong>"
<i>Graphical Models (Special Issue of GMP)</i>, Vol. 74, No. 4, pp. 173-183, 2012.
[<a href="pubs/wang_gmp12.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#emd_gmp12">bibtex</a>]
<p>
Empirical Mode Decomposition (EMD) is a powerful tool for the analysis of non-stationary and 
nonlinear signals, and has drawn a great deal of attention in various areas. In this paper, 
we generalize the classical EMD from Euclidean space to surfaces represented as triangular meshes. 
Inspired by the EMD, we also make a first step in using the extremal envelope method for 
feature-preserving smoothing.
</TD> 
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/alhashim_tvc_detail.png">
<IMG width=200 src="pubs/images/alhashim_tvc_detail.png"> </a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://ialhashim.github.io/">Ibraheem Alhashim</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.math.zju.edu.cn/ligangliu/">Ligang Liu</a>,
"<strong>Detail-Replicating Shape Stretching,</strong>"
<i>the Visual Computer</i>, Vol. 28, No. 12, pp. 1153-1166, 2012.
[<a href="pubs/alhashim_tvc_detail_reduced.pdf">PDF</a> |
<a href="http://www.youtube.com/watch?v=VAXm7Wm-R7c">Video</a> |
<a href="http://code.google.com/p/extend-mesh/">Code</a> |
<a href="pubs/haoz_paper_bib.html#drss_tvc12">bibtex</a>]
<p>
We propose a simple and efficient method that helps create model 
variations by applying non-uniform stretching on 3D models with 
organic geometric details. The method replicates the geometric 
details and synthesizes extensions by adopting texture synthesis 
techniques on surface details.
</TD>
</TR>

</TABLE>

<p>

<a name="papers2011">

<h3>2011</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/sidi_siga11.png">
<IMG width=200 src="pubs/images/sidi_siga11.png"> </a>
</TD>
<TD vAlign=center align=left>
9. Oana Sidi,
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
Yanir Kleiman, <strong>Hao Zhang</strong>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Unsupervised Co-Segmentation of a Set of Shapes 
via Descriptor-Space Spectral Clustering,</strong>"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2011)</i>,
Volume 30, Number 6, Article 126, 2011.
[<a href="pubs/sidi_siga11_coseg.pdf">PDF</a> (11 MB) |
<a href="pubs/haoz_paper_bib.html#sidi_siga11">bibtex</a>]
<p>
We introduce an algorithm for unsupervised co-segmentation of a set of
shapes so as to reveal the semantic shape parts and establish
their correspondence across the set. 
Our algorithm exploits a key enabling feature of the input set, namely,
dissimilar parts may be ``linked'' through third-parties present in the
set ... 
</TD>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/lin_siga11.png">
<IMG width=200 src="pubs/images/lin_siga11_small.png"> </a>
</TD>
<TD vAlign=center align=left>
8. <a href="http://web.siat.ac.cn/~jinjie/">Jinjie Lin</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<strong>Hao Zhang</strong>,
Cheng Liang,
<a href="http://graphics.cs.ucdavis.edu/~asharf/">Andrei Sharf</a>,
<a href="http://graphics.uni-konstanz.de/mitarbeiter/deussen.php?language=english">Oliver Deussen</a>, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>,
"<strong>Structure-Preserving Retargeting of Irregular 3D Architecture,</strong>"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2011)</i>,
Volume 30, Number 6, Article 183, 2011.
[<a href="pubs/lin_siga11_retarget_reduced.pdf">PDF</a> |
<a href="pubs/lin_siga11_retarget.pdf">Highres PDF (29MB)</a> |
<a href="pubs/haoz_paper_bib.html#lin_siga11">bibtex</a>]
<p>
We present an algorithm for interactive <i>structure-preserving</i> retargeting
of <i>irregular</i> 3D architecture models, offering the modeler an easy-to-use
tool to quickly generate a variety of 3D models that resemble an input piece in its structural
style ...
</TD>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/tag_sgp11.png">
<IMG width=200 src="pubs/images/tag_sgp11.png"> </a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://sites.google.com/site/andreatagliasacchi/">Andrea Tagliassachi</a>,
<a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
and <a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>VASE: Volume-Aware Surface Evolution for Surface Reconstruction from
Incomplete Point Clouds,</strong>"
<i>Computer Graphics Forum</i> (Special Issue of SGP),
Volume 30, Number 5, pp. 1563-1571, 2011.
[<a href="http://gruvi.cs.sfu.ca/files/project/vase/tagliasacchi_sgp11.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#tag_sgp11">bibtex</a>]
<p>
Objects with many concavities are difficult to acquire using laser scanners.
The resulting point scan typically suffers from large amounts of missing data.
We introduce weak volumetric priors which assume that the
volume of a shape varies smoothly and that each point cloud sample is
visible from outside the shape. Specifically, the union of view-rays
given by the scanner implicitly carves the exterior volume, while
volumetric smoothness regularizes the internal volume.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/xu_sig11.png">
<IMG width=200 src="pubs/images/xu_sig11_small.png"> </a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://www.kevinkaixu.net/">Kai Xu</a>, Hanlin Zheng,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<a href="http://www.math.zju.edu.cn/ligangliu/">Ligang Liu</a>,
and Yueshan Xiong,
"<strong>Photo-Inspired Model-Driven 3D Object Modeling,</strong>"
<i>ACM Trans. on Graphics</i> (Proceedings of SIGGRAPH 2011),
Volume 30, Number 4, pp. 80:1-80:10, 2011.
[<a href="http://www.cs.sfu.ca/~haoz/pubs/xu_sig11_photo.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#xu_sig11">bibtex</a>]
 <p>
We introduce an algorithm for 3D object modeling where the user draws creative inspiration from 
an object captured in a single photograph. Our method leverages the rich source of photographs 
for creative 3D modeling. However, with only a photo as a guide, creating a 3D model from 
scratch is a daunting task. We support the modeling process by utilizing an available set of 3D 
candidate models. Specifically, the user creates a digital 3D model as a geometric variation 
from a 3D candidate.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/olson_smi11.png">
<IMG width=200 src="pubs/images/olson_smi11_small.png"> </a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a>,
<a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.ubc.ca/~sheffa/">Alla Sheffer</a>,
"<strong>Point Set Silhouettes via Local Reconstruction,</strong>"
<i>Computer & Graphics</i> (Special Issue of SMI 2011),
Volume 35, Number 3, pp. 500-509, 2011.
[<a href="pubs/olson_smi11_psil.pdf">PDF (4MB)</a> |
  <a href="pubs/haoz_paper_bib.html#olson_smi11">bibtex</a>]
<p>
We present an algorithm to compute the silhouette set of a point cloud. Previous 
methods extract point set silhouettes by thresholding point normals, which can 
lead to simultaneous over- and under-detection of silhouettes. We argue that 
additional information such as surface curvature is necessary to resolve these  
issues. To this end, we develop a local reconstruction scheme using Gabriel and 
intrinsic Delaunay criteria and defi?ne point set silhouettes 
based on the notion of a silhouette generating set ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/wang_eg11_small.png">
<IMG width=200 src="pubs/images/wang_eg11_small.png"> </a>
</TD>
<TD vAlign=center align=left>
4. <a href="http://www.computer-graphics.cn/~eric/">Yanzhen Wang</a>,
<a href="http://sites.google.com/site/kevinkaixu/">Kai Xu</a>, Jun Li,
<strong>Hao Zhang</strong>,
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel
Shamir</a>, <a href="http://www.math.zju.edu.cn/ligangliu/">Ligang Liu</a>,
Zhiquan Cheng, and Yueshan Xiong,
"<strong>Symmetry Hierarchy of Man-Made Objects,</strong>"
<i>Computer Graphics Forum</i> (Special Issue of Eurographics 2011),
Volume 30, Number 2, pp. 287-296, 2011.
[<a href="http://kevinkaixu.net/projects/symh.html">Project page</a> |
 <a href="pubs/wang_eg11_symh.pdf">PDF (14MB)</a> |
 <a href="pubs/wang_eg11_symh_reduced.pdf">PDF reduced (500K)</a> |
 <a href="pubs/haoz_paper_bib.html#wang_eg11">bibtex</a>]
 <p>
We introduce symmetry hierarchy of man-made objects, a high-level structural 
representation of a 3D model providing a symmetry-induced, hierarchical 
organization of the model's constituent parts. We show that symmetry hierarchy 
naturally implies a hierarchical segmentation that is more meaningful than those 
produced by local geometric considerations. We also develop an application of 
symmetry hierarchies for structural shape editing.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_eg11.png">
<IMG width=200 src="pubs/images/vanKaick_eg11_small.png"> </a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
<a href="http://sites.google.com/site/andreatagliasacchi/">Andrea
Tagliasacchi</a>, Oana Sidi,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<a href="http://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>, and
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
"<strong>Prior Knowledge for Part Correspondence,</strong>"
<i>Computer Graphics Forum</i> (Special Issue of Eurographics 2011),
Volume 30, Number 2, pp. 553-562, 2011.
[<a href="pubs/vanKaick_eg11_knowledge.pdf">PDF (10 MB)</a> |
 <a href="pubs/vanKaick_eg11_knowledge_reduced.pdf">PDF reduced</a> |
 <a href="pubs/haoz_paper_bib.html#vanKaick_eg11">bibtex</a>]
<p>
We stipulate that under challenging scenarios, shape correspondence by
humans involves recognition of the shape parts where prior knowledge on the
parts would play a more dominant role than geometric similarity.
We introduce an approach to part correspondence which incorporates prior
knowledge and combines the knowledge with content-driven analysis based on
geometric similarity between the matched shapes ...

<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_cgf11.png">
<IMG height=120 width=200 src="pubs/images/vanKaick_cgf11_small.png"> </a>
</TD>
<TD vAlign=center align=left>
2. <a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>A Survey on Shape Correspondence,</strong>"
<i>Computer Graphics Forum</i> (extended version of Eurographics STAR),
Volume 30, Number 6, pp. 1681-1707, 2011.
[<a href="pubs/vanKaick_cgf11_survey.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#vanKaick_cgf11">bibtex</a>]
<p>
<p>
We review methods that are designed to compute correspondences between
geometric shapes represented by triangle meshes, contours, or point sets.
This survey is motivated in part by some recent developments in space-time
registration, where one seeks to correspond non-rigid and time-varying
surfaces, and semantic shape analysis, which underlines a recent trend to
incorporate shape understanding into the analysis pipeline ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/kahlert_tvc10_long.jpg">
<IMG height=120 width=200 src="pubs/images/kahlert_tvc10.png"> </a>
</TD>
<TD vAlign=center align=left>
1. Joe Kahlert,
<a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a>,
and <strong>Hao Zhang</strong>,
"<strong>Width-Bounded Geodesic Strips for Surface Tiling</strong>,"
<i>The Visual Computer</i>, Vol. 27, No. 1, pp. 45-56, 2011. 
[<a href="pubs/kahlert_tvc10.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#kahlert_tvc10">bibtex</a>]
<p>
We present an algorithm for computing families
of geodesic curves over an open mesh patch to partition
the patch into strip-like segments. Specifically, the
segments can be well approximated using strips obtained
by trimming long, rectangular pieces of material possessing
a prescribed width. We call this width-bounded
geodesic strip tiling of a curved surface, a problem with
practical applications such as the surfacing of curved
roofs.
<p>
</TD>
</TR>

</table>

<a name="papers2010">

<h3>2010</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/xu_siga10.png">
<IMG height=160 width=200 src="pubs/images/xu_siga10_small.png"> </a>
</TD>
<TD vAlign=center align=left>
10. <a href="http://sites.google.com/site/kevinkaixu/">Kai Xu</a>, 
<a href="http://www.computer-graphics.cn/members/~hh/">Honghua Li</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
Yueshan Xiong, and Zhiquan Cheng,
"<strong>Style-Content Separation by Anisotropic Part Scales</strong>,"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2010)</i>,
Volume 29, Number 6, pp. 184:1-184:10, 2010.
[<a href="pubs/xu_siga10_style.pdf">PDF (10MB) </a> |
<a href="https://sites.google.com/site/kevinkaixu/publications/style">Project
page</a> |
<a href="pubs/haoz_paper_bib.html#xu_siga10">bibtex</a>]
<p>
We perform co-analysis of a set of man-made 3D objects to allow the 
creation of novel instances derived from the set. We analyze the objects 
at the part level and treat the anisotropic part scales as a shape 
style. The co-analysis then allows style transfer to synthesize new 
objects. The key to co-analysis is part correspondence, where a major 
challenge is the handling of large style variations and diverse geometric 
content in the shape set. We propose style-content separation as 
a means to address this challenge ... 
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/shalom_siga10_small.png">
<IMG height=120 width=200 src="pubs/images/shalom_siga10_small.png"> </a>
</TD>
<TD vAlign=center align=left>
9. Shy Shalom, 
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel 
Shamir</a>, <strong>Hao Zhang</strong>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Cone Carving for Surface Reconstruction</strong>,"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2010)</i>,
Volume 29, Number 6, Article 150, 2010.
[<a href="pubs/shalom_siga10_cone.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#shalom_siga10">bibtex</a>]
<p>
We present cone carving, a novel space carving technique towards 
topologically correct surface reconstruction from an incomplete scanned 
point cloud. The technique utilizes the point samples not only for local 
surface position estimation but also to obtain global visibility  
information under the assumption that each acquired point is visible from 
a point laying outside the shape. This enables associating each point 
with a generalized cone, called the <i>visibility cone</i>, that carves a 
portion of the outside ambient space of the shape from the inside out.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/livny_siga10.jpg">
<IMG height=120 width=200 src="pubs/images/livny_siga10_small.jpg"> </a>
</TD>
<TD vAlign=center align=left>
8. Yotam Livny, Feilong Yan,
<a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a>,
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>, 
<strong>Hao Zhang</strong>, and 
<a href="http://www.cs.bgu.ac.il/~el-sana/">Jihad El-Sana</a>,
"<strong>Automatic Reconstruction of Tree Skeletal Structures from 
Point Clouds</strong>,"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2010)</i>,
Volume 29, Number 6, Article 151, 2010.
[<a href="pubs/livny_siga10_tree.pdf">PDF (20MB)</a> | 
 <a href="pubs/livny_siga10_tree_reduced.pdf">PDF reduced (64K)</a> |
 <a href="pubs/haoz_paper_bib.html#livny_siga10">bibtex</a>]
<p>
In this paper, we perform active laser scanning of real world vegetation 
and present an automatic approach that robustly reconstructs skeletal
structures of trees, from which full geometry can be generated. The
core of our method is a series of {\it global optimizations} that
fit skeletal structures to the often sparse, incomplete,
and noisy point data. A significant benefit of our approach is its
ability to reconstruct multiple overlapping trees simultaneously
without segmentation.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/nan_sig10.png">
<IMG height=120 width=200 src="pubs/images/nan_sig10_small.png"> </a>
</TD>
<TD vAlign=center align=left>
7. <a href="http://web.siat.ac.cn/~liangliang">Liangliang Nan</a>,
<a href="http://graphics.cs.ucdavis.edu/~asharf/">Andrei Sharf</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>,
"<strong>SmartBoxes for Interactive Urban Reconstruction,</strong>"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH 2010)</i>,
Volume 29, Number 4, Article 93, 2010.
[<a href="pubs/nan_sig10.pdf">PDF</a> | 
<a href="pubs/nan_sig10_highres.pdf">Highres PDF (17MB)</a> |
<a href="pubs/haoz_paper_bib.html#nan_sig10">bibtex</a>]
<p>
We introduce an interactive tool which enables a user to quickly
assemble an architectural model directly over a 3D point cloud
acquired from large-scale scanning of an urban scene. The user loosely
defines and manipulates simple building blocks, which we call
SmartBoxes, over the point samples. These boxes quickly snap to
their proper locations to conform to common architectural
structures. The key idea is that the building blocks are smart ...
</TD>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/part_analogy_2.png">
<IMG height=120 width=200 src="pubs/images/part_analogy_1.png"></a>
</TD>
<TD vAlign=center align=left>
6. <a href="http://www.cs.tau.ac.il/~liors/">Lior Shapira</a>,
<a href="http://www.cs.biu.ac.il/people/view/index/topicID/32/itemID/287">Shy Shalom</a>,
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and
<strong>Hao Zhang</strong>,
"<strong>Contextual Part Analogies in 3D Objects</strong>,"
<i>International Journal of Computer Vision</i>,
Vol. 89, No. 1-2, pp. 309-326, 2010.
[<a href="pubs/ijcv10_analogy.pdf">PDF</a> |
<a href="pubs/haoz_paper_bib.html#analogy_ijcv10">bibtex</a>]
<p>
We address the problem of finding analogies between parts of 3D objects.
By partitioning an object into meaningful parts and finding analogous
parts in other objects, not necessarily of the same type, based on a
contextual signature, many analysis and modeling tasks could be enhanced
...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/cao_smi10.png">
<IMG height=120 width=200 src="pubs/images/cao_smi10_small.png"> </a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://sites.google.com/site/jjcaoshomepage/">Junjie Cao</a>,
<a href="http://sites.google.com/site/andreatagliasacchi/">Andrea
Tagliasacchi</a>,
<a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a>,
<strong>Hao Zhang</strong>, and
Zhixun Su,
"<strong>Point Cloud Skeletons via Laplacian-Based Contraction,</strong>"
<i>Proc. of IEEE SMI</i>,
pp. 187-197, 2010.
[<a href="pubs/cao_smi10.pdf">PDF</a> |
<a href="http://code.google.com/p/skeletonization/">Project and code page</a> |
<a href="pubs/haoz_paper_bib.html#cao_smi10">bibtex</a>]
<p>
 We present an algorithm for curve skeleton
 extraction via Laplacian-based contraction. Our
 algorithm can be applied to surfaces with boundaries,
 polygon soups, and point clouds. We develop
 a contraction operation that is designed to work
 on generalized discrete geometry data, particularly
 point clouds, via local Delaunay triangulation and
 topological thinning ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zhang_cgf10.png">
<IMG height=120 width=200 src="pubs/images/zhang_cgf10.png"> </a>
</TD>
<TD vAlign=center align=left>
4. <strong>Hao Zhang</strong>,
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
and <a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
"<strong>Spectral Mesh Processing</strong>," (revised and extended version
of Eurographics
2007 STAR report) <i>Computer Graphics Forum</i>, Volume 29, Number 6,
pp. 1865-1894, 2010.
[<a href="pubs/zhang_cgf10_spect_survey.pdf">PDF</a> |
  <a href="pubs/haoz_paper_bib.html#zhang_cgf10">bibtex</a>]
  <p>
  We provide the first comprehensive survey on spectral mesh processing.
  Spectral methods for mesh processing and analysis rely on eigenvalues,
  eigenvectors, or eigenspace projections derived from appropriately defined
  mesh operators to carry out desired tasks ...
  <p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_ar10.png">
<IMG height=120 width=200 src="pubs/images/vanKaick_ar10_small.png"> </a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van 
Kaick</a>,
<a href="http://imaging.robarts.ca/~ward/">Aaron Ward</a>,
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
Mark Schweitzer, and <strong>Hao Zhang</strong>,
"<strong>Learning Fourier Descriptors for Computer-Aided Diagnosis of the 
Supraspinatus,</strong>"
<i><a href="http://www.academicradiology.org/">Academic Radiology</a></i>, 
Vol. 17, No. 8, pp. 1040-1049, 2010.
[<a href="pubs/vanKaick_ar10.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#vanKaick_ar10">bibtex</a>]
<p>
Supraspinatus muscle disorders are frequent and debilitating, resulting in 
pain and a limited range of shoulder motion. The gold standard for 
diagnosis involves an invasive surgical procedure ... we present a method 
to classify 3D shapes of the muscle into the relevant pathology groups, 
based on MRIs. The method learns the Fourier coefficients that best 
distinguish the different classes ...
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_star10.png">
<IMG height=120 width=200 src="pubs/images/vanKaick_star10.png"> </a>
</TD>
<TD vAlign=center align=left>
2. <a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>A Survey on Shape Correspondence,</strong>"
<i>Eurographics 2010 State-of-the-Art Report</i>, TBA.
[<a href="pubs/vanKaick_egstar10.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#vanKaick_egstar10">bibtex</a>]
<p>
<p>
We present a review of the correspondence problem targeted towards the
computer graphics audience. This survey is motivated by recent developments
such as advances in the correspondence of non-rigid or isometric shapes and 
methods that extract semantic information from the shapes ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/zheng_eg10.png">
<IMG height=120 width=200 src="pubs/images/zheng_eg10_small.png"> </a>
</TD>
<TD vAlign=center align=left>
1. Qian Zheng,
<a href="http://graphics.cs.ucdavis.edu/~asharf/">Andrei Sharf</a>,
<a href="http://sites.google.com/site/andreatagliasacchi/">Andrea 
Tagliasacchi</a>, 
<a href="http://web.siat.ac.cn/~baoquan/">Baoquan Chen</a>, 
<strong>Hao Zhang</strong>,
<a href="http://www.cs.ubc.ca/~sheffa/">Alla Sheffer</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Consensus Skeleton for Non-Rigid Space-Time Registration,</strong>"
<i>Computer Graphics Forum (Proceeding of Eurographics 2010)</i>, Volume 29,
Number 2, pp. 635-644, 2010.
[<a href="pubs/zheng_eg10.pdf">PDF</a> |
 Slides |
 <a href="pubs/haoz_paper_bib.html#zheng_eg10">bibtex</a>]
<p>
<p>
We introduce the notion of consensus skeletons for non-rigid space-time registration 
of a deforming shape. Instead of basing the registration on point features, which are 
local and sensitive to noise, we adopt the curve skeleton of the shape as a global and 
descriptive feature for the task. Our method uses no template and only assumes
that the skeletal structure of the captured shape remains largely consistent over time 
...
<p>
</TD>
</TR>

</table>

<a name="papers2009">

<h3>2009</h3>

<table cellPadding=5 width="99%" border=0>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/siga_pirs_2.png">
<IMG height=120 width=200 src="pubs/images/siga_pirs_1.png"> </a>
</TD>
<TD vAlign=center align=left>
10. <a href="http://sites.google.com/site/kevinkaixu/">Kai Xu</a>,
<strong>Hao Zhang</strong>,
<a href="http://sites.google.com/site/andreatagliasacchi/">Andrea Tagliasacchi</a>,
<a href="http://www.math.zju.edu.cn/ligangliu/">Ligang Liu</a>,
Guo Li, Min Meng, and Yueshan Xiong,
"<strong>Partial Intrinsic Reflectional Symmetry of 3D Shapes,</strong>"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2009)</i>,
Article 138. 
[<a href="pubs/siga09_pirs.pdf">PDF (16 MB)</a> |
 <a href="pubs/siga09_pirs_reduced.pdf">PDF (reduced size: 7 MB)</a> |
<a href="http://sites.google.com/site/kevinkaixu/publications/pirs">Project page</a> | 
<a href="pubs/haoz_paper_bib.html#pirs_siga09">bibtex</a>]
<p>
<p>
While many 3D objects around us exhibit various forms of global
symmetries, prominent intrinsic symmetries which exist only on parts of an
object are also well recognized ... In this paper, we introduce algorithms
to extract and utilize partial intrinsic reflectional symmetries (PIRS) of
a 3D shape ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/siga_consolidate_2.png">
<IMG height=120 width=200 src="pubs/images/siga_consolidate_1.png"> </a>
</TD>
<TD vAlign=center align=left>
9. <a href="http://www.cs.ubc.ca/~hhzhiyan/">Hui Huang</a>, Dan Li,
<strong>Hao Zhang</strong>, 
<a href="http://www.cs.ubc.ca/~ascher/">Uri Ascher</a>, and 
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Consolidation of Unorganized Point Clouds for Surface Reconstruction</strong>,"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2009)</i>, Article 176.
[<a href="pubs/siga09_consolidater.pdf">PDF (8 MB)</a> |
 <a href="pubs/siga09_consolidater_reduced.pdf">PDF (reduced size: 2 MB)</a> |
 <a href="pubs/haoz_paper_bib.html#consolidate_siga09">bibtex</a>]
<p>
We consolidate an unorganized point cloud with noise, outliers,
non-uniformities, and interference between close-by surface sheets as a
preprocess to surface generation ... First, we present a weighted locally
optimal projection operator ... Next, we introduce an iterative framework
for robust normal estimation, ...
<p>
</TD>
</TR>

<p>


<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/siga_fast_2.png">
<IMG height=120 width=200 src="pubs/images/siga_fast_1.png"> </a>
</TD>
<TD vAlign=center align=left>
8. <a href="http://sites.google.com/site/kevinkaixu/">Kai Xu</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<a href="http://www.cs.wustl.edu/~taoju/">Tao Ju</a>,
<a href="http://www.math.zju.edu.cn/ligangliu/">Ligang Liu</a>,
<strong>Hao Zhang</strong>, 
Shizhe Zhou, and
Yueshan Xiong, 
"<strong>Feature-Aligned Shape Texturing</strong>,"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2009)</i>, Article 108.
[<a href="pubs/siga09_fast.pdf">PDF (20 MB)</a> |
 <a href="pubs/siga09_fast_reduced.pdf">PDF (reduced size: 10 MB)</a> |
 <a href="http://sites.google.com/site/kevinkaixu/publications/fast">Project
 page</a> |
 <a href="http://sites.google.com/site/kevinkaixu/publications/fast/code">Source Code</a> |
<a href="pubs/haoz_paper_bib.html#fast_siga09">bibtex</a>]
<p>
We explore the use of salient curves in synthesizing natural-looking,
shape-revealing textures on surfaces. Our synthesis is guided by two
principles: matching the direction of the texture patterns to those of the
salient curves, and aligning the prominent feature lines in the texture to
the salient curves exactly ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/dyer_gpm09.jpg">
<IMG height=120 width=200 src="pubs/images/dyer_gpm09.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
7. <a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~torsten">Torsten Moeller</a>,
"<strong>Gabriel meshes and Delaunay edge flips</strong>,"
<i>Proc. of SIAM/ACM Joint Conf. on Geometric and Physical Modeling (GPM)</i>,
pp. 295-300, 2009.
[<a href="pubs/dyer_gpm09_delgab.pdf">PDF</a> |
 <a href="http://www.cs.sfu.ca/~rhdyer/personal/pubs/gpm2009full.pdf">extended
  version with more proofs</a> |
<a href="pubs/haoz_paper_bib.html#dyer_et_al_gpm09">bibtex</a>]
<p>
We undertake a study of the local properties of 2-Gabriel meshes. We show
that, under mild constraints on the dihedral angles, such meshes are
Delaunay meshes. The analysis is done by means of the Delaunay edge
flipping algorithm and it reveals the details of the distinction between
these two mesh structures ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/sig_rosa_2.png">
<IMG height=120 width=200 src="pubs/images/sig_rosa_1.png"> </a>
</TD>
<TD vAlign=center 
align=left>
6. <a href="http://sites.google.com/site/andreatagliasacchi/">Andrea 
Tagliasacchi</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>Curve Skeleton Extraction from Incomplete Point Cloud</strong>,"
<i>ACM Trans. on Graphics (Proceeding of SIGGRAPH 2009)</i>, Volume 28, Number 3,
Article 71, 9 pages, DOI = 10.1145/1531326.1531377. 
[<a href="pubs/sig09_rosa.pdf">PDF</a> | 
<a href="http://sites.google.com/site/andreatagliasacchi/publications/rosa-project-page">Project page</a> | <a href="pubs/haoz_paper_bib.html#rosa_sig09">bibtex</a>]
<p>
We present an algorithm for curve skeleton extraction from imperfect point
clouds where large portions of the data may be missing. Our construction
is primarily based on a novel notion of generalized rotational symmetry
axis (ROSA) of a point set with normals, via a variational formulation ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/smi09_harmonic_2.png">
<IMG height=120 width=200 src="pubs/images/smi09_harmonic_1.png">
</a>
</TD>
<TD vAlign=center align=left>
5. Kai Xu,
<strong>Hao Zhang</strong>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, and Yueshan Xiong,
"<strong>Dynamic Harmonic Fields for Surface Processing</strong>,"
<i>Computers and Graphics (Special Issue of SMI)</i>, Vol. 33, pp. 391-398, 2009.
[<a href="pubs/xu_smi09_harmonic.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#xu_smi09_1">bibtex</a>]
<p>
We propose a method for fast updating of harmonic fields defined on
polygonal meshes, enabling real-time insertion and deletion of
constraints. Our approach utilizes the penalty method to enforce
constraints in harmonic field computation. It maintains the symmetry
of the Laplacian system ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_cvpr09.png">
<IMG height=120 width=200 src="pubs/images/li_cvpr09.png"> </a>
</TD>
<TD vAlign=center 
align=left>
4. <a href="http://filebox.vt.edu/users/xxli/">Xiaoxing Li</a>,
Tao Jia, and
<strong>Hao Zhang</strong>,
"<strong>Expression-Insensitive 3D Face Recognition using Sparse
Representation</strong>,"
<i>IEEE CS Conf. on Computer Vision and Pattern Recognition (CVPR 
2009)</i>, pp. 2575-2582.
[<a href="pubs/li_zhang_cvpr09.pdf">PDF</a> | 
 <a href="pubs/haoz_paper_bib.html#li_cvpr09">bibtex</a>]
<p>
We present a face recognition method based on sparse representation for
recognizing 3D face meshes under expressions using low-level geometric
features ... To handle facial expressions, we design a feature pooling and
ranking scheme to collect various types of low-level geometric features
and rank them ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/smi09_qdc_2.png">
<IMG height=120 width=200 src="pubs/images/smi09_qdc_1.png"> </a>
</TD>
<TD vAlign=center align=left>
3. Kai Xu,
<a href="http://cheng.zhiquan.googlepages.com/">Zhiquan Cheng</a>,
Yanzhen Wang, Yueshan Xiong, and <strong>Hao Zhang</strong>,
"<strong>Quality Encoding for Tetrahedral Mesh Optimization</strong>,"
<i>Computers and Graphics (Special Issue of SMI)</i>, Vol. 33, pp. 250-261, 2009.
[<a href="pubs/xu_smi09_tet.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#xu_smi09_2">bibtex</a>
]
<p>
We define <i>quality differential coordinates</i> (QDC) for per-vertex
encoding of the quality of a tetrahedral mesh. Our formulation allows the
incorporation of element quality metrics into QDC construction to penalize
badly shaped and inverted tetrahedra ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left width=200>
<a href="pubs/images/eg09_partaware_2.png">
<IMG height=120 width=200 src="pubs/images/eg09_partaware_1.jpg"></a>
</TD>
<TD vAlign=center align=left>
2. <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a>,
<strong>Hao Zhang</strong>,
<a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
and
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
"<strong>A Part-Aware Surface Metric for Shape Analysis</strong>,"
<i>Computer Graphics Forum (Special Issue of Eurographics 2009)</i>,
Vol. 28, No. 2, 397-406, 2009.
[<a href="pubs/liu_eg09_partaware.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#liu_zhang_eg09">bibtex</a>]
<p>
The notion of parts in a shape plays an important role in many geometry
problems. At the same time, many such problems utilize a surface metric to
assist shape analysis and understanding. The main contribution of our work
is to bring together these two fundamental concepts ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/cgf09_tdf_2.png">
<IMG height=120 width=200 src="pubs/images/cgf09_tdf_1.png">
</a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a> and
<strong>Hao Zhang</strong>,
"<strong>Tangential Distance Fields for Mesh Silhouette
Problems,</strong>," <i>Computer Graphics Forum</i>,
Vol. 28, No. 1, pp. 84-100, 2009.
[<a href="pubs/olson_zhang_cgf08.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#olson_zhang_cgf09">bibtex</a>]
<p>
We introduce a novel class of distance fields for a given surface defined
by its tangent planes. At each point in space, we assign a scalar value
which is a weighted sum of distances to these tangent planes. We use four
applications to illustrate the benefit of using the resulting TDF scalar
field: view point selection, ...
<p>
</TD>
</TR>

</table>

<a name="papers2008p">

<h3>2008</h3>

<TABLE cellPadding=5 width="99%" border=0>

<TBODY>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/dyer_sgp08.png">
<IMG height=120 width=200 src="pubs/images/dyer_sgp08.png">
</a>
</TD>
<TD vAlign=center align=left>
3. <a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~torsten/">Torsten Moeller</a>,
"<strong>Surface sampling and the intrinsic Voronoi diagram</strong>,"
<i>Computer Graphics Forum</i>
(Special Issue of SGP), Volume 27, Number 5, pp. 1431-1439, 2008.
(won <strong>Best Paper Award</strong> at SGP)
[<a href="pubs/dyer_sgp08.pdf">PDF</a> |
  <a href="pubs/haoz_paper_bib.html#dyer_sgp08">bibtex</a>]
<p>
We develop adaptive sampling criteria which guarantee a topologically
faithful mesh and demonstrate an improvement and simplification over
earlier results, albeit restricted to 2D surfaces. These sampling
criteria are based on the strong convexity radius and the injectivity
radius ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/sgp08_ddsc_2.png">
<IMG height=120 width=200 src="pubs/images/sgp08_ddsc_1.png">
</a>
</TD>
<TD vAlign=center align=left>
2. <strong>Hao Zhang</strong>,
<a href="http://www.cs.ubc.ca/~sheffa/">Alla Sheffer</a>,
<a href="http://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
<a href="http://www.cs.ubc.ca/~qnzhou/">Qingnan Zhou</a>,
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
and
<a href="http://sites.google.com/site/andreatagliasacchi/home">
Andrea Tagliasacchi</a>,
"<strong>Deformation-Driven Shape Correspondence</strong>,"
<i>Computer Graphics Forum</i>
(Special Issue of SGP), Volume 27, Number 5, pp. 1393-1402, 2008.
[<a href="pubs/zhang_sgp08.pdf">PDF</a> |
  <a href="pubs/haoz_paper_bib.html#zhang_sgp08">bibtex</a> |
  Project page (<a href="http://www.cs.ubc.ca/labs/imager/tr/2008/DeformationDriveShapeCorrespondence/">UBC</a> | 
  <a href="http://gruvi.cs.sfu.ca/researchProject.php?s=371">SFU</a>)]
<p>
We present an automatic feature correspondence algorithm capable of
handling large, non-rigid shape variations, as well as partial matching ...
The search is deformation-driven, prioritized by a self-distortion energy
measured on meshes deformed according to a given correspondence ...
<p>
</TD>
</TR>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/liu_gi08.jpg">
<IMG height=120 width=200 src="pubs/images/liu_gi08.jpg">
</a>
</TD>
<TD vAlign=center align=left>
1. <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a>,
<strong>Hao Zhang</strong>, and James Busby,
"<strong>Convex Hull Covering of Polygonal Scenes for Accurate
Collision Detection in Games</strong>,"
Proc. of <i>Graphics Interface 2008</i>, pp. 203-210.
[<a href="pubs/liu_zhang_gi08.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#liu_zhang_gi08">bibtex</a>]
<p>
We look at a particular instance of the convex decomposition problem
which arises from real-world game development. Given a collection of
polyhedral surfaces (possibly with boundaries, holes, and complex interior
structures) that model the scene geometry in a game environment, we wish
to find a small set of convex hulls ...
<p>
</TD>
</TR>

</TBODY>
</TABLE>

<p>

<h3>2007</h3>

<TABLE cellPadding=5 width="99%" border=0>

<TBODY>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/vanKaick_pg07.jpg">
<IMG height=120 width=200 src="pubs/images/vanKaick_pg07.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
8. <a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
<a href="http://www.cs.sfu.ca/~hamarneh/">Ghassan Hamarneh</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.bccrc.ca/ccr/people_pwighton.html">Paul Wighton</a>,
"<strong>Contour Correspondence via Ant Colony Optimization</strong>,"
Proc. of <i><a href="http://mm.cse.wustl.edu/pg07/">Pacific Graphics
2007</a></i>, pp. 271-280.
[<a href="http://www.cs.sfu.ca/~ovankaic/personal/aco/">Oliver's page
 with paper and MATLAB code</a> |
 <a href="pubs/haoz_paper_bib.html#vanKaick_pg07">bibtex</a>]
<p>
We formulate contour correspondence as a Quadratic Assignment Problem
(QAP), incorporating proximity information. By maintaining the
neighborhood relation between points this way, we show that better
matching results are obtained in practice. We propose the first Ant Colony
Optimization (ACO) algorithm ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/dyer_et_al_sgp07.jpg">
<IMG height=120 width=200 src="pubs/images/dyer_et_al_sgp07.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
7. <a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~torsten/">Torsten Moeller</a>,
"<strong>Delaunay Mesh Construction</strong>,"
<i>Proc. of Eurographics Symposium on Geometry Processing (SGP)</i>, pp. 273-282.
[<a href="pubs/dyer_et_al_sgp07.pdf">PDF</a> | 
<a href="pubs/haoz_paper_bib.html#dyer_et_al_sgp07">bibtex</a>]
<p>
We present algorithms to produce Delaunay meshes from arbitrary triangle
meshes by edge flipping and geometry-preserving refinement and prove their
correctness. In particular we show that edge flipping serves to reduce
mesh surface area, and that a poorly sampled input mesh may yield
unflippable edges necessitating refinement ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/zhang_eg07_star.jpg">
<IMG height=120 width=200 src="pubs/images/zhang_eg07_star.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
6. <strong>Hao Zhang</strong>,
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
and <a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
"<strong>Spectral Methods for Mesh Processing and Analysis</strong>,"
<i>Proc. of <a href="http://www.cgg.cvut.cz/eg07/">Eurographics
2007</a> State of the Art Report</i>, pp. 1-22.
[<a href="pubs/zhang_eg07star_spectral.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#zhang_eg07star">bibtex</a>]
<p>
Spectral methods for mesh processing and analysis rely on the eigenvalues,
eigenvectors, or eigenspace projections derived from appropriately defined
mesh operators to carry out desired tasks. This state-of-the-art report
aims to provide a comprehensive survey on the spectral approach ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/eg07_spect_2.png">
<IMG height=120 width=200 src="pubs/images/eg07_spect_1.png">
</a>
</TD>
<TD vAlign=center align=left>
5. <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a> and <strong>Hao Zhang</strong>,
"<strong>Mesh Segmentation via Spectral Embedding and Contour 
Analysis</strong>,"
<i>Computer Graphics Forum (Special Issue of <a
href="http://www.cgg.cvut.cz/eg07/">Eurographics 2007</a>)</i>,
Vol. 26, pp. 385-394, 2007.
[<a href="pubs/liu_zhang_eg07.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#liu_zhang_eg07">bibtex</a>]
<p>
We propose a mesh segmentation algorithm where at each step, a sub-mesh
embedded in 3D is first spectrally projected into the plane with a contour
extracted from the planar embedding. Transforming the shape analysis
problem to the 2D domain facilitates our segmentability analysis and
sampling tasks ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_zhang_smi07_1.jpg">
<IMG height=120 width=95 src="pubs/images/li_zhang_smi07_1.jpg">
</a>
<a href="pubs/images/li_zhang_smi07_2.jpg">
<IMG height=120 width=95 src="pubs/images/li_zhang_smi07_2.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
4. <a href="http://filebox.vt.edu/users/xxli/">Xiaoxing Li</a> and <strong>Hao Zhang</strong>,
"<strong>Adapting Geometric Attributes for Expression-Invariant 
3D Face Recognition</strong>,"
<i>Proc. of <a href="http://smi07.liris.cnrs.fr/">Shape
Modeling International (SMI) 2007</a></i>, pp. 21-32.
[<a href="pubs/li_zhang_smi07.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#li_zhang_smi07">bibtex</a>]
<p>
We investigate the use of multiple intrinsic geometric attributes,
including angles, geodesic distances, and curvatures, for 3D face
recognition ... As invariance to facial expressions holds the key to
improving recognition performance, we propose to train for the
component-wise weights ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/dyer_et_al_spm07.jpg">
<IMG height=120 width=200 src="pubs/images/dyer_et_al_spm07_teaser.jpg">
</a>
</TD>
<TD 
vAlign=center align=left>
3. <a href="http://www-sop.inria.fr/members/Ramsay.Dyer/">Ramsay Dyer</a>,
<strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~torsten/">Torsten Moeller</a>,
"<strong>Voronoi-Delaunay Duality and Delaunay Meshes</strong>,"
<i>Proc. of <a href="http://cg.cs.tsinghua.edu.cn/spm2007/">ACM 
Symposium on Solid and Physical Modeling (SPM) 2007</a></i>,
pp. 415-420.
[<a href="pubs/dyer_spm07.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#dyer_et_al_spm2007">bibtex</a>]
<p>
We define a Delaunay mesh to be a manifold triangle mesh whose edges form
an intrinsic Delaunay triangulation or iDT of its vertices ... We show
that meshes constructed from a smooth surface by taking an iDT or a
restricted Delaunay triangulation, do not in general yield a Delaunay
mesh ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/jain_zhang_ijsm07.jpg">
<IMG height=120 width=200 src="pubs/images/jain_zhang_ijsm07.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
2. Varun Jain, <strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
"<strong>Non-Rigid Spectral Correspondence of Triangle Meshes</strong>," 
<i>International Journal on Shape Modeling</i>
(via invitation to Special Issue of SMI 2006), Volume 13, Number 1,
pp. 101-124.
[<a href="pubs/jain_zhang_ijsm07.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#jain_zhang_ijsm07">bibtex</a>]
<p>
We present an algorithm for finding a meaningful correspondence between
two triangle meshes, which is designed to handle general non-rigid
transformations. Our algorithm operates on embeddings of the two shapes in
the spectral domain so as to normalize them with respect to uniform
scaling and rigid-body transformation.
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/jain_zhang_cad07.jpg">
<IMG height=120 width=200 src="pubs/images/jain_zhang_cad07_teaser.jpg">
</a>
</TD>
<TD vAlign=center align=left>
1. Varun Jain and <strong>Hao Zhang</strong>,
"<strong>A Spectral Approach to Shape-Based Retrieval of Articulated 3D
Models</strong>,"
<i>Computer-Aided Design</i> (via invitation to 
<a href="http://www.sciencedirect.com/science/journal/00104485">Special 
Issue of GMP 2006</a>), Vol. 39, Issue 5, pp. 398-407, 2007.
[<a href="pubs/jain_zhang_cad07.pdf">PDF</a> |
 <a href="http://dx.doi.org/10.1016/j.cad.2007.02.009">DOI</a> |
 <a href="pubs/haoz_paper_bib.html#jain_zhang_cad07">bibtex</a>]
<p>
We present an approach for robust shape retrieval from databases
containing articulated 3D models. Each shape is represented by the
eigenvectors of an appropriately defined affinity matrix, forming a
spectral embedding which achieves normalization against rigid-body
transformations, shape articulation ...
<p>
</TD>
</TR>
</TBODY></TABLE>

<h3>2006</h3>

<TABLE cellPadding=5 width="99%" border=0>

<TBODY>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/li_zhang_sgp06.jpg">
<IMG height=120 width=200 src="pubs/images/li_zhang_sgp06_teaser.jpg">
</a>
</TD>
<TD 
vAlign=center aligh=left>
8. John Li and <strong>Hao Zhang</strong>,
"<strong>Nonobtuse Remeshing and Decimation</strong>,"
in Proc. of <i>Symposium on Geometry Processing (SGP) 2006</i> 
(short paper), pp.235-238.
[<a href="pubs/li_zhang_sgp06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#li_zhang_sgp06">bibtex</a>]
<p>
We propose an algorithm for guaranteed nonobtuse remeshing and nonobtuse
mesh decimation. Our strategy for the remeshing problem is to first
convert an input mesh, using a modified Marching Cubes algorithm, into a
rough approximate mesh that is guaranteed to be nonobtuse. We then apply
iterative "deform-to-fit" ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/olson_zhang_eg06.jpg">
<IMG height=120 width=200 src="pubs/images/olson_zhang_eg06_teaser.jpg">
</a>
</TD>
<TD vAlign=center aligh=left>
7. <a href="http://www.cs.sfu.ca/~matto/personal/">Matt Olson</a> and
<strong>Hao Zhang</strong>,
"<strong>Silhouette Extraction in Hough Space</strong>," 
<i>Computer Graphics Forum</i>
(Special Issue on <i>Eurographics 2006</i>),
Volume 25, Number 3, pp. 273-282, 2006.
[<a href="pubs/olson_zhang_eg06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#olson_zhang_eg06">bibtex</a>]
<p>
We present an efficient silhouette extractor for triangle meshes under
perspective projection in the Hough space. The more favorable point
distribution in Hough space allows us to obtain significant performance
gains over the traditional dual-space based techniques ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/jain_zhang_gmp06.jpg">
<IMG height=120 width=200 src="pubs/images/jain_zhang_gmp06_teaser.jpg">
</a>
</TD>
<TD 
vAlign=center align=left>
6. Varun Jain and <strong>Hao Zhang</strong>,
"<strong>Shape-Based Retrieval of Articulated 3D Models Using Spectral
Embedding</strong>," in Proceeding of
<i>Geometric Modeling and Processing 2006</i>, pp. 295-308.
[<a href="pubs/jain_zhang_gmp06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#jain_zhang_gmp06">bibtex</a>]
<p>
We present a spectral approach for robust shape retrieval from databases
containing articulated 3D shapes. We show absolute
improvement in retrieval performance when conventional shape descriptors are
used in the spectral domain on the McGill database of articulated 3D
shapes. We also propose a simple eigenvalue-based descriptor ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/liu_zhang_gmp06.jpg">
<IMG height=120 width=200 src="pubs/images/liu_zhang_gmp06_teaser.jpg">
</a>
</TD>
<TD 
vAlign=center align=left>
5. <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a>, <strong>Hao Zhang</strong>, and
<a href="http://www.cs.sfu.ca/~ovankaic/personal/">Oliver van Kaick</a>,
"<strong>Spectral Sequencing based on Graph Distance</strong>,"
in Proceeding of
<i>Geometric Modeling and Processing 2006</i> (poster paper), pp. 632-638.
[<a href="pubs/liu_zhang_gmp06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#liu_zhang_gmp06">bibtex</a>]
<p>
In this paper, we treat optimal mesh layout generation as a problem of
preserving graph distances and propose to use the subdominant eigenvector
of a kernel (affinity) matrix for sequencing ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/liu_et_al_cgi06.jpg">
<IMG height=120 width=200 src="pubs/images/liu_et_al_cgi06_teaser.jpg">
</a>
</TD>
<TD 
vAlign=center align=left>
4. <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a>, Varun Jain,
and <strong>Hao Zhang</strong>,
"<strong>Subsampling for Efficient Spectral Mesh Processing</strong>," 
in Proceeding of <i>Computer Graphics International 2006, Lecture Notes 
in Computer Science 4035</i>, H.-P. Seidel, T. Nishita, and Q. Peng, 
Eds., pp. 172-184, 2006. (acceptance rate: 10%)
[<a href="pubs/liu_cgi06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#liu_et_al_cgi06">bibtex</a>]
<p>
We apply Nystrom method, a sub-sampling and reconstruction technique, to
speed up spectral mesh processing. We first relate this method to Kernel
Principal Component Analysis (KPCA). This enables us to derive a novel
measure in the form of a matrix trace, based soly on sampled data, to
quantify the quality of Nystrom approximation ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/jain_zhang_smi06.jpg">
<IMG height=120 width=200 src="pubs/images/jain_zhang_smi06.jpg">
</a>
</TD>
<TD vAlign=center 
align=left>
3. Varun Jain and <strong>Hao Zhang</strong>, 
"<strong>Robust 3D Shape Correspondence in the Spectral Domain</strong>," 
in Proceeding of <i>International Conference on Shape Modeling and 
Applications 
(SMI) 2006</i>, pp. 118-129, 2006.
[<a href="pubs/jain_zhang_smi06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#jain_zhang_smi06">bibtex</a>]
<p>
We present an algorithm for finding a meaningful correspondence between
two 3D shapes given as triangle meshes. Our algorithm operates on
embeddings of the two shapes in the spectral domain so as to normalize
them with respect to uniform scaling, rigid-body transformation and shape
bending ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
<a href="pubs/images/clements_zhang_smi06.jpg">
<IMG height=120 width=200
src="pubs/images/clements_zhang_smi06_teaser.jpg">
</a>
</TD>
<TD vAlign=center align=left>
2. Andrew Clements and <strong>Hao Zhang</strong>, 
"<strong>Minimum Ratio Contours on Surface Meshes</strong>," 
in Proceeding of <i>International Conference on Shape Modeling and 
Applications (SMI) 2006</i>, pp. 26-37, 2006.
[<a href="pubs/clements_zhang_smi06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#clements_zhang_smi06">bibtex</a>]
<p>
We present a novel approach for discretely optimizing contours on the
surface of a triangle mesh. This is achieved through the use of a minimum
ratio cycle (MRC) algorithm, where we compute a contour having the minimal
ratio between a novel contour energy term and the length of the
contour ...
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
<a href="pubs/images/li_zhang_crv06.jpg">
<IMG height=120 width=200 src="pubs/images/li_zhang_crv06_teaser.jpg">
</a>
</TD>
<TD 
vAlign=center align=left>
1. <a href="http://filebox.vt.edu/users/xxli/">Xiaoxing Li</a>,
<a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>, and <strong>Hao Zhang</strong>,
"<strong>Expression-Invariant Face Recognition with Expression 
Classification</strong>," 
in Proceeding of <i>Canadian Conference on Computer and Robot Vision 
(CRV) 2006</i>, pp. 77-83, 2006. 
[<a href="pubs/li_crv06.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#li_crv06">bibtex</a>]
<p>
Facial expression, which changes face geometry, usually has an adverse
effect on the performance of a face recognition system. On the other
hand, face geometry is a useful cue for recognition. Taking these into
account, we utilize the idea of separating geometry and texture
information in a face image ...
<p>
</TD>
</TR>
</TBODY>
</TABLE>

<h3>2005 - </h3>

<TABLE cellPadding=5 width="99%" border=0>

<TBODY>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center 
align=left>
9. <strong>Hao Zhang</strong> and <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a>,
"<strong>Mesh Segmentation via Recursive and Visually Salient Spectral 
Cuts</strong>," 
in Proceeding of <i>Vision, Modeling, and Visualization 2005</i>, 
pp. 429-436, 2005.
[<a href="pubs/zhang_liu_vmv05.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#zhang_liu_vmv05">bibtex</a>]
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center 
align=left>
8. Varun Jain and <strong>Hao Zhang</strong>, 
"<strong>Robust 2D Shape Correspondence using Geodesic Shape 
Context</strong>," 
in Proceeding of <i>Pacific Graphics 2005</i>, (short paper), pp. 
121-124, 2005.
[<a href="pubs/haoz_zhang_bib.html#jain_zhang_pg05">bibtex</a>]
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center 
align=left>
7. <strong>Hao Zhang</strong>, 
"<strong>Discrete Combinatorial Laplacian Operators for Digital Geometry 
Processing</strong>," 
in Proc. of <i>SIAM Conference on Geometric Design and 
Computing</i>, 
pp. 575-592, 2004.
[<a href="pubs/zhang_gdc04.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#zhang_gdc04">bibtex</a>]
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
6. <a href="http://www.cs.sfu.ca/~lrong/personal/">Rong Liu</a> and <strong>Hao Zhang</strong>,
"<strong>Segmentation of 3D Meshes through Spectral Clustering</strong>," 
in Proceeding of <i>Pacific Graphics 2004</i>, pp. 298-305.
[<a href="pubs/liu_zhang_pg04.pdf">PDF</a> |
 <a href="pubs/haoz_paper_bib.html#liu_zhang_pg04">bibtex</a>]
<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
5. <strong>Hao Zhang</strong> and Hendrik C. Blok, 
"<strong>Optimal Mesh Signal Transforms</strong>," 
in Proceeding of <i>IEEE Geometric Modeling and Processing 
2004</i> (poster paper), pp. 373-379.
[<a href="pubs/haoz_paper_bib.html#zhang_blok_gmp04">bibtex</a>]

<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
4. <strong>Hao Zhang</strong> and
<a href="http://www.dgp.toronto.edu/~elf/">Eugene Fiume</a>,
"<strong>Butterworth Filtering and Implicit Fairing of Irregular 
Meshes</strong>," 
in Proceedings of <i>Pacific Graphics 2003</i> (short paper), 
pp. 502-506.
[<a href="pubs/haoz_paper_bib.html#zhang_fiume_pg03">bibtex</a>]

<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
3. <strong>Hao Zhang</strong> and
<a href="http://www.dgp.toronto.edu/~elf/">Eugene Fiume</a>,
"<strong>Mesh Smoothing with Shape or Feature Preservation</strong>," 
in <i>Advances in Modeling, Animation, and Rendering</i>, 
J. Vince and R. Earnshaw, editors, pp. 167-182, Springer 2002. 
Also as Proceeding of <i>Computer Graphics International 2002</i>.

<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#EEEEEE>
<TD vAlign=center align=left>
2. <strong>Hao Zhang</strong> and
<a href="http://www.dgp.toronto.edu/~elf/">Eugene Fiume</a>,
"<strong>Shape Matching of 3-D Contours using Normalized Fourier 
Descriptors</strong>,"
in Proceeding of <i>International Conference on Shape Modeling and 
Applications (SMI)</i>, 
IEEE Computer Society, pp. 261-268, 2002. 
[<a href="pubs/haoz_paper_bib.html#zhang_fiume_smi02">bibtex</a>]

<p>
</TD>
</TR>

<p>

<TR vAlign=center align=left bgColor=#FFFFFF>
<TD vAlign=center align=left>
1. <a href="http://maveric.uwaterloo.ca/~brzozo/">John A. Brzozowski</a>
and <strong>Hao Zhang</strong>, 
"<strong>Delay-Insensitivity and Semi-Modularity</strong>," 
<i>Formal Methods in System Design</i>,
Kluwer Academic Publishers, March 2000, vol. 16, 
pp. 191-218, 2000.
<p>
</TD>

</TR>
</TBODY>
</TABLE>

<p>

<strong>SIGGRAPH/TOG: 58; ICCV/CVPR/ECCV/NeurIPS: 17; SGP: 7; Eurographics: 8; EGSTAR: 4; CGF: 19.
</strong>

</body>
</html>


