



<!DOCTYPE HTML>

<head>
            <title>Andreas Geiger</title>
        <meta property="og:title" content="Andreas Geiger" />
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" type="text/css" media="all" href="https://www.cvlibs.net/site/style.css">
        <link rel="shortcut icon" type="image/x-icon" href="https://www.cvlibs.net/favicon.ico" />
    </head>

<body>

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.6";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
  
    <ol id="menu">
        <li><a href="https://www.cvlibs.net/index.php" >Home</a></li>
        <li><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/team/" target="_blank">Team</a></li> 
        <li><a href="https://www.cvlibs.net/publications.php" style="color:#FFF">Publications</a></li>        
        <li><a href="https://www.cvlibs.net/projects.php" >Projects</a></li>
        <li><a href="https://autonomousvision.github.io/"  target="_blank">Blog</a></li>
        <li><a href="https://www.cvlibs.net/site/cv.pdf" target="_blank">CV</a></li>
    </ol>

<div id="container">
<div id="site_title_big"><a href="https://www.cvlibs.net/index.php" style="text-decoration: none">Andreas Geiger</a></div>




<script type="text/javascript" src="./site/jquery.min.js"></script>
<script type="text/javascript" src="./site/fancybox/jquery.fancybox-1.3.4.pack.js"></script>
<link rel="stylesheet" type="text/css" href="./site/fancybox/jquery.fancybox-1.3.4.css" media="screen" />
<script type="text/javascript">
	$(document).ready(function() {		
		$("a[rel=video]").click(function() {
      $.fancybox({
		  'padding'	     	: 0,
		  'transitionIn'	: 'none',
		  'transitionOut'	: 'none',
		  'width'		      : 1024,
		  'height'		    : 768,
		  'href'			    : this.href.replace(new RegExp("watch\\?v=", "i"), 'v/'),
		  'type'			    : 'swf',
		  'swf'			      : { 'wmode'	: 'transparent', 'allowfullscreen' : 'true' }
	    });
      return false;
    });
	});
</script>

<script language="javascript" type="text/javascript">
function toggleDetails (id) {
  var element = document.getElementById(id);
	if(element.style.display=='block') element.style.display = 'none';
	else	                             element.style.display = 'block';
}
</script>


Get updated via email on new publications or videos by following us on <a href="http://scholar.google.ca/citations?user=SrVnrPcAAAAJ&hl=en" target="blank">GoogleScholar</a>, on our <a href="https://autonomousvision.github.io/" target="blank">research blog</a> or on our <a href="http://www.youtube.com/user/cvlibs" target="blank">YouTube</a> channel!</br>
The Latex bibliography file for all papers from our group can be downloaded from here: <a href="publications/bibliography.bib">bibliography.bib</a> with strings <a href="publications/bibliography_long.bib">bibliography_long.bib</a>.

<div class="newline_bg"><h2>2022</h2></div>
<div class="publication_container" onclick="toggleDetails('Renz2022CORL');"><div class="publication_image"><img src="publications/Renz2022CORL.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>PlanT: Explainable Planning Transformers via Object-Level Representations </b><br/>K. Renz, K. Chitta, O. Mercea, A. Koepke, Z. Akata and A. Geiger <br/>Conference on Robot Learning (CoRL), 2022</div></div>
<div class="publication_detail" id="Renz2022CORL" name="Renz2022CORL"><div class="publication_abstract"><b>Abstract:</b> Planning an optimal route in a complex environment requires efficient reasoning about the surrounding scene. While human drivers prioritize important objects and ignore details not relevant to the decision, learning-based planners typically extract features from dense, high-dimensional grid representations of the scene containing all vehicle and road context information. In this paper, we propose PlanT, a novel approach for planning in the context of self-driving that uses a standard transformer architecture. PlanT is based on imitation learning with a compact object-level input representation. With this representation, we demonstrate that information regarding the ego vehicle's route provides sufficient context regarding the road layout for planning. On the challenging Longest6 benchmark for CARLA, PlanT outperforms all prior methods (matching the driving score of the expert) while being 5.3x faster than equivalent pixel-based planning baselines during inference. Furthermore, we propose an evaluation protocol to quantify the ability of planners to identify relevant objects, providing insights regarding their decision making. Our results indicate that PlanT can reliably focus on the most relevant object in the scene, even when this object is geometrically distant.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Renz2022CORL.pdf" target="_blank">Renz2022CORL</a>,<br/>
&nbsp; author = {Katrin Renz and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and Otniel-Bogdan Mercea and Almut Sophia Koepke and Zeynep Akata and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {PlanT: Explainable Planning Transformers via Object-Level Representations},<br/>&nbsp; booktitle = {Conference on Robot Learning (CoRL)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Renz2022CORL.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Renz2022CORL_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=_sNNEyjMmaY" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://www.katrinrenz.de/plant/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schwarz2022NEURIPS');"><div class="publication_image"><img src="publications/Schwarz2022NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids </b><br/>K. Schwarz, A. Sauer, M. Niemeyer, Y. Liao and A. Geiger <br/>Advances in Neural Information Processing Systems (NeurIPS), 2022</div></div>
<div class="publication_detail" id="Schwarz2022NEURIPS" name="Schwarz2022NEURIPS"><div class="publication_abstract"><b>Abstract:</b> State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization.
To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schwarz2022NEURIPS.pdf" target="_blank">Schwarz2022NEURIPS</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/kschwarz" target="blank">Katja Schwarz</a> and <a href="https://axelsauer.com/" target="blank">Axel Sauer</a> and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schwarz2022NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://arxiv.org/abs/2206.07695" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="https://katjaschwarz.github.io/voxgraf/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Yu2022NEURIPS');"><div class="publication_image"><img src="publications/Yu2022NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction </b><br/>Z. Yu, S. Peng, M. Niemeyer, T. Sattler and A. Geiger <br/>Advances in Neural Information Processing Systems (NeurIPS), 2022</div></div>
<div class="publication_detail" id="Yu2022NEURIPS" name="Yu2022NEURIPS"><div class="publication_abstract"><b>Abstract:</b> In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Yu2022NEURIPS.pdf" target="_blank">Yu2022NEURIPS</a>,<br/>
&nbsp; author = {Zehao Yu and Songyou Peng and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Yu2022NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://arxiv.org/abs/2206.00665" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="https://niujinshuchong.github.io/monosdf/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chen2022ARXIV');"><div class="publication_image"><img src="publications/Chen2022ARXIV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Fast-SNARF: A Fast Deformer for Articulated Neural Fields </b><br/>X. Chen, T. Jiang, J. Song, M. Rietmann, A. Geiger, M. Black and O. Hilliges <br/>Arxiv, 2022</div></div>
<div class="publication_detail" id="Chen2022ARXIV" name="Chen2022ARXIV"><div class="publication_abstract"><b>Abstract:</b> Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150×. These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Chen2022ARXIV.pdf" target="_blank">Chen2022ARXIV</a>,<br/>
&nbsp; author = {Xu Chen and Tianjian Jiang and Jie Song and Max Rietmann and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael J. Black</a> and Otmar Hilliges},<br/>
&nbsp; title = {Fast-SNARF: A Fast Deformer for Articulated Neural Fields},<br/>&nbsp; journal = {Arxiv},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="https://arxiv.org/abs/2211.15601" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="https://github.com/xuchen-ethz/fast-snarf" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Xu2022ARXIV');"><div class="publication_image"><img src="publications/Xu2022ARXIV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Unifying Flow, Stereo and Depth Estimation </b><br/>H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao and A. Geiger <br/>Arxiv, 2022</div></div>
<div class="publication_detail" id="Xu2022ARXIV" name="Xu2022ARXIV"><div class="publication_abstract"><b>Abstract:</b> We present a unified formulation and model for three motion and 3D perception tasks: optical flow, rectified stereo matching and unrectified stereo depth estimation from posed images. Unlike previous specialized architectures for each specific task, we formulate all three tasks as a unified dense correspondence matching problem, which can be solved with a single model by directly comparing feature similarities. Such a formulation calls for discriminative feature representations, which we achieve using a Transformer, in particular the cross-attention mechanism. We demonstrate that cross-attention enables integration of knowledge from another image via cross-view interactions, which greatly improves the quality of the extracted features. Our unified model naturally enables cross-task transfer since the model architecture and parameters are shared across tasks. We outperform RAFT with our unified model on the challenging Sintel dataset, and our final model that uses a few additional task-specific refinement steps outperforms or compares favorably to recent state-of-the-art methods on 10 popular flow, stereo and depth datasets, while being simpler and more efficient in terms of model design and inference speed.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Xu2022ARXIV.pdf" target="_blank">Xu2022ARXIV</a>,<br/>
&nbsp; author = {Haofei Xu and Jing Zhang and Jianfei Cai and Hamid Rezatofighi and Fisher Yu and Dacheng Tao and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Unifying Flow, Stereo and Depth Estimation},<br/>&nbsp; journal = {Arxiv},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="https://arxiv.org/abs/2211.05783" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="https://haofeixu.github.io/unimatch/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Wang2022ECCV');"><div class="publication_image"><img src="publications/Wang2022ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>ARAH: Animatable Volume Rendering of Articulated Human SDFs </b><br/>S. Wang, K. Schwarz, A. Geiger and S. Tang <br/>European Conference on Computer Vision (ECCV), 2022</div></div>
<div class="publication_detail" id="Wang2022ECCV" name="Wang2022ECCV"><div class="publication_abstract"><b>Abstract:</b> Combining human body models with differentiable rendering has recently enabled animatable avatars of clothed humans from sparse sets of multi-view RGB videos. While state-of-the-art approaches achieve a realistic appearance with neural radiance fields (NeRF), the inferred geometry often lacks detail due to missing geometric constraints. Further, animating avatars in out-of-distribution poses is not yet possible because the mapping from observation space to canonical space does not generalize faithfully to unseen poses. In this work, we address these shortcomings and propose a model to create animatable clothed human avatars with detailed geometry that generalize well to out-of-distribution poses. To achieve detailed geometry, we combine an articulated implicit surface representation with volume rendering. For generalization, we propose a novel joint root-finding algorithm for simultaneous ray-surface intersection search and correspondence search. Our algorithm enables efficient point sampling and accurate point canonicalization while generalizing well to unseen poses. We demonstrate that our proposed pipeline can generate clothed avatars with high-quality pose-dependent geometry and appearance from a sparse set of multi-view RGB videos. Our method achieves state-of-the-art performance on geometry and appearance reconstruction while creating animatable avatars that generalize well to out-of-distribution poses beyond the small number of training poses.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Wang2022ECCV.pdf" target="_blank">Wang2022ECCV</a>,<br/>
&nbsp; author = {Shaofei Wang and <a href="https://avg.is.tuebingen.mpg.de/person/kschwarz" target="blank">Katja Schwarz</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Siyu Tang},<br/>
&nbsp; title = {ARAH: Animatable Volume Rendering of Articulated Human SDFs},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Wang2022ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Wang2022ECCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=PzumWPOpvDI" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=VKSWlt_7FYw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://neuralbodies.github.io/arah/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chen2022ECCV');"><div class="publication_image"><img src="publications/Chen2022ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>TensoRF: Tensorial Radiance Fields </b><br/>A. Chen, Z. Xu, A. Geiger, J. Yu and H. Su <br/>European Conference on Computer Vision (ECCV), 2022</div></div>
<div class="publication_detail" id="Chen2022ECCV" name="Chen2022ECCV"><div class="publication_abstract"><b>Abstract:</b> We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition - that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality,
our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Chen2022ECCV.pdf" target="_blank">Chen2022ECCV</a>,<br/>
&nbsp; author = {Anpei Chen and Zexiang Xu and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Jingyi Yu and Hao Su},<br/>
&nbsp; title = {TensoRF: Tensorial Radiance Fields},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Chen2022ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Chen2022ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Chen2022ECCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=ujOMgaKV3lA" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://apchenstu.github.io/TensoRF/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Hanselmann2022ECCV');"><div class="publication_image"><img src="publications/Hanselmann2022ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients </b> <b><span style="color:#a51e37">(oral)</span></b><br/>N. Hanselmann, K. Renz, K. Chitta, A. Bhattacharyya and A. Geiger <br/>European Conference on Computer Vision (ECCV), 2022</div></div>
<div class="publication_detail" id="Hanselmann2022ECCV" name="Hanselmann2022ECCV"><div class="publication_abstract"><b>Abstract:</b> Simulators offer the possibility of safe, low-cost development of self-driving systems. However, current driving simulators exhibit naïve behavior models for background traffic. Hand-tuned scenarios are typically added during simulation to induce safety-critical situations. An alternative approach is to adversarially perturb the background traffic trajectories. In this paper, we study this approach to safety-critical driving scenario generation using the CARLA simulator. We use a kinematic bicycle model as a proxy to the simulator's true dynamics and observe that gradients through this proxy model are sufficient for optimizing the background traffic trajectories. Based on this finding, we propose KING, which generates safety-critical driving scenarios with a 20% higher success rate than black-box optimization. By solving the scenarios generated by KING using a privileged rule-based expert algorithm, we obtain training data for an imitation learning policy. After fine-tuning on this new data, we show that the policy becomes better at avoiding collisions. Importantly, our generated data leads to reduced collisions on both held-out scenarios generated via KING as well as traditional hand-crafted scenarios, demonstrating improved robustness.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Hanselmann2022ECCV.pdf" target="_blank">Hanselmann2022ECCV</a>,<br/>
&nbsp; author = {Niklas Hanselmann and Katrin Renz and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and Apratim Bhattacharyya and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Hanselmann2022ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Hanselmann2022ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Hanselmann2022ECCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=zZ0AptvdC1s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=qkDQLmqY5AQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://lasnik.github.io/king/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Fu2022THREEDV');"><div class="publication_image"><img src="publications/Fu2022THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation </b><br/>X. Fu, S. Zhang, T. Chen, Y. Lu, L. Zhu, X. Zhou, A. Geiger and Y. Liao <br/>International Conference on 3D Vision (3DV), 2022</div></div>
<div class="publication_detail" id="Fu2022THREEDV" name="Fu2022THREEDV"><div class="publication_abstract"><b>Abstract:</b> Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multi-view consistent by design. Experimental results show that Panoptic NeRF outperforms existing semantic and instance label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Fu2022THREEDV.pdf" target="_blank">Fu2022THREEDV</a>,<br/>
&nbsp; author = {Xiao Fu and Shangzhan Zhang and Tianrun Chen and Yichong Lu and Lanyun Zhu and Xiaowei Zhou and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a>},<br/>
&nbsp; title = {Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Fu2022THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Fu2022THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Fu2022THREEDV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=5QKTeFLciWo" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://fuxiao0719.github.io/projects/panopticnerf/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chitta2022PAMI');"><div class="publication_image"><img src="publications/Chitta2022PAMI.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving </b><br/>K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz and A. Geiger <br/>Pattern Analysis and Machine Intelligence (PAMI), 2022</div></div>
<div class="publication_detail" id="Chitta2022PAMI" name="Chitta2022PAMI"><div class="publication_abstract"><b>Abstract:</b> How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g. object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48%.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Chitta2022PAMI.pdf" target="_blank">Chitta2022PAMI</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and <a href="https://avg.is.tuebingen.mpg.de/person/aprakash" target="blank">Aditya Prakash</a> and Bernhard Jaeger and Zehao Yu and Katrin Renz and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving},<br/>&nbsp; journal = {Pattern Analysis and Machine Intelligence (PAMI)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Chitta2022PAMI.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Chitta2022PAMI_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Chitta2022PAMI_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=-GMhYcxOiEU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=uO4xYM0FVPk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://github.com/autonomousvision/transfuser" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Sauer2022SIGGRAPH');"><div class="publication_image"><img src="publications/Sauer2022SIGGRAPH.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets </b><br/>A. Sauer, K. Schwarz and A. Geiger <br/>International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), 2022</div></div>
<div class="publication_detail" id="Sauer2022SIGGRAPH" name="Sauer2022SIGGRAPH"><div class="publication_abstract"><b>Abstract:</b> Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN  in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN's performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 1024x1024 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object~classes. Code, models, and supplementary videos can be found at https://sites.google.com/view/stylegan-xl/.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Sauer2022SIGGRAPH.pdf" target="_blank">Sauer2022SIGGRAPH</a>,<br/>
&nbsp; author = {<a href="https://axelsauer.com/" target="blank">Axel Sauer</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kschwarz" target="blank">Katja Schwarz</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},<br/>&nbsp; booktitle = {International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Sauer2022SIGGRAPH.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Sauer2022SIGGRAPH_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=c-PjRXVG8ZI" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://sites.google.com/view/stylegan-xl/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chen2022CVPR');"><div class="publication_image"><img src="publications/Chen2022CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>gDNA: Towards Generative Detailed Neural Avatars </b><br/>X. Chen, T. Jiang, J. Song, J. Yang, M. Black, A. Geiger and O. Hilliges <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2022</div></div>
<div class="publication_detail" id="Chen2022CVPR" name="Chen2022CVPR"><div class="publication_abstract"><b>Abstract:</b> To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods that represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Chen2022CVPR.pdf" target="_blank">Chen2022CVPR</a>,<br/>
&nbsp; author = {Xu Chen and Tianjian Jiang and Jie Song and Jinlong Yang and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Otmar Hilliges},<br/>
&nbsp; title = {gDNA: Towards Generative Detailed Neural Avatars},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Chen2022CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Chen2022CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Chen2022CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=jl4U_Ja3ARU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://xuchen-ethz.github.io/gdna" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Dong2022CVPR');"><div class="publication_image"><img src="publications/Dong2022CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence </b><br/>Z. Dong, C. Guo, J. Song, X. Chen, A. Geiger and O. Hilliges <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2022</div></div>
<div class="publication_detail" id="Dong2022CVPR" name="Dong2022CVPR"><div class="publication_abstract"><b>Abstract:</b> We present a novel method to learn Personalized Implicit Neural Avatars (PINA) from a short RGB-D sequence. This allows non-expert users to create a detailed and personalized virtual copy of themselves, which can be animated with realistic clothing deformations. PINA does not require complete scans, nor does it require a prior learned from large datasets of clothed humans. Learning a complete avatar in this setting is challenging, since only few depth observations are available, which are noisy and incomplete (i.e.only partial visibility of the body per frame). We propose a method to learn the shape and non-rigid deformations via a pose-conditioned implicit surface and a deformation field, defined in canonical space. This allows us to fuse all partial observations into a single consistent canonical representation. Fusion is formulated as a global optimization problem over the pose, shape and skinning parameters. The method can learn neural avatars from real noisy RGB-D sequences for a diverse set of people and clothing styles and these avatars can be animated given unseen motion sequences.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Dong2022CVPR.pdf" target="_blank">Dong2022CVPR</a>,<br/>
&nbsp; author = {Zijian Dong and Chen Guo and Jie Song and Xu Chen and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Otmar Hilliges},<br/>
&nbsp; title = {PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Dong2022CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Dong2022CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Dong2022CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=oGpKUuD54Qk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://zj-dong.github.io/pina" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Niemeyer2022CVPR');"><div class="publication_image"><img src="publications/Niemeyer2022CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs </b> <b><span style="color:#a51e37">(oral)</span></b><br/>M. Niemeyer, J. Barron, B. Mildenhall, M. Sajjadi, A. Geiger and N. Radwan <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2022</div></div>
<div class="publication_detail" id="Niemeyer2022CVPR" name="Niemeyer2022CVPR"><div class="publication_abstract"><b>Abstract:</b> Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Niemeyer2022CVPR.pdf" target="_blank">Niemeyer2022CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and Jonathan Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Noha Radwan},<br/>
&nbsp; title = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Niemeyer2022CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Niemeyer2022CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Niemeyer2022CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=QyyyvA4-Kwc" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://m-niemeyer.github.io/regnerf" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liao2022PAMI');"><div class="publication_image"><img src="publications/Liao2022PAMI.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D </b><br/>Y. Liao, J. Xie and A. Geiger <br/>Pattern Analysis and Machine Intelligence (PAMI), 2022</div></div>
<div class="publication_detail" id="Liao2022PAMI" name="Liao2022PAMI"><div class="publication_abstract"><b>Abstract:</b> For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k semantic and instance annotated images and 1B annotated 3D points. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset. KITTI-360 will enable progress at the intersection of these research areas and thus contributing towards solving one of our grand challenges: the development of fully autonomous self-driving systems.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Liao2022PAMI.pdf" target="_blank">Liao2022PAMI</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="http://www.clairexie.org/" target="blank">Jun Xie</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D},<br/>&nbsp; journal = {Pattern Analysis and Machine Intelligence (PAMI)},<br/>
&nbsp; year = {2022}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liao2022PAMI.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=OonvYU5bx3s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=sM9630Ih8xM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/datasets/kitti-360/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2021</h2></div>
<div class="publication_container" onclick="toggleDetails('Sauer2021NEURIPS');"><div class="publication_image"><img src="publications/Sauer2021NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Projected GANs Converge Faster </b><br/>A. Sauer, K. Chitta, J. Müller and A. Geiger <br/>Advances in Neural Information Processing Systems (NeurIPS), 2021</div></div>
<div class="publication_detail" id="Sauer2021NEURIPS" name="Sauer2021NEURIPS"><div class="publication_abstract"><b>Abstract:</b> Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Sauer2021NEURIPS.pdf" target="_blank">Sauer2021NEURIPS</a>,<br/>
&nbsp; author = {<a href="https://axelsauer.com/" target="blank">Axel Sauer</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and Jens Müller and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Projected GANs Converge Faster},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Sauer2021NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Sauer2021NEURIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Sauer2021NEURIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="https://sites.google.com/view/projected-gan" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Wang2021NEURIPS');"><div class="publication_image"><img src="publications/Wang2021NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images </b><br/>S. Wang, M. Mihajlovic, Q. Ma, A. Geiger and S. Tang <br/>Advances in Neural Information Processing Systems (NeurIPS), 2021</div></div>
<div class="publication_detail" id="Wang2021NEURIPS" name="Wang2021NEURIPS"><div class="publication_abstract"><b>Abstract:</b> In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Wang2021NEURIPS.pdf" target="_blank">Wang2021NEURIPS</a>,<br/>
&nbsp; author = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Siyu Tang},<br/>
&nbsp; title = {MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Wang2021NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Wang2021NEURIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Wang2021NEURIPS_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Wang2021NEURIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=EVrZX8qkqqw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=o_snpEZrl7k" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://neuralbodies.github.io/metavatar/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Paschalidou2021NEURIPS');"><div class="publication_image"><img src="publications/Paschalidou2021NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>ATISS: Autoregressive Transformers for Indoor Scene Synthesis </b><br/>D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger and S. Fidler <br/>Advances in Neural Information Processing Systems (NeurIPS), 2021</div></div>
<div class="publication_detail" id="Paschalidou2021NEURIPS" name="Paschalidou2021NEURIPS"><div class="publication_abstract"><b>Abstract:</b> The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the
room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8x faster than existing methods.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Paschalidou2021NEURIPS.pdf" target="_blank">Paschalidou2021NEURIPS</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/dpaschalidou" target="blank">Despoina Paschalidou</a> and Amlan Kar and Maria Shugrina and Karsten Kreis and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Sanja Fidler},<br/>
&nbsp; title = {ATISS: Autoregressive Transformers for Indoor Scene Synthesis},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Paschalidou2021NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Paschalidou2021NEURIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Paschalidou2021NEURIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=VNY0BFMi2j4" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://nv-tlabs.github.io/ATISS/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schwarz2021NEURIPS');"><div class="publication_image"><img src="publications/Schwarz2021NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>On the Frequency Bias of Generative Models </b><br/>K. Schwarz, Y. Liao and A. Geiger <br/>Advances in Neural Information Processing Systems (NeurIPS), 2021</div></div>
<div class="publication_detail" id="Schwarz2021NEURIPS" name="Schwarz2021NEURIPS"><div class="publication_abstract"><b>Abstract:</b> The key objective of Generative Adversarial Networks (GANs) is to generate new data with the same statistics as the provided training data. However, multiple recent works show that state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an elevated amount of high frequencies in the spectral statistics which makes it straightforward to distinguish real and generated images. Explanations for this phenomenon are controversial: While most works attribute the artifacts to the generator, other works point to the discriminator.  We take a sober look at those explanations and provide insights on what makes proposed measures against high-frequency artifacts effective. To achieve this, we first independently assess the architectures of both the generator and discriminator and investigate if they exhibit a frequency bias that makes learning the distribution of high-frequency content particularly problematic. Based on these experiments, we make the following four observations: 1) Different upsampling operations bias the generator towards different spectral properties. 2) Checkerboard artifacts introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able to compensate for these artifacts. 3) The discriminator does not struggle with detecting high frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling operations in the discriminator can impair the quality of the training signal it provides. In light of these findings, we analyze proposed measures against high-frequency artifacts in state-of-the-art GAN training but find that none of the existing approaches can fully resolve spectral artifacts yet. Our results suggest that there is great potential in improving the discriminator and that this could be key to match the distribution of the training data more closely.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf" target="_blank">Schwarz2021NEURIPS</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/kschwarz" target="blank">Katja Schwarz</a> and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {On the Frequency Bias of Generative Models},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schwarz2021NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Schwarz2021NEURIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Schwarz2021NEURIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=jbcB-hHoOIA" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/frequency_bias" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Peng2021NEURIPS');"><div class="publication_image"><img src="publications/Peng2021NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Shape As Points: A Differentiable Poisson Solver </b> <b><span style="color:#a51e37">(oral)</span></b><br/>S. Peng, C. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys and A. Geiger <br/>Advances in Neural Information Processing Systems (NeurIPS), 2021</div></div>
<div class="publication_detail" id="Peng2021NEURIPS" name="Peng2021NEURIPS"><div class="publication_abstract"><b>Abstract:</b> In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference times and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) which allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Peng2021NEURIPS.pdf" target="_blank">Peng2021NEURIPS</a>,<br/>
&nbsp; author = {Songyou Peng and Chiyu Max Jiang and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Shape As Points: A Differentiable Poisson Solver},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Peng2021NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Peng2021NEURIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Peng2021NEURIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=TgR0NvYty0A" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/shape_as_points" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Niemeyer2021THREEDV');"><div class="publication_image"><img src="publications/Niemeyer2021THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields </b><br/>M. Niemeyer and A. Geiger <br/>International Conference on 3D Vision (3DV), 2021</div></div>
<div class="publication_detail" id="Niemeyer2021THREEDV" name="Niemeyer2021THREEDV"><div class="publication_abstract"><b>Abstract:</b> Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, ie, scenes are modeled in 3D and then rendered differentiably to the image plane. While this leads to impressive 3D~consistency, the camera needs to be modelled as well and we show in this work that these methods are sensitive to the choice of prior camera distributions. Current approaches assume fixed intrinsics and predefined priors over camera pose ranges, and parameter tuning is typically required for real-world data. If the data distribution is not matched, results degrade significantly. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Niemeyer2021THREEDV.pdf" target="_blank">Niemeyer2021THREEDV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Niemeyer2021THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Niemeyer2021THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Niemeyer2021THREEDV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=rrIIEc2qYjM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/campari" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Weber2021NEURIPSDATA');"><div class="publication_image"><img src="publications/Weber2021NEURIPSDATA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>STEP: Segmenting and Tracking Every Pixel </b><br/>M. Weber, J. Xie, M. Collins, Y. Zhu, P. Voigtlaender, H. Adam, B. Green, A. Geiger, B. Leibe, D. Cremers,  et al.<br/>Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021</div></div>
<div class="publication_detail" id="Weber2021NEURIPSDATA" name="Weber2021NEURIPSDATA"><div class="publication_abstract"><b>Abstract:</b> The task of assigning semantic classes and track identities to every pixel in a video is called video panoptic segmentation. Our work is the first that targets this task in a real-world setting requiring dense interpretation in both spatial and temporal domains. As the ground-truth for this task is difficult and expensive to obtain, existing datasets are either constructed synthetically or only sparsely annotated within short video clips. To overcome this, we introduce a new benchmark encompassing two datasets, KITTI-STEP, and MOTChallenge-STEP. The datasets contain long video sequences, providing challenging examples and a test-bed for studying long-term pixel-precise segmentation and tracking under real-world conditions. We further propose a novel evaluation metric Segmentation and Tracking Quality (STQ) that fairly balances semantic and tracking aspects of this task and is more appropriate for evaluating sequences of arbitrary length. Finally, we provide several baselines to evaluate the status of existing methods on this new challenging dataset. We have made our datasets, metric, benchmark servers, and baselines publicly available, and hope this will inspire future research.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Weber2021NEURIPSDATA.pdf" target="_blank">Weber2021NEURIPSDATA</a>,<br/>
&nbsp; author = {<a href="https://dvl.in.tum.de/team/weber/" target="blank">Mark Weber</a> and <a href="http://www.clairexie.org/" target="blank">Jun Xie</a> and Maxwell Collins and Yukun Zhu and <a href="https://www.vision.rwth-aachen.de/person/197/" target="blank">Paul Voigtlaender</a> and Hartwig Adam and Bradley Green and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="https://www.vision.rwth-aachen.de/persons/" target="blank">Bastian Leibe</a> and Daniel Cremers and Aljosa Osep and Laura Leal-Taixe and Liang-Chieh Chen},<br/>
&nbsp; title = {STEP: Segmenting and Tracking Every Pixel},<br/>&nbsp; booktitle = {Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Weber2021NEURIPSDATA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Weber2021NEURIPSDATA_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="https://arxiv.org/abs/2102.11859" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="http://www.youtube.com/watch?v=bf_YoKU_R54" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/google-research/deeplab2" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chitta2021ICCV');"><div class="publication_image"><img src="publications/Chitta2021ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>NEAT: Neural Attention Fields for End-to-End Autonomous Driving </b><br/>K. Chitta, A. Prakash and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2021</div></div>
<div class="publication_detail" id="Chitta2021ICCV" name="Chitta2021ICCV"><div class="publication_abstract"><b>Abstract:</b> Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial pre-requisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end Imitation Learning (IL) models. Our representation is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. NEAT nearly matches the state-of-the-art on the CARLA Leaderboard while being far less resource-intensive. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability. On a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Chitta2021ICCV.pdf" target="_blank">Chitta2021ICCV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and <a href="https://avg.is.tuebingen.mpg.de/person/aprakash" target="blank">Aditya Prakash</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Chitta2021ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Chitta2021ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Chitta2021ICCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Chitta2021ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=hYm6LPTyHHA" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=uO4xYM0FVPk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=gtO-ghjKkRs" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="https://github.com/autonomousvision/neat" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Reiser2021ICCV');"><div class="publication_image"><img src="publications/Reiser2021ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs </b><br/>C. Reiser, S. Peng, Y. Liao and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2021</div></div>
<div class="publication_detail" id="Reiser2021ICCV" name="Reiser2021ICCV"><div class="publication_abstract"><b>Abstract:</b> NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that significant speed-ups are possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by two orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality..<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Reiser2021ICCV.pdf" target="_blank">Reiser2021ICCV</a>,<br/>
&nbsp; author = {Christian Reiser and Songyou Peng and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Reiser2021ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Reiser2021ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=PNh0LvMpovU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=qvsMMDonF28" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://creiser.github.io/kilonerf/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/kilonerf/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Oechsle2021ICCV');"><div class="publication_image"><img src="publications/Oechsle2021ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction </b> <b><span style="color:#a51e37">(oral)</span></b><br/>M. Oechsle, S. Peng and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2021</div></div>
<div class="publication_detail" id="Oechsle2021ICCV" name="Oechsle2021ICCV"><div class="publication_abstract"><b>Abstract:</b> Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Oechsle2021ICCV.pdf" target="_blank">Oechsle2021ICCV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/moechsle" target="blank">Michael Oechsle</a> and Songyou Peng and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Oechsle2021ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Oechsle2021ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=WXUfHvZge0E" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=OSHlNS6ytkc" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://github.com/autonomousvision/unisurf" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chen2021ICCV');"><div class="publication_image"><img src="publications/Chen2021ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes </b><br/>X. Chen, Y. Zheng, M. Black, O. Hilliges and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2021</div></div>
<div class="publication_detail" id="Chen2021ICCV" name="Chen2021ICCV"><div class="publication_abstract"><b>Abstract:</b> Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Chen2021ICCV.pdf" target="_blank">Chen2021ICCV</a>,<br/>
&nbsp; author = {Xu Chen and Yufeng Zheng and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a> and Otmar Hilliges and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Chen2021ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Chen2021ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Chen2021ICCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Chen2021ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=k5rdKC7gfFU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=EVrZX8qkqqw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://xuchen-ethz.github.io/snarf/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/snarf/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Baur2021ICCV');"><div class="publication_image"><img src="publications/Baur2021ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation </b> <b><span style="color:#a51e37">(oral)</span></b><br/>S. Baur, D. Emmerichs, F. Moosmann, P. Pinggera, B. Ommer and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2021</div></div>
<div class="publication_detail" id="Baur2021ICCV" name="Baur2021ICCV"><div class="publication_abstract"><b>Abstract:</b> Recently, several frameworks for self-supervised learning of 3D scene flow on point clouds have emerged. Scene flow inherently separates every scene into multiple moving agents and a large class of points following a single rigid sensor motion. However, existing methods do not leverage this property of the data in their self-supervised training routines which could improve and stabilize flow predictions. Based on the discrepancy between a robust rigid ego-motion estimate and a raw flow prediction, we generate a self-supervised motion segmentation signal. The predicted motion segmentation, in turn, is used by our algorithm to attend to stationary points for aggregation of motion information in static parts of the scene. We learn our model end-to-end by backpropagating gradients through Kabsch's algorithm and demonstrate that this leads to accurate ego-motion which in turn improves the scene flow estimate. Using our method, we show state-of-the-art results across multiple scene flow metrics for different real-world datasets, showcasing the robustness and generalizability of this approach. We further analyze the performance gain when performing joint motion segmentation and scene flow in an ablation study. We also present a novel network architecture for 3D LiDAR scene flow which is capable of handling an order of magnitude more points during training than previously possible.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Baur2021ICCV.pdf" target="_blank">Baur2021ICCV</a>,<br/>
&nbsp; author = {Stefan Baur and David Emmerichs and <a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_moosmann.php" target="blank">Frank Moosmann</a> and Peter Pinggera and Bjorn Ommer and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Baur2021ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Baur2021ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=z34H43BOEVw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=kWH8_ZLFw28" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://baurst.github.io/slim/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Weis2021JMLR');"><div class="publication_image"><img src="publications/Weis2021JMLR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Benchmarking Unsupervised Object Representations for Video Sequences </b><br/>M. Weis, K. Chitta, Y. Sharma, W. Brendel, M. Bethge, A. Geiger and A. Ecker <br/>Journal of Machine Learning Research (JMLR), 2021</div></div>
<div class="publication_detail" id="Weis2021JMLR" name="Weis2021JMLR"><div class="publication_abstract"><b>Abstract:</b> Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: ViMON, a video-extension of MONet, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Weis2021JMLR.pdf" target="_blank">Weis2021JMLR</a>,<br/>
&nbsp; author = {Marissa Weis and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and Yash Sharma and Wieland Brendel and Matthias Bethge and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Alexander Ecker},<br/>
&nbsp; title = {Benchmarking Unsupervised Object Representations for Video Sequences},<br/>&nbsp; journal = {Journal of Machine Learning Research (JMLR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Weis2021JMLR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://github.com/ecker-lab/object-centric-representation-benchmark" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Hanselmann2021IV');"><div class="publication_image"><img src="publications/Hanselmann2021IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation </b><br/>N. Hanselmann, N. Schneider, B. Ortelt and A. Geiger <br/>Intelligent Vehicles Symposium (IV), 2021</div></div>
<div class="publication_detail" id="Hanselmann2021IV" name="Hanselmann2021IV"><div class="publication_abstract"><b>Abstract:</b> In order to handle the challenges of autonomous driving, deep learning has proven to be crucial in tackling increasingly complex tasks, such as 3D detection or instance segmentation. State-of-the-art approaches for image-based detection tasks tackle this complexity by operating in a cascaded fashion: they first extract a 2D bounding box based on which additional attributes, e.g. instance masks, are inferred. While these methods perform well, a key challenge remains the lack of accurate and cheap annotations for the growing variety of tasks. Synthetic data presents a promising solution but, despite the effort in domain adaptation research, the gap between synthetic and real data remains an open problem. In this work, we propose a weakly supervised domain adaptation setting which exploits the structure of cascaded detection tasks. In particular, we learn to infer the attributes solely from the source domain while leveraging 2D bounding boxes as weak labels in both domains to explain the domain shift. We further encourage domain-invariant features through class-wise feature alignment using ground-truth class information, which is not available in the unsupervised setting. As our experiments demonstrate, the approach is competitive with fully supervised settings while outperforming unsupervised adaptation approaches by a large margin.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Hanselmann2021IV.pdf" target="_blank">Hanselmann2021IV</a>,<br/>
&nbsp; author = {Niklas Hanselmann and <a href="http://nick-schneider.me/" target="blank">Nick Schneider</a> and Benedikt Ortelt and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Hanselmann2021IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=B8osSrZEIrQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://lasnik.github.io/wsda/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Paschalidou2021CVPR');"><div class="publication_image"><img src="publications/Paschalidou2021CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks </b><br/>D. Paschalidou, A. Katharopoulos, A. Geiger and S. Fidler <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2021</div></div>
<div class="publication_detail" id="Paschalidou2021CVPR" name="Paschalidou2021CVPR"><div class="publication_abstract"><b>Abstract:</b> Impressive progress in 3D shape extraction led to representations that can capture object geometries with high fidelity. In parallel, primitive-based methods seek to represent objects as semantically consistent part arrangements. However, due to the simplicity of existing primitive representations, these methods fail to accurately reconstruct 3D shapes using a small number of primitives/parts. We address the trade-off between reconstruction quality and number of parts with Neural Parts, a novel 3D primitive representation that defines primitives using an Invertible Neural Network (INN) which implements homeomorphic mappings between a sphere and the target object. The INN allows us to compute the inverse mapping of the homeomorphism, which in turn, enables the efficient computation of both the implicit surface function of a primitive and its mesh, without any additional post-processing. Our model learns to parse 3D objects into semantically consistent part arrangements without any part-level supervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our primitives can capture complex geometries and thus simultaneously achieve geometrically accurate as well as interpretable reconstructions using an order of magnitude fewer primitives than state-of-the-art shape abstraction methods.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Paschalidou2021CVPR.pdf" target="_blank">Paschalidou2021CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/dpaschalidou" target="blank">Despoina Paschalidou</a> and Angelos Katharopoulos and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Sanja Fidler},<br/>
&nbsp; title = {Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Paschalidou2021CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Paschalidou2021CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Paschalidou2021CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Paschalidou2021CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=6WK3B0IZJsw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://paschalidoud.github.io/neural_parts" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Wang2021CVPR');"><div class="publication_image"><img src="publications/Wang2021CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration </b><br/>S. Wang, A. Geiger and S. Tang <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2021</div></div>
<div class="publication_detail" id="Wang2021CVPR" name="Wang2021CVPR"><div class="publication_abstract"><b>Abstract:</b> Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, despite recent effort on rotation representation learning in neural networks, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Wang2021CVPR.pdf" target="_blank">Wang2021CVPR</a>,<br/>
&nbsp; author = {Shaofei Wang and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Siyu Tang},<br/>
&nbsp; title = {Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Wang2021CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Wang2021CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=OYMCUClMkD8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://taconite.github.io/PTF/website/PTF.html" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Prakash2021CVPR');"><div class="publication_image"><img src="publications/Prakash2021CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving </b><br/>A. Prakash, K. Chitta and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2021</div></div>
<div class="publication_detail" id="Prakash2021CVPR" name="Prakash2021CVPR"><div class="publication_abstract"><b>Abstract:</b> How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 80% compared to geometry-based fusion.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Prakash2021CVPR.pdf" target="_blank">Prakash2021CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/aprakash" target="blank">Aditya Prakash</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Multi-Modal Fusion Transformer for End-to-End Autonomous Driving},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Prakash2021CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Prakash2021CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Prakash2021CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Prakash2021CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=WxadQyQ2gMs" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://ap229997.github.io/projects/transfuser/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/transfuser/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Niemeyer2021CVPR');"><div class="publication_image"><img src="publications/Niemeyer2021CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields </b> <b><span style="color:#a51e37">(oral, best paper award)</span></b><br/>M. Niemeyer and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2021</div></div>
<div class="publication_detail" id="Niemeyer2021CVPR" name="Niemeyer2021CVPR"><div class="publication_abstract"><b>Abstract:</b> Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf" target="_blank">Niemeyer2021CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Niemeyer2021CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Niemeyer2021CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Niemeyer2021CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Niemeyer2021CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=fIaDXC-qRSg" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=kcuhkcUtVj0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://m-niemeyer.github.io/project-pages/giraffe/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/giraffe/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Tosi2021CVPR');"><div class="publication_image"><img src="publications/Tosi2021CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>SMD-Nets: Stereo Mixture Density Networks </b><br/>F. Tosi, Y. Liao, C. Schmitt and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2021</div></div>
<div class="publication_detail" id="Tosi2021CVPR" name="Tosi2021CVPR"><div class="publication_abstract"><b>Abstract:</b> Despite stereo matching accuracy has greatly improved by deep learning in the last few years, recovering sharp boundaries and high-resolution outputs efficiently remains challenging. In this paper, we propose Stereo Mixture Density Networks (SMD-Nets), a simple yet effective learning framework compatible with a wide class of 2D and 3D architectures which ameliorates both issues. Specifically, we exploit bimodal mixture densities as output representation and show that this allows for sharp and precise disparity estimates near discontinuities while explicitly modeling the  aleatoric uncertainty inherent in the observations. Moreover, we formulate disparity estimation as a continuous problem in the image domain, allowing our model to query disparities at arbitrary spatial precision. We carry out comprehensive experiments on a new high-resolution and highly realistic synthetic stereo dataset, consisting of stereo pairs at 8Mpx resolution, as well as on real-world stereo datasets. Our experiments demonstrate increased depth accuracy near object boundaries and prediction of ultra high-resolution disparity maps on standard GPUs. We demonstrate the flexibility of our technique by improving the performance of a variety of stereo backbones.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Tosi2021CVPR.pdf" target="_blank">Tosi2021CVPR</a>,<br/>
&nbsp; author = {Fabio Tosi and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://avg.is.tue.mpg.de/person/cschmitt" target="blank">Carolin Schmitt</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {SMD-Nets: Stereo Mixture Density Networks},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Tosi2021CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Tosi2021CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Tosi2021CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=tvVGuUSe2n8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/fabiotosi92/SMD-Nets" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/smdnets/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/><a href="https://www.rsipvision.com/ComputerVisionNews-2021July/30/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Best of CVPR</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Sauer2021ICLR');"><div class="publication_image"><img src="publications/Sauer2021ICLR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Counterfactual Generative Networks </b><br/>A. Sauer and A. Geiger <br/>International Conference on Learning Representations (ICLR), 2021</div></div>
<div class="publication_detail" id="Sauer2021ICLR" name="Sauer2021ICLR"><div class="publication_abstract"><b>Abstract:</b> Neural networks are prone to learning shortcuts -they often model simple correlations, ignoring more complex ones that potentially generalize better. Prior works on image classification show that instead of learning a connection to object shape, deep classifiers tend to exploit spurious correlations with low-level texture or the background for solving the classification task. In this work, we take a step towards more robust and interpretable classifiers that explicitly expose the task's causal structure. Building on current advances in deep generative modeling, we propose to decompose the image generation process into independent causal mechanisms that we train without direct supervision. By exploiting appropriate inductive biases, these mechanisms disentangle object shape, object texture, and background; hence, they allow for generating counterfactual images. We demonstrate the ability of our model to generate such images on MNIST and ImageNet. Further, we show that the counterfactual images can improve out-of-distribution robustness with a marginal drop in performance on the original classification task, despite being synthetic. Lastly, our generative model can be trained efficiently on a single GPU, exploiting common pre-trained models as inductive biases.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Sauer2021ICLR.pdf" target="_blank">Sauer2021ICLR</a>,<br/>
&nbsp; author = {<a href="https://axelsauer.com/" target="blank">Axel Sauer</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Counterfactual Generative Networks},<br/>&nbsp; booktitle = {International Conference on Learning Representations (ICLR)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Sauer2021ICLR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=JDwaLueR35U" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://sites.google.com/view/counterfactual-generation/home" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/cgn/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liu2021TIP');"><div class="publication_image"><img src="publications/Liu2021TIP.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Steering Kernels for Guided Depth Completion </b><br/>L. Liu, Y. Liao, Y. Wang, A. Geiger and Y. Liu <br/>Transactions on Image Processing (TIP), 2021</div></div>
<div class="publication_detail" id="Liu2021TIP" name="Liu2021TIP"><div class="publication_abstract"><b>Abstract:</b> This paper addresses the guided depth completion task in which the goal is to predict a dense depth map given a guidance RGB image and sparse depth measurements. Recent advances on this problem nurture hopes that one day we can acquire accurate and dense depth at a very low cost. A major challenge of guided depth completion is to effectively make use of extremely sparse measurements, eg, measurements covering less than 1% of the image pixels. In this paper, we propose a fully differentiable model that avoids convolving on sparse tensors by jointly learning depth interpolation and refinement. More specifically, we propose a differentiable kernel regression layer that interpolates the sparse depth measurements via learned kernels. We further refine the interpolated depth map using a residual depth refinement layer which leads to improved performance compared to learning absolute depth prediction using a vanilla network. We provide experimental evidence that our differentiable kernel regression layer not only enables end-to-end training from very sparse measurements using standard convolutional network architectures, but also leads to better depth interpolation results compared to existing heuristically motivated methods. We demonstrate that our method outperforms many state-of-the-art guided depth completion techniques on both NYUv2 and KITTI. We further show the generalization ability of our method with respect to the density and spatial statistics of the sparse depth measurements.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Liu2021TIP.pdf" target="_blank">Liu2021TIP</a>,<br/>
&nbsp; author = {Lina Liu and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://ywang-zju.github.io/" target="blank">Yue Wang</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Yong Liu},<br/>
&nbsp; title = {Learning Steering Kernels for Guided Depth Completion},<br/>&nbsp; journal = {Transactions on Image Processing (TIP)},<br/>
&nbsp; year = {2021}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liu2021TIP.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2020</h2></div>
<div class="publication_container" onclick="toggleDetails('Schwarz2020NEURIPS');"><div class="publication_image"><img src="publications/Schwarz2020NEURIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis </b><br/>K. Schwarz, Y. Liao, M. Niemeyer and A. Geiger <br/>Advances in Neural Information Processing Systems (NeurIPS), 2020</div></div>
<div class="publication_detail" id="Schwarz2020NEURIPS" name="Schwarz2020NEURIPS"><div class="publication_abstract"><b>Abstract:</b> While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, eg, the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf" target="_blank">Schwarz2020NEURIPS</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/kschwarz" target="blank">Katja Schwarz</a> and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schwarz2020NEURIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Schwarz2020NEURIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Schwarz2020NEURIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=akQf7WaCOHo" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=kcuhkcUtVj0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://github.com/autonomousvision/graf" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Alhaija2020THREEDV');"><div class="publication_image"><img src="publications/Alhaija2020THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition </b><br/>H. Alhaija, S. Mustikovela, V. Jampani, J. Thies, M. Niessner, A. Geiger and C. Rother <br/>International Conference on 3D Vision (3DV), 2020</div></div>
<div class="publication_detail" id="Alhaija2020THREEDV" name="Alhaija2020THREEDV"><div class="publication_abstract"><b>Abstract:</b> Neural rendering techniques promise efficient photo-realistic image synthesis while providing rich control over scene parameters by learning the physical image formation process. While several supervised methods have been pro-posed for this task, acquiring a dataset of images with accurately aligned 3D models is very difficult. The main contribution of this work is to lift this restriction by training a neural rendering algorithm from unpaired data. We pro-pose an auto encoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties. In contrast to a traditional graphics pipeline, our approach does not require to specify all scene properties, such as material parameters and lighting by hand.Instead, we learn photo-realistic deferred rendering from a small set of 3D models and a larger set of unaligned real images, both of which are easy to acquire in practice. Simultaneously, we obtain accurate intrinsic decompositions of real images while not requiring paired ground truth. Our experiments confirm that a joint treatment of rendering and de-composition is indeed beneficial and that our approach out-performs state-of-the-art image-to-image translation base-lines both qualitatively and quantitatively.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Alhaija2020THREEDV.pdf" target="_blank">Alhaija2020THREEDV</a>,<br/>
&nbsp; author = {Hassan Alhaija and Siva Mustikovela and Varun Jampani and Justus Thies and Matthias Niessner and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Carsten Rother},<br/>
&nbsp; title = {Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Alhaija2020THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Alhaija2020THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=FOWoCeOAiug" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Oechsle2020THREEDV');"><div class="publication_image"><img src="publications/Oechsle2020THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Implicit Surface Light Fields </b><br/>M. Oechsle, M. Niemeyer, C. Reiser, L. Mescheder, T. Strauss and A. Geiger <br/>International Conference on 3D Vision (3DV), 2020</div></div>
<div class="publication_detail" id="Oechsle2020THREEDV" name="Oechsle2020THREEDV"><div class="publication_abstract"><b>Abstract:</b> Implicit representations of 3D objects have recently achieved impressive results on learning-based 3D reconstruction tasks. While existing works use simple texture models to represent object appearance, photo-realistic image synthesis requires reasoning about the complex interplay of light, geometry and surface properties. In this work, we propose a novel implicit representation for capturing the visual appearance of an object in terms of its surface light field. In contrast to existing representations, our implicit model represents surface light fields in a continuous fashion and independent of the geometry. Moreover, we condition the surface light field with respect to the location and color of a small light source. Compared to traditional surface light field models, this allows us to manipulate the light source and relight the object using environment maps. We further demonstrate the capabilities of our model to predict the visual appearance of an unseen object from a single real RGB image and corresponding 3D shape information. As evidenced by our experiments, our model is able to infer rich visual appearance including shadows and specular reflections. Finally, we show that the proposed representation can be embedded into a variational auto-encoder for generating novel appearances that conform to the specified illumination conditions.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Oechsle2020THREEDV.pdf" target="_blank">Oechsle2020THREEDV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/moechsle" target="blank">Michael Oechsle</a> and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and Christian Reiser and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_strauss.php" target="blank">Thilo Strauss</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Implicit Surface Light Fields},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Oechsle2020THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Oechsle2020THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Oechsle2020THREEDV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="https://github.com/autonomousvision/cslf" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Luiten2020IJCV');"><div class="publication_image"><img src="publications/Luiten2020IJCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking </b><br/>J. Luiten, A. Osep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taixe and B. Leibe <br/>International Journal of Computer Vision (IJCV), 2020</div></div>
<div class="publication_detail" id="Luiten2020IJCV" name="Luiten2020IJCV"><div class="publication_abstract"><b>Abstract:</b> Multi-Object Tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, HOTA (Higher Order Tracking Accuracy), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Luiten2020IJCV.pdf" target="_blank">Luiten2020IJCV</a>,<br/>
&nbsp; author = {<a href="https://www.vision.rwth-aachen.de/person/216/" target="blank">Jonathon Luiten</a> and Aljosa Osep and Patrick Dendorfer and Philip Torr and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Laura Leal-Taixe and <a href="https://www.vision.rwth-aachen.de/persons/" target="blank">Bastian Leibe</a>},<br/>
&nbsp; title = {HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking},<br/>&nbsp; journal = {International Journal of Computer Vision (IJCV)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Luiten2020IJCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Behl2020IROS');"><div class="publication_image"><img src="publications/Behl2020IROS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Label Efficient Visual Abstractions for Autonomous Driving </b><br/>A. Behl, K. Chitta, A. Prakash, E. Ohn-Bar and A. Geiger <br/>International Conference on Intelligent Robots and Systems (IROS), 2020</div></div>
<div class="publication_detail" id="Behl2020IROS" name="Behl2020IROS"><div class="publication_abstract"><b>Abstract:</b> It is well known that semantic segmentation can be used as an effective intermediate representation for learning driving policies. However, the task of street scene semantic segmentation requires expensive annotations. Furthermore, segmentation algorithms are often trained irrespective of the actual driving task, using auxiliary image-space loss functions which are not guaranteed to maximize driving metrics such as safety or distance traveled per intervention. In this work, we seek to quantify the impact of reducing segmentation annotation costs on learned behavior cloning agents. We analyze several segmentation-based intermediate representations. We use these visual abstractions to systematically study the trade-off between annotation efficiency and driving performance, ie, the types of classes labeled, the number of image samples used to learn the visual abstraction model, and their granularity (eg, object masks vs. 2D bounding boxes). Our analysis uncovers several practical insights into how segmentation-based visual abstractions can be exploited in a more label efficient manner. Surprisingly, we find that state-of-the-art driving performance can be achieved with orders of magnitude reduction in annotation cost. Beyond label efficiency, we find several additional training benefits when leveraging visual abstractions, such as a significant reduction in the variance of the learned policy when compared to state-of-the-art end-to-end driving models.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Behl2020IROS.pdf" target="_blank">Behl2020IROS</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and <a href="https://avg.is.tuebingen.mpg.de/person/aprakash" target="blank">Aditya Prakash</a> and <a href="https://avg.is.tuebingen.mpg.de/person/eohn-bar" target="blank">Eshed Ohn-Bar</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Label Efficient Visual Abstractions for Autonomous Driving},<br/>&nbsp; booktitle = {International Conference on Intelligent Robots and Systems (IROS)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Behl2020IROS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Behl2020IROS_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=5SszfDWrqzo" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/visual_abstractions" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/visual-abstractions/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Chen2020ECCV');"><div class="publication_image"><img src="publications/Chen2020ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Category Level Object Pose Estimation via Neural Analysis-by-Synthesis </b><br/>X. Chen, Z. Dong, J. Song, A. Geiger and O. Hilliges <br/>European Conference on Computer Vision (ECCV), 2020</div></div>
<div class="publication_detail" id="Chen2020ECCV" name="Chen2020ECCV"><div class="publication_abstract"><b>Abstract:</b> Many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. In this paper we combine a gradient-based fitting procedure with a parametric neural image synthesis module that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus rendering the need for explicit CAD models per object instance unnecessary. The image synthesis network is designed to efficiently span the pose configuration space so that model capacity can be used to capture the shape and local appearance (i.e., texture) variations jointly. At inference time the synthesized images are compared to the target via an appearance based loss and the error signal is backpropagated through the network to the input parameters. Keeping the network parameters fixed, this allows for iterative optimization of the object pose, shape and appearance in a joint manner and we experimentally show that the method can recover orientation of objects with high accuracy from 2D images alone. When provided with depth measurements, to overcome scale ambiguities, the method can accurately recover the full 6DOF pose successfully.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Chen2020ECCV.pdf" target="_blank">Chen2020ECCV</a>,<br/>
&nbsp; author = {Xu Chen and Zijian Dong and Jie Song and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Otmar Hilliges},<br/>
&nbsp; title = {Category Level Object Pose Estimation via Neural Analysis-by-Synthesis},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Chen2020ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Chen2020ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://ait.ethz.ch/projects/2020/neural-object-fitting" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Peng2020ECCV');"><div class="publication_image"><img src="publications/Peng2020ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Convolutional Occupancy Networks </b> <b><span style="color:#a51e37">(spotlight)</span></b><br/>S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys and A. Geiger <br/>European Conference on Computer Vision (ECCV), 2020</div></div>
<div class="publication_detail" id="Peng2020ECCV" name="Peng2020ECCV"><div class="publication_abstract"><b>Abstract:</b> Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Peng2020ECCV.pdf" target="_blank">Peng2020ECCV</a>,<br/>
&nbsp; author = {Songyou Peng and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Convolutional Occupancy Networks},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Peng2020ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Peng2020ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Peng2020ECCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=EmauovgrDSM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/convolutional_occupancy_networks" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/convolutional-occupancy-networks/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Sanzenbacher2020ARXIV');"><div class="publication_image"><img src="publications/Sanzenbacher2020ARXIV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Neural Light Transport </b><br/>P. Sanzenbacher, L. Mescheder and A. Geiger <br/>Arxiv, 2020</div></div>
<div class="publication_detail" id="Sanzenbacher2020ARXIV" name="Sanzenbacher2020ARXIV"><div class="publication_abstract"><b>Abstract:</b> In recent years, deep generative models have gained significance due to their ability to synthesize natural-looking images with applications ranging from virtual reality to data augmentation for training computer vision models. While existing models are able to faithfully learn the image distribution of the training set, they often lack controllability as they operate in 2D pixel space and do not model the physical image formation process. In this work, we investigate the importance of 3D reasoning for photorealistic rendering. We present an approach for learning light transport in static and dynamic 3D scenes using a neural network with the goal of predicting photorealistic images. In contrast to existing approaches that operate in the 2D image domain, our approach reasons in both 3D and 2D space, thus enabling global illumination effects and manipulation of 3D scene geometry. Experimentally, we find that our model is able to produce photorealistic renderings of static and dynamic scenes. Moreover, it compares favorably to baselines which combine path tracing and image denoising at the same computational budget.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Sanzenbacher2020ARXIV.pdf" target="_blank">Sanzenbacher2020ARXIV</a>,<br/>
&nbsp; author = {Paul Sanzenbacher and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Neural Light Transport},<br/>&nbsp; journal = {Arxiv},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Sanzenbacher2020ARXIV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Sanzenbacher2020ARXIV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="https://arxiv.org/abs/2006.03427" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Janai2020');"><div class="publication_image"><img src="publications/Janai2020.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art </b><br/>J. Janai, F. Güney, A. Behl and A. Geiger <br/>Foundations and Trends in Computer Graphics and Vision, 2020</div></div>
<div class="publication_detail" id="Janai2020" name="Janai2020"><div class="publication_abstract"><b>Abstract:</b> Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This monograph attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@BOOK{<a href="https://www.cvlibs.net/publications/Janai2020.pdf" target="_blank">Janai2020</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/jjanai" target="blank">Joel Janai</a> and <a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art},<br/>&nbsp; publisher = {Foundations and Trends in Computer Graphics and Vision},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Janai2020.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://arxiv.org/abs/1704.05519" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="http://dx.doi.org/10.1561/0600000079" target="_blank"><div class="publication_icon"><img src="site/icon_link.png"/></div>Link</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Ohn-Bar2020CVPR');"><div class="publication_image"><img src="publications/Ohn-Bar2020CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Situational Driving </b><br/>E. Ohn-Bar, A. Prakash, A. Behl, K. Chitta and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2020</div></div>
<div class="publication_detail" id="Ohn-Bar2020CVPR" name="Ohn-Bar2020CVPR"><div class="publication_abstract"><b>Abstract:</b> Human drivers have a remarkable ability to drive in diverse visual conditions and situations, e.g., from maneuvering in rainy, limited visibility conditions with no lane markings to turning in a busy intersection while yielding to pedestrians. In contrast, we find that state-of-the-art sensorimotor driving models struggle when encountering diverse settings with varying relationships between observation and action. To generalize when making decisions across diverse conditions, humans leverage multiple types of situation-specific reasoning and learning strategies. Motivated by this observation, we develop a framework for learning a situational driving policy that effectively captures reasoning under varying types of scenarios. Our key idea is to learn a mixture model with a set of policies that can capture multiple driving modes. We first optimize the mixture model through behavior cloning, and show it to result in significant gains in terms of driving performance in diverse conditions. We then refine the model by directly optimizing for the driving task itself, i.e., supervised with the navigation task reward. Our method is more scalable than methods assuming access to privileged information, e.g., perception labels, as it only assumes demonstration and reward-based supervision. We achieve over 98% success rate on the CARLA driving benchmark as well as state-of-the-art performance on a newly introduced generalization benchmark.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Ohn-Bar2020CVPR.pdf" target="_blank">Ohn-Bar2020CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/eohn-bar" target="blank">Eshed Ohn-Bar</a> and <a href="https://avg.is.tuebingen.mpg.de/person/aprakash" target="blank">Aditya Prakash</a> and <a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Situational Driving},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Ohn-Bar2020CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Ohn-Bar2020CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Ohn-Bar2020CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=3PEEKoBWsL4" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=XkZyEqO1l5o" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=g9-KRpkDYOk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="https://github.com/autonomousvision/lsd" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schmitt2020CVPR');"><div class="publication_image"><img src="publications/Schmitt2020CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>On Joint Estimation of Pose, Geometry and svBRDF from a Handheld Scanner </b><br/>C. Schmitt, S. Donne, G. Riegler, V. Koltun and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2020</div></div>
<div class="publication_detail" id="Schmitt2020CVPR" name="Schmitt2020CVPR"><div class="publication_abstract"><b>Abstract:</b> We propose a novel formulation for joint recovery of camera pose, object geometry and spatially-varying BRDF. The input to our approach is a sequence of RGB-D images captured by a mobile, hand-held scanner that actively illuminates the scene with point light sources. Compared to previous works that jointly estimate geometry and materials from a hand-held scanner, we formulate this problem using a single objective function that can be minimized using off-the-shelf gradient-based solvers. By integrating material clustering as a differentiable operation into the optimization process, we avoid pre-processing heuristics and demonstrate that our model is able to determine the correct number of specular materials independently. We provide a study on the importance of each component in our formulation and on the requirements of the initial geometry. We show that optimizing over the poses is crucial for accurately recovering fine details and that our approach naturally results in a semantically meaningful material segmentation.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schmitt2020CVPR.pdf" target="_blank">Schmitt2020CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/cschmitt" target="blank">Carolin Schmitt</a> and <a href="https://avg.is.tue.mpg.de/person/sdonne" target="blank">Simon Donne</a> and <a href="https://rvlab.icg.tugraz.at/personal_page/personal_page_gernot.html" target="blank">Gernot Riegler</a> and <a href="http://vladlen.info/" target="blank">Vladlen Koltun</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {On Joint Estimation of Pose, Geometry and svBRDF from a Handheld Scanner},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schmitt2020CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Schmitt2020CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Schmitt2020CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Schmitt2020CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=_xxSQPD9qU0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/handheld_svbrdf_geometry" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Prakash2020CVPR');"><div class="publication_image"><img src="publications/Prakash2020CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Exploring Data Aggregation in Policy Learning for Vision-based Urban Autonomous Driving </b><br/>A. Prakash, A. Behl, E. Ohn-Bar, K. Chitta and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2020</div></div>
<div class="publication_detail" id="Prakash2020CVPR" name="Prakash2020CVPR"><div class="publication_abstract"><b>Abstract:</b> Data aggregation techniques can significantly improve vision-based policy learning within a training environment, e.g., learning to drive in a specific simulation condition. However, as on-policy data is sequentially sampled and added in an iterative manner, the policy can specialize and overfit to the training conditions. For real-world applications, it is useful for the learned policy to generalize to novel scenarios that differ from the training conditions. To improve policy learning while maintaining robustness when training end-to-end driving policies, we perform an extensive analysis of data aggregation techniques in the CARLA environment. We demonstrate how the majority of them have poor generalization performance, and develop a novel approach with empirically better generalization performance compared to existing techniques. Our two key ideas are (1) to sample critical states from the collected on-policy data based on the utility they provide to the learned policy in terms of driving behavior, and (2) to incorporate a replay buffer which progressively focuses on the high uncertainty regions of the policy's state distribution. We evaluate the proposed approach on the CARLA NoCrash benchmark, focusing on the most challenging driving scenarios with dense pedestrian and vehicle traffic. Our approach improves driving success rate by 16% over state-of-the-art, achieving 87% of the expert performance while also reducing the collision rate by an order of magnitude without the use of any additional modality, auxiliary tasks, architectural modifications or reward from the environment.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Prakash2020CVPR.pdf" target="_blank">Prakash2020CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/aprakash" target="blank">Aditya Prakash</a> and <a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and <a href="https://avg.is.tuebingen.mpg.de/person/eohn-bar" target="blank">Eshed Ohn-Bar</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kchitta" target="blank">Kashyap Chitta</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Exploring Data Aggregation in Policy Learning for Vision-based Urban Autonomous Driving},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Prakash2020CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Prakash2020CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Prakash2020CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=Aj8hvgtPocU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=VxYUM5VTnAI" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=g9-KRpkDYOk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="https://github.com/autonomousvision/data_aggregation" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/dagger-urban-driving/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Paschalidou2020CVPR');"><div class="publication_image"><img src="publications/Paschalidou2020CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image </b><br/>D. Paschalidou, L. Gool and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2020</div></div>
<div class="publication_detail" id="Paschalidou2020CVPR" name="Paschalidou2020CVPR"><div class="publication_abstract"><b>Abstract:</b> Humans perceive the 3D world as a set of distinct objects that are characterized by various low-level (geometry, reflectance) and high-level (connectivity, adjacency, symmetry) properties. Recent methods based on convolutional neural networks (CNNs) demonstrated impressive progress in 3D reconstruction, even when using a single 2D image as input. However, the majority of these methods focuses on recovering the local 3D geometry of an object without considering its part-based decomposition or relations between parts. We address this challenging problem by proposing a novel formulation that allows to jointly recover the geometry of a 3D object as a set of primitives as well as their latent hierarchical structure without part-level supervision. Our model recovers the higher level structural decomposition of various objects in the form of a binary tree of primitives, where simple parts are represented with fewer primitives and more complex parts are modeled with more components. Our experiments on the ShapeNet and D-FAUST datasets demonstrate that considering the organization of parts indeed facilitates reasoning about 3D geometry.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Paschalidou2020CVPR.pdf" target="_blank">Paschalidou2020CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/dpaschalidou" target="blank">Despoina Paschalidou</a> and <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" target="blank">Luc Gool</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Paschalidou2020CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Paschalidou2020CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Paschalidou2020CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Paschalidou2020CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=QgD0NHbWVlU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=Gd9Xa3Fy4wc" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://github.com/paschalidoud/hierarchical_primitives" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/hierarchical-primitives/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Niemeyer2020CVPR');"><div class="publication_image"><img src="publications/Niemeyer2020CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision </b><br/>M. Niemeyer, L. Mescheder, M. Oechsle and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2020</div></div>
<div class="publication_detail" id="Niemeyer2020CVPR" name="Niemeyer2020CVPR"><div class="publication_abstract"><b>Abstract:</b> Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Niemeyer2020CVPR.pdf" target="_blank">Niemeyer2020CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://avg.is.tuebingen.mpg.de/person/moechsle" target="blank">Michael Oechsle</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Niemeyer2020CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Niemeyer2020CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Niemeyer2020CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Niemeyer2020CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=lcub1KH-mmk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=gIha5kvSX9s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=U_jIN3qWVEw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="http://www.youtube.com/watch?v=9r9TDr2Aq5A" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 4</a><br/><a href="https://github.com/autonomousvision/differentiable_volumetric_rendering" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/differentiable-volumetric-rendering/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liao2020CVPR');"><div class="publication_image"><img src="publications/Liao2020CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis </b><br/>Y. Liao, K. Schwarz, L. Mescheder and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2020</div></div>
<div class="publication_detail" id="Liao2020CVPR" name="Liao2020CVPR"><div class="publication_abstract"><b>Abstract:</b> In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Liao2020CVPR.pdf" target="_blank">Liao2020CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://avg.is.tuebingen.mpg.de/person/kschwarz" target="blank">Katja Schwarz</a> and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liao2020CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Liao2020CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Liao2020CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Liao2020CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=ygQCgGC0Lm8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=NSX-ornLgZI" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=LyZCRXNS0eg" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="https://github.com/autonomousvision/controllable_image_synthesis" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/controllable-gan/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liu2020RAL');"><div class="publication_image"><img src="publications/Liu2020RAL.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Self-Supervised Linear Motion Deblurring </b><br/>P. Liu, J. Janai, M. Pollefeys, T. Sattler and A. Geiger <br/>Robotics and Automation Letters (RA-L), 2020</div></div>
<div class="publication_detail" id="Liu2020RAL" name="Liu2020RAL"><div class="publication_abstract"><b>Abstract:</b> Motion blurry images challenge many computer vision algorithms, e.g., feature detection, motion estimation, or object recognition. Deep convolutional neural networks are state-of-the-art for image deblurring. However, obtaining training data with corresponding sharp and blurry image pairs can be difficult. In this paper, we present a differentiable reblur model for self-supervised motion deblurring, which enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our key insight is that motion cues obtained from consecutive images yield sufficient information to inform the deblurring task. We therefore formulate deblurring as an inverse rendering problem, taking into account the physical image formation process: we first predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We use both synthetic and real dataset for experimental evaluations. Our experiments demonstrate that self-supervised
single image deblurring is really feasible and leads to visually compelling results.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Liu2020RAL.pdf" target="_blank">Liu2020RAL</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/liup/" target="blank">Peidong Liu</a> and <a href="https://avg.is.tue.mpg.de/person/jjanai" target="blank">Joel Janai</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Self-Supervised Linear Motion Deblurring},<br/>&nbsp; journal = {Robotics and Automation Letters (RA-L)},<br/>
&nbsp; year = {2020}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liu2020RAL.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://github.com/ethliup/SelfDeblur" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/motion-deblur" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2019</h2></div>
<div class="publication_container" onclick="toggleDetails('Ranjan2019ICCV');"><div class="publication_image"><img src="publications/Ranjan2019ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Attacking Optical Flow </b><br/>A. Ranjan, J. Janai, A. Geiger and M. Black <br/>International Conference on Computer Vision (ICCV), 2019</div></div>
<div class="publication_detail" id="Ranjan2019ICCV" name="Ranjan2019ICCV"><div class="publication_abstract"><b>Abstract:</b> Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Ranjan2019ICCV.pdf" target="_blank">Ranjan2019ICCV</a>,<br/>
&nbsp; author = {<a href="https://ps.is.tuebingen.mpg.de/person/aranjan" target="blank">Anurag Ranjan</a> and <a href="https://avg.is.tue.mpg.de/person/jjanai" target="blank">Joel Janai</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a>},<br/>
&nbsp; title = {Attacking Optical Flow},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Ranjan2019ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Ranjan2019ICCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=5nQ7loiPmdA" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://flowattack.is.tue.mpg.de/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Niemeyer2019ICCV');"><div class="publication_image"><img src="publications/Niemeyer2019ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics </b><br/>M. Niemeyer, L. Mescheder, M. Oechsle and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2019</div></div>
<div class="publication_detail" id="Niemeyer2019ICCV" name="Niemeyer2019ICCV"><div class="publication_abstract"><b>Abstract:</b> Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while state-of-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Niemeyer2019ICCV.pdf" target="_blank">Niemeyer2019ICCV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://avg.is.tuebingen.mpg.de/person/moechsle" target="blank">Michael Oechsle</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Niemeyer2019ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Niemeyer2019ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Niemeyer2019ICCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Niemeyer2019ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=c0yOugTgrWc" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/occupancy_flow" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/occupancy-flow/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Oechsle2019ICCV');"><div class="publication_image"><img src="publications/Oechsle2019ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Texture Fields: Learning Texture Representations in Function Space </b> <b><span style="color:#a51e37">(oral)</span></b><br/>M. Oechsle, L. Mescheder, M. Niemeyer, T. Strauss and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2019</div></div>
<div class="publication_detail" id="Oechsle2019ICCV" name="Oechsle2019ICCV"><div class="publication_abstract"><b>Abstract:</b> In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach  circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.
<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Oechsle2019ICCV.pdf" target="_blank">Oechsle2019ICCV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tuebingen.mpg.de/person/moechsle" target="blank">Michael Oechsle</a> and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_strauss.php" target="blank">Thilo Strauss</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Texture Fields: Learning Texture Representations in Function Space},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Oechsle2019ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Oechsle2019ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Oechsle2019ICCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Oechsle2019ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=y8XHkl3vtpI" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/autonomousvision/texture_fields" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/texture-fields/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Coors2019THREEDV');"><div class="publication_image"><img src="publications/Coors2019THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>NoVA: Learning to See in Novel Viewpoints and Domains </b><br/>B. Coors, A. Condurache and A. Geiger <br/>International Conference on 3D Vision (3DV), 2019</div></div>
<div class="publication_detail" id="Coors2019THREEDV" name="Coors2019THREEDV"><div class="publication_abstract"><b>Abstract:</b> Domain adaptation techniques enable the re-use and transfer of existing labeled datasets from a source to a target domain in which little or no labeled data exists. Recently, image-level domain adaptation approaches have demonstrated impressive results in adapting from synthetic to real-world environments by translating source images to the style of a target domain. However, the domain gap between source and target may not only be caused by a different style but also by a change in viewpoint. This case necessitates a semantically consistent translation of source images and labels to the style and viewpoint of the target domain. In this work, we propose the Novel Viewpoint Adaptation (NoVA) model, which enables unsupervised adaptation to a novel viewpoint in a target domain for which no labeled data is available. NoVA utilizes an explicit representation of the 3D scene geometry to translate source view images and labels to the target view. Experiments on adaptation to synthetic and real-world datasets show the benefit of NoVA compared to state-of-the-art domain adaptation approaches on the task of semantic segmentation.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Coors2019THREEDV.pdf" target="_blank">Coors2019THREEDV</a>,<br/>
&nbsp; author = {<a href="https://is.tuebingen.mpg.de/person/bcoors" target="blank">Benjamin Coors</a> and <a href="https://www.isip.uni-luebeck.de/people/alexandru-condurache.html" target="blank">Alexandru Paul Condurache</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {NoVA: Learning to See in Novel Viewpoints and Domains},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Coors2019THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Coors2019THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Coors2019THREEDV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Coors2019THREEDV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=rsJWXtgdSUg" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Lv2019CVPR');"><div class="publication_image"><img src="publications/Lv2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Taking a Deeper Look at the Inverse Compositional Algorithm </b> <b><span style="color:#a51e37">(oral, best paper finalist)</span></b><br/>Z. Lv, F. Dellaert, J. Rehg and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Lv2019CVPR" name="Lv2019CVPR"><div class="publication_abstract"><b>Abstract:</b> In this paper, we provide a modern synthesis of the classic inverse compositional algorithm for dense image alignment. We first discuss the assumptions made by this well-established technique, and subsequently propose to relax these assumptions by incorporating data-driven priors into this model. More specifically, we unroll a robust version of the inverse compositional algorithm and replace multiple components of this algorithm using more expressive models whose parameters we train in an end-to-end fashion from data. Our experiments on several challenging 3D rigid motion estimation tasks demonstrate the advantages of combining optimization with learning-based techniques, outperforming the classic inverse compositional algorithm as well as data-driven image-to-pose regression approaches.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Lv2019CVPR.pdf" target="_blank">Lv2019CVPR</a>,<br/>
&nbsp; author = {<a href="https://www.cc.gatech.edu/~zlv30/" target="blank">Zhaoyang Lv</a> and <a href="https://www.cc.gatech.edu/~dellaert/FrankDellaert/Frank_Dellaert/Frank_Dellaert.html" target="blank">Frank Dellaert</a> and <a href="https://rehg.org/" target="blank">James M. Rehg</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Taking a Deeper Look at the Inverse Compositional Algorithm},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Lv2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Lv2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Lv2019CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Lv2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=doTjXDFtyK0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Mescheder2019CVPR');"><div class="publication_image"><img src="publications/Mescheder2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Occupancy Networks: Learning 3D Reconstruction in Function Space </b> <b><span style="color:#a51e37">(oral, best paper finalist)</span></b><br/>L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Mescheder2019CVPR" name="Mescheder2019CVPR"><div class="publication_abstract"><b>Abstract:</b> With the advent of deep neural networks, learning-based approaches for 3D~reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D~reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D~reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Mescheder2019CVPR.pdf" target="_blank">Mescheder2019CVPR</a>,<br/>
&nbsp; author = {<a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://avg.is.tuebingen.mpg.de/person/moechsle" target="blank">Michael Oechsle</a> and <a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="blank">Michael Niemeyer</a> and <a href="http://www.nowozin.net/sebastian/" target="blank">Sebastian Nowozin</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Mescheder2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Mescheder2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Mescheder2019CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Mescheder2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=w1Qo3bOiPaE&t=6s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=9r9TDr2Aq5A" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://github.com/LMescheder/Occupancy-Networks" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/occupancy-networks/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Riegler2019CVPR');"><div class="publication_image"><img src="publications/Riegler2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Connecting the Dots: Learning Representations for Active Monocular Depth Estimation </b><br/>G. Riegler, Y. Liao, S. Donne, V. Koltun and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Riegler2019CVPR" name="Riegler2019CVPR"><div class="publication_abstract"><b>Abstract:</b> We propose a technique for depth estimation with a monocular structured-light camera, \ie, a calibrated stereo set-up with one camera and one laser projector. Instead of formulating the depth estimation via a correspondence search problem, we show that a simple convolutional architecture is sufficient for high-quality disparity estimates in this setting. As accurate ground-truth is hard to obtain, we train our model in a self-supervised fashion with a combination of photometric and geometric losses. Further, we demonstrate that the projected pattern of the structured light sensor can be reliably separated from the ambient information. This can then be used to improve depth boundaries in a weakly supervised fashion by modeling the joint statistics of image and depth edges. The model trained in this fashion compares favorably to the state-of-the-art on challenging synthetic and real-world datasets. In addition, we contribute a novel simulator, which allows to benchmark active depth prediction algorithms in controlled conditions.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Riegler2019CVPR.pdf" target="_blank">Riegler2019CVPR</a>,<br/>
&nbsp; author = {<a href="https://rvlab.icg.tugraz.at/personal_page/personal_page_gernot.html" target="blank">Gernot Riegler</a> and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://avg.is.tue.mpg.de/person/sdonne" target="blank">Simon Donne</a> and <a href="http://vladlen.info/" target="blank">Vladlen Koltun</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Connecting the Dots: Learning Representations for Active Monocular Depth Estimation},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Riegler2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Riegler2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Riegler2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="https://github.com/autonomousvision/connecting_the_dots" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Voigtlaender2019CVPR');"><div class="publication_image"><img src="publications/Voigtlaender2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>MOTS: Multi-Object Tracking and Segmentation </b><br/>P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. Sekar, A. Geiger and B. Leibe <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Voigtlaender2019CVPR" name="Voigtlaender2019CVPR"><div class="publication_abstract"><b>Abstract:</b> This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Voigtlaender2019CVPR.pdf" target="_blank">Voigtlaender2019CVPR</a>,<br/>
&nbsp; author = {<a href="https://www.vision.rwth-aachen.de/person/197/" target="blank">Paul Voigtlaender</a> and Michael Krause and Aljosa Osep and <a href="https://www.vision.rwth-aachen.de/person/216/" target="blank">Jonathon Luiten</a> and Berin Balachandar Gnana Sekar and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="https://www.vision.rwth-aachen.de/persons/" target="blank">Bastian Leibe</a>},<br/>
&nbsp; title = {MOTS: Multi-Object Tracking and Segmentation},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Voigtlaender2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Voigtlaender2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Voigtlaender2019CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Voigtlaender2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=K38_pZw_P9s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://www.vision.rwth-aachen.de/page/mots" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Behl2019CVPR');"><div class="publication_image"><img src="publications/Behl2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds </b><br/>A. Behl, D. Paschalidou, S. Donne and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Behl2019CVPR" name="Behl2019CVPR"><div class="publication_abstract"><b>Abstract:</b> Despite significant progress in image-based 3D scene flow estimation, the performance of such approaches has not yet reached the fidelity required by many applications. Simultaneously, these applications are often not restricted to image-based estimation: laser scanners provide a popular alternative to traditional cameras, for example in the context of self-driving cars, as they directly yield a 3D point cloud. In this paper, we propose to estimate 3D motion from such unstructured point clouds using a deep neural network. In a single forward pass, our model jointly predicts 3D scene flow as well as the 3D bounding box and rigid body motion of objects in the scene. While the prospect of estimating 3D scene flow from unstructured point clouds is  promising, it is also a challenging task. We show that the traditional global representation of rigid body motion prohibits inference by CNNs, and propose a translation equivariant representation to circumvent this problem. For training our deep network, a large dataset is required. Because of this, we augment real scans from KITTI with virtual objects, realistically modeling occlusions and simulating sensor noise. A thorough comparison with classic and learning-based techniques highlights the robustness of the proposed approach.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Behl2019CVPR.pdf" target="_blank">Behl2019CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and <a href="https://avg.is.tue.mpg.de/person/dpaschalidou" target="blank">Despoina Paschalidou</a> and <a href="https://avg.is.tue.mpg.de/person/sdonne" target="blank">Simon Donne</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Behl2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Behl2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Behl2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=cjJhzYCUNTY" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/aseembehl/pointflownet" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Donne2019CVPR');"><div class="publication_image"><img src="publications/Donne2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Non-volumetric Depth Fusion using Successive Reprojections </b><br/>S. Donne and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Donne2019CVPR" name="Donne2019CVPR"><div class="publication_abstract"><b>Abstract:</b> Given a set of input views, multi-view stereopsis techniques estimate depth maps to represent the 3D reconstruction of the scene; these are fused into a single, consistent, reconstruction -- most often a point cloud. In this work we propose to learn an auto-regressive depth refinement directly from data. While deep learning has improved the accuracy and speed of depth estimation significantly, learned MVS techniques remain limited to the planesweeping paradigm. We refine a set of input depth maps by successively reprojecting information from neighbouring views to leverage multi-view constraints. Compared to learning-based volumetric fusion techniques, an image-based representation allows significantly more detailed reconstructions; compared to traditional point-based techniques, our method learns noise suppression and surface completion in a data-driven fashion. Due to the limited availability of high-quality reconstruction datasets with ground truth, we introduce two novel synthetic datasets to (pre-)train our network. Our approach is able to improve both the output depth maps and the reconstructed point cloud, for both learned and traditional depth estimation front-ends, on both synthetic and real data.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Donne2019CVPR.pdf" target="_blank">Donne2019CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/sdonne" target="blank">Simon Donne</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Non-volumetric Depth Fusion using Successive Reprojections},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Donne2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Donne2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Donne2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=Cz7zz7Fuqlg" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/simon-donne/defusr/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/defusr/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Paschalidou2019CVPR');"><div class="publication_image"><img src="publications/Paschalidou2019CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids </b><br/>D. Paschalidou, A. Ulusoy and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2019</div></div>
<div class="publication_detail" id="Paschalidou2019CVPR" name="Paschalidou2019CVPR"><div class="publication_abstract"><b>Abstract:</b> Abstracting complex 3D shapes with parsimonious part-based representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Paschalidou2019CVPR.pdf" target="_blank">Paschalidou2019CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/dpaschalidou" target="blank">Despoina Paschalidou</a> and <a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Paschalidou2019CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Paschalidou2019CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Paschalidou2019CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Paschalidou2019CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=eaZHYOsv9Lw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/paschalidoud/superquadric_parsing" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/superquadrics-revisited/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Heng2019ICRA');"><div class="publication_image"><img src="publications/Heng2019ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Project AutoVision: Localization and 3D Scene Perception for an Autonomous Vehicle with a Multi-Camera System </b><br/>L. Heng, B. Choi, Z. Cui, M. Geppert, S. Hu, B. Kuan, P. Liu, R. Nguyen, Y. Yeo, A. Geiger,  et al.<br/>International Conference on Robotics and Automation (ICRA), 2019</div></div>
<div class="publication_detail" id="Heng2019ICRA" name="Heng2019ICRA"><div class="publication_abstract"><b>Abstract:</b> Project AutoVision aims to develop localization and 3D scene perception capabilities for a self-driving vehicle. Such capabilities will enable autonomous navigation in urban and rural environments, in day and night, and with cameras as the only exteroceptive sensors. The sensor suite employs many cameras for both 360-degree coverage and accurate multi-view stereo; the use of low-cost cameras keeps the cost of this sensor suite to a minimum. In addition, the project seeks to extend the operating envelope to include GNSS-less conditions which are typical for environments with tall buildings, foliage, and tunnels. Emphasis is placed on leveraging multi-view geometry and deep learning to enable the vehicle to localize and perceive in 3D space. This paper presents an overview of the project, and describes the sensor suite and current progress in the areas of calibration, localization, and perception.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Heng2019ICRA.pdf" target="_blank">Heng2019ICRA</a>,<br/>
&nbsp; author = {<a href="http://www.lionel.work/" target="blank">Lionel Heng</a> and Benjamin Choi and Zhaopeng Cui and <a href="https://www.cvg.ethz.ch/people/researchStaff/" target="blank">Marcel Geppert</a> and Sixing Hu and Benson Kuan and <a href="http://people.inf.ethz.ch/liup/" target="blank">Peidong Liu</a> and Rang M. H. Nguyen and Ye Chuan Yeo and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="https://sites.google.com/site/gimheelee/" target="blank">Gim Hee Lee</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a>},<br/>
&nbsp; title = {Project AutoVision: Localization and 3D Scene Perception for an Autonomous Vehicle with a Multi-Camera System},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Heng2019ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Cui2019ICRA');"><div class="publication_image"><img src="publications/Cui2019ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Real-Time Dense Mapping for Self-Driving Vehicles using Fisheye Cameras </b><br/>Z. Cui, L. Heng, Y. Yeo, A. Geiger, M. Pollefeys and T. Sattler <br/>International Conference on Robotics and Automation (ICRA), 2019</div></div>
<div class="publication_detail" id="Cui2019ICRA" name="Cui2019ICRA"><div class="publication_abstract"><b>Abstract:</b> We present a real-time dense geometric mapping algorithm for large-scale environments. Unlike existing methods which use pinhole cameras, our implementation is based on fisheye cameras which have larger field of view and benefit some other tasks including Visual-Inertial Odometry, localization and object detection around vehicles. Our algorithm runs on in-vehicle PCs at 15 Hz approximately, enabling vision-only 3D scene perception for self-driving vehicles. For each synchronized set of images captured by multiple cameras, we first compute a depth map for a reference camera using plane-sweeping stereo. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a rather low resolution, we recover the depths using multiple image resolutions. We adopt the fast object detection framework YOLOv3 to remove potentially dynamic objects. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function (TSDF) volume to obtain a 3D map. We evaluate our method on large-scale urban datasets, and results show that our method works well even in complex environments.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Cui2019ICRA.pdf" target="_blank">Cui2019ICRA</a>,<br/>
&nbsp; author = {Zhaopeng Cui and <a href="http://www.lionel.work/" target="blank">Lionel Heng</a> and Ye Chuan Yeo and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a>},<br/>
&nbsp; title = {Real-Time Dense Mapping for Self-Driving Vehicles using Fisheye Cameras},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2019}<br/>
}
</div>
<div class="publication_links"><a href="publications/Cui2019ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Cui2019ICRA_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=ILQzmnQX7DM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://zhpcui.github.io/projects/arxiv18_densemapping/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2018</h2></div>
<div class="publication_container" onclick="toggleDetails('Sauer2018CORL');"><div class="publication_image"><img src="publications/Sauer2018CORL.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Conditional Affordance Learning for Driving in Urban Environments </b> <b><span style="color:#a51e37">(oral)</span></b><br/>A. Sauer, N. Savinov and A. Geiger <br/>Conference on Robot Learning (CoRL), 2018</div></div>
<div class="publication_detail" id="Sauer2018CORL" name="Sauer2018CORL"><div class="publication_abstract"><b>Abstract:</b> Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs. A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations. However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits. In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 \% in goal-directed navigation on the challenging CARLA simulation benchmark. In addition, our approach is the first to handle traffic lights, speed signs and smooth car-following, resulting in a significant reduction of traffic accidents.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Sauer2018CORL.pdf" target="_blank">Sauer2018CORL</a>,<br/>
&nbsp; author = {<a href="https://axelsauer.com/" target="blank">Axel Sauer</a> and <a href="http://people.inf.ethz.ch/nsavinov/" target="blank">Nikolay Savinov</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Conditional Affordance Learning for Driving in Urban Environments},<br/>&nbsp; booktitle = {Conference on Robot Learning (CoRL)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Sauer2018CORL.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Sauer2018CORL_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=UtUbpigMgr0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/xl-sr/CAL" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/><a href="https://autonomousvision.github.io/conditional-affordance-learning/" target="_blank"><div class="publication_icon"><img src="site/icon_blog.png"/></div>Blog</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Sevilla-Lara2018GCPR');"><div class="publication_image"><img src="publications/Sevilla-Lara2018GCPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>On the Integration of Optical Flow and Action Recognition </b> <b><span style="color:#a51e37">(oral)</span></b><br/>L. Sevilla-Lara, Y. Liao, F. Güney, V. Jampani, A. Geiger and M. Black <br/>German Conference on Pattern Recognition (GCPR), 2018</div></div>
<div class="publication_detail" id="Sevilla-Lara2018GCPR" name="Sevilla-Lara2018GCPR"><div class="publication_abstract"><b>Abstract:</b> Most of the top performing action recognition methods use optical flow as a black box input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Sevilla-Lara2018GCPR.pdf" target="_blank">Sevilla-Lara2018GCPR</a>,<br/>
&nbsp; author = {Laura Sevilla-Lara and <a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and Varun Jampani and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a>},<br/>
&nbsp; title = {On the Integration of Optical Flow and Action Recognition},<br/>&nbsp; booktitle = {German Conference on Pattern Recognition (GCPR)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Sevilla-Lara2018GCPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Stutz2018IJCV');"><div class="publication_image"><img src="publications/Stutz2018IJCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning 3D Shape Completion under Weak Supervision </b><br/>D. Stutz and A. Geiger <br/>International Journal of Computer Vision (IJCV), 2018</div></div>
<div class="publication_detail" id="Stutz2018IJCV" name="Stutz2018IJCV"><div class="publication_abstract"><b>Abstract:</b> We address the problem of 3D shape completion from sparse and noisy point clouds, a fundamental problem in computer vision and robotics. Recent approaches are either data-driven or learning-based: Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations; Learning-based approaches, in contrast, avoid the expensive optimization step by learning to directly predict complete shapes from incomplete observations in a fully-supervised setting. However, full supervision is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. On synthetic benchmarks based on ShapeNet and ModelNet as well as on real robotics data from KITTI and Kinect, we demonstrate that the proposed amortized maximum likelihood approach is able to compete with a fully supervised baseline and outperforms the data-driven approach of Engelmann et al., while requiring less supervision and being significantly faster.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Stutz2018IJCV.pdf" target="_blank">Stutz2018IJCV</a>,<br/>
&nbsp; author = {<a href="http://davidstutz.de/" target="blank">David Stutz</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning 3D Shape Completion under Weak Supervision},<br/>&nbsp; booktitle = {International Journal of Computer Vision (IJCV)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Stutz2018IJCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://avg.is.tue.mpg.de/research_projects/3d-shape-completion" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liu2018IROS');"><div class="publication_image"><img src="publications/Liu2018IROS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Towards Robust Visual Odometry with a Multi-Camera System </b><br/>P. Liu, M. Geppert, L. Heng, T. Sattler, A. Geiger and M. Pollefeys <br/>International Conference on Intelligent Robots and Systems (IROS), 2018</div></div>
<div class="publication_detail" id="Liu2018IROS" name="Liu2018IROS"><div class="publication_abstract"><b>Abstract:</b> We present a visual odometry (VO) algorithm for a multi-camera system and robust operation in challenging environments. Our algorithm consists of a pose tracker and a local mapper. The tracker estimates the current pose by minimizing photometric errors between the most recent keyframe and the current frame. The mapper initializes the depths of all sampled feature points using plane-sweeping stereo. To reduce pose drift, a sliding window optimizer is used to refine poses and structure jointly. Our formulation is flexible enough to support an arbitrary number of stereo cameras. We evaluate our algorithm thoroughly on five datasets. The datasets were captured in different conditions: daytime, night-time with near-infrared (NIR) illumination and night-time without NIR illumination. Experimental results show that a multi-camera setup makes the VO more robust to challenging environments, especially night-time conditions, in which a single stereo configuration fails easily due to the lack of features.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Liu2018IROS.pdf" target="_blank">Liu2018IROS</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/liup/" target="blank">Peidong Liu</a> and <a href="https://www.cvg.ethz.ch/people/researchStaff/" target="blank">Marcel Geppert</a> and <a href="http://www.lionel.work/" target="blank">Lionel Heng</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a>},<br/>
&nbsp; title = {Towards Robust Visual Odometry with a Multi-Camera System},<br/>&nbsp; booktitle = {International Conference on Intelligent Robots and Systems (IROS)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liu2018IROS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Alhaija2018ACCV');"><div class="publication_image"><img src="publications/Alhaija2018ACCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Geometric Image Synthesis </b><br/>H. Alhaija, S. Mustikovela, A. Geiger and C. Rother <br/>Asian Conference on Computer Vision (ACCV), 2018</div></div>
<div class="publication_detail" id="Alhaija2018ACCV" name="Alhaija2018ACCV"><div class="publication_abstract"><b>Abstract:</b> The task of generating natural images from 3D scenes has been a long standing goal in computer graphics. On the other hand, recent developments in deep neural networks allow for trainable models that can produce natural-looking images with little or no knowledge about the scene structure. While the generated images often consist of realistic looking local patterns, the overall structure of the generated images is often inconsistent. In this work we propose a trainable, geometry-aware image generation method that leverages various types of scene information, including geometry and segmentation, to create realistic looking natural images that match the desired scene structure. Our geometrically-consistent image synthesis method is a deep neural network, called Geometry to Image Synthesis (GIS) framework, which retains the advantages of a trainable method, e.g., differentiability and adaptiveness, but, at the same time, makes a step towards the generalizability, control and quality output of modern graphics rendering engines. We utilize the GIS framework to insert vehicles in outdoor driving scenes, as well as to generate novel views of objects from the Linemod dataset. We qualitatively show that our network is able to generalize beyond the training set to novel scene geometries, object shapes and segmentations. Furthermore, we quantitatively show that the GIS framework can be used to synthesize large amounts of training data which proves beneficial for training instance segmentation models.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Alhaija2018ACCV.pdf" target="_blank">Alhaija2018ACCV</a>,<br/>
&nbsp; author = {Hassan Alhaija and Siva Mustikovela and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Carsten Rother},<br/>
&nbsp; title = {Geometric Image Synthesis},<br/>&nbsp; booktitle = {Asian Conference on Computer Vision (ACCV)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Alhaija2018ACCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=W2tFCz9xJoU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Coors2018ECCV');"><div class="publication_image"><img src="publications/Coors2018ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>SphereNet: Learning Spherical Representations for Detection and Classification in Omnidirectional Images </b><br/>B. Coors, A. Condurache and A. Geiger <br/>European Conference on Computer Vision (ECCV), 2018</div></div>
<div class="publication_detail" id="Coors2018ECCV" name="Coors2018ECCV"><div class="publication_abstract"><b>Abstract:</b> Omnidirectional cameras offer great benefits over classical cameras wherever a wide field of view is essential, such as in virtual reality applications or in autonomous robots. Unfortunately, standard convolutional neural networks are not well suited for this scenario as the natural projection surface is a sphere which cannot be unwrapped to a plane without introducing significant distortions, particularly in the polar regions. In this work, we present SphereNet, a novel deep learning framework which encodes invariance against such distortions explicitly into convolutional neural networks. Towards this goal, SphereNet adapts the sampling locations of the convolutional filters, effectively reversing distortions, and wraps the filters around the sphere. By building on regular convolutions, SphereNet enables the transfer of existing perspective convolutional neural network models to the omnidirectional case. We demonstrate the effectiveness of our method on the tasks of image classification and object detection, exploiting two newly created semi-synthetic and real-world omnidirectional datasets.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Coors2018ECCV.pdf" target="_blank">Coors2018ECCV</a>,<br/>
&nbsp; author = {<a href="https://is.tuebingen.mpg.de/person/bcoors" target="blank">Benjamin Coors</a> and <a href="https://www.isip.uni-luebeck.de/people/alexandru-condurache.html" target="blank">Alexandru Paul Condurache</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {SphereNet: Learning Spherical Representations for Detection and Classification in Omnidirectional Images},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Coors2018ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Coors2018ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Cherabier2018ECCV');"><div class="publication_image"><img src="publications/Cherabier2018ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Priors for Semantic 3D Reconstruction </b><br/>I. Cherabier, J. Schönberger, M. Oswald, M. Pollefeys and A. Geiger <br/>European Conference on Computer Vision (ECCV), 2018</div></div>
<div class="publication_detail" id="Cherabier2018ECCV" name="Cherabier2018ECCV"><div class="publication_abstract"><b>Abstract:</b> We present a novel semantic 3D reconstruction framework which embeds variational regularization into a neural network. Our network performs a fixed number of unrolled multi-scale optimization iterations with shared interaction weights. In contrast to existing variational methods for semantic 3D reconstruction, our model is end-to-end trainable and captures more complex dependencies between the semantic labels and the 3D geometry. Compared to previous learning-based approaches to 3D reconstruction, we integrate powerful long-range dependencies using variational coarse-to-fine optimization. As a result, our network architecture requires only a moderate number of parameters while keeping a high level of expressiveness which enables learning from very little data. Experiments on real and synthetic datasets demonstrate that our network achieves higher accuracy compared to a purely variational approach while at the same time requiring two orders of magnitude less iterations to converge. Moreover, our approach handles ten times more semantic class labels using the same computational resources.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Cherabier2018ECCV.pdf" target="_blank">Cherabier2018ECCV</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/cian" target="blank">Ian Cherabier</a> and <a href="http://people.inf.ethz.ch/jschoenb/" target="blank">Johannes Schönberger</a> and <a href="http://people.inf.ethz.ch/moswald" target="blank">Martin Oswald</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Priors for Semantic 3D Reconstruction},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Cherabier2018ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Cherabier2018ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=AjIM0t3rdSc" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Janai2018ECCV');"><div class="publication_image"><img src="publications/Janai2018ECCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Unsupervised Learning of Multi-Frame Optical Flow with Occlusions </b><br/>J. Janai, F. Güney, A. Ranjan, M. Black and A. Geiger <br/>European Conference on Computer Vision (ECCV), 2018</div></div>
<div class="publication_detail" id="Janai2018ECCV" name="Janai2018ECCV"><div class="publication_abstract"><b>Abstract:</b> Learning optical flow with neural networks is hampered by the need for obtaining training data with associated ground truth. Unsupervised learning is a promising direction, yet the performance of current unsupervised methods is still limited. In particular, the lack of proper occlusion handling in commonly used data terms constitutes a major source of error. While most optical flow methods process pairs of consecutive frames, more advanced occlusion reasoning can be realized when considering multiple frames. In this paper, we propose a framework for unsupervised learning of optical flow and occlusions over multiple frames. More specifically, we exploit the minimal configuration of three frames to strengthen the photometric loss and explicitly reason about occlusions. We demonstrate that our multi-frame, occlusion-sensitive formulation outperforms existing unsupervised two-frame methods and even produces results on par with some fully supervised methods.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Janai2018ECCV.pdf" target="_blank">Janai2018ECCV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/jjanai" target="blank">Joel Janai</a> and <a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://ps.is.tuebingen.mpg.de/person/aranjan" target="blank">Anurag Ranjan</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Unsupervised Learning of Multi-Frame Optical Flow with Occlusions},<br/>&nbsp; booktitle = {European Conference on Computer Vision (ECCV)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Janai2018ECCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Janai2018ECCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=z7TZsAFDMMk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/JJanai/back2future" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Mescheder2018ICML');"><div class="publication_image"><img src="publications/Mescheder2018ICML.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Which Training Methods for GANs do actually Converge? </b> <b><span style="color:#a51e37">(oral)</span></b><br/>L. Mescheder, A. Geiger and S. Nowozin <br/>International Conference on Machine learning (ICML), 2018</div></div>
<div class="publication_detail" id="Mescheder2018ICML" name="Mescheder2018ICML"><div class="publication_abstract"><b>Abstract:</b> Recent work has shown local convergence of GAN training for absolutely continuous
data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Mescheder2018ICML.pdf" target="_blank">Mescheder2018ICML</a>,<br/>
&nbsp; author = {<a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.nowozin.net/sebastian/" target="blank">Sebastian Nowozin</a>},<br/>
&nbsp; title = {Which Training Methods for GANs do actually Converge?},<br/>&nbsp; booktitle = {International Conference on Machine learning (ICML)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Mescheder2018ICML.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Mescheder2018ICML_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Mescheder2018ICML_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Mescheder2018ICML_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://on-demand.gputechconf.com/gtc-eu/2018/video/e8401/" target="_blank"><div class="publication_icon"><img src="site/icon_talk.png"/></div>Talk</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schoenberger2018CVPR');"><div class="publication_image"><img src="publications/Schoenberger2018CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Semantic Visual Localization </b><br/>J. Schönberger, M. Pollefeys, A. Geiger and T. Sattler <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2018</div></div>
<div class="publication_detail" id="Schoenberger2018CVPR" name="Schoenberger2018CVPR"><div class="publication_abstract"><b>Abstract:</b> Robust visual localization under a wide range of viewing conditions is a fundamental problem in computer vision. Handling the difficult cases of this problem is not only very challenging but also of high practical relevance, eg, in the context of life-long localization for augmented reality or autonomous robots. In this paper, we propose a novel approach based on a joint 3D geometric and semantic understanding of the world, enabling it to succeed under conditions where previous approaches failed. Our method leverages a novel generative model for descriptor learning, trained on semantic scene completion as an auxiliary task. The resulting 3D descriptors are robust to missing observations by encoding high-level 3D geometric and semantic information. Experiments on several challenging large-scale localization datasets demonstrate reliable localization under extreme viewpoint, illumination, and geometry changes.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schoenberger2018CVPR.pdf" target="_blank">Schoenberger2018CVPR</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/jschoenb/" target="blank">Johannes Schönberger</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a>},<br/>
&nbsp; title = {Semantic Visual Localization},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schoenberger2018CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Schoenberger2018CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Schoenberger2018CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Stutz2018CVPR');"><div class="publication_image"><img src="publications/Stutz2018CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning 3D Shape Completion from Laser Scan Data with Weak Supervision </b><br/>D. Stutz and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2018</div></div>
<div class="publication_detail" id="Stutz2018CVPR" name="Stutz2018CVPR"><div class="publication_abstract"><b>Abstract:</b> 3D shape completion from partial point clouds is a fundamental problem in computer vision and computer graphics. Recent approaches can be characterized as either data-driven or learning-based. Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations. Learning-based approaches, in contrast, avoid the expensive optimization step and instead directly predict the complete shape from the incomplete observations using deep neural networks. However, full supervision is required which is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, ie, learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. Tackling 3D shape completion of cars on ShapeNet and KITTI, we demonstrate that the proposed amortized maximum likelihood approach is able to compete with a fully supervised baseline and a state-of-the-art data-driven approach while being significantly faster. On ModelNet, we additionally show that the approach is able to generalize to other object categories as well.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Stutz2018CVPR.pdf" target="_blank">Stutz2018CVPR</a>,<br/>
&nbsp; author = {<a href="http://davidstutz.de/" target="blank">David Stutz</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning 3D Shape Completion from Laser Scan Data with Weak Supervision},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Stutz2018CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Stutz2018CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Stutz2018CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="https://avg.is.tue.mpg.de/research_projects/3d-shape-completion" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liao2018CVPR');"><div class="publication_image"><img src="publications/Liao2018CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Deep Marching Cubes: Learning Explicit Surface Representations </b><br/>Y. Liao, S. Donne and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2018</div></div>
<div class="publication_detail" id="Liao2018CVPR" name="Liao2018CVPR"><div class="publication_abstract"><b>Abstract:</b> Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (eg, TSDF) from which 3D surface meshes must be extracted in a post-processing step (eg, via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Liao2018CVPR.pdf" target="_blank">Liao2018CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/yliao" target="blank">Yiyi Liao</a> and <a href="https://avg.is.tue.mpg.de/person/sdonne" target="blank">Simon Donne</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Deep Marching Cubes: Learning Explicit Surface Representations},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liao2018CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Liao2018CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Liao2018CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=vhrvl9qOSKM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://avg.is.tue.mpg.de/research_projects/deep-marching-cubes" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Paschalidou2018CVPR');"><div class="publication_image"><img src="publications/Paschalidou2018CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials </b> <b><span style="color:#a51e37">(spotlight)</span></b><br/>D. Paschalidou, A. Ulusoy, C. Schmitt, L. Gool and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2018</div></div>
<div class="publication_detail" id="Paschalidou2018CVPR" name="Paschalidou2018CVPR"><div class="publication_abstract"><b>Abstract:</b> In this paper, we consider the problem of reconstructing a dense 3D model using images captured from different views. Recent methods based on convolutional neural networks (CNN) allow learning the entire task from data. However, they do not incorporate the physics of image formation such as perspective geometry and occlusion. Instead, classical approaches based on Markov Random Fields (MRF) with ray-potentials explicitly model these physical processes, but they cannot cope with large surface appearance variations across different viewpoints. In this paper, we propose RayNet, which combines the strengths of both frameworks. RayNet integrates a CNN that learns view-invariant feature representations with an MRF that explicitly encodes the physics of perspective projection and occlusion. We train RayNet end-to-end using empirical risk minimization. We thoroughly evaluate our approach on challenging real-world datasets and demonstrate its benefits over a piece-wise trained baseline, hand-crafted models as well as other learning-based approaches.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Paschalidou2018CVPR.pdf" target="_blank">Paschalidou2018CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/dpaschalidou" target="blank">Despoina Paschalidou</a> and <a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="https://avg.is.tue.mpg.de/person/cschmitt" target="blank">Carolin Schmitt</a> and <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" target="blank">Luc Gool</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Paschalidou2018CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Paschalidou2018CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Paschalidou2018CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=ebLuwu5kiGQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://avg.is.tue.mpg.de/research_projects/raynet" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Barsan2018ICRA');"><div class="publication_image"><img src="publications/Barsan2018ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Robust Dense Mapping for Large-Scale Dynamic Environments </b><br/>I. Barsan, P. Liu, M. Pollefeys and A. Geiger <br/>International Conference on Robotics and Automation (ICRA), 2018</div></div>
<div class="publication_detail" id="Barsan2018ICRA" name="Barsan2018ICRA"><div class="publication_abstract"><b>Abstract:</b> We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Barsan2018ICRA.pdf" target="_blank">Barsan2018ICRA</a>,<br/>
&nbsp; author = {<a href="http://www.cs.toronto.edu/~iab/" target="blank">Ioan Andrei Barsan</a> and <a href="http://people.inf.ethz.ch/liup/" target="blank">Peidong Liu</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Robust Dense Mapping for Large-Scale Dynamic Environments},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Barsan2018ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=jVK6UKMo-60" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://andreibarsan.github.io/dynslam" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Menze2018JPRS');"><div class="publication_image"><img src="publications/Menze2018JPRS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Object Scene Flow </b><br/>M. Menze, C. Heipke and A. Geiger <br/>ISPRS Journal of Photogrammetry and Remote Sensing (JPRS), 2018</div></div>
<div class="publication_detail" id="Menze2018JPRS" name="Menze2018JPRS"><div class="publication_abstract"><b>Abstract:</b> This work investigates the estimation of dense three-dimensional motion fields, commonly referred to as scene flow. While great progress has been made in recent years, large displacements and adverse imaging conditions as observed in natural outdoor environments are still very challenging for current approaches to reconstruction and motion estimation. In this paper, we propose a unified random field model which reasons jointly about 3D scene flow as well as the location, shape and motion of vehicles in the observed scene. We formulate the problem as the task of decomposing the scene into a small number of rigidly moving objects sharing the same motion parameters. Thus, our formulation effectively introduces long-range spatial dependencies which commonly employed local rigidity priors are lacking. Our inference algorithm then estimates the association of image segments and object hypotheses together with their three-dimensional shape and motion. We demonstrate the potential of the proposed approach by introducing a novel challenging scene flow benchmark which allows for a thorough comparison of the proposed scene flow approach with respect to various baseline models. In contrast to previous benchmarks, our evaluation is the first to provide stereo and optical flow ground truth for dynamic real-world urban scenes at large scale. Our experiments reveal that rigid motion segmentation can be utilized as an effective regularizer for the scene flow problem, improving upon existing two-frame scene flow methods. At the same time, our method yields plausible object segmentations without requiring an explicitly trained recognition model for a specific object class.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Menze2018JPRS.pdf" target="_blank">Menze2018JPRS</a>,<br/>
&nbsp; author = {<a href="http://www.ipi.uni-hannover.de/tmm.html" target="blank">Moritz Menze</a> and <a href="http://www.ipi.uni-hannover.de/chei.html" target="blank">Christian Heipke</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Object Scene Flow},<br/>&nbsp; journal = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Menze2018JPRS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=jCKHyhdI31w" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=DBJmR6hx2UE" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/projects/objectsceneflow" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Coors2018VISAPP');"><div class="publication_image"><img src="publications/Coors2018VISAPP.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Learning Transformation Invariant Representations with Weak Supervision </b><br/>B. Coors, A. Condurache, A. Mertins and A. Geiger <br/>International Conference on Computer Vision Theory and Applications (VISAPP), 2018</div></div>
<div class="publication_detail" id="Coors2018VISAPP" name="Coors2018VISAPP"><div class="publication_abstract"><b>Abstract:</b> Deep convolutional neural networks are the current state-of-the-art solution to many computer vision tasks. However, their ability to handle large global and local image transformations is limited. Consequently, extensive data augmentation is often utilized to incorporate prior knowledge about desired invariances to geometric transformations such as rotations or scale changes. In this work, we combine data augmentation with an unsupervised loss which enforces similarity between the predictions of augmented copies of an input sample. Our loss acts as an effective regularizer which facilitates the learning of transformation invariant representations. We investigate the effectiveness of the proposed similarity loss on rotated MNIST and the German Traffic Sign Recognition Benchmark (GTSRB) in the context of different classification models including ladder networks. Our experiments demonstrate improvements with respect to the standard data augmentation approach for supervised and semi-supervised learning tasks, in particular in the presence of little annotated data. In addition, we analyze the performance of the proposed approach with respect to its hyperparameters, including the strength of the regularization as well as the layer where representation similarity is enforced.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Coors2018VISAPP.pdf" target="_blank">Coors2018VISAPP</a>,<br/>
&nbsp; author = {<a href="https://is.tuebingen.mpg.de/person/bcoors" target="blank">Benjamin Coors</a> and <a href="https://www.isip.uni-luebeck.de/people/alexandru-condurache.html" target="blank">Alexandru Condurache</a> and <a href="https://www.isip.uni-luebeck.de/people/alfred-mertins.html" target="blank">Alfred Mertins</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Learning Transformation Invariant Representations with Weak Supervision},<br/>&nbsp; booktitle = {International Conference on Computer Vision Theory and Applications (VISAPP)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Coors2018VISAPP.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Alhaija2018IJCV');"><div class="publication_image"><img src="publications/Alhaija2018IJCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes </b><br/>H. Alhaija, S. Mustikovela, L. Mescheder, A. Geiger and C. Rother <br/>International Journal of Computer Vision (IJCV), 2018</div></div>
<div class="publication_detail" id="Alhaija2018IJCV" name="Alhaija2018IJCV"><div class="publication_abstract"><b>Abstract:</b> The success of deep learning in computer vision is based on the availability of large annotated datasets. To lower the need for hand labeled images, virtually rendered 3D worlds have recently gained popularity. Unfortunately, creating realistic 3D content is challenging on its own and requires significant human effort. In this work, we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation and object detection models. Exploiting the fact that not all aspects of the scene are equally important for this task, we propose to augment real-world imagery with virtual objects of the target category. Capturing real-world images at large scale is easy and cheap, and directly provides real background appearances without the need for creating complex 3D models of the environment. We present an efficient procedure to augment these images with virtual objects. In contrast to modeling complete 3D environments, our data augmentation approach requires only a few user interactions in combination with 3D models of the target object category. Leveraging our approach, we introduce a novel dataset of augmented urban driving scenes with 360 degree images that are used as environment maps to create realistic lighting and reflections on rendered objects. We analyze the significance of realistic object placement by comparing manual placement by humans to automatic methods based on semantic scene analysis. This allows us to create composite images which exhibit both realistic background appearance as well as a large number of complex object arrangements. Through an extensive set of experiments, we conclude the right set of parameters to produce augmented data which can maximally enhance the performance of instance segmentation models. Further, we demonstrate the utility of the proposed approach on training standard deep models for semantic instance segmentation and object detection of cars in outdoor driving scenarios. We test the models trained on our augmented data on the KITTI 2015 dataset, which we have annotated with pixel-accurate ground truth, and on the Cityscapes dataset. Our experiments demonstrate that the models trained on augmented imagery generalize better than those trained on fully synthetic data or models trained on limited amounts of annotated real data.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Alhaija2018IJCV.pdf" target="_blank">Alhaija2018IJCV</a>,<br/>
&nbsp; author = {Hassan Alhaija and Siva Mustikovela and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Carsten Rother},<br/>
&nbsp; title = {Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes},<br/>&nbsp; journal = {International Journal of Computer Vision (IJCV)},<br/>
&nbsp; year = {2018}<br/>
}
</div>
<div class="publication_links"><a href="publications/Alhaija2018IJCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2017</h2></div>
<div class="publication_container" onclick="toggleDetails('Mescheder2017NIPS');"><div class="publication_image"><img src="publications/Mescheder2017NIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>The Numerics of GANs </b> <b><span style="color:#a51e37">(spotlight)</span></b><br/>L. Mescheder, S. Nowozin and A. Geiger <br/>Advances in Neural Information Processing Systems (NIPS), 2017</div></div>
<div class="publication_detail" id="Mescheder2017NIPS" name="Mescheder2017NIPS"><div class="publication_abstract"><b>Abstract:</b> In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Mescheder2017NIPS.pdf" target="_blank">Mescheder2017NIPS</a>,<br/>
&nbsp; author = {<a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="http://www.nowozin.net/sebastian/" target="blank">Sebastian Nowozin</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {The Numerics of GANs},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NIPS)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Mescheder2017NIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Mescheder2017NIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Behl2017ICCV');"><div class="publication_image"><img src="publications/Behl2017ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Bounding Boxes, Segmentations and Object Coordinates: How Important is Recognition for 3D Scene Flow Estimation  ...</b><br/>A. Behl, O. Jafari, S. Mustikovela, H. Alhaija, C. Rother and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2017</div></div>
<div class="publication_detail" id="Behl2017ICCV" name="Behl2017ICCV"><div class="publication_abstract"><b>Abstract:</b> Existing methods for 3D scene flow estimation often fail in the presence of large displacement or local ambiguities, e.g., at texture-less or reflective surfaces. However, these challenges are omnipresent in dynamic road scenes, which is the focus of this work. Our main contribution is to overcome these 3D motion estimation problems by exploiting recognition. In particular, we investigate the importance of recognition granularity, from coarse 2D bounding box estimates over 2D instance segmentations to fine-grained 3D object part predictions. We compute these cues using CNNs trained on a newly annotated dataset of stereo images and integrate them into a CRF-based model for robust 3D scene flow estimation - an approach we term Instance Scene Flow. We analyze the importance of each recognition cue in an ablation study and observe that the instance segmentation cue is by far strongest, in our setting. We demonstrate the effectiveness of our method on the challenging KITTI 2015 scene flow benchmark where we achieve state-of-the-art performance at the time of submission.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Behl2017ICCV.pdf" target="_blank">Behl2017ICCV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and Omid Hosseini Jafari and Siva Karthik Mustikovela and Hassan Abu Alhaija and Carsten Rother and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Bounding Boxes, Segmentations and Object Coordinates: How Important is Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios?},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Behl2017ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Behl2017ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Behl2017ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Uhrig2017THREEDV');"><div class="publication_image"><img src="publications/Uhrig2017THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Sparsity Invariant CNNs </b> <b><span style="color:#a51e37">(oral, best student paper award)</span></b><br/>J. Uhrig, N. Schneider, L. Schneider, U. Franke, T. Brox and A. Geiger <br/>International Conference on 3D Vision (3DV), 2017</div></div>
<div class="publication_detail" id="Uhrig2017THREEDV" name="Uhrig2017THREEDV"><div class="publication_abstract"><b>Abstract:</b> In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth upsampling from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the benefits of the proposed network architecture in synthetic and real experiments \wrt various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising 93k depth annotated RGB images. Our dataset allows for training and evaluating depth upsampling and depth prediction techniques in challenging real-world settings.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Uhrig2017THREEDV.pdf" target="_blank">Uhrig2017THREEDV</a>,<br/>
&nbsp; author = {<a href="https://lmb.informatik.uni-freiburg.de/people/uhrigj/index.en.html" target="blank">Jonas Uhrig</a> and <a href="http://nick-schneider.me/" target="blank">Nick Schneider</a> and Lukas Schneider and <a href="http://www.uwe-franke.de/" target="blank">Uwe Franke</a> and <a href="https://lmb.informatik.uni-freiburg.de/people/brox/index.en.html" target="blank">Thomas Brox</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Sparsity Invariant CNNs},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Uhrig2017THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Uhrig2017THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Riegler2017THREEDV');"><div class="publication_image"><img src="publications/Riegler2017THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>OctNetFusion: Learning Depth Fusion from Data </b> <b><span style="color:#a51e37">(oral)</span></b><br/>G. Riegler, A. Ulusoy, H. Bischof and A. Geiger <br/>International Conference on 3D Vision (3DV), 2017</div></div>
<div class="publication_detail" id="Riegler2017THREEDV" name="Riegler2017THREEDV"><div class="publication_abstract"><b>Abstract:</b> In this paper, we present a learning based approach to depth fusion, i.e., dense 3D reconstruction from multiple depth images. The most common approach to depth fusion is based on averaging truncated signed distance functions, which was originally proposed by Curless and Levoy in 1996. While this method is simple and provides great results, it is not able to reconstruct (partially) occluded surfaces and requires a large number frames to filter out sensor noise and outliers. Motivated by the availability of large 3D model repositories and recent advances in deep learning, we present a novel 3D CNN architecture that learns to predict an implicit surface representation from the input depth maps. Our learning based method significantly outperforms the traditional volumetric fusion approach in terms of noise reduction and outlier suppression. By learning the structure of real world 3D objects and scenes, our approach is further able to reconstruct occluded regions and to fill in gaps in the reconstruction. We demonstrate that our learning based approach outperforms both vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric fusion. Further, we demonstrate state-of-the-art 3D shape completion results.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Riegler2017THREEDV.pdf" target="_blank">Riegler2017THREEDV</a>,<br/>
&nbsp; author = {<a href="https://rvlab.icg.tugraz.at/personal_page/personal_page_gernot.html" target="blank">Gernot Riegler</a> and <a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="https://www.tugraz.at/institute/icg/research/team-bischof/people/team-about/horst-bischof/" target="blank">Horst Bischof</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {OctNetFusion: Learning Depth Fusion from Data},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Riegler2017THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.cvlibs.net/videos/gtc_17_talk.mp4" target="_blank"><div class="publication_icon"><img src="site/icon_talk.png"/></div>Talk</a><br/><a href="http://www.youtube.com/watch?v=NM0_wJHWnTk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=GrFDKtUm6ek" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="https://github.com/griegler/octnetfusion" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Alhaija2017BMVC');"><div class="publication_image"><img src="publications/Alhaija2017BMVC.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Augmented Reality Meets Deep Learning for Car Instance Segmentation in Urban Scenes </b><br/>H. Alhaija, S. Mustikovela, L. Mescheder, A. Geiger and C. Rother <br/>British Machine Vision Conference (BMVC), 2017</div></div>
<div class="publication_detail" id="Alhaija2017BMVC" name="Alhaija2017BMVC"><div class="publication_abstract"><b>Abstract:</b> The success of deep learning in computer vision is based on the availability of large annotated datasets. To lower the need for hand labeled images, virtually rendered 3D worlds have recently gained popularity. Unfortunately, creating realistic 3D content is challenging on its own and requires significant human effort. In this work, we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation models. Exploiting the fact that not all aspects of the scene are equally important for this task, we propose to augment real-world imagery with virtual objects of the target category. Capturing real-world images at large scale is easy and cheap, and directly provides real background appearances without the need for creating complex 3D models of the environment. We present an efficient procedure to augment these images with virtual objects. This allows us to create realistic composite images which exhibit both realistic background appearance as well as a large number of complex object arrangements. In contrast to modeling complete 3D environments, our data augmentation approach requires only a few user interactions in combination with 3D shapes of the target object category. We demonstrate the utility of the proposed approach for training a state-of-the-art high-capacity deep model for semantic instance segmentation. In particular, we consider the task of segmenting car instances on the KITTI dataset which we have annotated with pixel-accurate ground truth. Our experiments demonstrate that models trained on augmented imagery generalize better than those trained on synthetic data or models trained on limited amounts of annotated real data.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Alhaija2017BMVC.pdf" target="_blank">Alhaija2017BMVC</a>,<br/>
&nbsp; author = {Hassan Abu Alhaija and Siva Karthik Mustikovela and <a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Carsten Rother},<br/>
&nbsp; title = {Augmented Reality Meets Deep Learning for Car Instance Segmentation in Urban Scenes},<br/>&nbsp; booktitle = {British Machine Vision Conference (BMVC)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Alhaija2017BMVC.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://arxiv.org/abs/1708.01566" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Liu2017IROS');"><div class="publication_image"><img src="publications/Liu2017IROS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Direct Visual Odometry for a Fisheye-Stereo Camera </b><br/>P. Liu, L. Heng, T. Sattler, A. Geiger and M. Pollefeys <br/>International Conference on Intelligent Robots and Systems (IROS), 2017</div></div>
<div class="publication_detail" id="Liu2017IROS" name="Liu2017IROS"><div class="publication_abstract"><b>Abstract:</b> We present a direct visual odometry algorithm for a fisheye-stereo camera. Our algorithm performs simultaneous camera motion estimation and semi-dense reconstruction. The pipeline consists of two threads: a tracking thread and a mapping thread. In the tracking thread, we estimate the camera pose via semi-dense direct image alignment. To have a wider field of view (FoV) which is important for robotic perception, we use fisheye images directly without converting them to conventional pinhole images which come with a limited FoV. To address the epipolar curve problem, plane-sweeping stereo is used for stereo matching and depth initialization. Multiple depth hypotheses are tracked for selected pixels to better capture the uncertainty characteristics of stereo matching. Temporal motion stereo is then used to refine the depth and remove false positive depth hypotheses. Our implementation runs at an average of 20 Hz on a low-end PC. We run experiments in outdoor environments to validate our algorithm, and discuss the experimental results. We experimentally show that we are able to estimate 6D poses with low drift, and at the same time, do semi-dense 3D reconstruction with high accuracy.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Liu2017IROS.pdf" target="_blank">Liu2017IROS</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/liup/" target="blank">Peidong Liu</a> and <a href="http://www.lionel.work/" target="blank">Lionel Heng</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a>},<br/>
&nbsp; title = {Direct Visual Odometry for a Fisheye-Stereo Camera},<br/>&nbsp; booktitle = {International Conference on Intelligent Robots and Systems (IROS)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Liu2017IROS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Mescheder2017ICML');"><div class="publication_image"><img src="publications/Mescheder2017ICML.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks </b><br/>L. Mescheder, S. Nowozin and A. Geiger <br/>International Conference on Machine learning (ICML), 2017</div></div>
<div class="publication_detail" id="Mescheder2017ICML" name="Mescheder2017ICML"><div class="publication_abstract"><b>Abstract:</b> Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Mescheder2017ICML.pdf" target="_blank">Mescheder2017ICML</a>,<br/>
&nbsp; author = {<a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="http://www.nowozin.net/sebastian/" target="blank">Sebastian Nowozin</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks},<br/>&nbsp; booktitle = {International Conference on Machine learning (ICML)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Mescheder2017ICML.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Mescheder2017ICML_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="https://github.com/LMescheder/AdversarialVariationalBayes" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Camposeco2017CVPR');"><div class="publication_image"><img src="publications/Camposeco2017CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Toroidal Constraints for Two Point Localization Under High Outlier Ratios </b><br/>F. Camposeco, T. Sattler, A. Cohen, A. Geiger and M. Pollefeys <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2017</div></div>
<div class="publication_detail" id="Camposeco2017CVPR" name="Camposeco2017CVPR"><div class="publication_abstract"><b>Abstract:</b> Localizing a query image against a 3D model at large scale is a hard problem, since 2D-3D matches become more and more ambiguous as the model size increases. This creates a need for pose estimation strategies that can handle very low inlier ratios. In this paper, we draw new insights on the geometric information available from the 2D-3D matching process. As modern descriptors are not invariant against large variations in viewpoint, we are able to find the rays in space used to triangulate a given point that are closest to a query descriptor. It is well known that two correspondences constrain the camera to lie on the surface of a torus. Adding the knowledge of direction of triangulation, we are able to approximate the position of the camera from \emphtwo matches alone. We derive a geometric solver that can compute this position in under 1 microsecond. Using this solver, we propose a simple yet powerful outlier filter which scales quadratically in the number of matches. We validate the accuracy of our solver and demonstrate the usefulness of our method in real world settings.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Camposeco2017CVPR.pdf" target="_blank">Camposeco2017CVPR</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/fcampose/" target="blank">Federico Camposeco</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a> and <a href="http://people.inf.ethz.ch/acohen/" target="blank">Andrea Cohen</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a>},<br/>
&nbsp; title = {Toroidal Constraints for Two Point Localization Under High Outlier Ratios},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Camposeco2017CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Camposeco2017CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.cvg.ethz.ch/research/toroidal-constraints/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schoeps2017CVPR');"><div class="publication_image"><img src="publications/Schoeps2017CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>A Multi-View Stereo Benchmark with High-Resolution Images and Multi-Camera Videos </b><br/>T. Schöps, J. Schönberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2017</div></div>
<div class="publication_detail" id="Schoeps2017CVPR" name="Schoeps2017CVPR"><div class="publication_abstract"><b>Abstract:</b> Motivated by the limitations of existing multi-view stereo benchmarks, we present a novel dataset for this task. Towards this goal, we recorded a variety of indoor and outdoor scenes using a high-precision laser scanner and captured both high-resolution DSLR imagery as well as synchronized low-resolution stereo videos with varying fields-of-view. To align the images with the laser scans, we propose a robust technique which minimizes photometric errors conditioned on the geometry. In contrast to previous datasets, our benchmark provides novel challenges and covers a diverse set of viewpoints and scene types, ranging from natural scenes to man-made indoor and outdoor environments. Furthermore, we provide data at significantly higher temporal and spatial resolution. Our benchmark is the first to cover the important use case of hand-held mobile devices while also providing high-resolution DSLR camera images. We make our datasets and an online evaluation server available at http://www.eth3d.net.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schoeps2017CVPR.pdf" target="_blank">Schoeps2017CVPR</a>,<br/>
&nbsp; author = {<a href="http://people.inf.ethz.ch/schoepst/" target="blank">Thomas Schöps</a> and <a href="http://people.inf.ethz.ch/jschoenb/" target="blank">Johannes Schönberger</a> and <a href="https://www1.ethz.ch/igp/photogrammetry/people/galliani" target="blank">Silvano Galliani</a> and <a href="http://people.inf.ethz.ch/sattlert/" target="blank">Torsten Sattler</a> and <a href="https://www1.ethz.ch/igp/photogrammetry/people/Schindler" target="blank">Konrad Schindler</a> and <a href="http://people.inf.ethz.ch/pomarc/" target="blank">Marc Pollefeys</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {A Multi-View Stereo Benchmark with High-Resolution Images and Multi-Camera Videos},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schoeps2017CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Schoeps2017CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=Ni9tw2RfDAw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.eth3d.net" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Ulusoy2017CVPR');"><div class="publication_image"><img src="publications/Ulusoy2017CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Semantic Multi-view Stereo: Jointly Estimating Objects and Voxels </b><br/>A. Ulusoy, M. Black and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2017</div></div>
<div class="publication_detail" id="Ulusoy2017CVPR" name="Ulusoy2017CVPR"><div class="publication_abstract"><b>Abstract:</b> Dense 3D reconstruction from RGB images is a highly ill-posed problem due to occlusions, textureless or reflective surfaces, as well as other challenges. We propose object-level shape priors to address these ambiguities. Towards this goal, we formulate a probabilistic model that integrates multi-view image evidence with 3D shape information from multiple objects. Inference in this model yields a dense 3D reconstruction of the scene as well as the existence and precise 3D pose of the objects in it. Our approach is able to recover fine details not captured in the input shapes while defaulting to the input models in occluded regions where image evidence is weak. Due to its probabilistic nature, the approach is able to cope with the approximate geometry of the 3D models as well as input shapes that are not present in the scene. We evaluate the approach quantitatively on several challenging indoor and outdoor datasets.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Ulusoy2017CVPR.pdf" target="_blank">Ulusoy2017CVPR</a>,<br/>
&nbsp; author = {<a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Semantic Multi-view Stereo: Jointly Estimating Objects and Voxels},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Ulusoy2017CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Ulusoy2017CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=FiR02QygYo0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://ps.is.tue.mpg.de/project/Volumetric_Reconstruction" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Janai2017CVPR');"><div class="publication_image"><img src="publications/Janai2017CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data </b> <b><span style="color:#a51e37">(oral)</span></b><br/>J. Janai, F. Güney, J. Wulff, M. Black and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2017</div></div>
<div class="publication_detail" id="Janai2017CVPR" name="Janai2017CVPR"><div class="publication_abstract"><b>Abstract:</b> Existing optical flow datasets are limited in size and variability due to the difficulty of capturing dense ground truth. In this paper, we tackle this problem by tracking pixels through densely sampled space-time volumes recorded with a high-speed video camera. Our model exploits the linearity of small motions and reasons about occlusions from multiple frames. Using our technique, we are able to establish accurate reference flow fields outside the laboratory in natural environments. Besides, we show how our predictions can be used to augment the input images with realistic motion blur. We demonstrate the quality of the produced flow fields on synthetic and real-world datasets. Finally, we collect a novel challenging optical flow dataset by applying our technique on data from a high-speed camera and analyze the performance of the state-of-the-art in optical flow under various levels of motion blur.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Janai2017CVPR.pdf" target="_blank">Janai2017CVPR</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/jjanai" target="blank">Joel Janai</a> and <a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://ps.is.tuebingen.mpg.de/person/jwulff" target="blank">Jonas Wulff</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Janai2017CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Janai2017CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=q_6JVM39hNw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cvlibs.net/projects/slow_flow" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Riegler2017CVPR');"><div class="publication_image"><img src="publications/Riegler2017CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>OctNet: Learning Deep 3D Representations at High Resolutions </b> <b><span style="color:#a51e37">(oral)</span></b><br/>G. Riegler, A. Ulusoy and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2017</div></div>
<div class="publication_detail" id="Riegler2017CVPR" name="Riegler2017CVPR"><div class="publication_abstract"><b>Abstract:</b> We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Riegler2017CVPR.pdf" target="_blank">Riegler2017CVPR</a>,<br/>
&nbsp; author = {<a href="https://rvlab.icg.tugraz.at/personal_page/personal_page_gernot.html" target="blank">Gernot Riegler</a> and <a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {OctNet: Learning Deep 3D Representations at High Resolutions},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Riegler2017CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Riegler2017CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.cvlibs.net/videos/gtc_17_talk.mp4" target="_blank"><div class="publication_icon"><img src="site/icon_talk.png"/></div>Talk</a><br/><a href="http://www.youtube.com/watch?v=qYyephF2BBw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="https://github.com/griegler/octnet" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Janai2017ARXIV');"><div class="publication_image"><img src="publications/Janai2017ARXIV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art </b><br/>J. Janai, F. Güney, A. Behl and A. Geiger <br/>Arxiv, 2017</div></div>
<div class="publication_detail" id="Janai2017ARXIV" name="Janai2017ARXIV"><div class="publication_abstract"><b>Abstract:</b> Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Janai2017ARXIV.pdf" target="_blank">Janai2017ARXIV</a>,<br/>
&nbsp; author = {<a href="https://avg.is.tue.mpg.de/person/jjanai" target="blank">Joel Janai</a> and <a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://avg.is.tue.mpg.de/person/abehl" target="blank">Aseem Behl</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art},<br/>&nbsp; journal = {Arxiv},<br/>
&nbsp; year = {2017}<br/>
}
</div>
<div class="publication_links"><a href="publications/Janai2017ARXIV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://arxiv.org/abs/1704.05519" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/><a href="http://www.cvlibs.net/projects/autonomous_vision_survey/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2016</h2></div>
<div class="publication_container" onclick="toggleDetails('Brubaker2016PAMI');"><div class="publication_image"><img src="publications/Brubaker2016PAMI.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Map-Based Probabilistic Visual Self-Localization </b><br/>M. Brubaker, A. Geiger and R. Urtasun <br/>Pattern Analysis and Machine Intelligence (PAMI), 2016</div></div>
<div class="publication_detail" id="Brubaker2016PAMI" name="Brubaker2016PAMI"><div class="publication_abstract"><b>Abstract:</b> Accurate and efficient self-localization is a critical problem for autonomous systems. This paper describes an affordable solution to vehicle self-localization which uses odometry computed from two video cameras and road maps as the sole inputs. The core of the method is a probabilistic model for which an efficient approximate inference algorithm is derived. The inference algorithm is able to utilize distributed computation in order to meet the real-time requirements of autonomous systems in some instances. Because of the probabilistic nature of the model the method is capable of coping with various sources of uncertainty including noise in the visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, the proposed method is able to localize a vehicle to 4m on average after 52 seconds of driving on maps which contain more than 2,150km of drivable roads.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Brubaker2016PAMI.pdf" target="_blank">Brubaker2016PAMI</a>,<br/>
&nbsp; author = {<a href="http://www.cs.toronto.edu/~mbrubake" target="blank">Marcus A. Brubaker</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Map-Based Probabilistic Visual Self-Localization},<br/>&nbsp; journal = {Pattern Analysis and Machine Intelligence (PAMI)},<br/>
&nbsp; year = {2016}<br/>
}
</div>
<div class="publication_links"><a href="publications/Brubaker2016PAMI.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=rAuTgsiM2-8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cs.toronto.edu/~mbrubake/projects/map" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Mescheder2016ARXIV');"><div class="publication_image"><img src="publications/Mescheder2016ARXIV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Probabilistic Duality for Parallel Gibbs Sampling without Graph Coloring </b><br/>L. Mescheder, S. Nowozin and A. Geiger <br/>Arxiv, 2016</div></div>
<div class="publication_detail" id="Mescheder2016ARXIV" name="Mescheder2016ARXIV"><div class="publication_abstract"><b>Abstract:</b> We present a new notion of probabilistic duality for random variables involving mixture distributions. Using this notion, we show how to implement a highly-parallelizable Gibbs sampler for weakly coupled discrete pairwise graphical models with strictly positive factors that requires almost no preprocessing and is easy to implement. Moreover, we show how our method can be combined with blocking to improve mixing. Even though our method leads to inferior mixing times compared to a sequential Gibbs sampler, we argue that our method is still very useful for large dynamic networks, where factors are added and removed on a continuous basis, as it is hard to maintain a graph coloring in this setup. Similarly, our method is useful for parallelizing Gibbs sampling in graphical models that do not allow for graph colorings with a small number of colors such as densely connected graphs.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Mescheder2016ARXIV.pdf" target="_blank">Mescheder2016ARXIV</a>,<br/>
&nbsp; author = {<a href="http://avg.is.tuebingen.mpg.de/person/lmescheder" target="blank">Lars Mescheder</a> and <a href="http://www.nowozin.net/sebastian/" target="blank">Sebastian Nowozin</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Probabilistic Duality for Parallel Gibbs Sampling without Graph Coloring},<br/>&nbsp; journal = {Arxiv},<br/>
&nbsp; year = {2016}<br/>
}
</div>
<div class="publication_links"><a href="publications/Mescheder2016ARXIV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="https://arxiv.org/abs/1611.06684" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Arxiv</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Guney2016ACCV');"><div class="publication_image"><img src="publications/Guney2016ACCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Deep Discrete Flow </b><br/>F. Güney and A. Geiger <br/>Asian Conference on Computer Vision (ACCV), 2016</div></div>
<div class="publication_detail" id="Guney2016ACCV" name="Guney2016ACCV"><div class="publication_abstract"><b>Abstract:</b> Motivated by the success of deep learning techniques in matching problems, we present a method for learning context-aware features for solving optical flow using discrete optimization. Towards this goal, we present an efficient way of training a context network with a large receptive field size on top of a local network using dilated convolutions on patches. We perform feature matching by comparing each pixel in the reference image to every pixel in the target image, utilizing fast GPU matrix multiplication. The matching cost volume from the network's output forms the data term for discrete MAP inference in a pairwise Markov random field. We provide an extensive empirical investigation of network architectures and model parameters. At the time of submission, our method ranks second on the challenging MPI Sintel test set.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Guney2016ACCV.pdf" target="_blank">Guney2016ACCV</a>,<br/>
&nbsp; author = {<a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Deep Discrete Flow},<br/>&nbsp; booktitle = {Asian Conference on Computer Vision (ACCV)},<br/>
&nbsp; year = {2016}<br/>
}
</div>
<div class="publication_links"><a href="publications/Guney2016ACCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Guney2016ACCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Guney2016ACCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Xie2016CVPR');"><div class="publication_image"><img src="publications/Xie2016CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer </b><br/>J. Xie, M. Kiefel, M. Sun and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2016</div></div>
<div class="publication_detail" id="Xie2016CVPR" name="Xie2016CVPR"><div class="publication_abstract"><b>Abstract:</b> Semantic annotations are vital for training models for object recognition, semantic segmentation or scene understanding. Unfortunately, pixelwise annotation of images at very large scale is labor-intensive and only little labeled data is available, particularly at instance level and for street scenes. In this paper, we propose to tackle this problem by lifting the semantic instance labeling task from 2D into 3D. Given reconstructions from stereo or laser data, we annotate static 3D scene elements with rough bounding primitives and develop a probabilistic model which transfers this information into the image domain. We leverage our method to obtain 2D labels for a novel suburban video dataset which we have collected, resulting in 400k semantic and instance image annotations. A comparison of our method to state-of-the-art label transfer baselines reveals that 3D information enables more efficient annotation while at the same time resulting in improved accuracy and time-coherent labels.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Xie2016CVPR.pdf" target="_blank">Xie2016CVPR</a>,<br/>
&nbsp; author = {<a href="http://www.clairexie.org/" target="blank">Jun Xie</a> and <a href="https://ps.is.tuebingen.mpg.de/person/mkiefel" target="blank">Martin Kiefel</a> and <a href="http://www.ee.washington.edu/faculty/sun/" target="blank">Ming-Ting Sun</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2016}<br/>
}
</div>
<div class="publication_links"><a href="publications/Xie2016CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Xie2016CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.cvlibs.net/datasets/kitti-360/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Ulusoy2016CVPR');"><div class="publication_image"><img src="publications/Ulusoy2016CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Patches, Planes and Probabilities: A Non-local Prior for Volumetric 3D Reconstruction </b><br/>A. Ulusoy, M. Black and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2016</div></div>
<div class="publication_detail" id="Ulusoy2016CVPR" name="Ulusoy2016CVPR"><div class="publication_abstract"><b>Abstract:</b> We propose a non-local structured prior for volumetric multi-view 3D reconstruction. Towards this goal, we present a random field model based on ray potentials in which assumptions about 3D surface patches such as planarity or Manhattan world constraints can be efficiently encoded as probabilistic priors. We further derive an inference algorithm that reasons jointly about voxels, pixels and image segments, and estimates marginal distributions of appearance, occupancy, depth, normals and planarity. Key to tractable inference is a novel hybrid representation that spans both voxel and pixel space and that integrates non-local information from 2D image segmentations in a principled way. We compare our non-local prior to commonly employed local smoothness assumptions and a variety of state-of-the-art volumetric reconstruction baselines on challenging outdoor scenes with textureless and reflective surfaces. Our experiments indicate that regularizing over larger distances has the potential to resolve ambiguities where local regularizers fail.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Ulusoy2016CVPR.pdf" target="_blank">Ulusoy2016CVPR</a>,<br/>
&nbsp; author = {<a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael Black</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Patches, Planes and Probabilities: A Non-local Prior for Volumetric 3D Reconstruction},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2016}<br/>
}
</div>
<div class="publication_links"><a href="publications/Ulusoy2016CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=FUYZi4jezzk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://ps.is.tue.mpg.de/project/Volumetric_Reconstruction" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2015</h2></div>
<div class="publication_container" onclick="toggleDetails('Zhou2015ICCV');"><div class="publication_image"><img src="publications/Zhou2015ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Exploiting Object Similarity in 3D Reconstruction </b><br/>C. Zhou, F. Güney, Y. Wang and A. Geiger <br/>International Conference on Computer Vision (ICCV), 2015</div></div>
<div class="publication_detail" id="Zhou2015ICCV" name="Zhou2015ICCV"><div class="publication_abstract"><b>Abstract:</b> Despite recent progress, reconstructing outdoor scenes in 3D from movable platforms remains a highly difficult endeavor. Challenges include low frame rates, occlusions, large distortions and difficult lighting conditions. In this paper, we leverage the fact that the larger the reconstructed area, the more likely objects of similar type and shape will occur in the scene. This is particularly true for outdoor scenes where buildings and vehicles often suffer from missing texture or reflections, but share similarity in 3D shape. We take advantage of this shape similarity by locating objects using detectors and jointly reconstructing them while learning a volumetric model of their shape. This allows us to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations for the respective category. We evaluate our approach with respect to LIDAR ground truth on a novel challenging suburban dataset and show its advantages over the state-of-the-art.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Zhou2015ICCV.pdf" target="_blank">Zhou2015ICCV</a>,<br/>
&nbsp; author = {<a href="http://ps.is.tuebingen.mpg.de/person/zhou" target="blank">Chen Zhou</a> and <a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://ywang-zju.github.io/" target="blank">Yizhou Wang</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Exploiting Object Similarity in 3D Reconstruction},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Zhou2015ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Zhou2015ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=OnaV5CFhGWs" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cvlibs.net/projects/similarity_reconstruction" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Lenz2015ICCV');"><div class="publication_image"><img src="publications/Lenz2015ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation </b><br/>P. Lenz, A. Geiger and R. Urtasun <br/>International Conference on Computer Vision (ICCV), 2015</div></div>
<div class="publication_detail" id="Lenz2015ICCV" name="Lenz2015ICCV"><div class="publication_abstract"><b>Abstract:</b> One of the most popular approaches to multi-target tracking is tracking-by-detection. Current min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm which solves the data association problem optimally while reusing computation, resulting in faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrary length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Lenz2015ICCV.pdf" target="_blank">Lenz2015ICCV</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" target="blank">Philip Lenz</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Lenz2015ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Lenz2015ICCV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Lenz2015ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=B_5vXjgKNR0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=IrFtQ2NPaw8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cs.toronto.edu/boundTracking/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Menze2015ISA');"><div class="publication_image"><img src="publications/Menze2015ISA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Joint 3D Estimation of Vehicles and Scene Flow </b> <b><span style="color:#a51e37">(oral)</span></b><br/>M. Menze, C. Heipke and A. Geiger <br/>ISPRS Workshop on Image Sequence Analysis (ISA), 2015</div></div>
<div class="publication_detail" id="Menze2015ISA" name="Menze2015ISA"><div class="publication_abstract"><b>Abstract:</b> Three-dimensional reconstruction of dynamic scenes is an important prerequisite for applications like mobile robotics or autonomous driving. While much progress has been made in recent years, imaging conditions in natural outdoor environments are still very challenging for current reconstruction and recognition methods. In this paper, we propose a novel unified approach which reasons jointly about 3D scene flow as well as the pose, shape and motion of vehicles in the scene. Towards this goal, we incorporate a deformable CAD model into a slanted-plane conditional random field for scene flow estimation and enforce shape consistency between the rendered 3D models and the parameters of all superpixels in the image. The association of superpixels to objects is established by an index variable which implicitly enables model selection. We evaluate our approach on the challenging KITTI scene flow dataset in terms of object and scene flow estimation. Our results provide a prove of concept and demonstrate the usefulness of our method.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Menze2015ISA.pdf" target="_blank">Menze2015ISA</a>,<br/>
&nbsp; author = {<a href="http://www.ipi.uni-hannover.de/tmm.html" target="blank">Moritz Menze</a> and <a href="http://www.ipi.uni-hannover.de/chei.html" target="blank">Christian Heipke</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Joint 3D Estimation of Vehicles and Scene Flow},<br/>&nbsp; booktitle = {ISPRS Workshop on Image Sequence Analysis (ISA)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Menze2015ISA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.cvlibs.net/projects/objectsceneflow,http://www.ipi.uni-hannover.de/projekt_abt1.html?&tx_tkforschungsberichte_pi1[backpid]=38&cHash=7741b141c63283480e94ec5834205268&tx_tkforschungsberichte_pi1[showUid]=152" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Ulusoy2015THREEDV');"><div class="publication_image"><img src="publications/Ulusoy2015THREEDV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Towards Probabilistic Volumetric Reconstruction using Ray Potentials </b> <b><span style="color:#a51e37">(oral, best paper award)</span></b><br/>A. Ulusoy, A. Geiger and M. Black <br/>International Conference on 3D Vision (3DV), 2015</div></div>
<div class="publication_detail" id="Ulusoy2015THREEDV" name="Ulusoy2015THREEDV"><div class="publication_abstract"><b>Abstract:</b> This paper presents a novel probabilistic foundation for volumetric 3-d reconstruction. We formulate the problem as inference in a Markov random field, which accurately captures the dependencies between the occupancy and appearance of each voxel, given all input images. Our main contribution is an approximate highly parallelized discrete-continuous inference algorithm to compute the marginal distributions of each voxel's occupancy and appearance. In contrast to the MAP solution, marginals encode the underlying uncertainty and ambiguity in the reconstruction. Moreover, the proposed algorithm allows for a Bayes optimal prediction with respect to a natural reconstruction loss. We compare our method to two state-of-the-art volumetric reconstruction algorithms on three challenging aerial datasets with LIDAR ground truth. Our experiments demonstrate that the proposed algorithm compares favorably in terms of reconstruction accuracy and the ability to expose reconstruction uncertainty.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Ulusoy2015THREEDV.pdf" target="_blank">Ulusoy2015THREEDV</a>,<br/>
&nbsp; author = {<a href="http://ps.is.tuebingen.mpg.de/person/ulusoy" target="blank">Ali Osman Ulusoy</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ps.is.tuebingen.mpg.de/person/black" target="blank">Michael J. Black</a>},<br/>
&nbsp; title = {Towards Probabilistic Volumetric Reconstruction using Ray Potentials},<br/>&nbsp; booktitle = {International Conference on 3D Vision (3DV)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Ulusoy2015THREEDV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Ulusoy2015THREEDV_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="http://www.youtube.com/watch?v=NGj9sGaeOVY" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://ps.is.tue.mpg.de/project/Volumetric_Reconstruction" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Menze2015GCPR');"><div class="publication_image"><img src="publications/Menze2015GCPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Discrete Optimization for Optical Flow </b> <b><span style="color:#a51e37">(oral)</span></b><br/>M. Menze, C. Heipke and A. Geiger <br/>German Conference on Pattern Recognition (GCPR), 2015</div></div>
<div class="publication_detail" id="Menze2015GCPR" name="Menze2015GCPR"><div class="publication_abstract"><b>Abstract:</b> We propose to look at large-displacement optical flow from a discrete point of view. Motivated by the observation that sub-pixel accuracy is easily obtained given pixel-accurate optical flow, we conjecture that computing the integral part is the hardest piece of the problem. Consequently, we formulate optical flow estimation as a discrete inference problem in a conditional random field, followed by sub-pixel refinement. Naive discretization of the 2D flow space, however, is intractable due to the resulting size of the label set. In this paper, we therefore investigate three different strategies, each able to reduce computation and memory demands by several orders of magnitude. Their combination allows us to estimate large-displacement optical flow both accurately and efficiently and demonstrates the potential of discrete optimization for optical flow. We obtain state-of-the-art performance on MPI Sintel and KITTI.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Menze2015GCPR.pdf" target="_blank">Menze2015GCPR</a>,<br/>
&nbsp; author = {<a href="http://www.ipi.uni-hannover.de/tmm.html" target="blank">Moritz Menze</a> and <a href="http://www.ipi.uni-hannover.de/chei.html" target="blank">Christian Heipke</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Discrete Optimization for Optical Flow},<br/>&nbsp; booktitle = {German Conference on Pattern Recognition (GCPR)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Menze2015GCPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Menze2015GCPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Menze2015GCPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.cvlibs.net/projects/discrete_flow" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2015GCPR');"><div class="publication_image"><img src="publications/Geiger2015GCPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Joint 3D Object and Layout Inference from a single RGB-D Image </b> <b><span style="color:#a51e37">(oral, best paper award)</span></b><br/>A. Geiger and C. Wang <br/>German Conference on Pattern Recognition (GCPR), 2015</div></div>
<div class="publication_detail" id="Geiger2015GCPR" name="Geiger2015GCPR"><div class="publication_abstract"><b>Abstract:</b> Inferring 3D objects and the layout of indoor scenes from a single RGB-D image captured with a Kinect camera is a challenging task. Towards this goal, we propose a high-order graphical model and jointly reason about the layout, objects and superpixels in the image. In contrast to existing holistic approaches, our model leverages detailed 3D geometry using inverse graphics and explicitly enforces occlusion and visibility constraints for respecting scene properties and projective geometry. We cast the task as MAP inference in a factor graph and solve it efficiently using message passing. We evaluate our method with respect to several baselines on the challenging NYUv2 indoor dataset using 21 object categories. Our experiments demonstrate that the proposed method is able to infer scenes with a large degree of clutter and occlusions.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2015GCPR.pdf" target="_blank">Geiger2015GCPR</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://igm.univ-mlv.fr/~cwang/index.php" target="blank">Chaohui Wang</a>},<br/>
&nbsp; title = {Joint 3D Object and Layout Inference from a single RGB-D Image},<br/>&nbsp; booktitle = {German Conference on Pattern Recognition (GCPR)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2015GCPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2015GCPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Geiger2015GCPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=R2s6wzCHSYg" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cvlibs.net/projects/indoor_scenes" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Menze2015CVPR');"><div class="publication_image"><img src="publications/Menze2015CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Object Scene Flow for Autonomous Vehicles </b><br/>M. Menze and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2015</div></div>
<div class="publication_detail" id="Menze2015CVPR" name="Menze2015CVPR"><div class="publication_abstract"><b>Abstract:</b> This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently moving objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also reveal novel challenges which can't be handled by existing methods.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Menze2015CVPR.pdf" target="_blank">Menze2015CVPR</a>,<br/>
&nbsp; author = {<a href="http://www.ipi.uni-hannover.de/tmm.html" target="blank">Moritz Menze</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Object Scene Flow for Autonomous Vehicles},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Menze2015CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Menze2015CVPR_abstract.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Extended Abstract</a><br/><a href="publications/Menze2015CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Menze2015CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Menze2015CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=jCKHyhdI31w" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=DBJmR6hx2UE" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/projects/objectsceneflow" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Guney2015CVPR');"><div class="publication_image"><img src="publications/Guney2015CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Displets: Resolving Stereo Ambiguities using Object Knowledge </b><br/>F. Güney and A. Geiger <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2015</div></div>
<div class="publication_detail" id="Guney2015CVPR" name="Guney2015CVPR"><div class="publication_abstract"><b>Abstract:</b> Stereo techniques have witnessed tremendous progress over the last decades, yet some aspects of the problem still remain challenging today. Striking examples are reflecting and textureless surfaces which cannot easily be recovered using traditional local regularizers. In this paper, we therefore propose to regularize over larger distances using object-category specific disparity proposals (displets) which we sample using inverse graphics techniques based on a sparse disparity estimate and a semantic segmentation of the image. The proposed displets encode the fact that objects of certain categories are not arbitrarily shaped but typically exhibit regular structures. We integrate them as non-local regularizer for the challenging object class 'car' into a superpixel based CRF framework and demonstrate its benefits on the KITTI stereo evaluation.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Guney2015CVPR.pdf" target="_blank">Guney2015CVPR</a>,<br/>
&nbsp; author = {<a href="http://ps.is.tuebingen.mpg.de/person/g%C3%BCney" target="blank">Fatma Güney</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Displets: Resolving Stereo Ambiguities using Object Knowledge},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="publications/Guney2015CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Guney2015CVPR_abstract.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Extended Abstract</a><br/><a href="publications/Guney2015CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Guney2015CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="publications/Guney2015CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=19JXFXIj0EM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=2WMcS7YYZ1k" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=O2rADk19a84" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="http://www.cvlibs.net/projects/displets" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Winner2015eng');"><div class="publication_image"><img src="publications/Winner2015eng.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Handbook of Driver Assistance Systems </b><br/>H. Winner, S. Hakuli, F. Lotz, C. Singer, A. Geiger  et al.<br/>Springer Vieweg, 2015</div></div>
<div class="publication_detail" id="Winner2015eng" name="Winner2015eng"><div class="publication_abstract"><b>Abstract:</b> This fundamental work explains in detail systems for active safety and driver assistance, considering both their structure and their function. These include the well-known standard systems such as Anti-lock braking system (ABS), Electronic Stability Control (ESC) or Adaptive Cruise Control (ACC). But it includes also new systems for protecting collisions protection, for changing the lane, or for convenient parking. The book aims at giving a complete picture focusing on the entire system. First, it describes the components which are necessary for assistance systems, such as sensors, actuators, mechatronic subsystems, and control elements. Then, it explains key features for the user-friendly design of human-machine interfaces between driver and assistance system. Finally, important characteristic features of driver assistance systems for particular vehicles are presented: Systems for commercial vehicles and motorcycles.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@BOOK{<a href="https://www.cvlibs.net/publications/Winner2015eng.pdf" target="_blank">Winner2015eng</a>,<br/>
&nbsp; author = {H. Winner and S. Hakuli and F. Lotz and C. Singer and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and  others},<br/>
&nbsp; title = {Handbook of Driver Assistance Systems},<br/>&nbsp; publisher = {Springer Vieweg},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="http://www.springer.com/de/book/9783319123516" target="_blank"><div class="publication_icon"><img src="site/icon_link.png"/></div>Link</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Winner2015');"><div class="publication_image"><img src="publications/Winner2015.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Handbuch Fahrerassistenzsysteme </b><br/>H. Winner, S. Hakuli, F. Lotz, C. Singer, A. Geiger  et al.<br/>Springer Vieweg, 2015</div></div>
<div class="publication_detail" id="Winner2015" name="Winner2015"><div class="publication_abstract"><b>Abstract:</b> In diesem Grundlagenwerk werden Fahrerassistenzsysteme für aktive Sicherheit und Fahrerentlastung in Aufbau und Funktion ausführlich erklärt. Darüber hinaus enthält es eine Übersicht der Rahmenbedingungen für die Fahrerassistenzentwicklung sowie Erläuterungen der angewandten Entwicklungs- und Testwerkzeuge. Die Beschreibung umfasst die heute bekannten Assistenzsysteme für die Fahrzeugstabilisierung (z. B. ABS und ESC), die Bahnführung (z. B. ACC, Einparkassistenz) und die Navigation sowie einen Ausblick auf die zukünftigen Entwicklungen, insbesondere der zunehmenden Automatisierung des Fahrens. Die Darstellung bezieht Funktionsprinzipien und Ausführungsformen der dazu erforderlichen Komponenten wie Sensoren, Aktoren, mechatronische Subsysteme und Betätigungselemente ein. Außerdem werden Konzepte der Datenfusion und Umfeldrepräsentation sowie der nutzergerechten Gestaltung der Mensch-Maschine-Schnittstelle zwischen Assistenzsystem und Fahrer vorgestellt. Kapitel über die Besonderheiten von Fahrerassistenzsystemen bei Nutzfahrzeugen und Motorrädern runden den umfassenden Ansatz ab.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@BOOK{<a href="https://www.cvlibs.net/publications/Winner2015.pdf" target="_blank">Winner2015</a>,<br/>
&nbsp; author = {H. Winner and S. Hakuli and F. Lotz and C. Singer and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and  others},<br/>
&nbsp; title = {Handbuch Fahrerassistenzsysteme},<br/>&nbsp; publisher = {Springer Vieweg},<br/>
&nbsp; year = {2015}<br/>
}
</div>
<div class="publication_links"><a href="http://www.springer.com/de/book/9783658057336" target="_blank"><div class="publication_icon"><img src="site/icon_link.png"/></div>Link</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2014</h2></div>
<div class="publication_container" onclick="toggleDetails('Geiger2014PAMI');"><div class="publication_image"><img src="publications/Geiger2014PAMI.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>3D Traffic Scene Understanding from Movable Platforms </b><br/>A. Geiger, M. Lauer, C. Wojek, C. Stiller and R. Urtasun <br/>Pattern Analysis and Machine Intelligence (PAMI), 2014</div></div>
<div class="publication_detail" id="Geiger2014PAMI" name="Geiger2014PAMI"><div class="publication_abstract"><b>Abstract:</b> In this paper, we present a novel probabilistic generative model for multi-object traffic scene understanding from movable platforms which reasons jointly about the 3D scene layout as well as the location and orientation of objects in the scene. In particular, the scene topology, geometry and traffic activities are inferred from short video sequences. Inspired by the impressive driving capabilities of humans, our model does not rely on GPS, lidar or map knowledge. Instead, it takes advantage of a diverse set of visual cues in the form of vehicle tracklets, vanishing points, semantic scene labels, scene flow and occupancy grids. For each of these cues we propose likelihood functions that are integrated into a probabilistic generative model. We learn all model parameters from training data using contrastive divergence. Experiments conducted on videos of 113 representative intersections show that our approach successfully infers the correct layout in a variety of very challenging scenarios. To evaluate the importance of each feature cue, experiments using different feature combinations are conducted. Furthermore, we show how by employing context derived from the proposed method we are able to improve over the state-of-the-art in terms of object detection and object orientation estimation in challenging and cluttered urban environments.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Geiger2014PAMI.pdf" target="_blank">Geiger2014PAMI</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_lauer.php" target="blank">Martin Lauer</a> and <a href="http://www.d2.mpi-inf.mpg.de/cwojek" target="blank">Christian Wojek</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" target="blank">Christoph Stiller</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {3D Traffic Scene Understanding from Movable Platforms},<br/>&nbsp; journal = {Pattern Analysis and Machine Intelligence (PAMI)},<br/>
&nbsp; year = {2014}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2014PAMI.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=glH9jYxwS2M" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=uSGnGoUJBXQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/projects/intersection" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Roser2014ICRA');"><div class="publication_image"><img src="publications/Roser2014ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Simultaneous Underwater Visibility Assessment, Enhancement and Improved Stereo </b><br/>M. Roser, M. Dunbabin and A. Geiger <br/>International Conference on Robotics and Automation (ICRA), 2014</div></div>
<div class="publication_detail" id="Roser2014ICRA" name="Roser2014ICRA"><div class="publication_abstract"><b>Abstract:</b> Vision-based underwater navigation and obstacle avoidance demands robust computer vision algorithms, particularly for operation in turbid water with reduced visibility. This paper describes a novel method for the simultaneous underwater image quality assessment, visibility enhancement and disparity computation to increase stereo range resolution under dynamic, natural lighting and turbid conditions. The technique estimates the visibility properties from a sparse 3D map of the original degraded image using a physical underwater light attenuation model. Firstly, an iterated distance-adaptive image contrast enhancement enables a dense disparity computation and visibility estimation. Secondly, using a light attenuation model for ocean water, a color corrected stereo underwater image is obtained along with a visibility distance estimate. Experimental results in shallow, naturally lit, high-turbidity coastal environments show the proposed technique improves range estimation over the original images as well as image quality and color for habitat classification. Furthermore, the recursiveness and robustness of the technique allows real-time implementation onboard an Autonomous Underwater Vehicles for improved navigation and obstacle avoidance performance.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Roser2014ICRA.pdf" target="_blank">Roser2014ICRA</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_roser.php" target="blank">Martin Roser</a> and Matthew Dunbabin and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Simultaneous Underwater Visibility Assessment, Enhancement and Improved Stereo},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2014}<br/>
}
</div>
<div class="publication_links"><a href="publications/Roser2014ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schoenbein2014ICRA');"><div class="publication_image"><img src="publications/Schoenbein2014ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Calibrating and Centering Quasi-Central Catadioptric Cameras </b><br/>M. Schönbein, T. Strauss and A. Geiger <br/>International Conference on Robotics and Automation (ICRA), 2014</div></div>
<div class="publication_detail" id="Schoenbein2014ICRA" name="Schoenbein2014ICRA"><div class="publication_abstract"><b>Abstract:</b> Non-central catadioptric models are able to cope with irregular camera setups and inaccuracies in the manufacturing process but are computationally demanding and thus not suitable for robotic applications. On the other hand, calibrating a quasi-central (almost central) system with a central model introduces errors due to a wrong relationship between the viewing ray orientations and the pixels on the image sensor. In this paper, we propose a central approximation to quasi-central catadioptric camera systems that is both accurate and efficient. We observe that the distance to points in 3D is typically large compared to deviations from the single viewpoint. Thus, we first calibrate the system using a state-of-the-art non-central camera model. Next, we show that by remapping the observations we are able to match the orientation of the viewing rays of a much simpler single viewpoint model with the true ray orientations. While our approximation is general and applicable to all quasi-central camera systems, we focus on one of the most common cases in practice: hypercatadioptric cameras. We compare our model to a variety of baselines in synthetic and real localization and motion estimation experiments. We show that by using the proposed model we are able to achieve near non-central accuracy while obtaining speed-ups of more than three orders of magnitude compared to state-of-the-art non-central models.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schoenbein2014ICRA.pdf" target="_blank">Schoenbein2014ICRA</a>,<br/>
&nbsp; author = {Miriam Schönbein and <a href="http://www.mrt.kit.edu/mitarbeiter_strauss.php" target="blank">Tobias Strauss</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Calibrating and Centering Quasi-Central Catadioptric Cameras},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2014}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schoenbein2014ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.cvlibs.net/projects/omnicam/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Schoenbein2014IROS');"><div class="publication_image"><img src="publications/Schoenbein2014IROS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Omnidirectional 3D Reconstruction in Augmented Manhattan Worlds </b><br/>M. Schönbein and A. Geiger <br/>International Conference on Intelligent Robots and Systems (IROS), 2014</div></div>
<div class="publication_detail" id="Schoenbein2014IROS" name="Schoenbein2014IROS"><div class="publication_abstract"><b>Abstract:</b> This paper proposes a method for high-quality omnidirectional 3D reconstruction of augmented Manhattan worlds from catadioptric stereo video sequences. In contrast to existing works we do not rely on constructing virtual perspective views, but instead propose to optimize depth jointly in a unified omnidirectional space. Furthermore, we show that plane-based prior models can be applied even though planes in 3D do not project to planes in the omnidirectional domain. Towards this goal, we propose an omnidirectional slanted-plane Markov random field model which relies on plane hypotheses extracted using a novel voting scheme for 3D planes in omnidirectional space. To quantitatively evaluate our method we introduce a dataset which we have captured using our autonomous driving platform AnnieWAY which we equipped with two horizontally aligned catadioptric cameras and a Velodyne HDL-64E laser scanner for precise ground truth depth measurements. As evidenced by our experiments, the proposed method clearly benefits from the unified view and significantly outperforms existing stereo matching techniques both quantitatively and qualitatively. Furthermore, our method is able to reduce noise and the obtained depth maps can be represented very compactly by a small number of image segments and plane parameters.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Schoenbein2014IROS.pdf" target="_blank">Schoenbein2014IROS</a>,<br/>
&nbsp; author = {Miriam Schönbein and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Omnidirectional 3D Reconstruction in Augmented Manhattan Worlds},<br/>&nbsp; booktitle = {International Conference on Intelligent Robots and Systems (IROS)},<br/>
&nbsp; year = {2014}<br/>
}
</div>
<div class="publication_links"><a href="publications/Schoenbein2014IROS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.cvlibs.net/projects/omnicam/" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2013</h2></div>
<div class="publication_container" onclick="toggleDetails('Zhang2013ICCV');"><div class="publication_image"><img src="publications/Zhang2013ICCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Understanding High-Level Semantics by Modeling Traffic Patterns </b><br/>H. Zhang, A. Geiger and R. Urtasun <br/>International Conference on Computer Vision (ICCV), 2013</div></div>
<div class="publication_detail" id="Zhang2013ICCV" name="Zhang2013ICCV"><div class="publication_abstract"><b>Abstract:</b> In this paper, we are interested in understanding the semantics of outdoor scenes in the context of autonomous driving. Towards this goal, we propose a generative model of 3D urban scenes which is able to reason not only about the geometry and objects present in the scene, but also about the high-level semantics in the form of traffic patterns. We found that a small number of patterns is sufficient to model the vast majority of traffic scenes and show how these patterns can be learned. As evidenced by our experiments, this high-level reasoning significantly improves the overall scene estimation as well as the vehicle-to-lane association when compared to state-of-the-art approaches. All data and code will be made available upon publication.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Zhang2013ICCV.pdf" target="_blank">Zhang2013ICCV</a>,<br/>
&nbsp; author = {Hongyi Zhang and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Understanding High-Level Semantics by Modeling Traffic Patterns},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV)},<br/>
&nbsp; year = {2013}<br/>
}
</div>
<div class="publication_links"><a href="publications/Zhang2013ICCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Zhang2013ICCV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=1x0Io_NZ5JA" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cvlibs.net/projects/intersection" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2013IJRR');"><div class="publication_image"><img src="publications/Geiger2013IJRR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Vision meets Robotics: The KITTI Dataset </b><br/>A. Geiger, P. Lenz, C. Stiller and R. Urtasun <br/>International Journal of Robotics Research (IJRR), 2013</div></div>
<div class="publication_detail" id="Geiger2013IJRR" name="Geiger2013IJRR"><div class="publication_abstract"><b>Abstract:</b> We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Geiger2013IJRR.pdf" target="_blank">Geiger2013IJRR</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" target="blank">Philip Lenz</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" target="blank">Christoph Stiller</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Vision meets Robotics: The KITTI Dataset},<br/>&nbsp; journal = {International Journal of Robotics Research (IJRR)},<br/>
&nbsp; year = {2013}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2013IJRR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=KXpZ6B1YB_k" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=_imrrzn8NDk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=_3QVQNCn1Cs" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="http://www.cvlibs.net/datasets/kitti" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2013');"><div class="publication_image"><img src="publications/Geiger2013.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Probabilistic Models for 3D Urban Scene Understanding from Movable Platforms </b><br/>A. Geiger <br/>Ph.D. Thesis, 2013</div></div>
<div class="publication_detail" id="Geiger2013" name="Geiger2013"><div class="publication_abstract"><b>Abstract:</b> Visual 3D scene understanding is an important component in autonomous driving and robot navigation. Intelligent vehicles for example often base their decisions on observations obtained from video cameras as they are cheap and easy to employ. Inner-city intersections represent an interesting but also very challenging scenario in this context: The road layout may be very complex and observations are often noisy or even missing due to heavy occlusions. While Highway navigation and autonomous driving on simple and annotated intersections have already been demonstrated successfully, understanding and navigating general inner-city crossings with little prior knowledge remains an unsolved problem. This thesis is a contribution to understanding multi-object traffic scenes from video sequences. All data is provided by a camera system which is mounted on top of the autonomous driving platform AnnieWAY. The proposed probabilistic generative model reasons jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, the scene topology, geometry as well as traffic activities are inferred from short video sequences. The model takes advantage of monocular information in the form of vehicle tracklets, vanishing lines and semantic labels. Additionally, the benefit of stereo features such as 3D scene flow and occupancy grids is investigated. Motivated by the impressive driving capabilities of humans, no further information such as GPS, lidar, radar or map knowledge is required. Experiments conducted on 113 representative intersection sequences show that the developed approach successfully infers the correct layout in a variety of difficult scenarios. To evaluate the importance of each feature cue, experiments with different feature combinations are conducted. Additionally, the proposed method is shown to improve object detection and object orientation estimation performance.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@PHDTHESIS{<a href="https://www.cvlibs.net/publications/Geiger2013.pdf" target="_blank">Geiger2013</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Probabilistic Models for 3D Urban Scene Understanding from Movable Platforms},<br/>&nbsp; school = {KIT},<br/>
&nbsp; year = {2013}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2013.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=glH9jYxwS2M" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=uSGnGoUJBXQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/projects/intersection" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Brubaker2013CVPR');"><div class="publication_image"><img src="publications/Brubaker2013CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization </b> <b><span style="color:#a51e37">(oral, best paper runner up award)</span></b><br/>M. Brubaker, A. Geiger and R. Urtasun <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2013</div></div>
<div class="publication_detail" id="Brubaker2013CVPR" name="Brubaker2013CVPR"><div class="publication_abstract"><b>Abstract:</b> In this paper we propose an affordable solution to self-localization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Brubaker2013CVPR.pdf" target="_blank">Brubaker2013CVPR</a>,<br/>
&nbsp; author = {<a href="http://www.cs.toronto.edu/~mbrubake" target="blank">Marcus A. Brubaker</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2013}<br/>
}
</div>
<div class="publication_links"><a href="publications/Brubaker2013CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Brubaker2013CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Brubaker2013CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=rAuTgsiM2-8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cs.toronto.edu/~mbrubake/projects/map" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Fritsch2013ITSC');"><div class="publication_image"><img src="publications/Fritsch2013ITSC.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>A New Performance Measure and Evaluation Benchmark for Road Detection Algorithms </b><br/>J. Fritsch, T. Kuehnl and A. Geiger <br/>International Conference on Intelligent Transportation Systems (ITSC), 2013</div></div>
<div class="publication_detail" id="Fritsch2013ITSC" name="Fritsch2013ITSC"><div class="publication_abstract"><b>Abstract:</b> Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and egolane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted egolane area for driver assistance applications by fitting a corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Fritsch2013ITSC.pdf" target="_blank">Fritsch2013ITSC</a>,<br/>
&nbsp; author = {Jannik Fritsch and Tobias Kuehnl and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {A New Performance Measure and Evaluation Benchmark for Road Detection Algorithms},<br/>&nbsp; booktitle = {International Conference on Intelligent Transportation Systems (ITSC)},<br/>
&nbsp; year = {2013}<br/>
}
</div>
<div class="publication_links"><a href="publications/Fritsch2013ITSC.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.cvlibs.net/datasets/kitti" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2012</h2></div>
<div class="publication_container" onclick="toggleDetails('Geiger2012CVPR');"><div class="publication_image"><img src="publications/Geiger2012CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite </b> <b><span style="color:#a51e37">(oral)</span></b><br/>A. Geiger, P. Lenz and R. Urtasun <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2012</div></div>
<div class="publication_detail" id="Geiger2012CVPR" name="Geiger2012CVPR"><div class="publication_abstract"><b>Abstract:</b> Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry / SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2012CVPR.pdf" target="_blank">Geiger2012CVPR</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" target="blank">Philip Lenz</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2012}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2012CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2012CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Geiger2012CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://techtalks.tv/talks/are-we-ready-for-autonomous-driving-the-kitti-vision-benchmark-suite/56385" target="_blank"><div class="publication_icon"><img src="site/icon_talk.png"/></div>Talk</a><br/><a href="http://www.youtube.com/watch?v=KXpZ6B1YB_k" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=_imrrzn8NDk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=_3QVQNCn1Cs" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="http://www.cvlibs.net/datasets/kitti" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2012ICRA');"><div class="publication_image"><img src="publications/Geiger2012ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Automatic Calibration of Range and Camera Sensors using a single Shot </b> <b><span style="color:#a51e37">(oral)</span></b><br/>A. Geiger, F. Moosmann, O. Car and B. Schuster <br/>International Conference on Robotics and Automation (ICRA), 2012</div></div>
<div class="publication_detail" id="Geiger2012ICRA" name="Geiger2012ICRA"><div class="publication_abstract"><b>Abstract:</b> As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present an online toolbox for fully automatic camera-to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within less than one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experiments using a variety of sensors such as greyscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2012ICRA.pdf" target="_blank">Geiger2012ICRA</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_moosmann.php" target="blank">Frank Moosmann</a> and Oemer Car and Bernhard Schuster},<br/>
&nbsp; title = {Automatic Calibration of Range and Camera Sensors using a single Shot},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2012}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2012ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2012ICRA_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://techtalks.tv/talks/automatic-camera-and-range-sensor-calibration-using-a-single-shot/55714" target="_blank"><div class="publication_icon"><img src="site/icon_talk.png"/></div>Talk</a><br/><a href="http://www.cvlibs.net/software/calibration" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Lategahn2012IV');"><div class="publication_image"><img src="publications/Lategahn2012IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Motion-without-Structure: Real-time Multipose Optimization for Accurate Visual Odometry </b><br/>H. Lategahn, A. Geiger, B. Kitt and C. Stiller <br/>Intelligent Vehicles Symposium (IV), 2012</div></div>
<div class="publication_detail" id="Lategahn2012IV" name="Lategahn2012IV"><div class="publication_abstract"><b>Abstract:</b> State of the art visual odometry systems use bundle adjustment (BA) like methods to jointly optimize motion and scene structure. Fusing measurements from multiple time steps and optimizing an error criterion in a batch fashion seems to deliver the most accurate results. However, often the scene structure is of no interest and is a mere auxiliary quantity although it contributes heavily to the complexity of the problem. Herein we propose to use a recently developed incremental motion estimator which delivers relative pose displacements between each two frames within a sliding window inducing a pose graph. Moreover, we introduce a method to learn the uncertainty associated with each of the pose displacements. The pose graph is adjusted by non-linear least squares optimization while incorporating a motion model. Thereby we fuse measurements from multiple time steps much in the same sense as BA does. However, we obviate the need to estimate the scene structure yielding a very efficient estimator: Solving the nonlinear least squares problem by a Gauss-Newton method takes approximately 1ms. We show the effectiveness of our method on simulated and real world data and demonstrate substantial improvements over incremental methods.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Lategahn2012IV.pdf" target="_blank">Lategahn2012IV</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/mitarbeiter_lategahn.php" target="blank">Henning Lategahn</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_kitt.php" target="blank">Bernd Kitt</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" target="blank">Christoph Stiller</a>},<br/>
&nbsp; title = {Motion-without-Structure: Real-time Multipose Optimization for Accurate Visual Odometry},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2012}<br/>
}
</div>
<div class="publication_links"><a href="publications/Lategahn2012IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Lategahn2012IV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2012TITS');"><div class="publication_image"><img src="publications/Geiger2012TITS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Team AnnieWAY's entry to the Grand Cooperative Driving Challenge 2011 </b><br/>A. Geiger, M. Lauer, F. Moosmann, B. Ranft, H. Rapp, C. Stiller and J. Ziegler <br/>Transactions on Intelligent Transportation Systems (TITS), 2012</div></div>
<div class="publication_detail" id="Geiger2012TITS" name="Geiger2012TITS"><div class="publication_abstract"><b>Abstract:</b> In this paper we present the concepts and methods developed for the autonomous vehicle AnnieWAY, our winning entry to the Grand Cooperative Driving Challenge of 2011. We describe algorithms for sensor fusion, vehicle-to-vehicle communication and cooperative control. Furthermore, we analyze the performance of the proposed methods and compare them to those of competing teams. We close with our results from the competition and lessons learned.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@ARTICLE{<a href="https://www.cvlibs.net/publications/Geiger2012TITS.pdf" target="_blank">Geiger2012TITS</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_lauer.php" target="blank">Martin Lauer</a> and <a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_moosmann.php" target="blank">Frank Moosmann</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_ranft.php" target="blank">Benjamin Ranft</a> and <a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_rapp.php" target="blank">Holger Rapp</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" target="blank">Christoph Stiller</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_ziegler.php" target="blank">Julius Ziegler</a>},<br/>
&nbsp; title = {Team AnnieWAY's entry to the Grand Cooperative Driving Challenge 2011},<br/>&nbsp; journal = {Transactions on Intelligent Transportation Systems (TITS)},<br/>
&nbsp; year = {2012}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2012TITS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=bneZ8zGsOas" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=ceV6Ex2ExnM" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=lmRifLzw8iA" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="http://www.youtube.com/watch?v=TaIfFBNJAk0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 4</a><br/><a href="http://www.mrt.kit.edu/annieway" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2011</h2></div>
<div class="publication_container" onclick="toggleDetails('Geiger2011CVPR');"><div class="publication_image"><img src="publications/Geiger2011CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>A Generative Model for 3D Urban Scene Understanding from Movable Platforms </b> <b><span style="color:#a51e37">(oral)</span></b><br/>A. Geiger, M. Lauer and R. Urtasun <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2011</div></div>
<div class="publication_detail" id="Geiger2011CVPR" name="Geiger2011CVPR"><div class="publication_abstract"><b>Abstract:</b> 3D scene understanding is key for the success of applications such as autonomous driving and robot navigation. However, existing approaches either produce a mild level of understanding, e.g., segmentation, object detection, or are not accurate enough for these applications, e.g., 3D pop-ups. In this paper we propose a principled generative model of 3D urban scenes that takes into account dependencies between static and dynamic features. We derive a reversible jump MCMC scheme that is able to infer the geometric (e.g., street orientation) and topological (e.g., number of intersecting streets) properties of the scene layout, as well as the semantic activities occurring in the scene, e.g., traffic situations at an intersection. Furthermore, we show that this global level of understanding provides the context necessary to disambiguate current state-of-the-art detectors. We demonstrate the effectiveness of our approach on a dataset composed of short stereo video sequences of 113 different scenes captured by a car driving around a mid-size city.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2011CVPR.pdf" target="_blank">Geiger2011CVPR</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_lauer.php" target="blank">Martin Lauer</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {A Generative Model for 3D Urban Scene Understanding from Movable Platforms},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2011}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2011CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2011CVPR_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Geiger2011CVPR_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://techtalks.tv/talks/a-generative-model-for-3d-urban-scene-understanding-from-movable-platforms/54225" target="_blank"><div class="publication_icon"><img src="site/icon_talk.png"/></div>Talk</a><br/><a href="http://www.youtube.com/watch?v=QXR2wIxA4UQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cvlibs.net/projects/intersection" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Lategahn2011ICRA');"><div class="publication_image"><img src="publications/Lategahn2011ICRA.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Visual SLAM for Autonomous Ground Vehicles </b><br/>H. Lategahn, A. Geiger and B. Kitt <br/>International Conference on Robotics and Automation (ICRA), 2011</div></div>
<div class="publication_detail" id="Lategahn2011ICRA" name="Lategahn2011ICRA"><div class="publication_abstract"><b>Abstract:</b> In this paper we propose a dense stereo V-SLAM algorithm that estimates a dense 3D map representation which is more accurate than raw stereo measurements. Thereto, we run a sparse VSLAM system, take the resulting pose estimates to compute a locally dense representation from dense stereo correspondences. This dense representation is expressed in local coordinate systems which are tracked as part of the SLAM estimate. This allows the dense part to be continuously updated. Our system is driven by visual odometry priors to achieve high robustness when tracking landmarks. Moreover, the sparse part of the SLAM system uses recently published submapping techniques to achieve constant runtime complexity most of the time. The improved accuracy over raw stereo measurements is shown in a Monte Carlo simulation. Finally, we demonstrate the feasibility of our method by presenting outdoor experiments of a car like robot.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Lategahn2011ICRA.pdf" target="_blank">Lategahn2011ICRA</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/mitarbeiter_lategahn.php" target="blank">Henning Lategahn</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_kitt.php" target="blank">Bernd Kitt</a>},<br/>
&nbsp; title = {Visual SLAM for Autonomous Ground Vehicles},<br/>&nbsp; booktitle = {International Conference on Robotics and Automation (ICRA)},<br/>
&nbsp; year = {2011}<br/>
}
</div>
<div class="publication_links"><a href="publications/Lategahn2011ICRA.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Lategahn2011ICRA_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Lenz2011IV');"><div class="publication_image"><img src="publications/Lenz2011IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Sparse Scene Flow Segmentation for Moving Object Detection in Urban Environments </b><br/>P. Lenz, J. Ziegler, A. Geiger and M. Roser <br/>Intelligent Vehicles Symposium (IV), 2011</div></div>
<div class="publication_detail" id="Lenz2011IV" name="Lenz2011IV"><div class="publication_abstract"><b>Abstract:</b> This paper presents an approach for object detection utilizing sparse scene flow. For consecutive stereo images taken from a moving vehicle, corresponding interest points are extracted. Thus, for every interest point, disparity and optical flow values are known and consequently, scene flow can be calculated. Adjacent interest points describing similar scene flow are considered to belong to one rigid object. The proposed method does not rely on object classes and allows for a robust detection of dynamic objects in traffic scenes. Leading vehicles are continuously detected for several frames. Oncoming objects are detected within five frames after their appearance.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Lenz2011IV.pdf" target="_blank">Lenz2011IV</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" target="blank">Philip Lenz</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_ziegler.php" target="blank">Julius Ziegler</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_roser.php" target="blank">Martin Roser</a>},<br/>
&nbsp; title = {Sparse Scene Flow Segmentation for Moving Object Detection in Urban Environments},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2011}<br/>
}
</div>
<div class="publication_links"><a href="publications/Lenz2011IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Lenz2011IV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=ML3suY0HzgU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=Rkl0-1-oFC8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2011IV');"><div class="publication_image"><img src="publications/Geiger2011IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>StereoScan: Dense 3D Reconstruction in Real-time </b> <b><span style="color:#a51e37">(oral)</span></b><br/>A. Geiger, J. Ziegler and C. Stiller <br/>Intelligent Vehicles Symposium (IV), 2011</div></div>
<div class="publication_detail" id="Geiger2011IV" name="Geiger2011IV"><div class="publication_abstract"><b>Abstract:</b> This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2011IV.pdf" target="_blank">Geiger2011IV</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_ziegler.php" target="blank">Julius Ziegler</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" target="blank">Christoph Stiller</a>},<br/>
&nbsp; title = {StereoScan: Dense 3D Reconstruction in Real-time},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2011}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2011IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2011IV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=KAL2vjGvwts" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=QdLYeck1pBw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/software/libviso" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2011NIPS');"><div class="publication_image"><img src="publications/Geiger2011NIPS.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Joint 3D Estimation of Objects and Scene Layout </b><br/>A. Geiger, C. Wojek and R. Urtasun <br/>Advances in Neural Information Processing Systems (NIPS), 2011</div></div>
<div class="publication_detail" id="Geiger2011NIPS" name="Geiger2011NIPS"><div class="publication_abstract"><b>Abstract:</b> We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2011NIPS.pdf" target="_blank">Geiger2011NIPS</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.d2.mpi-inf.mpg.de/cwojek" target="blank">Christian Wojek</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Joint 3D Estimation of Objects and Scene Layout},<br/>&nbsp; booktitle = {Advances in Neural Information Processing Systems (NIPS)},<br/>
&nbsp; year = {2011}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2011NIPS.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2011NIPS_supplementary.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Supplementary Material</a><br/><a href="publications/Geiger2011NIPS_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=Lj855LQTX6s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://www.cvlibs.net/projects/intersection" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2010</h2></div>
<div class="publication_container" onclick="toggleDetails('Geiger2010ACCV');"><div class="publication_image"><img src="publications/Geiger2010ACCV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Efficient Large-Scale Stereo Matching </b> <b><span style="color:#a51e37">(oral)</span></b><br/>A. Geiger, M. Roser and R. Urtasun <br/>Asian Conference on Computer Vision (ACCV), 2010</div></div>
<div class="publication_detail" id="Geiger2010ACCV" name="Geiger2010ACCV"><div class="publication_abstract"><b>Abstract:</b> In this paper we propose a novel approach to binocular stereo for fast matching of high-resolution images. Our approach builds a prior on the disparities by forming a triangulation on a set of support points which can be robustly matched, reducing the matching ambiguities of the remaining points. This allows for efficient exploitation of the disparity search space, yielding accurate dense reconstruction without the need for global optimization. Moreover, our method automatically determines the disparity range and can be easily parallelized. We demonstrate the effectiveness of our approach on the large-scale Middlebury benchmark, and show that state-of-the-art performance can be achieved with significant speedups. Computing the left and right disparity maps for a one Megapixel image pair takes about one second on a single CPU core.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2010ACCV.pdf" target="_blank">Geiger2010ACCV</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_roser.php" target="blank">Martin Roser</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a>},<br/>
&nbsp; title = {Efficient Large-Scale Stereo Matching},<br/>&nbsp; booktitle = {Asian Conference on Computer Vision (ACCV)},<br/>
&nbsp; year = {2010}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2010ACCV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2010ACCV_slides.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Slides</a><br/><a href="http://www.youtube.com/watch?v=pFsV8UY4sT4" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=fDNqGg-DrFI" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.youtube.com/watch?v=XiTZVi2A7Cc" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 3</a><br/><a href="http://www.youtube.com/watch?v=NiMlVgEu7mg" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 4</a><br/><a href="http://www.youtube.com/watch?v=QdLYeck1pBw" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 5</a><br/><a href="http://www.youtube.com/watch?v=4LIUnGnYbN0" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 6</a><br/><a href="http://www.youtube.com/watch?v=rTQ_ZmQ0wNk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 7</a><br/><a href="http://www.cvlibs.net/software/libelas" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Roser2010ACCVWORK');"><div class="publication_image"><img src="publications/Roser2010ACCVWORK.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Realistic Modeling of Water Droplets for Monocular Adherent Raindrop Recognition using Bezier Curves </b><br/>M. Roser, J. Kurz and A. Geiger <br/>Asian Conference on Computer Vision (ACCV) Workshops, 2010</div></div>
<div class="publication_detail" id="Roser2010ACCVWORK" name="Roser2010ACCVWORK"><div class="publication_abstract"><b>Abstract:</b> In this paper, we propose a novel raindrop shape model for the detection of view-disturbing, adherent raindrops on inclined surfaces. Whereas state-of-the-art techniques do not consider inclined surfaces because they assume the droplets as sphere sections with equal contact angles, our model incorporates cubic Bezier curves that provide a low dimensional and physically interpretable representation of a raindrop surface. The parameters are empirically deduced from numerous observations of different raindrop sizes and surface inclination angles. It can be easily integrated into a probabilistic framework for raindrop recognition, using geometrical optics to simulate the visual raindrop appearance. In comparison to a sphere section model, the proposed model yields an improved droplet surface accuracy up to three orders of magnitude.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Roser2010ACCVWORK.pdf" target="_blank">Roser2010ACCVWORK</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_roser.php" target="blank">Martin Roser</a> and Julian Kurz and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Realistic Modeling of Water Droplets for Monocular Adherent Raindrop Recognition using Bezier Curves},<br/>&nbsp; booktitle = {Asian Conference on Computer Vision (ACCV) Workshops},<br/>
&nbsp; year = {2010}<br/>
}
</div>
<div class="publication_links"><a href="publications/Roser2010ACCVWORK.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2010IV');"><div class="publication_image"><img src="publications/Geiger2010IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>ObjectFlow: A Descriptor for Classifying Traffic Motion </b><br/>A. Geiger and B. Kitt <br/>Intelligent Vehicles Symposium (IV), 2010</div></div>
<div class="publication_detail" id="Geiger2010IV" name="Geiger2010IV"><div class="publication_abstract"><b>Abstract:</b> We present and evaluate a novel scene descriptor for classifying urban traffic by object motion. Atomic 3D flow vectors are extracted and compensated for the vehicle's egomotion, using stereo video sequences. Votes cast by each flow vector are accumulated in a bird's eye view histogram grid. Since we are directly using low-level object flow, no prior object detection or tracking is needed. We demonstrate the effectiveness of the proposed descriptor by comparing it to two simpler baselines on the task of classifying more than 100 challenging video sequences into intersection and non-intersection scenarios. Our experiments reveal good classification performance in busy traffic situations, making our method a valuable complement to traditional approaches based on lane markings.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2010IV.pdf" target="_blank">Geiger2010IV</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_kitt.php" target="blank">Bernd Kitt</a>},<br/>
&nbsp; title = {ObjectFlow: A Descriptor for Classifying Traffic Motion},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2010}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2010IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2010IV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Kitt2010IV');"><div class="publication_image"><img src="publications/Kitt2010IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Visual Odometry based on Stereo Image Sequences with RANSAC-based Outlier Rejection Scheme </b> <b><span style="color:#a51e37">(oral)</span></b><br/>B. Kitt, A. Geiger and H. Lategahn <br/>Intelligent Vehicles Symposium (IV), 2010</div></div>
<div class="publication_detail" id="Kitt2010IV" name="Kitt2010IV"><div class="publication_abstract"><b>Abstract:</b> A common prerequisite for many vision-based driver assistance systems is the knowledge of the vehicle's own movement. In this paper we propose a novel approach for estimating the egomotion of the vehicle from a sequence of stereo images. Our method is directly based on the trifocal geometry between image triples, thus no time expensive recovery of the 3-dimensional scene structure is needed. The only assumption we make is a known camera geometry, where the calibration may also vary over time. We employ an Iterated Sigma Point Kalman Filter in combination with a RANSAC-based outlier rejection scheme which yields robust frame-to-frame motion estimation even in dynamic environments. A high-accuracy inertial navigation system is used to evaluate our results on challenging real-world video sequences. Experiments show that our approach is clearly superior compared to other filtering techniques in terms of both, accuracy and run-time.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Kitt2010IV.pdf" target="_blank">Kitt2010IV</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/mitarbeiter_kitt.php" target="blank">Bernd Kitt</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.mrt.kit.edu/mitarbeiter_lategahn.php" target="blank">Henning Lategahn</a>},<br/>
&nbsp; title = {Visual Odometry based on Stereo Image Sequences with RANSAC-based Outlier Rejection Scheme},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2010}<br/>
}
</div>
<div class="publication_links"><a href="publications/Kitt2010IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=EPTJz7w_AqU" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 1</a><br/><a href="http://www.youtube.com/watch?v=DPLh6MoxPAk" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video 2</a><br/><a href="http://www.cvlibs.net/software/libviso" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2009</h2></div>
<div class="publication_container" onclick="toggleDetails('Geiger2009CVPR');"><div class="publication_image"><img src="publications/Geiger2009CVPR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Rank Priors for Continuous Non-Linear Dimensionality Reduction </b><br/>A. Geiger, R. Urtasun and T. Darrell <br/>Conference on Computer Vision and Pattern	Recognition (CVPR), 2009</div></div>
<div class="publication_detail" id="Geiger2009CVPR" name="Geiger2009CVPR"><div class="publication_abstract"><b>Abstract:</b> Discovering the underlying low-dimensional latent structure in high-dimensional perceptual observations (e.g., images, video) can, in many cases, greately improve performance in recognition and tracking. However, non-linear dimensionality reduction methods are often susceptible to local minima and perform poorly when initialized far from the global optimum, even when the intrinsic dimensionality is known a priori. In this work we introduce a prior over the dimensionality of the latent space that penalizes high dimensional spaces, and simultaneously optimize both the latent space and its intrinsic dimensionality in a continuous fashion. Ad-hoc initialization schemes are unnecessarywith our approach; we initialize the latent space to the observation space and automatically infer the latent dimensionality. We report results applying our prior to various probabilistic non-linear dimensionality reduction tasks, and show that our method can outperform graph-based dimensionality reduction techniques as well as previously suggested initialization strategies. We demonstrate the effectiveness of our approach when tracking and classifying human motion.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2009CVPR.pdf" target="_blank">Geiger2009CVPR</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a> and <a href="http://www.eecs.berkeley.edu/~trevor" target="blank">Trevor Darrell</a>},<br/>
&nbsp; title = {Rank Priors for Continuous Non-Linear Dimensionality Reduction},<br/>&nbsp; booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},<br/>
&nbsp; year = {2009}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2009CVPR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2009CVPR_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/><a href="http://www.youtube.com/watch?v=KeLtE537e_A" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Roser2009ICCVWORK');"><div class="publication_image"><img src="publications/Roser2009ICCVWORK.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Video-based Raindrop Detection for Improved Image Registration </b><br/>M. Roser and A. Geiger <br/>International Conference on Computer Vision (ICCV) Workshops, 2009</div></div>
<div class="publication_detail" id="Roser2009ICCVWORK" name="Roser2009ICCVWORK"><div class="publication_abstract"><b>Abstract:</b> In this paper we present a novel approach to improved image registration in rainy weather situations. To this end, we perform monocular raindrop detection in single images based on a photometric raindrop model. Our method is capable of detecting raindrops precisely, even in front of complex backgrounds. The effectiveness is demonstrated by a significant increase in image registration accuracy which also allows for successful image restoration. Experiments on video sequences taken from within a moving vehicle prove the applicability to real-world scenarios.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Roser2009ICCVWORK.pdf" target="_blank">Roser2009ICCVWORK</a>,<br/>
&nbsp; author = {<a href="http://www.mrt.kit.edu/ehemalige_mitarbeiter_roser.php" target="blank">Martin Roser</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Video-based Raindrop Detection for Improved Image Registration},<br/>&nbsp; booktitle = {International Conference on Computer Vision (ICCV) Workshops},<br/>
&nbsp; year = {2009}<br/>
}
</div>
<div class="publication_links"><a href="publications/Roser2009ICCVWORK.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=m1WjanZ0c2s" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2009IV');"><div class="publication_image"><img src="publications/Geiger2009IV.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Monocular road mosaicing for urban environments </b><br/>A. Geiger <br/>Intelligent Vehicles Symposium (IV), 2009</div></div>
<div class="publication_detail" id="Geiger2009IV" name="Geiger2009IV"><div class="publication_abstract"><b>Abstract:</b> Marking-based lane recognition requires an unobstructed view onto the road. In practice however, heavy traffic often constrains the visual field, especially in urban scenarios such as urban crossroads. In this paper we present a novel approach to road mosaicing for dynamic environments. Our method is based on a multistage registration procedure and uses blending techniques. We show that under modest assumptions accurate registration is possible from monocular image sequences. We further demonstrate that fusing visual information from previous frames into the current view can greatly extend the camera's field of view.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Geiger2009IV.pdf" target="_blank">Geiger2009IV</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Monocular road mosaicing for urban environments},<br/>&nbsp; booktitle = {Intelligent Vehicles Symposium (IV)},<br/>
&nbsp; year = {2009}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2009IV.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="publications/Geiger2009IV_poster.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Poster</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2008</h2></div>
<div class="publication_container" onclick="toggleDetails('Urtasun2008ICML');"><div class="publication_image"><img src="publications/Urtasun2008ICML.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Topologically-Constrained Latent Variable Models </b><br/>R. Urtasun, D. Fleet, A. Geiger, J. Popovic, T. Darrell and N. Lawrence <br/>International Conference on Machine learning (ICML), 2008</div></div>
<div class="publication_detail" id="Urtasun2008ICML" name="Urtasun2008ICML"><div class="publication_abstract"><b>Abstract:</b> In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Urtasun2008ICML.pdf" target="_blank">Urtasun2008ICML</a>,<br/>
&nbsp; author = {<a href="http://ttic.uchicago.edu/~rurtasun" target="blank">Raquel Urtasun</a> and <a href="http://www.cs.toronto.edu/~fleet" target="blank">David Fleet</a> and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and <a href="http://www.adobe.com/technology/people/seattle/jovan-popovic.html" target="blank">Jovan Popovic</a> and <a href="http://www.eecs.berkeley.edu/~trevor" target="blank">Trevor Darrell</a> and <a href="http://staffwww.dcs.shef.ac.uk/people/N.Lawrence" target="blank">Neil Lawrence</a>},<br/>
&nbsp; title = {Topologically-Constrained Latent Variable Models},<br/>&nbsp; booktitle = {International Conference on Machine learning (ICML)},<br/>
&nbsp; year = {2008}<br/>
}
</div>
<div class="publication_links"><a href="publications/Urtasun2008ICML.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=JDSOKHoxrqQ" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="publication_container" onclick="toggleDetails('Geiger2008');"><div class="publication_image"><img src="publications/Geiger2008.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>Human Body Tracking with Rank Priors for Non-Linear Dimensionality Reduction </b><br/>A. Geiger <br/>Masters Thesis, 2008</div></div>
<div class="publication_detail" id="Geiger2008" name="Geiger2008"><div class="publication_abstract"><b>Abstract:</b> Non-linear dimensionality reduction methods are powerful techniques to deal with high-dimensional datasets. However, they often are susceptible to local minima and perform poorly when initialized far from the global optimum, even when the intrinsic dimensionality is known a priori. In this work we introduce a prior over the dimensionality of the latent space, and simultaneously optimize both the latent space and its intrinsic dimensionality. Ad-hoc initialization schemes are unnecessary with our approach; we initialize the latent space to the observation space and automatically infer the latent dimensionality using an optimization scheme that drops dimensions in a continuous fashion. We report results applying our prior to various tasks involving probabilistic non-linear dimensionality reduction, and show that our method can outperform graph-based dimensionality reduction techniques as well as previously suggested ad-hoc initialization strategies.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@MASTERSTHESIS{<a href="https://www.cvlibs.net/publications/Geiger2008.pdf" target="_blank">Geiger2008</a>,<br/>
&nbsp; author = {<a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a>},<br/>
&nbsp; title = {Human Body Tracking with Rank Priors for Non-Linear Dimensionality Reduction},<br/>&nbsp; school = {Massachusetts Institute of Technology},<br/>
&nbsp; year = {2008}<br/>
}
</div>
<div class="publication_links"><a href="publications/Geiger2008.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=KeLtE537e_A" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/></div>
</div>
<div class="newline_10"></div> 
<div class="newline_bg"><h2>2006</h2></div>
<div class="publication_container" onclick="toggleDetails('Pilet2006ISMAR');"><div class="publication_image"><img src="publications/Pilet2006ISMAR.jpg" width="65" height="65"/></div>
<div class="publication_title"><b>An All-in-One Solution to Geometric and Photometric Calibration </b><br/>J. Pilet, A. Geiger, P. Lagger, V. Lepetit and P. Fua <br/>International Symposium on Mixed and Augmented Reality (ISMAR), 2006</div></div>
<div class="publication_detail" id="Pilet2006ISMAR" name="Pilet2006ISMAR"><div class="publication_abstract"><b>Abstract:</b> We propose a fully automated approach to calibrating multiple cameras whose fields of view may not all overlap. Our technique only requires waving an arbitrary textured planar pattern in front of the cameras, which is the only manual intervention that is required. The pattern is then automatically detected in the frames where it is visible and used to simultaneously recover geometric and photometric camera calibration parameters. In other words, even a novice user can use our system to extract all the information required to add virtual 3D objects into the scene and light them convincingly. This makes it ideal for Augmented Reality applications and we distribute the code under a GPL license.<br/><div class="newline_10"></div>
<b>Latex Bibtex Citation:</b><br/>@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Pilet2006ISMAR.pdf" target="_blank">Pilet2006ISMAR</a>,<br/>
&nbsp; author = {Julien Pilet and <a href="https://www.cvlibs.net" target="blank">Andreas Geiger</a> and Pascal Lagger and Vincent Lepetit and Pascal Fua},<br/>
&nbsp; title = {An All-in-One Solution to Geometric and Photometric Calibration},<br/>&nbsp; booktitle = {International Symposium on Mixed and Augmented Reality (ISMAR)},<br/>
&nbsp; year = {2006}<br/>
}
</div>
<div class="publication_links"><a href="publications/Pilet2006ISMAR.pdf" target="_blank"><div class="publication_icon"><img src="site/icon_pdf.png"/></div>Paper</a><br/><a href="http://www.youtube.com/watch?v=dG3WCXndSS8" target="_blank"><div class="publication_icon"><img src="site/icon_video.png"/></div>Video</a><br/><a href="http://cvlab.epfl.ch/software/bazar/index.php" target="_blank"><div class="publication_icon"><img src="site/icon_project.png"/></div>Project Page</a><br/></div>
</div>
<div class="newline_10"></div> 

<div class="tracker">
<div id="eXTReMe"><br><br><a href="http://extremetracking.com/open?login=votec">
<img src="http://t1.extreme-dm.com/i.gif" style="border: 0;"
height="38" width="41" id="EXim" alt="eXTReMe Tracker" /></a>
<script type="text/javascript"><!--
var EXlogin='votec' // Login
var EXvsrv='s10' // VServer
EXs=screen;EXw=EXs.width;navigator.appName!="Netscape"?
EXb=EXs.colorDepth:EXb=EXs.pixelDepth;
navigator.javaEnabled()==1?EXjv="y":EXjv="n";
EXd=document;EXw?"":EXw="na";EXb?"":EXb="na";
EXd.write("<img src=http://e1.extreme-dm.com",
"/"+EXvsrv+".g?login="+EXlogin+"&amp;",
"jv="+EXjv+"&amp;j=y&amp;srw="+EXw+"&amp;srb="+EXb+"&amp;",
"l="+escape(EXd.referrer)+" height=1 width=1>");//-->
</script><noscript><div id="neXTReMe"><img height="1" width="1" alt=""
src="http://e1.extreme-dm.com/s10.g?login=votec&amp;j=n&amp;jv=n" />
</div></noscript></div>
</div>

<div id="newline_5"></div>
<div id="footer">
© 2022 | 
Andreas Geiger | <a href="https://www.cvlibs.net">cvlibs.net</a> | <a href="http://www.csstemplateheaven.com" target="blank">css</a>
</div>
</div>

</body>
</html>


