<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        
        <meta name="keywords" content="Chen Change Loy, Ziwei Liu, Bo Dai, Yixin Cao, Super-resolution, Deep Learning, Computer Vision, Convolutional Neural Network, Enhancement, Inpainting, Knowledge Distillation, Restoration, GAN, Generative Adversarial Networks, Generation, Manipulation, Editing, Object Detection, Segmentation, Recognition, Continual Learning, Domain Generalization, Self-Supervised Learning, Singapore, NTU, Nanyang Technological University, SCSE" />
        
        <!-- Page Title -->
        <title>Publications | MMLab@NTU</title>

        <!-- Favicons -->
        <link rel="apple-touch-icon" sizes="180x180" href="./assets/img/favicons/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="./assets/img/favicons/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="./assets/img/favicons/favicon-16x16.png">
        <link rel="manifest" href="./assets/img/favicons/site.webmanifest">
        <link rel="mask-icon" href="./assets/img/favicons/safari-pinned-tab.svg" color="#f23838">
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff">

        <!-- Vendor Stylesheets -->
        <link href="https://fonts.googleapis.com/css?family=Oswald:300,400,500,700%7CRoboto:300,400,700" rel="stylesheet">
        <link href="./assets/vendor/material-design-iconic-font/dist/css/material-design-iconic-font.min.css" rel="stylesheet">
        <link href="./assets/vendor/jquery.mb.vimeo_player/dist/css/jquery.mb.vimeo_player.min.css" rel="stylesheet">
        <link href="./assets/vendor/@fortawesome/fontawesome-free/css/all.css" rel="stylesheet">

        <!-- Theme Stylesheets -->
        <link href="./assets/css/theme.css" rel="stylesheet">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-22940424-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-22940424-1');
        </script>
    </head>

    <body>
        <!-- Preloader -->
        <div class="preloader">
            <div class="spinner">
                <div class="circles"></div>
            </div>
        </div>
        <!-- End of Preloader -->


        <!-- Header -->
        <header class="spyre-navbar navbar navbar-expand-lg bg-secondary navbar-dark fixed-top align-items-center" data-transparent data-text-color="#ffffff">
            <div class="container">
                <a class="navbar-brand mr-lg-5 mr-xl-7" href="./index.html">
                    <img src="./assets/img/logo.png" class="d-none d-lg-block" alt="MMLab" width="183" />
                    <img src="./assets/img/logo.png" class="d-block d-lg-none" alt="MMLab" width="150" />
                </a>

                <!-â Desktop Menu -->
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav mr-auto">
                        <li class="pl-2 nav-item navbar-text"><a href="./index.html" class="nav-link text-400">Home</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="./team.html" class="nav-link text-400">Team</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="./research.html" class="nav-link text-400">Research</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="./publication_topic.html" class="nav-link">Publications</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="./downloads.html" class="nav-link text-400">Code | Datasets</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="./careers.html" class="nav-link text-400">Join Us</a></li>
                    </ul>
                </div>
                <!-â End of Desktop Menu -->

                <div class="menu-toggle d-block d-lg-none">
                    <div class="hamburger">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                    <div class="cross">
                        <span></span>
                        <span></span>
                    </div>
                </div>
            </div>

            <!-- Spyrenav Overlay -->
            <div class="spyre-navbar-overlay overlay-slide">
                <div class="container">
                    <div class="row">
                        <div class="spyre-navbar-nav-container col-md-6 col-lg-5 col-xl-4 bg-white ext-l">
                            <nav class="spyre-navbar-nav">
                                <ul class="spyre-nav">
                                    <li class="spyre-nav-item"><a href="./index.html" class="spyre-nav-link">Home</a></li>
                                    <li class="spyre-nav-item"><a href="./research.html" class="spyre-nav-link">Our Research</a></li>
                                    <li class="spyre-nav-item"><a href="./team.html" class="spyre-nav-link">Team</a></li>
                                    <li class="spyre-nav-item dropdown">
                                        <a href="#" class="spyre-nav-link dropdown-toggle" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications</a>
                                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                                            <li class="dropdown-menu-item"><a href="./publication_topic.html" class="dropdown-menu-link">By Topic</a></li>
                                            <li class="dropdown-menu-item"><a href="./publication_year.html" class="dropdown-menu-link">By Year</a></li>
                                        </ul>
                                    </li>
                                    <li class="spyre-nav-item"><a href="./downloads.html" class="spyre-nav-link">Code and Datasets</a></li>
                                    <li class="spyre-nav-item"><a href="./careers.html" class="spyre-nav-link">Join Us</a></li>
                                </ul>
                            </nav>
                        </div>
        
                        <div class="col-lg-7 col-xl-8 d-none d-md-block">
                            <div class="d-flex flex-column h-100">
                                <div class="d-flex h-100">
                                    <div class="align-self-center">
                                        <<div class="text-uppercase"
                                            data-background-text="computer vision"
                                            data-color="#7079a2"
                                            data-opacity="0.02"
                                            data-font-size="85px"
                                            data-font-weight="500"
                                            data-offset-x="-5%"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="mmlab"
                                            data-color="#7079a2"
                                            data-opacity="0.04"
                                            data-font-size="175px"
                                            data-font-weight="500"
                                            data-offset-x="29%"
                                            data-padding="7vh 0 2vh 0"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="deep learning"
                                            data-color="#7079a2"
                                            data-opacity="0.03"
                                            data-font-size="140px"
                                            data-font-weight="500"
                                            data-offset-x="15%"
                                            data-letter-spacing="5px"
                                        ></div>
                                    </div>
                                </div>
                                
                                <div class="mt-auto">
                                    <ul class="nav flex-nowrap float-right">
                                        <li class="nav-item">
                                            <a class="nav-link px-2" href="https://twitter.com/MMLabNTU" target="_blank">
                                                <i class="zmdi zmdi-twitter text-white"></i>
                                            </a>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- End of Spyrenav Overlay -->
        </header>
        <!-- End of Header -->


        <!-- Main Content -->
        <main class="main minh-100vh">
            <!-- Section -->
            <section class="py-0 overflow-hidden text-center">
                <div class="bg-container overlay parallax" data-rellax-percentage="0.5" style="background-image: url(./assets/img/backgrounds/bg-02.jpg)">
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section -->
            <section id="section-1" class="pb-0">
                <div class="container">
                    <span id="restoration"></span>
                    <div class="row">
                        <div class="col-lg-4 order-lg-1">
                            <div class="pb-6 pt-6 py-lg-3" data-toggle="sticky" data-sticky-offset-top="100">
                                <h5 class="mb-4 text-uppercase text-600">Topics</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="#restoration" class="text-600">Low-Level Vision</a></li>
                                    <li class="mb-1"><a href="#editing" class="text-600">Editing and Generation</a></li>
                                    <li class="mb-1"><a href="#detection" class="text-600">Image and Video Understanding</a></li>
                                    <li class="mb-1"><a href="#3D" class="text-600">3D Scene Understanding and Reconstruction</a></li>
                                    <li class="mb-1"><a href="#learning" class="text-600">Deep Learning</a></li>
                                    <li class="mb-1"><a href="#forensics" class="text-600">Media Forensics</a></li>
                                </ul>

                                <h5 class="mb-4 text-uppercase text-600">Sort by</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="./publication_year.html#section-1" class="text-600">Years</a></li>
                                </ul>

                                <h5 class="mb-4 text-uppercase text-600">Highlights</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="./conference/neurips2022/index.html" class="text-600">NeurIPS 2022</a></li>
                                    <li class="mb-1"><a href="./conference/eccv2022/index.html" class="text-600">ECCV 2022</a></li>
                                    <li class="mb-1"><a href="./conference/cvpr2022/index.html" class="text-600">CVPR 2022</a></li>
                                </ul>
                            </div>
                        </div>

                        <div class="col-lg-8 order-lg-2 pb-6 pb-lg-0 pt-lg-3"> 
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Low-Level Vision</h2>
                                <p class="text-700 fw-medium">Let's enhance! Our team has been working on various image/video restoration and enhancement problems such as super-resolution, denoising, low-light enhancement etc. Some notable methods developed by us include SRCNN, ESRGAN, EDVR, BasicVSR, GLEAN, CodeFormer and Zero-DCE. </p>
                                
                                <img src="./assets/img/backgrounds/bg-sr.jpg" class="mt-4 img-fluid w-100" alt="" />
                                <p></p>

                                <h4 class="mb-4 text-uppercase">Super-Resolution</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Reference-based Image and Video Super-Resolution via C<sup>2</sup>-Matching </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Jiang, K. C. K. Chan, X. Wang, C. C. Loy, Z. Liu <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023 <strong>(TPAMI)</strong><br />  
                                       </span>
                                       [<a href="https://arxiv.org/abs/2212.09581" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/yumingj/C2-Matching" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>   
                                    <li>
                                        <span class="text-primary">DifFace: Blind Face Restoration with Diffused Error Contraction </span> 
                                        <br />
                                        <span class="text-500">
                                        Z. Yue, C. C. Loy <br /> 
                                        Technical report, arXiv:2212.06512, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2212.06512" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/zsyOAOA/DifFace" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://huggingface.co/spaces/OAOA/DifFace" target="_blank"><span class="text-muted">Demo</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Towards Robust Blind Face Restoration with Codebook Lookup Transformer </span> 
                                        <br />
                                        <span class="text-500">
                                        S. Zhou, K. C. K. Chan, C. Li, C. C. Loy <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2206.11253" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://shangchenzhou.com/projects/CodeFormer/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://huggingface.co/spaces/sczhou/CodeFormer" target="_blank"><span class="text-muted">Demo</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond </span> 
                                       <br />
                                       <span class="text-500">
                                       K. C. K. Chan, X. Wang, X. Xu, J. Gu, C. C. Loy <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 <strong>(TPAMI)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2022.3186715" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2207.14812" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="./project/glean/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li> 
                                    <li>
                                       <span class="text-primary">Investigating Trade-offs in Real-World Video Super-Resolution </span> 
                                       <br />
                                       <span class="text-500">
                                       K. C. K. Chan, S. Zhou, X. Xu, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_Investigating_Tradeoffs_in_Real-World_Video_Super-Resolution_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2111.12704" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Chan_Investigating_Tradeoffs_in_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/ckkelvinchan/RealBasicVSR" target="_blank"><span class="text-muted">Project Page</span></a>]                                    </li>
                                    <li>
                                       <span class="text-primary">BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment </span> 
                                       <br />
                                       <span class="text-500">
                                       K. C. K. Chan, S. Zhou, X. Xu, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.13371" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Chan_BasicVSR_Improving_Video_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://ckkelvinchan.github.io/projects/BasicVSR++/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                   
                                    <li>
                                        <span class="text-primary">GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution </span> 
                                        <br />
                                        <span class="text-500">
                                        K. C. K. Chan, X. Wang, X. Xu, J. Gu, C. C. Loy <br /> 
                                        in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR, Oral)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_GLEAN_Generative_Latent_Bank_for_Large-Factor_Image_Super-Resolution_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2012.00739" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Chan_GLEAN_Generative_Latent_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/glean/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond </span> 
                                        <br />
                                        <span class="text-500">
                                        K. C. K. Chan, X. Wang, K. Yu, C. Dong, C. C. Loy <br /> 
                                        in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_BasicVSR_The_Search_for_Essential_Components_in_Video_Super-Resolution_and_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2012.02181" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Chan_BasicVSR_The_Search_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://ckkelvinchan.github.io/projects/BasicVSR" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Robust Reference-based Super-Resolution via C<sup>2</sup>-Matching </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Jiang, K. C. K. Chan, X. Wang, C. C. Loy, Z. Liu <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jiang_Robust_Reference-Based_Super-Resolution_via_C2-Matching_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2106.01863" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Jiang_Robust_Reference-Based_Super-Resolution_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/yumingj/C2-Matching" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                     
                                    <li>
                                       <span class="text-primary">Understanding Deformable Alignment in Video Super-Resolution </span> 
                                       <br />
                                       <span class="text-500">
                                       K. C. K. Chan, X. Wang, K. Yu, C. Dong, C. C. Loy <br /> 
                                       in Proceedings of AAAI Conference on Artificial Intelligence, 2021 <strong>(AAAI)</strong><br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2009.07265" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://ckkelvinchan.github.io/projects/DCN/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Cross-Scale Internal Graph Neural Network for Image Super-Resolution </span> 
                                       <br />
                                       <span class="text-500">
                                       S. Zhou, J. Zhang, W. Zuo, C. C. Loy  <br /> 
                                       in Proceedings of Neural Information Processing Systems, 2020 <strong>(NeurIPS)</strong> <br />    
                                       </span>
                                       [<a href="https://proceedings.neurips.cc/paper/2020/file/23ad3e314e2a2b43b4c720507cec0723-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2006.16673" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://papers.nips.cc/paper/2020/file/23ad3e314e2a2b43b4c720507cec0723-Supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/sczhou/IGNN" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Non-Local Recurrent Network for Image Restoration</span> 
                                       <br />
                                       <span class="text-500">
                                       D. Liu, B. Wen, Y. Fan, C. C. Loy, T. S. Huang  <br /> 
                                       in Proceedings of Neural Information Processing Systems, 2018 <strong>(NeurIPS)</strong> <br />    
                                       </span>
                                       [<a href="https://papers.nips.cc/paper/2018/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1806.02919" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/Ding-Liu/NLRN" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</span> 
                                       <br />
                                       <span class="text-500">
                                       X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, C. C. Loy <br /> 
                                       in Workshop Proceedings of European Conference on Computer Vision, 2018 <strong>(ECCVW)</strong></strong><br />  
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1809.00219" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/xinntao/ESRGAN" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>   
                                </ul>

                                <h4 class="mb-4 text-uppercase">Restoration | Enhancement</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Exploring CLIP for Assessing the Look and Feel of Images </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Wang, K. C. K. Chan, C. C. Loy <br /> 
                                        in Proceedings of AAAI Conference on Artificial Intelligence, 2023 <strong>(AAAI)</strong><br />   
                                        </span>
                                        [<a href="https://arxiv.org/abs/2207.12396" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/IceClear/CLIP-IQA" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies </span> 
                                        <br />
                                        <span class="text-500">
                                        S-Y. Li, Y. Li, B. Li, C. Dong, Z. Liu, C. C. Loy <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2211.05709" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://lisiyao21.github.io/projects/AnimeRun" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Flare7K: A Phenomenological Nighttime Flare Removal Dataset </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Dai, C. Li, S. Zhou, R. Feng, C. C. Loy <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2210.06570" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://nukaliad.github.io/projects/Flare7K" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Deep Fourier Up-Sampling </span> 
                                        <br />
                                        <span class="text-500">
                                        M. Zhou, H. Yu, J. Huang, F. Zhao, J. Gu, C. C. Loy, D. Meng, C. Li <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2210.05171" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://li-chongyi.github.io/FourierUp_files/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment </span> 
                                        <br />
                                        <span class="text-500">
                                        C. Li, C. Guo, R. Feng, S. Zhou, C. C. Loy <br /> 
                                        Technical report, arXiv:2207.14273, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2207.14273" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://li-chongyi.github.io/CuDi_files/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">LEDNet: Joint Low-light Enhancement and Deblurring in the Dark </span> 
                                        <br />
                                        <span class="text-500">
                                        S. Zhou, C. Li, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660562.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2202.03373" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660562-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://shangchenzhou.com/projects/LEDNet/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">On the Generalization of BasicVSR++ to Video Deblurring and Denoising </span> 
                                        <br />
                                        <span class="text-500">
                                        K. C. K. Chan, S. Zhou, X. Xu, C. C. Loy <br /> 
                                        Technical report, arXiv:2204.05308, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2204.05308" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://ckkelvinchan.github.io/projects/BasicVSR++/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Low-Light Image and Video Enhancement Using Deep Learning: A Survey </span> 
                                       <br />
                                       <span class="text-500">
                                       C. Li, C. Guo, L. Han, J. Jiang, M. Cheng, J. Gu, C. C. Loy <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021 <strong>(TPAMI)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2021.3126387" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.10729" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://www.mmlab-ntu.com/project/lliv_survey/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Temporally Consistent Video Colorization with Deep Feature Propagation and Self-regularization Learning </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Liu, H. Zhao, K. C. K. Chan, X. Wang, C. C. Loy, Y. Qiao, C. Dong  <br /> 
                                       Technical report, arXiv:2110.04562, 2021 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2110.04562" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation</span> 
                                       <br />
                                       <span class="text-500">
                                       X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, P. Luo  <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021 <strong>(TPAMI)</strong><br />   
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2021.3115428" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://github.com/XingangPan/deep-generative-prior" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">ReconfigISP: Reconfigurable Camera Image Processing Pipeline </span>
                                        <br />
                                        <span class="text-500">
                                        K. Yu, Z. Li, Y. Peng, C. C. Loy, J. Gu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_ReconfigISP_Reconfigurable_Camera_Image_Processing_Pipeline_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2109.04760" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Yu_ReconfigISP_Reconfigurable_Camera_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/reconfigisp/index.html" target="_blank"><span class="text-muted">Project Page</span></a>] 
                                    </li>
                                    <li>
                                       <span class="text-primary">Path-Restore: Learning Network Path Selection for Image Restoration</span> 
                                       <br />
                                       <span class="text-500">
                                       K. Yu, X. Wang, C. Dong, X. Tang, C. C. Loy  <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021 <strong>(TPAMI)</strong><br />   
                                       </span>
                                       [<a href="https://doi.org/10.1109/tpami.2021.3096255" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/1904.10343" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="./project/pathrestore/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                       
                                    <li>
                                       <span class="text-primary">Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation </span> 
                                       <br />
                                       <span class="text-500">
                                       C. Li, C. Guo, C. C. Loy <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021 <strong>(TPAMI)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2021.3063604" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2103.00860" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network </span> 
                                       <br />
                                       <span class="text-500">
                                       R. Feng, C. Li, H. Chen, S. Li, C. C. Loy, J. Gu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Removing_Diffraction_Image_Artifacts_in_Under-Display_Camera_via_Dynamic_Skip_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.09556" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Feng_Removing_Diffraction_Image_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://jnjaby.github.io/projects/UDC/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                     
                                    <li>
                                       <span class="text-primary">Deep Animation Video Interpolation in the Wild </span> 
                                       <br />
                                       <span class="text-500">
                                       S-Y. Li, S. Zhao, W. Yu, W. Sun, D. Metaxas, C. C. Loy, Z. Liu <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Siyao_Deep_Animation_Video_Interpolation_in_the_Wild_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.02495" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Siyao_Deep_Animation_Video_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/lisiyao21/AnimeInterp/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, P. Luo  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV, Oral)</strong><br />    
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470256.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2003.13659" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/XingangPan/deep-generative-prior" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Flexible Piecewise Curves Estimation for Photo Enhancement </span> 
                                       <br />
                                       <span class="text-500">
                                       C. Li, C. Guo, Q. Ai, S. Zhou, C. C. Loy <br /> 
                                       Technical report, arXiv:2010.13412, 2020 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2010.13412" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>                            
                                    <li>
                                       <span class="text-primary">Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement </span> 
                                       <br />
                                       <span class="text-500">
                                       C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, R. Cong  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2001.06826" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</span> 
                                       <br />
                                       <span class="text-500">
                                       X. Wang, C. K. Chan, K. Yu, C. Dong, X. Tang, C. C. Loy  <br /> 
                                       in Workshop Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, NTIRE, 2019 <strong>(CVPRW)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Wang_EDVR_Video_Restoration_With_Enhanced_Deformable_Convolutional_Networks_CVPRW_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1905.02716" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://xinntao.github.io/projects/EDVR" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Deep Network Interpolation for Continuous Imagery Effect Transition</span> 
                                       <br />
                                       <span class="text-500">
                                       X. Wang, K. Yu, C. Dong, X. Tang, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Deep_Network_Interpolation_for_Continuous_Imagery_Effect_Transition_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1811.10515" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://xinntao.github.io/projects/DNI" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>

                                <h4 class="mb-4 text-uppercase">Optical Flow Estimation</h4>
                                <ul>                                    
                                    <li>
                                       <span class="text-primary">LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation</span> 
                                       <br />
                                       <span class="text-500">
                                       T.-W. Hui, C. C. Loy  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV)</strong><br />  
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650171.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2007.09319" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650171-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/twhui/LiteFlowNet3" target="_blank"><span class="text-muted">Project Page</span>]</a></a>
                                    </li>
                                    <li>
                                       <span class="text-primary">A Lightweight Optical Flow CNN - Revisiting Data Fidelity and Regularization</span> 
                                       <br />
                                       <span class="text-500">
                                       T.-W. Hui, X. Tang, C. C. Loy  <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020 <strong>(TPAMI)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2020.2976928" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/1903.07414" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/twhui/LiteFlowNet2" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li> 
                                </ul> 
                                <span id="editing"></span>
                            </div>   

                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Editing and Generation</h2>
                                <p class="text-700 fw-medium">We like algorithms that could generate new visual contents, e.g., face generation, face reenactment, image inpainting, scene de-occlusion, etc. </p>

                                <img src="./assets/img/backgrounds/bg-edit.jpg" class="mt-4 img-fluid w-100" alt="" />
                                <p></p>

                                <h4 class="mb-4 text-uppercase">Face Editing</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">VToonify: Controllable High-Resolution Portrait Video Style Transfer </span> 
                                        <br />
                                        <span class="text-500">
                                        S. Yang, L. Jiang, Z. Liu, C. C. Loy <br /> 
                                        ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH ASIA - TOG)</strong><br />    
                                        </span>
                                        [<a href="https://dl.acm.org/doi/10.1145/3550454.3555437" target="_blank"><span class="text-muted">DOI</span></a>]
                                        [<a href="https://arxiv.org/abs/2209.11224" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="./project/vtoonify/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=0_OmVhDgYuY" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Audio-driven Dubbing for User Generated Contents via Style-aware Semi-parametric Synthesis</span> 
                                       <br />
                                       <span class="text-500">
                                       L. Song, W. Wu, C. Fu, C. C. Loy, R. He  <br /> 
                                       IEEE Transactions on Circuits and Systems for Video Technology, 2022 <strong>(TCVST)</strong><br />
                                       </span>
                                       [<a href="https://doi.org/10.1109/TCSVT.2022.3210002" target="_blank"><span class="text-muted">DOI</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset </span> 
                                        <br />
                                        <span class="text-500">
                                        H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670641.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.12393" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670641-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://celebv-hq.github.io/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Xu, Y. Yin, L. Jiang, Q. Wu, C. Zheng, C. C. Loy, B. Dai, W. Wu <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_TransEditor_Transformer-Based_Dual-Space_GAN_for_Highly_Controllable_Facial_Editing_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.17266" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Xu_TransEditor_Transformer-Based_Dual-Space_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://billyxyb.github.io/TransEditor/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Talking Faces: Audio-to-Video Face Generation</span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Wang, L. Song, W. Wu, C. Qian, R. He, C. C. Loy <br /> 
                                        In C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, C. Busch (Eds.), Handbook of Digital Face Manipulation and Detection, Springer, 2022<br /> 
                                        </span>
                                        [<a href="https://link.springer.com/book/10.1007/978-3-030-87664-7" target="_blank"><span class="text-muted">Book Link</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Everybodyâs Talkinâ: Let Me Talk as You Want </span> 
                                       <br />
                                       <span class="text-500">
                                       L. Song, W. Wu, C. Qian, R. He, C. C. Loy  <br /> 
                                       IEEE Transactions on Information Forensics and Security, 2022 <strong>(TIFS)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TIFS.2022.3146783" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2001.05201" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://wywu.github.io/projects/EBT/EBT.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li> 
                                    <li>
                                        <span class="text-primary">Talk-to-Edit: Fine-Grained Facial Editing via Dialog </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Jiang,  Z. Huang, X. Pan, C. C. Loy, Z. Liu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Talk-To-Edit_Fine-Grained_Facial_Editing_via_Dialog_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2109.04425" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Jiang_Talk-To-Edit_Fine-Grained_Facial_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/talkedit/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation </span> 
                                       <br />
                                       <span class="text-500">
                                       H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, Z. Liu <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Representation_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.11116" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhou_Pose-Controllable_Talking_Face_CVPR_2021_supplemental.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                               
                                    <li>
                                       <span class="text-primary">Pareidolia Face Reenactment </span> 
                                       <br />
                                       <span class="text-500">
                                       L. Song, W. Wu, C. Fu, C. Qian, C. C. Loy, R. He  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Pareidolia_Face_Reenactment_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.03061" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Song_Pareidolia_Face_Reenactment_CVPR_2021_supplemental.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://wywu.github.io/projects/ETT/ETT.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="https://www.youtube.com/watch?v=lVYZ3IAVM_U" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li> 
                                    <li>
                                       <span class="text-primary">Audio-Driven Emotional Video Portraits </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Ji, H. Zhou, K. Wang, W. Wu, X. Cao, C. C. Loy, F. Xu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Audio-Driven_Emotional_Video_Portraits_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.07452" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Ji_Audio-Driven_Emotional_Video_CVPR_2021_supplemental.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://jixinya.github.io/projects/evp/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>       
                                    <li>
                                       <span class="text-primary">MEAD: A Large-scale Audio-visual Dataset for Emotional Talking Face Generation</span> 
                                       <br />
                                       <span class="text-500">
                                       K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, C. C. Loy  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV)</strong><br />  
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698-supp.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://wywu.github.io/projects/MEAD/MEAD.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">One-shot Face Reenactment</span> 
                                       <br />
                                       <span class="text-500">
                                       Y.  Zhang, S. Zhang, Y. He, C. Li, C. C. Loy, Z. Liu  <br /> 
                                       in Proceedings of British Machine Vision Conference, 2019 <strong>(BMVC, Spotlight)</strong><br />  
                                       </span>
                                       [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/bmvc_2019_one.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1908.03251" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://wywu.github.io/projects/ReenactGAN/OneShotReenact.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Instance-level Facial Attributes Transfer with Geometry-aware Flow</span> 
                                       <br />
                                       <span class="text-500">
                                       W. Yin, Z. Liu, C. C. Loy  <br /> 
                                       in Proceedings of AAAI Conference on Artificial Intelligence, 2019 <strong>(AAAI, Spotlight)</strong><br />  
                                       </span>
                                       [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2019_instance.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1811.12670" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://mmlab.ie.cuhk.edu.hk/projects/attribute-transfer/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">ReenactGAN: Learning to Reenact Faces via Boundary Transfer</span> 
                                       <br />
                                       <span class="text-500">
                                       W. Wu, Y. Zhang, C. Li, C. Qian, C. C. Loy <br /> 
                                       in Proceedings of European Conference on Computer Vision, 2018 <strong>(ECCV)</strong></strong><br />  
                                       </span>
                                       [<a href="https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Wayne_Wu_Learning_to_Reenact_ECCV_2018_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1807.11079" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="https://www.youtube.com/watch?v=LBAfeKrHMys" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                </ul>

                                <h4 class="mb-4 text-uppercase">Image and Video Generation</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Text2Light: Zero-Shot Text-Driven HDR Panorama Generation </span> 
                                        <br />
                                        <span class="text-500">
                                        Z. Chen, G. Wang, Z. Liu <br /> 
                                        ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH ASIA - TOG)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2209.09898" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://frozenburning.github.io/projects/text2light/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=XDx6tOHigPE" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3 </span> 
                                        <br />
                                        <span class="text-500">
                                        H. Qiu, Y. Jiang, H. Zhou, W. Wu, Z. Liu <br /> 
                                        Technical report, arXiv:2208.07862, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2208.07862" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="http://haonanqiu.com/projects/StyleFaceV.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis </span> 
                                        <br />
                                        <span class="text-500">
                                        D. Moltisanti, J. Wu, B. Dai, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680321.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.10120" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680321-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://github.com/dmoltisanti/brace" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">StyleLight: HDR Panorama Generation for Lighting Estimation and Editing </span> 
                                        <br />
                                        <span class="text-500">
                                        G. Wang, Y. Yang, C. C. Loy, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750474.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.14811" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750474-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://style-light.github.io/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis </span> 
                                        <br />
                                        <span class="text-500">
                                        L. Zhuo, G. Wang, S. Li, W. Wu, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750285.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.05049" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750285-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://fast-vid2vid.github.io/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">StyleGAN-Human: A Data-Centric Odyssey of Human Generation </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Fu, S. Li, Y. Jiang, K.-Y. Lin, C. Qian, C. C. Loy, W. Wu, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136760001.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2204.11823" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136760001-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://stylegan-human.github.io/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=nIrb9hwsdcI" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Text2Human: Text-Driven Controllable Human Image Generation </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Jiang, S. Yang, H. Qiu, W. Wu, C. C. Loy, Z. Liu <br /> 
                                        ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH - TOG)</strong><br />    
                                        </span>
                                        [<a href="https://dl.acm.org/doi/abs/10.1145/3528223.3530104" target="_blank"><span class="text-muted">DOI</span></a>]
                                        [<a href="https://arxiv.org/abs/2205.15996" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://yumingj.github.io/projects/Text2Human.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=yKh4VORA_E0" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer </span> 
                                       <br />
                                       <span class="text-500">
                                       S. Yang, L. Jiang, Z. Liu, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Pastiche_Master_Exemplar-Based_High-Resolution_Portrait_Style_Transfer_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.13248" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Yang_Pastiche_Master_Exemplar-Based_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="./project/dualstylegan/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="https://www.youtube.com/watch?v=scZTu77jixI" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Unsupervised Image-to-Image Translation with Generative Prior </span> 
                                       <br />
                                       <span class="text-500">
                                       S. Yang, L. Jiang, Z. Liu, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unsupervised_Image-to-Image_Translation_With_Generative_Prior_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2204.03641" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Yang_Unsupervised_Image-to-Image_Translation_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="./project/gpunit/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory </span> 
                                       <br />
                                       <span class="text-500">
                                       S-Y. Li, W. Yu, T. Gu, C. Lin, Q. Wang, C. Qian, C. C. Loy, Z. Liu <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Siyao_Bailando_3D_Dance_Generation_by_Actor-Critic_GPT_With_Choreographic_Memory_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.13055" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Siyao_Bailando_3D_Dance_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="./project/bailando/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="https://www.youtube.com/watch?v=YbXOcuMTzD8" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Full-Range Virtual Try-On with Recurrent Tri-Level Transformation </span>
                                       <br />
                                       <span class="text-500">
                                       H. Yang, X. Yu, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Full-Range_Virtual_Try-On_With_Recurrent_Tri-Level_Transform_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Yang_Full-Range_Virtual_Try-On_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Towards Diverse and Natural Scene-aware 3D Human Motion Synthesis </span>
                                       <br />
                                       <span class="text-500">
                                       J. Wang, Y. Rong, J. Liu, S. Yan, D. Lin, B. Dai  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Towards_Diverse_and_Natural_Scene-Aware_3D_Human_Motion_Synthesis_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2205.13001" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_Towards_Diverse_and_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks </span> 
                                       <br />
                                       <span class="text-500">
                                       W. Zhu, Z. Yang, Z. Di, W. Wu, Y. Wang, C. C. Loy <br /> 
                                       in Proceedings of AAAI Conference on Artificial Intelligence, 2022 <strong>(AAAI)</strong><br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2112.10082" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://yzhq97.github.io/mocanet/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">The Nuts and Bolts of Adopting Transformer in GANs </span> 
                                       <br />
                                       <span class="text-500">
                                       R. Xu and X. Xu and K. Chen and B. Zhou and C. C. Loy  <br /> 
                                       Technical report, arXiv:2110.13107, 2021 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2110.13107" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://nbei.github.io/stransgan.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data </span> 
                                        <br />
                                        <span class="text-500">
                                        L. Jiang, B. Dai, W. Wu, C. C. Loy <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://papers.nips.cc/paper/2021/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2111.06849" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="./project/apa/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=3Luz817WpZM" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Focal Frequency Loss for Image Reconstruction and Synthesis </span> 
                                        <br />
                                        <span class="text-500">
                                        L. Jiang, B. Dai, W. Wu, C. C. Loy <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Focal_Frequency_Loss_for_Image_Reconstruction_and_Synthesis_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2012.12821" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Jiang_Focal_Frequency_Loss_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/ffl/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">BlockPlanner: City Block Generation with Vectorized Graph Representation </span> 
                                        <br />
                                        <span class="text-500">
                                        L. Xu, Y. Xiangli, A. Rao, N. Zhao, B. Dai, Z. Liu, D. Lin <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_BlockPlanner_City_Block_Generation_With_Vectorized_Graph_Representation_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Xu_BlockPlanner_City_Block_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>] 
                                    </li>
                                    <li>
                                       <span class="text-primary">Positional Encoding as Spatial Inductive Bias in GANs </span> 
                                       <br />
                                       <span class="text-500">
                                       R. Xu, X. Wang, K. Chen, B. Zhou, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />   
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Positional_Encoding_As_Spatial_Inductive_Bias_in_GANs_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2012.05217" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Xu_Positional_Encoding_As_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://nbei.github.io/gan-pos-encoding.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="https://www.youtube.com/watch?v=n6B01YqC1ng&feature=emb_title" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Scene-aware Generative Network for Human Motion Synthesis </span> 
                                       <br />
                                       <span class="text-500">
                                       J. Wang, S. Yan, B. Dai, D. Lin <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Scene-Aware_Generative_Network_for_Human_Motion_Synthesis_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Scene-Aware_Generative_Network_CVPR_2021_supplemental.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="http://wangjingbo.top/papers/CVPR2021_PoseGeneration/Posegeneration.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Pan, B. Dai, Z. Liu, C. C. Loy, P. Luo <br /> 
                                       International Conference on Learning Representations, 2021 <strong>(ICLR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openreview.net/pdf?id=FGqiDsBUKL0" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2011.00844" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://xingangpan.github.io/projects/GAN2Shape.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Texture Memory-Augmented Deep Patch-Based Image Inpainting </span> 
                                       <br />
                                       <span class="text-500">
                                       R. Xu, M. Guo, J. Wang, X. Li, B. Zhou, C. C. Loy <br /> 
                                       IEEE Transactions on Image Processing, 2021 <strong>(TIP)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TIP.2021.3122930" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2009.13240" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>                    
                                    <li>
                                       <span class="text-primary">Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, P. Luo  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV, Oral)</strong><br />    
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470256.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2003.13659" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/XingangPan/deep-generative-prior" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">TSIT: A Simple and Versatile Framework for Image-to-Image Translation</span> 
                                       <br />
                                       <span class="text-500">
                                       L. Jiang, C. Zhang, M. Huang, C. Liu, J. Shi, C. C. Loy  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV, Spotlight)</strong><br />  
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480205.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2007.12072" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480205-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/EndlessSora/TSIT" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Self-Supervised Scene De-occlusion </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Zhan, X. Pan, B. Dai, Z. Liu, D. Lin, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR, Oral)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Self-Supervised_Scene_De-Occlusion_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2004.02788" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Zhan_Self-Supervised_Scene_De-Occlusion_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://xiaohangzhan.github.io/projects/deocclusion/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="https://www.youtube.com/watch?v=xIHCyyaB5gU&feature=emb_title" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting </span> 
                                       <br />
                                       <span class="text-500">
                                       Z. Yang, W. Zhu, W. Wu, C. Qian, Q. Zhou, B. Zhou, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_TransMoMo_Invariance-Driven_Unsupervised_Video_Motion_Retargeting_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2003.14401" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yang_TransMoMo_Invariance-Driven_Unsupervised_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://yzhq97.github.io/transmomo/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Real or Not Real, That is the Question </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Xiangli, Y. Deng, B. Dai, C. C. Loy, D. Lin  <br /> 
                                       International Conference on Learning Representations, 2020 <strong>(ICLR, Spotlight)</strong><br />    
                                       </span>
                                       [<a href="https://openreview.net/pdf?id=B1lPaCNtPB" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://github.com/kam1107/RealnessGAN" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">High-Quality Video Generation from Static Structural Annotations</span> 
                                       <br />
                                       <span class="text-500">
                                       L. Sheng, J. Pan, J. Guo, J. Shao, C. C. Loy   <br /> 
                                       International Journal of Computer Vision, 2020 <strong>(IJCV)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1007/s11263-020-01334-x" target="_blank"><span class="text-muted">DOI</span></a>]
                                    </li> 
                                    <li>
                                       <span class="text-primary">TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</span> 
                                       <br />
                                       <span class="text-500">
                                       W. Wu, K. Cao, C. Li, C. Qian, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_TransGaGa_Geometry-Aware_Unsupervised_Image-To-Image_Translation_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1904.09571" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Wu_TransGaGa_Geometry-Aware_Unsupervised_CVPR_2019_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://wywu.github.io/projects/TGaGa/TGaGa.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Deep Flow-Guided Video Inpainting</span> 
                                       <br />
                                       <span class="text-500">
                                       R. Xu, X. Li, B. Zhou, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Deep_Flow-Guided_Video_Inpainting_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1905.02884" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://nbei.github.io/video-inpainting.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Dense Intrinsic Appearance Flow for Human Pose Transfer</span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Li, C. Huang, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Dense_Intrinsic_Appearance_Flow_for_Human_Pose_Transfer_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Li_Dense_Intrinsic_Appearance_CVPR_2019_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="http://mmlab.ie.cuhk.edu.hk/projects/pose-transfer/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Disentangling Content and Style via Unsupervised Geometry Distillation</span> 
                                       <br />
                                       <span class="text-500">
                                       W. Wu, K. Cao, C. Li, C. Qian, C. C. Loy  <br /> 
                                       International Conference on Learning Representations Workshop, 2019 <strong>(ICLRW)</strong> <br />    
                                       </span>
                                       [<a href="https://openreview.net/pdf?id=SkgsQ8LK_E" target="_blank"><span class="text-muted">PDF</span></a>]
                                    </li>                                    
                                </ul>

                                <h4 class="mb-4 text-uppercase">Simulation</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Transformer with Implicit Edges for Particle-based Physics Simulation </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Shao, C. C. Loy, B. Dai <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136790539.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.10860" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.mmlab-ntu.com/project/tie/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>

                                <h4 class="mb-4 text-uppercase">Visual and Sound</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation </span>
                                       <br />
                                       <span class="text-500">
                                       X. Liu, Q. Wu, H. Zhou, Y. Xu, R. Qian, X. Lin, X. Zhou, W. Wu, B. Dai, B. Zhou  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Hierarchical_Cross-Modal_Association_for_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.13161" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Liu_Learning_Hierarchical_Cross-Modal_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://alvinliu0.github.io/projects/HA2G" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Visual Sound Localization in-the-Wild by Cross-Modal Interference Erasing </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Liu, R. Qian, H. Zhou, W. lin, Z. Liu, B. Zhou, X. Zhou <br /> 
                                       in Proceedings of AAAI Conference on Artificial Intelligence, 2022 <strong>(AAAI)</strong><br />    
                                       </span>
                                       [<a href="https://www.aaai.org/AAAI22Papers/AAAI-140.LiuX.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2202.06406" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">SepFusion: Finding Optimal Fusion Structures for Visual Sound Separation </span> 
                                       <br />
                                       <span class="text-500">
                                       D. Zhou, X. Zhou, D. Hu, H. Zhou, L. Bai, Z. Liu, W. Ouyang <br /> 
                                       in Proceedings of AAAI Conference on Artificial Intelligence, 2022 <strong>(AAAI)</strong><br />    
                                       </span>
                                       [<a href="https://www.aaai.org/AAAI22Papers/AAAI-11517.ZhouD.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                    </li> 
                                    <li>
                                       <span class="text-primary">Visually Informed Binaural Audio Generation without Binaural Audios </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Xu, H. Zhou, Z. Liu, B. Dai, X. Wang, D. Lin <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Visually_Informed_Binaural_Audio_Generation_without_Binaural_Audios_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.06162" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Xu_Visually_Informed_Binaural_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://sheldontsui.github.io/projects/PseudoBinaural" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>
                                <span id="detection"></span>
                            </div>  

                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Image and Video Understanding</h2>
                                <p class="text-700 fw-medium">We explore effective and efficient methods to detect, segment and recognize objects in complex scenes.</p>

                                <img src="./assets/img/backgrounds/bg-seg.jpg" class="mt-4 img-fluid w-100" alt="" />
                                <p></p>

                                <h4 class="mb-4 text-uppercase">Image Recognition</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. He, Ge. Huang, S. Chen, J. Teng, K. Wang, Z. Yin, L. Sheng, Z. Liu, Y. Qiao, J. Shao <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860495.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2203.08764" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860495-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Benchmarking Omni-Vision Representation through the Lens of Visual Realms </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zhang, Z. Yin, J. Shao, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670587.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.07106" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670587-supp.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>][<a href="https://arxiv.org/abs/2207.07106" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://zhangyuanhan-ai.github.io/OmniBenchmark/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Panoptic Scene Graph Generation </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Yang, Y. Z. Ang, Z. Guo, K. Zhou, W. Zhang, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870175.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.11247" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870175-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://psgdataset.org/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zhang, Q. Sun, Y. Zhou, Z. He, Z. Yin, K. Wang, L. Sheng, Y. Qiao, J. Shao, Z. Liu <br /> 
                                        Technical report, arXiv:2203.07845, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2203.07845" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/Davidzhangyuanhan/Bamboo" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Incorporating Convolution Designs into Visual Transformers </span> 
                                        <br />
                                        <span class="text-500">
                                        K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, W. Wu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Incorporating_Convolution_Designs_Into_Visual_Transformers_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2103.11816" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Differentiable Dynamic Wirings for Neural Networks </span> 
                                        <br />
                                        <span class="text-500">
                                        K. Yuan, Q. Li,  S. Guo, D. Chen, A. Zhou, F. Yu, Z. Liu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Differentiable_Dynamic_Wirings_for_Neural_Networks_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>] 
                                    </li>
                                </ul>

                                <h4 class="mb-4 text-uppercase">Object Detection</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Semi-Supervised and Long-Tailed Object Detection with CascadeMatch </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Zang, K. Zhou, C. Huang, C. C. Loy  <br /> 
                                       International Journal of Computer Vision, 2022 <strong>(IJCV)</strong><br />   
                                       </span>
                                       [<span class="text-muted">Coming Soon</span>]
                                    </li> 
                                    <li>
                                        <span class="text-primary">Open-Vocabulary DETR with Conditional Matching </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zang, W. Li, K. Zhou, C. Huang, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV, Oral)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690107.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2203.11876" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690107-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/ovdetr/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Few-Shot Object Detection via Association and Discrimination </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Cao, J. Wang, Y. Jin, T. Wu, K. Chen, Z. Liu, D. Lin <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://papers.nips.cc/paper/2021/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2111.11656" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/yhcao6/FADI" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">CARAFE++: Unified Content-Aware ReAssembly of FEatures</span> 
                                       <br />
                                       <span class="text-500">
                                       J. Wang, K. Chen, R. Xu, Z. Liu, C. C. Loy, D. Lin  <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021 <strong>(TPAMI)</strong> <br />  
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2021.3074370" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2012.04733" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Side-Aware Boundary Localization for More Precise Object Detection</span> 
                                       <br />
                                       <span class="text-500">
                                       J. Wang, W. Zhang, Y. Cao, K. Chen, J. Pang, T. Gong, J. Shi, C. C. Loy, D. Lin  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV)</strong><br />  
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490392.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1912.04260" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490392-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</span> 
                                       <br />
                                       <span class="text-500">
                                       C. Li, R. Cong, Y. Piao, Q. Xu, C. C. Loy  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV)</strong><br />  
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2007.07051" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://li-chongyi.github.io/Proj_ECCV20" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Feature Pyramid Grids </span> 
                                       <br />
                                       <span class="text-500">
                                       K. Chen, Y. Cao, C. C. Loy, D. Lin, C. Feichtenhofer  <br /> 
                                       Technical report, arXiv:2004.03580, 2020 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2004.03580" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>                                     
                                    <li>
                                       <span class="text-primary">Prime Sample Attention in Object Detection</span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Cao, K. Chen, C. C. Loy, D. Lin  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_Prime_Sample_Attention_in_Object_Detection_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1904.04821" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Cao_Prime_Sample_Attention_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/open-mmlab/mmdetection" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">CARAFE: Content-Aware ReAssembly of FEatures</span> 
                                       <br />
                                       <span class="text-500">
                                       J. Wang, K. Chen, R. Xu, Z. Liu, C. C. Loy, D. Lin  <br /> 
                                       in Proceedings of International Conference on Computer Vision, 2019 <strong>(ICCV, Oral)</strong> <br />  
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1905.02188" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Wang_CARAFE_Content-Aware_ReAssembly_ICCV_2019_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Region Proposal by Guided Anchoring</span> 
                                       <br />
                                       <span class="text-500">
                                       J. Wang, K. Chen, S. Yang, C. C. Loy, D. Lin  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Region_Proposal_by_Guided_Anchoring_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1901.03278" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/open-mmlab/mmdetection/blob/master/configs/guided_anchoring/README.md" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul> 

                                <h4 class="mb-4 text-uppercase">Semantic Segmentation</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">LaserMix for Semi-Supervised LiDAR Semantic Segmentation </span> 
                                        <br />
                                        <span class="text-500">
                                        L. Kong, J. Ren, L. Pan, Z. Liu <br /> 
                                        Technical report, arXiv:2207.00026, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2207.00026" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Video K-Net: A Simple, Strong, and Unified Baseline For End-to-End Dense Video Segmentation </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Li, W. Zhang, J. Pang, K. Chen, G. Cheng, Y. Tong, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Video_K-Net_A_Simple_Strong_and_Unified_Baseline_for_Video_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2204.04656" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_Video_K-Net_A_CVPR_2022_supplemental.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/lxtGH/Video-K-Net" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets </span> 
                                        <br />
                                        <span class="text-500">
                                        K. T. R. Voo, L. Jiang, C. C. Loy <br /> 
                                        in Workshop Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, VDU, 2022 <strong>(CVPRW)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Voo_Delving_Into_High-Quality_Synthetic_Face_Occlusion_Segmentation_Datasets_CVPRW_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2205.06218" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/CVPR2022W/VDU/supplemental/Voo_Delving_Into_High-Quality_CVPRW_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://github.com/kennyvoo/face-occlusion-generation" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">K-Net: Towards Unified Image Segmentation </span> 
                                       <br />
                                       <span class="text-500">
                                       W. Zhang, J. Pang, K. Chen, C. C. Loy <br /> 
                                       in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                       </span>
                                       [<a href="https://papers.nips.cc/paper/2021/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2106.14855" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="./project/knet/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zang, C. Huang, C. C. Loy <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zang_FASA_Feature_Augmentation_and_Sampling_Adaptation_for_Long-Tailed_Instance_Segmentation_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2102.12867" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Zang_FASA_Feature_Augmentation_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/fasa/index.html" target="_blank"><span class="text-muted">Project Page</span></a>] 
                                    </li>            
                                    <li>
                                       <span class="text-primary">Seesaw Loss for Long-Tailed Instance Segmentation </span> 
                                       <br />
                                       <span class="text-500">
                                       J. Wang, W. Zhang, Y. Zang, Y. Cao, J. Pang, T. Gong, K. Chen, Z. Liu, C. C. Loy, D. Lin  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Seesaw_Loss_for_Long-Tailed_Instance_Segmentation_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2008.10032" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Seesaw_Loss_for_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Hybrid Task Cascade for Instance Segmentation</span> 
                                       <br />
                                       <span class="text-500">
                                       K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, C. C. Loy, D. Lin  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid_Task_Cascade_for_Instance_Segmentation_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1901.07518" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://mmlab.ie.cuhk.edu.hk/projects/HybridTaskCascade/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation</span> 
                                       <br />
                                       <span class="text-500">
                                       X. Li, C. C. Loy <br /> 
                                       in Proceedings of European Conference on Computer Vision, 2018 <strong>(ECCV)</strong></strong><br />  
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaoxiao_Li_Video_Object_Segmentation_ECCV_2018_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1803.04242" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">PSANet: Point-wise Spatial Attention Network for Scene Parsing</span> 
                                       <br />
                                       <span class="text-500">
                                       H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, J. Jia <br /> 
                                       in Proceedings of European Conference on Computer Vision, 2018 <strong>(ECCV)</strong></strong><br />  
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://hszhao.github.io/projects/psanet/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>  

                                <h4 class="mb-4 text-uppercase">Tracking and Association</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">TCTrack: Temporal Contexts for Aerial Tracking </span>
                                       <br />
                                       <span class="text-500">
                                       Z. Cao, Z. Huang, L. Pan, S. Zhang, Z. Liu, C. Fu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_TCTrack_Temporal_Contexts_for_Aerial_Tracking_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.01885" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Cao_TCTrack_Temporal_Contexts_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/vision4robotics/TCTrack" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">MessyTable: Instance Association in Multiple Camera Views</span> 
                                       <br />
                                       <span class="text-500">
                                       Z. Cai, J. Zhang, D. Ren, C. Yu, H. Zhao, S. Yi, C. K. Yeo, C. C. Loy  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV)</strong><br />  
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560001.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2007.14878" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560001-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://caizhongang.github.io/projects/MessyTable/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Robust Multi-Modality Multi-Object Tracking</span> 
                                       <br />
                                       <span class="text-500">
                                       W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, C. C. Loy  <br /> 
                                       in Proceedings of International Conference on Computer Vision, 2019 <strong>(ICCV)</strong> <br />
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Robust_Multi-Modality_Multi-Object_Tracking_ICCV_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1909.03850" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/ZwwWayne/mmMOT" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>  
                                </ul>

                                <h4 class="mb-4 text-uppercase">Action Recognition</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition </span>
                                       <br />
                                       <span class="text-500">
                                       Y. Xu, F. Wei, X. Sun, C. Yang, Y. Shen, B. Dai, B. Zhou, S. Lin  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Cross-Model_Pseudo-Labeling_for_Semi-Supervised_Action_Recognition_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2112.09690" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Xu_Cross-Model_Pseudo-Labeling_for_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://justimyhxu.github.io/projects/cmpl/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Revisiting Skeleton-based Action Recognition </span>
                                       <br />
                                       <span class="text-500">
                                       H. Duan, Y. Zhao, K. Chen, D. Lin, B. Dai  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.13586" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Duan_Revisiting_Skeleton-Based_Action_CVPR_2022_supplemental.zip" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/kennymckormick/pyskl" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">TAda! Temporally-Adaptive Convolutions for Video Understanding </span> 
                                       <br />
                                       <span class="text-500">
                                       Z. Huang, S. Zhang, L. Pan, Z. Qing, M. Tang, Z. Liu, M. H. Ang Jr  <br /> 
                                       International Conference on Learning Representations, 2022 <strong>(ICLR)</strong><br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2110.06178" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                </ul>                              
                                <span id="3D"></span>
                            </div> 

                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">3D Scene Understanding and Reconstruction</h2>
                                <p class="text-700 fw-medium">Our team has been working on various tasks related to 3D reconstruction and perception, e.g, 3D shape generation and 3D human recovery</p>

                                <img src="./assets/img/backgrounds/bg-3d.jpg" class="mt-4 img-fluid w-100" alt="" />
                                <p></p>

                                <h4 class="mb-4 text-uppercase">3D Reconstruction | Generation</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Correspondence Distillation from NeRF-based GAN </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Lan, C. C. Loy, B. Dai <br /> 
                                        Technical report, arXiv:2212.09735, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2212.09735" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Lan, X. Meng, S. Yang, C. C. Loy, B. Dai <br /> 
                                        Technical report, arXiv:2212.07409, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2212.07409" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">EVA3D: Compositional 3D Human Generation from 2D Image Collections </span> 
                                        <br />
                                        <span class="text-500">
                                        F. Hong, Z. Chen, Y. Lan, L. Pan, Z. Liu <br /> 
                                        Technical report, arXiv:2210.04888, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2210.04888" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://hongfz16.github.io/projects/EVA3D.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=JNV0FJ0aDWM" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model </span> 
                                        <br />
                                        <span class="text-500">
                                        M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, Z. Liu <br /> 
                                        Technical report, arXiv:2208.15001, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2208.15001" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://mingyuan-zhang.github.io/projects/MotionDiffuse.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Relighting4D: Neural Relightable Human from Videos </span> 
                                        <br />
                                        <span class="text-500">
                                        Z. Chen, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136740589.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.07104" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136740589-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://frozenburning.github.io/projects/relighting4d/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Monocular 3D Object Reconstruction with GAN Inversion </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Zhang, D. Ren, Z. Cai, C. K. Yeo, B. Dai, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610665.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.10061" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610665-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/meshinversion/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling </span> 
                                        <br />
                                        <span class="text-500">
                                        Z. Cai, D. Ren, A. Zeng, Z. Lin, T. Yu, W. Wang, X. Fan, Y. Gao, Y. Yu, L. Pan, F. Hong, M. Zhang, C. C. Loy, L. Yang, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV, Oral)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2204.13686" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://caizhongang.github.io/projects/HuMMan/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                        [<a href="https://www.youtube.com/watch?v=Q_lxIrV3UgE" target="_blank"><span class="text-muted">YouTube</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars </span> 
                                        <br />
                                        <span class="text-500">
                                        F. Hong, M. Zhang, L. Pan, Z. Cai, L. Yang, Z. Liu <br /> 
                                        ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH - TOG)</strong><br />    
                                        </span>
                                        [<a href="https://dl.acm.org/doi/10.1145/3528223.3530094" target="_blank"><span class="text-muted">DOI</span></a>]
                                        [<a href="https://arxiv.org/abs/2205.08535" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://hongfz16.github.io/projects/AvatarCLIP.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Chasing the Tail in Monocular 3D Human Reconstruction with Prototype Memory </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Rong, Z. Liu, C. C. Loy  <br /> 
                                       IEEE Transactions on Image Processing, 2022 <strong>(TIP)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TIP.2022.3154606" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2012.14739" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://penincillin.github.io/pmnet_tip2022" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">CityNeRF: Building NeRF at City Scale </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, D. Lin <br /> 
                                        Technical report, arXiv:2112.05504, 2021 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2112.05504" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://city-super.github.io/citynerf/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Robust Partial-to-Partial Point Cloud Registration in a Full Range </span> 
                                        <br />
                                        <span class="text-500">
                                        L. Pan, Z. Cai, Z. Liu <br /> 
                                        Technical report, arXiv:2111.15606, 2021 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2111.15606" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/paul007pl/GMCNet" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Playing for 3D Human Recovery </span> 
                                       <br />
                                       <span class="text-500">
                                       Z. Cai, M. Zhang, J. Ren, C. Wei, D. Ren, J. Li, Z. Lin, H. Zhao, S. Yi, L. Yang, C. C. Loy, Z. Liu  <br /> 
                                       Technical report, arXiv:2110.07588, 2021 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2110.07588" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://caizhongang.github.io/projects/GTA-Human/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Pan, X. Xu, C. C. Loy, C. Theobalt, B. Dai <br /> 
                                       in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                       </span>
                                       [<a href="https://papers.nips.cc/paper/2021/file/a64c94baaf368e1840a1324e839230de-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2110.15678" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://xingangpan.github.io/projects/ShadeGAN.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Garment4D: Garment Reconstruction from Point Cloud Sequences </span> 
                                        <br />
                                        <span class="text-500">
                                        F. Hong, L. Pan, Z. Cai, Z. Liu <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://papers.nips.cc/paper/2021/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://github.com/hongfz16/Garment4D" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Density-aware Chamfer Distance as a Comprehensive Metric for Point Cloud Completion </span> 
                                        <br />
                                        <span class="text-500">
                                        T. Wu, L. Pan, J. Zhang, T. Wang, Z. Liu, D. Lin <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://papers.nips.cc/paper/2021/file/f3bd5ad57c8389a8a1a541a76be463bf-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2111.12702" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/wutong16/Density_aware_Chamfer_Distance" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Generative Occupancy Fields for 3D Surface-Aware Image Synthesis </span> 
                                        <br />
                                        <span class="text-500">
                                        X. Xu, X. Pan, D. Lin, B. Dai <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://papers.nips.cc/paper/2021/file/acab0116c354964a558e65bdd07ff047-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2111.00969" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://sheldontsui.github.io/projects/GOF" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Rong, J. Wang, Z. Liu, C. C. Loy <br /> 
                                       in Proceedings of International Conference on 3D Vision, 2021 <strong>(3DV)</strong><br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2111.00763" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://penincillin.github.io/ihmr_3dv2021" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">3D Human Texture Estimation from a Single Image with Transformers </span> 
                                        <br />
                                        <span class="text-500">
                                        X. Xu, C. C. Loy <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV, Oral)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_3D_Human_Texture_Estimation_From_a_Single_Image_With_Transformers_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>] 
                                        [<a href="https://arxiv.org/abs/2109.02563" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="./project/texformer/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]   
                                    </li>
                                    <li>
                                       <span class="text-primary">Variational Relational Point Completion Network </span>
                                       <br />
                                       <span class="text-500">
                                       L. Pan, X. Chen, Z. Cai, J. Zhang, H. Zhao, S. Yi, Z. Liu <br />
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR, Oral)</strong><br />
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_Variational_Relational_Point_Completion_Network_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.10154" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Pan_Variational_Relational_Point_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://paul007pl.github.io/projects/VRCNet" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Unsupervised 3D Shape Completion through GAN Inversion </span> 
                                       <br />
                                       <span class="text-500">
                                       J. Zhang, X. Chen, Z. Cai, L. Pan, H. Zhao, S. Yi, C. K. Yeo, B. Dai, C. C. Loy <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Unsupervised_3D_Shape_Completion_Through_GAN_Inversion_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.13366" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhang_Unsupervised_3D_Shape_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]                                       
                                       [<a href="https://junzhezhang.github.io/projects/ShapeInversion/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li> 
                                    <li>
                                       <span class="text-primary">Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Pan, B. Dai, Z. Liu, C. C. Loy, P. Luo <br /> 
                                       International Conference on Learning Representations, 2021 <strong>(ICLR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openreview.net/pdf?id=FGqiDsBUKL0" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2011.00844" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://xingangpan.github.io/projects/GAN2Shape.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild</span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Rong, Z. Liu, C. Li, K. Cao, C. C. Loy  <br /> 
                                       in Proceedings of International Conference on Computer Vision, 2019 <strong>(ICCV)</strong> <br />
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Rong_Delving_Deep_Into_Hybrid_Annotations_for_3D_Human_Recovery_in_ICCV_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1908.06442" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://penincillin.github.io/dct_iccv2019" target="_blank"><span class="text-muted">Project Page</span></a>]
                                       [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/iccv_2019_delving_supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                </ul>  

                                <h4 class="mb-4 text-uppercase">3D Perception</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Benchmarking and Analyzing 3D Human Pose and Shape Estimation Beyond Algorithms </span> 
                                        <br />
                                        <span class="text-500">
                                        H. E. Pang, Z. Cai, L. Yang, T. Zhang, Z. Liu <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2209.10529" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/smplbody/hmr-benchmarks" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Benchmarking and Analyzing Point Cloud Classification under Corruptions </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Ren, L. Pan, Z. Liu <br /> 
                                        in Proceedings of International Conference on Machine Learning, 2022 <strong>(ICML)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2202.03377" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://sites.google.com/view/modelnetc" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Versatile Multi-Modal Pre-Training for Human-Centric Perception </span> 
                                       <br />
                                       <span class="text-500">
                                       F. Hong, L. Pan, Z. Cai, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Versatile_Multi-Modal_Pre-Training_for_Human-Centric_Perception_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.13815" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://hongfz16.github.io/projects/HCMoCo.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">LiDAR-based 4D Panoptic Segmentation via Dynamic Shifting Network </span> 
                                        <br />
                                        <span class="text-500">
                                        F. Hong, H. Zhou, X. Zhu, H. Li, Z. Liu <br /> 
                                        Technical report, arXiv:2203.07186, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2203.07186" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/hongfz16/DS-Net" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency </span> 
                                        <br />
                                        <span class="text-500">
                                        Z. Luo, Z. Cai, C. Zhou, G. Zhang, H. Zhao, S. Yi, S. Lu, H. Li, S. Zhang, Z. Liu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_Unsupervised_Domain_Adaptive_3D_Detection_With_Multi-Level_Consistency_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2107.11355" target="_blank"><span class="text-muted">arXiv</span></a>] 
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Luo_Unsupervised_Domain_Adaptive_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>] 
                                    </li>
                                    <li>
                                       <span class="text-primary">LiDAR-based Panoptic Segmentation via Dynamic Shifting Network</span> 
                                       <br />
                                       <span class="text-500">
                                       F. Hong, H. Zhou, X. Zhu, H. Li, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong> <br />
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_LiDAR-Based_Panoptic_Segmentation_via_Dynamic_Shifting_Network_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2011.11964" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Hong_LiDAR-Based_Panoptic_Segmentation_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/hongfz16/DS-Net" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Exploring Data Augmentation for Multi-Modality 3D Object Detection </span> 
                                       <br />
                                       <span class="text-500">
                                       W. Zhang, Z. Wang, C. C. Loy  <br /> 
                                       Technical report, arXiv:2012.12741, 2020 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2012.12741" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/open-mmlab/mmdetection3d" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                </ul>                               
                                <span id="learning"></span>
                            </div>

                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Deep Learning</h2>
                                <p class="text-700 fw-medium">We investigate new deep learning methods that are more efficient, robust, accurate, scalable, transferable, and explainable.</p>

                                <img src="./assets/img/backgrounds/bg-04.jpg" class="mt-4 img-fluid w-100" alt="" />
                                <p></p>

                                <h4 class="mb-4 text-uppercase">Unsupervised | Self-Supervised Learning</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Delving into Inter-Image Invariance for Unsupervised Visual Representations </span> 
                                       <br />
                                       <span class="text-500">
                                       J. Xie, X. Zhan, Z. Liu, Y. S. Ong, C. C. Loy  <br /> 
                                       International Journal of Computer Vision, 2022 <strong>(IJCV)</strong><br />   
                                       </span>
                                       [<a href="https://rdcu.be/cWgRI" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://doi.org/10.1007/s11263-022-01681-x" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2008.11702" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/open-mmlab/mmselfsup" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li> 
                                    <li>
                                        <span class="text-primary">Dense Siamese Network for Dense Unsupervised Learning </span> 
                                        <br />
                                        <span class="text-500">
                                        W. Zhang, J. Pang, K. Chen, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136900460.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2203.11075" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136900460-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://github.com/ZwwWayne/DenseSiam"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Masked Frequency Modeling for Self-Supervised Visual Pre-Training </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Xie, W. Li, X. Zhan, Z. Liu, Y. S. Ong, C. C. Loy <br /> 
                                        Technical report, arXiv:2206.04673, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2206.07706" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="./project/mfm/index.html"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Self-Supervised Representation Learning: Introduction, Advances and Challenges </span> 
                                       <br />
                                       <span class="text-500">
                                       L. Ericsson, H. Gouk, C. C. Loy, T. M. Hospedales  <br /> 
                                       IEEE Signal Processing Magazine, 2022 <strong>(SPM)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/MSP.2021.3134634" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2110.09327" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Unsupervised Object-Level Representation Learning from Scene Images </span> 
                                       <br />
                                       <span class="text-500">
                                       J. Xie, X. Zhan, Z. Liu, Y. S. Ong, C. C. Loy <br /> 
                                       in Proceedings of Neural Information Processing Systems, 2021 <strong>(NeurIPS)</strong><br />    
                                       </span>
                                       [<a href="https://papers.nips.cc/paper/2021/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2106.11952" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="./project/orl/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>  
                                    <li>
                                       <span class="text-primary">Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Wang, Z. Liu, S. X. Yu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Unsupervised_Feature_Learning_by_Cross-Level_Instance-Group_Discrimination_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2008.03813" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Unsupervised_Feature_Learning_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/frank-xwang/CLD-UnsupervisedLearning" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Online Deep Clustering for Unsupervised Representation Learning </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Zhan, J. Xie, Z. Liu, Y. S. Ong, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Online_Deep_Clustering_for_Unsupervised_Representation_Learning_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2006.10645" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/open-mmlab/OpenSelfSup" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Self-Supervised Learning via Conditional Motion Propagation</span> 
                                       <br />
                                       <span class="text-500">
                                       X. Zhan, X. Pan, Z. Liu, D. Lin, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_Self-Supervised_Learning_via_Conditional_Motion_Propagation_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1903.11412" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://mmlab.ie.cuhk.edu.hk/projects/CMP/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>                                

                                <h4 class="mb-4 text-uppercase">Knowledge Distillation</h4>
                                <ul>
                                    <!-- <li>
                                       <span class="text-primary">Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup </span> 
                                       <br />
                                       <span class="text-500">
                                       G. Xu, Z. Liu, C. C. Loy  <br /> 
                                       Pattern Recognition, 2023 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2012.09413" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/xuguodong03/UNIXKD" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li> -->
                                    <li>
                                        <span class="text-primary">Mind the Gap in Distilling StyleGANs </span> 
                                        <br />
                                        <span class="text-500">
                                        G. Xu, Y. Hou, Z. Liu, C. C. Loy <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930416.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2208.08840" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930416-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://github.com/xuguodong03/StyleKD" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                  
                                    <li>
                                       <span class="text-primary">Knowledge Distillation Meets Self-Supervision </span> 
                                       <br />
                                       <span class="text-500">
                                       G. Xu, Z. Liu, X. Li, C. C. Loy  <br /> 
                                       European Conference on Computer Vision, 2020 <strong>(ECCV)</strong><br />    
                                       </span>
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2006.07114" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/xuguodong03/SSKD" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Residual Knowledge Distillation </span> 
                                       <br />
                                       <span class="text-500">
                                       M. Gao, Y. Shen, Q. Li, C. C. Loy  <br /> 
                                       Technical report, arXiv:2002.09168, 2020 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2002.09168" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Inter-Region Affinity Distillation for Road Marking Segmentation </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Hou, Z. Ma, C. Liu, T.-W. Hui, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Inter-Region_Affinity_Distillation_for_Road_Marking_Segmentation_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2004.05304" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Hou_Inter-Region_Affinity_Distillation_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/cardwing/Codes-for-IntRA-KD" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Hou, Z. Ma, C, Liu, C. C. Loy  <br /> 
                                       in Proceedings of International Conference on Computer Vision, 2019 <strong>(ICCV)</strong> <br />
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Learning_Lightweight_Lane_Detection_CNNs_by_Self_Attention_Distillation_ICCV_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1908.00821" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Hou_Learning_Lightweight_Lane_ICCV_2019_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/cardwing/Codes-for-Lane-Detection" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks</span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Hou, Z. Ma, C. Liu, C. C. Loy  <br /> 
                                       in Proceedings of AAAI Conference on Artificial Intelligence, 2019 <strong>(AAAI, Oral)</strong><br />  
                                       </span>
                                       [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2019_learning.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/1811.02759" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://cardwing.github.io/projects/FM-Net" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">An Embarrassingly Simple Approach for Knowledge Distillation</span> 
                                       <br />
                                       <span class="text-500">
                                       M. Gao, Y. Shen, Q. Li, C. C. Loy, X. Tang  <br /> 
                                       Technical report, arXiv:1812.01819, 2018<br />  
                                       </span>
                                       [<a href="https://arxiv.org/abs/1812.01819" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                </ul> 

                                <h4 class="mb-4 text-uppercase">Continual Learning</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Retrospective Class Incremental Learning </span> 
                                       <br />
                                       <span class="text-500">
                                       Q. Tao, C. C. Loy, J. Cai, Z. Ge, S. See <br /> 
                                       in Proceedings of IEEE International Conference on Multimedia and Expo, 2021 <strong>(ICME)</strong><br />    
                                       </span>
                                       [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/icme_2021_restropective.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Learning a Unified Classifier Incrementally via Rebalancing</span> 
                                       <br />
                                       <span class="text-500">
                                       S. Hou, X. Pan, C. C. Loy, Z. Wang, D. Lin  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2019 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hou_Learning_a_Unified_Classifier_Incrementally_via_Rebalancing_CVPR_2019_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="http://mmlab.ie.cuhk.edu.hk/projects/rebalanced-learning/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Lifelong Learning via Progressive Distillation and Retrospection</span> 
                                       <br />
                                       <span class="text-500">
                                       S. Hou, X. Pan, C. C. Loy, Z. Wang, D. Lin <br /> 
                                       in Proceedings of European Conference on Computer Vision, 2018 <strong>(ECCV)</strong></strong><br />  
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Saihui_Hou_Progressive_Lifelong_Learning_ECCV_2018_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="http://mmlab.ie.cuhk.edu.hk/projects/lifelong/" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>

                                <h4 class="mb-4 text-uppercase">Long-Tailed Recognition</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Open Long-Tailed Recognition in a Dynamic World </span> 
                                       <br />
                                       <span class="text-500">
                                       Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, S. X. Yu <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 <strong>(TPAMI)</strong><br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2208.08349" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Iterative Human and Automated Identification of Wildlife Images</span> 
                                       <br />
                                       <span class="text-500">
                                       Z. Miao, Z. Liu, K. M. Gaynor, M. S. Palmer, S. X. Yu, W. M. Getz  <br /> 
                                       Nature Machine Intelligence, vol. 3, pp. 885â895, 2021 <strong>(Nat Mach Intell)</strong><br />   
                                       </span>
                                       [<a href="https://doi.org/10.1038/s42256-021-00393-0" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2105.02320" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zang, C. Huang, C. C. Loy <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/iccv_2021_fasa.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2102.12867" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/iccv_2021_fasa_supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/fasa/index.html" target="_blank"><span class="text-muted">Project Page</span></a>] 
                                    </li>
                                    <li>
                                       <span class="text-primary">Adversarial Robustness under Long-Tailed Distribution </span>
                                       <br />
                                       <span class="text-500">
                                       T. Wu, Z. Liu, Q. Huang, Y. Wang, D. Lin <br />
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR, Oral)</strong><br />
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Adversarial_Robustness_Under_Long-Tailed_Distribution_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2104.02703" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wu_Adversarial_Robustness_Under_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/wutong16/Adversarial_Long-Tail" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>                                    
                                    <li>
                                       <span class="text-primary">Long-Tailed Recognition by Routing Diverse Distribution-Aware Experts </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Wang, L. Lian, Z. Miao, Z. Liu, S. X. Yu  <br /> 
                                       International Conference on Learning Representations, 2021 <strong>(ICLR)</strong> <br />    
                                       </span>
                                       [<a href="https://openreview.net/pdf?id=D9I3drBz4UC" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2010.01809" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/frank-xwang/RIDE-LongTailRecognition" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul> 

                                <h4 class="mb-4 text-uppercase">Model Uncertainty and Robustness</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">OpenOOD: Benchmarking Generalized Out-of-Distribution Detection </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Yang, P. Wang, D. Zou, Z. Zhou, K. Ding, W. Peng, H. Wang, G. Chen, B. Li, Y. Sun, X. Du, K. Zhou, W. Zhang, D. Hendrycks, Y. Li, Z. Liu <br /> 
                                        in Proceedings of Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2210.07242" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/Jingkang50/OpenOOD" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">On-Device Domain Generalization </span> 
                                        <br />
                                        <span class="text-500">
                                        K. Zhou, Y. Zhang, Y. Zang, J. Yang, C. C. Loy, Z. Liu <br /> 
                                        Technical report, arXiv:2209.07521, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2209.07521" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Domain Generalization: A Survey </span> 
                                       <br />
                                       <span class="text-500">
                                       K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy <br /> 
                                       IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 <strong>(TPAMI)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1109/TPAMI.2022.3195549" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2103.02503" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners </span> 
                                        <br />
                                        <span class="text-500">
                                        B. Li, J. Yang, J. Ren, Y. Wang, Z. Liu <br /> 
                                        Technical report, arXiv:2206.04046, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2206.04046" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/Luodian/SF-MoE-DG" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Robust Face Anti-Spoofing with Dual Probabilistic Modeling </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Zhang, Y. Wu, Z. Yin, J. Shao, Z. Liu <br /> 
                                       Technical report, arXiv:2204.12685, 2022 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2204.12685" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Full-Spectrum Out-of-Distribution Detection </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Yang Â· K. Zhou Â· Z. Liu <br /> 
                                        Technical report, arXiv:2204.05306, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2204.05306" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/Jingkang50/OpenOOD" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Balanced MSE for Imbalanced Visual Regression </span>
                                       <br />
                                       <span class="text-500">
                                       J. Ren, M. Zhang, C. Yu, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR, Oral)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Balanced_MSE_for_Imbalanced_Visual_Regression_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.16427" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Ren_Balanced_MSE_for_CVPR_2022_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://github.com/jiawei-ren/BalancedMSE" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Delving Deep into the Generalization of Vision Transformers under Distribution Shifts </span>
                                       <br />
                                       <span class="text-500">
                                       C. Zhang, M. Zhang, S. Zhang, D. Jin, Q. Zhou, Z. Cai, H. Zhao, S. Yi, X. Liu, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Delving_Deep_Into_the_Generalization_of_Vision_Transformers_Under_Distribution_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2106.07617" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/Phoenix1153/ViT_OOD_generalization" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Generalized Out-of-Distribution Detection: A Survey </span> 
                                       <br />
                                       <span class="text-500">
                                       J. Yang, K. Zhou, Y. Li, Z. Liu  <br /> 
                                       Technical report, arXiv:2110.11334, 2021 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2110.11334" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/Jingkang50/OODSurvey" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Semantically Coherent Out-of-Distribution Detection </span> 
                                        <br />
                                        <span class="text-500">
                                        J. Yang, H. Wang, L. Feng, X. Yan, H. Zheng,  W. Zhang, Z. Liu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Semantically_Coherent_Out-of-Distribution_Detection_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2108.11941" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Yang_Semantically_Coherent_Out-of-Distribution_ICCV_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://jingkang50.github.io/projects/scood" target="_blank"><span class="text-muted">Project Page</span></a>] 
                                    </li>
                                    <li>
                                        <span class="text-primary">Energy-Based Open-World Uncertainty Modeling for Confidence Calibration </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Wang, B. Li, T. Che, K. Zhou, D. Li, Z. Liu <br /> 
                                        in Proceedings of IEEE/CVF International Conference on Computer Vision, 2021 <strong>(ICCV)</strong><br />    
                                        </span>
                                        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Energy-Based_Open-World_Uncertainty_Modeling_for_Confidence_Calibration_ICCV_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2107.12628" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Optimization Variance: Exploring Generalization Properties of DNNs </span> 
                                       <br />
                                       <span class="text-500">
                                       X. Zhang, D. Wu, H. Xiong, B. Dai <br /> 
                                       Technical report, arXiv:2106.01714, 2021 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2106.01714" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Semi-Supervised Domain Generalization with Stochastic StyleMatch </span> 
                                       <br />
                                       <span class="text-500">
                                       K. Zhou, C. C. Loy, Z. Liu <br /> 
                                       in Workshop on Distribution Shifts of Neural Information Processing Systems, 2021 <strong>(NeurIPS DistShift)</strong><br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2106.00592" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/KaiyangZhou/ssdg-benchmark" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>
                        
                                <h4 class="mb-4 text-uppercase">Network Compression</h4>
                                <ul>
                                    <li>
                                       <span class="text-primary">Network Pruning via Resource Reallocation </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. Hou, Z. Ma, C. Liu, Z. Wang, C. C. Loy <br /> 
                                       Technical report, arXiv:2103.01847, 2021 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2103.01847" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>   
                                    <li>
                                       <span class="text-primary">EcoNAS: Finding Proxies for Economical Neural Architecture Search </span> 
                                       <br />
                                       <span class="text-500">
                                       D. Zhou, X. Zhou, W. Zhang, C. C. Loy, S. Yi, X. Zhang, W. Ouyang  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_EcoNAS_Finding_Proxies_for_Economical_Neural_Architecture_Search_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2001.01233" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Zhou_EcoNAS_Finding_Proxies_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                    </li>
                                </ul>

                                <h4 class="mb-4 text-uppercase">Zero/Few-shot Learning</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Unified Vision and Language Prompt Learning </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zang, W. Li, K. Zhou, C. Huang, C. C. Loy <br /> 
                                        Technical report, arXiv:2210.07225, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2210.07225" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Extract Free Dense Labels from CLIP </span> 
                                        <br />
                                        <span class="text-500">
                                        C. Zhou, C. C. Loy, B. Dai <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV, Oral)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880687.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2112.01071" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880687-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="./project/maskclip/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                        <span class="text-primary">Neural Prompt Search </span> 
                                        <br />
                                        <span class="text-500">
                                        Y. Zhang, K. Zhou, Z. Liu <br /> 
                                        Technical report, arXiv:2206.04673, 2022 <br />    
                                        </span>
                                        [<a href="https://arxiv.org/abs/2206.04673" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://github.com/Davidzhangyuanhan/NOAH" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Conditional Prompt Learning for Vision-Language Models </span> 
                                       <br />
                                       <span class="text-500">
                                       K. Zhou, J. Yang, C. C. Loy, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2203.05557" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/KaiyangZhou/CoOp" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Learning to Prompt for Vision-Language Models </span> 
                                       <br />
                                       <span class="text-500">
                                       K. Zhou, J. Yang, C. C. Loy, Z. Liu <br /> 
                                       International Journal of Computer Vision, 2022 <strong>(IJCV)</strong><br />    
                                       </span>
                                       [<a href="https://doi.org/10.1007/s11263-022-01653-1" target="_blank"><span class="text-muted">DOI</span></a>]
                                       [<a href="https://arxiv.org/abs/2109.01134" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://github.com/KaiyangZhou/CoOp" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>  
                                </ul>                                  
                                <span id="forensics"></span>
                            </div> 

                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Media Forensics</h2>
                                <p class="text-700 fw-medium">We collect large-scale datasets and develop new methods for face forgery detection.</p>

                                <img src="./assets/img/backgrounds/bg-03.jpg" class="mt-4 img-fluid w-100" alt="" />
                                <p></p>

                                <h4 class="mb-4 text-uppercase">Forgery Detection and Anti-Deepfake</h4>
                                <ul>
                                    <li>
                                        <span class="text-primary">Detecting and Recovering Sequential DeepFake Manipulation </span> 
                                        <br />
                                        <span class="text-500">
                                        R. Shao, T. Wu, Z. Liu <br /> 
                                        European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />    
                                        </span>
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730710.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                        [<a href="https://arxiv.org/abs/2207.02204" target="_blank"><span class="text-muted">arXiv</span></a>]
                                        [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730710-supp.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                        [<a href="https://rshaojimmy.github.io/Projects/SeqDeepFake" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">Few-shot Forgery Detection via Guided Adversarial Interpolation </span> 
                                       <br />
                                       <span class="text-500">
                                       H. Qiu, S. Chen, B. Gan, K. Wang, H. Shi, J. Shao, Z. Liu <br /> 
                                       Technical report, arXiv:2204.05905, 2022 <br />    
                                       </span>
                                       [<a href="https://arxiv.org/abs/2204.05905" target="_blank"><span class="text-muted">arXiv</span></a>]
                                    </li>  
                                    <li>
                                        <span class="text-primary">DeepFakes Detection: the DeeperForensics Dataset and Challenge</span> 
                                        <br />
                                        <span class="text-500">
                                        L. Jiang, W. Wu, C. Qian, C. C. Loy <br /> 
                                        In C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, C. Busch (Eds.), Handbook of Digital Face Manipulation and Detection, Springer, 2022<br /> 
                                        </span>
                                        [<a href="https://link.springer.com/book/10.1007/978-3-030-87664-7" target="_blank"><span class="text-muted">Book Link</span></a>]
                                    </li>    
                                    <li>
                                       <span class="text-primary">ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis </span> 
                                       <br />
                                       <span class="text-500">
                                       Y. He, B. Gan, S. Chen, Y. Zhou, G. Yin, L. Song, L. Sheng, J. Shao, Z. Liu  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2021 <strong>(CVPR, Oral)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2103.05630" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/He_ForgeryNet_A_Versatile_CVPR_2021_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://yinanhe.github.io/projects/forgerynet.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                    <li>
                                       <span class="text-primary">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection </span> 
                                       <br />
                                       <span class="text-500">
                                       L. Jiang, R. Li, W. Wu, C. Qian, C. C. Loy  <br /> 
                                       in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2020 <strong>(CVPR)</strong> <br />    
                                       </span>
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_DeeperForensics-1.0_A_Large-Scale_Dataset_for_Real-World_Face_Forgery_Detection_CVPR_2020_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                       [<a href="https://arxiv.org/abs/2001.03024" target="_blank"><span class="text-muted">arXiv</span></a>]
                                       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Jiang_DeeperForensics-1.0_A_Large-Scale_CVPR_2020_supplemental.pdf" target="_blank"><span class="text-muted">Supplementary Material</span></a>]
                                       [<a href="https://liming-jiang.com/projects/DrF1/DrF1.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                                    </li>
                                </ul>
                            </div>                                                           
                        </div>
                        
                    </div>
                </div>
            </section>
            <!-- End of Section -->
        </main>
        <!-- End of Main Content -->


        <!-- Footer -->
        <footer class="footer text-white" style="background-image: url(./assets/img/footer-bg.jpg)">
            <div class="container d-flex h-100">
                <div class="row flex-grow-1">
                    <div class="col-lg-3 pt-3 ext-l bg-secondary text-center text-lg-left">
                        <div class="d-flex flex-column h-100">
                            <div class="pt-5 pt-lg-8 pb-4">
                                <img src="./assets/img/logo-small.png" alt="" width="108" class="mb-4" />
                                <p class="mb-4 mt-3 fs--1"><br />
                                Academic Block North <br />
                                Nanyang Technological University<br />
                                61 Nanyang Dr, Singapore 637335</p>
        
                                <p class="fs--1">
                                <span class="text-white"><i class="zmdi zmdi-email zmdi-hc-fw mr-1"></i>mmlab-contact at e.ntu.edu.sg</span><br />
                                <span class="text-white"><i class="zmdi zmdi-twitter zmdi-hc-fw mr-1"></i><a href="https://twitter.com/MMLabNTU" target="_blank" class="text-white">@MMLabNTU</a></span></p>
                            </div>
    
                            <!-- <ul class="mt-4 mt-lg-auto mb-5 mb-lg-0 list-unstyled list-inline">
                                <li class="mr-3 list-inline-item">
                                    <a href="https://twitter.com/MMLabNTU" target="_blank">
                                        <i class="zmdi zmdi-twitter text-white"></i>
                                    </a>
                                </li>
                            </ul> -->
                        </div>
                    </div>

                    <div class="col d-flex flex-column mb-2 mt-3 pl-lg-7">
                        <div class="row pt-5 pt-lg-8 pb-4 pb-lg-6">
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Publications</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="./publication_topic.html" class="text-white">By Topic</a></li>
                                    <li class="my-1"><a href="./publication_year.html" class="text-white">By Year</a></li>
                                </ul>
                            </div>
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">About</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="./research.html" class="text-white">Our Research</a></li>
                                    <li class="my-1"><a href="./team.html" class="text-white">Team</a></li>
                                    <li class="my-1"><a href="./careers.html" class="text-white">Join Us</a></li>
                                </ul>
                            </div>
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Open Source</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="https://openmmlab.com/" target="_blank" class="text-white">OpenMMLab</a></li>
                                    <li class="my-1"><a href="./downloads.html" class="text-white">Code and Datasets</a></li>
                                </ul>
                            </div>
                        </div>

                        <div class="mt-auto d-flex justify-content-between">
                            <span class="fs--3 fs-lg--2">&copy; MMLab@NTU, 2021</span>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- End of Footer -->

        <!-- Top Button -->
        <a id="back-to-top" href="#" class="btn btn-light btn-lg back-to-top" role="button"><i class="fas fa-chevron-up"></i></a>
        <!-- End of Top Button -->

        <!-- Core Javascripts -->
        <script src="./assets/vendor/jquery/dist/jquery.min.js"></script>
        <script src="./assets/vendor/popper.js/dist/umd/popper.min.js"></script>
        <script src="./assets/vendor/bootstrap/dist/js/bootstrap.min.js"></script>
        <script src="./assets/vendor/typed.js/lib/typed.min.js"></script>

        <!-- Vendor Javascripts -->
        <script src="./assets/vendor/rellax/rellax.min.js"></script>
        <script src="./assets/vendor/sticky-kit/dist/sticky-kit.min.js"></script>
        <script src="./assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
        <script src="./assets/vendor/isotope-layout/dist/isotope.pkgd.min.js"></script>
        <script src="./assets/vendor/isotope-packery/packery-mode.pkgd.min.js"></script>
        <script src="./assets/vendor/aos/dist/aos.js"></script>

        <!-- Theme Javascripts -->
        <script src="./assets/js/theme.js"></script>
        <script src="./assets/js/top.js"></script>
    </body>
</html>