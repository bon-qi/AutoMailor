<!DOCTYPE html>
<html lang="en">
<head>
	<title>Publications | Perceiving Systems - Max Planck Institute for Intelligent Systems</title>


	<link rel="stylesheet" media="all" href="/assets/application_critical-a6522b239b1b542c8c2d91c165ff7764.css" data-turbolinks-track="true" defer="defer" />

	<!-- <link rel="preload" href="" as="style" onload="this.onload=null;this.rel='stylesheet'">
	<noscript><link rel="stylesheet" href=""></noscript> -->

	<link rel="preload" href="/assets/font-awesome/css/font-awesome-pure-041bc8eacc6e69913e9ea9c0c28684e0.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
	<noscript><link rel="stylesheet" href="/assets/font-awesome/css/font-awesome-pure-041bc8eacc6e69913e9ea9c0c28684e0.css"></noscript>

	




	<script src="/assets/publications-ec66260bc24be475f375e9c62598fcf1.js" data-turbolinks-track="true" defer="defer"></script>
	<link rel="stylesheet" media="all" href="/assets/publications-436435d47281e03d604776fb5bba64e3.css" data-turbolinks-track="true" defer="defer" />



	<script src="/assets/jquery.tokeninput-c92512a1c3822c6a8fbf1f3b79aaba06.js" data-turbolinks-track="true" defer="defer"></script>
	
		<script src="/javascripts/jcrop/jquery.Jcrop.min.js" data-turbolinks-track="true" defer="defer"></script>


		<link rel="stylesheet" media="all" href="/assets/purified/page_search_inner-c01bd9362a6d0fab5c68113e6afd56b1.css" data-turbolinks-track="true" defer="defer" />



















	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="keywords" content="Computer Vision, Max Planck Institute for Intelligent Systems, Perceiving Systems Department, Computer Vision, Computer Vision Tuebingen, Computer Vision Tübingen, Computer Vision Germany, Computer Vision Deutschland, Computer Vision Research, Computer Vision Research Tuebingen, Computer Vision Research Germany"/>
	<meta name="description" content="Using computer vision, computer graphics, and machine learning, we teach computers to see people and understand their behavior in complex 3D scenes. We are located in Tübingen, Germany."/>
  	<meta name="author" content="Jon Williams"/>
	<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
	<meta name="google-site-verification" content="ECPD8vOhMbNjxILeTmFab1vP33FWWRBf-0RbcRl4eFo" />


	<meta name="turbolinks-cache-control" content="no-cache">

	<!-- sharing -->
	<meta property="og:site_name" content="Max Planck Institute for Intelligent Systems">

<meta name="twitter:card" content="summary_large_image" />

	<meta property="og:url" content="https://is.mpg.de" />




<meta property="og:locale:alternate" content="en_US" />
<meta property="og:type" content="website" />

	<link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-32ffa17fc0e0588a030547e20f53c4be.png" />


	
	<meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="Iv1vtM0xVDTMHVbvFctwj+K3cSb4iuNt6lCQ7CZHxwxk/DG3h7h1mwKSgJTMqgP2QS945eQ8RB6HtHVIgTPRng==" />
</head>

<body class="header  header-location default-link eupopup eupopup-bottom">

	<header>
			<!-- IW-798 -->
			<div id='headerColorBar' class='header-color-bar header-color-bar-ps'></div>

<!-- IW-716 show corona header -->



		
		<div id="unstickyheader" class="unstickyheader-signed-out">

	<div class="topbar">
		<div class="container" >
			<div class="header-image-container">

					<!-- IW-815 -->

							<a href="https://ps.is.mpg.de"><img width="368" height="102" class="header-image" title="Perceiving Systems" alt="Perceiving Systems, Computer Vision" src="/assets/header/header_logo_is_ps-ee500b0b056108c7b343b0703cb1ee57.png" /></a>
			
			</div>

			<div class="text-right" >
				
				<div class="social-icons-header-container hidden-xs">
					<ul class="social-icons">

							<!-- 
								<li><a href="https://www.facebook.com/PerceivingSystems" aria-label="Facebook - Perceiving Systems" alt="Facebook - Perceiving Systems" data-original-title="Facebook" width="28" height="28" class="rounded-x custom-social-facebook" target="_blank" rel="noopener noreferrer" ></a></li>
								<li><a href="https://twitter.com/PerceivingSys" aria-label="Twitter - Perceiving Systems" alt="Twitter - Perceiving Systems" data-original-title="Twitter" width="28" height="28" class="rounded-x custom-social-twitter" target="_blank" rel="noopener noreferrer" ></a></li>
								<li><a href="https://www.youtube.com/channel/UCqNJuPO0tyV6eWfYB7lcsvw" aria-label="YouTube - Perceiving Systems" alt="YouTube - Perceiving Systems" data-original-title="Youtube" width="28" height="28" class="rounded-x custom-social-youtube" target="_blank" rel="noopener noreferrer" ></a></li>
								<li><a href="https://www.youtube.com/channel/UCWLlVCbk_g2bYv36txh8VKA"  aria-label="YouTube - Modern Magnetic Systems" alt="YouTube - Modern Magnetic Systems" data-original-title="Youtube" width="28" height="28" class="rounded-x custom-social-youtube" target="_blank" rel="noopener noreferrer" ></a></li>
								<li><a href="https://www.facebook.com/profile.php?id=100012738961011" aria-label="FaceBook - Neural Capture and Synthesis Group" alt="FaceBook - Neural Capture and Synthesis Group" data-original-title="Facebook" width="28" height="28" class="rounded-x custom-social-facebook" target="_blank" rel="noopener noreferrer" ></a></li>
								<li><a href="https://twitter.com/JustusThies" data-original-title="Twitter" aria-label="Twitter - Neural Capture and Synthesis Group" alt="Twitter - Neural Capture and Synthesis Group" width="28" height="28" class="rounded-x custom-social-twitter" target="_blank" rel="noopener noreferrer" ></a></li>
								<li><a href="https://www.youtube.com/channel/UCwmSTvnV-sjtlIlNvWYC6Ow" aria-label="YouTube - Neural Capture and Synthesis Group" alt="YouTube - Neural Capture and Synthesis Group" data-original-title="Youtube" width="28" height="28" class="rounded-x custom-social-youtube" target="_blank" rel="noopener noreferrer" ></a></li>
							 -->

								<!-- IW-594 -->
								<li><a href="https://www.facebook.com/PerceivingSystems" aria-label="Facebook - Perceiving Systems" alt="Facebook - Perceiving Systems" class="custom-social" target="_blank" rel="noopener noreferrer" ><i class="fa fa-facebook-square" aria-hidden="true"></i></a></li>
								<li><a href="https://twitter.com/PerceivingSys" aria-label="Twitter - Perceiving Systems" alt="Twitter - Perceiving Systems" class="custom-social" target="_blank" rel="noopener noreferrer" ><i class="fa fa-twitter-square" aria-hidden="true"></i></a></li>
								<li><a href="https://www.youtube.com/channel/UCqNJuPO0tyV6eWfYB7lcsvw" aria-label="YouTube - Perceiving Systems" alt="YouTube - Perceiving Systems" class="custom-social" target="_blank" rel="noopener noreferrer" ><i class="fa fa-youtube-square" aria-hidden="true"></i></a></li>
					</ul>
				</div>
			</div>
		</div>
	</div>
</div>
</div>



<nav>
		<div id="stickyheader">		

		<div class="header-lower">
			
		<div class="navbar navbar-default custom-navbar-default mega-menu" role="navigation">
			<div class="container">

				<div class="navbar-header">
					<button type="button" class="navbar-toggle custom-navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
						<span class="sr-only">Toggle navigation</span>
						<span class="fa fa-bars"></span>
					</button>
				</div>

				<!-- IW-798 -->

					
							<!-- department toc -->
							<div class="collapse navbar-collapse custom-navbar-collapse navbar-responsive-collapse">
								<ul id="mainNav" class="nav navbar-nav custom-navbar-nav overflow">
										<li class='dropdown'><a href="https://ps.is.mpg.de">Perceiving Systems</a><ul class='dropdown-menu dropdown-menu-ps'><li class='sperate-link-toc-item-bottom'><a href="https://is.mpg.de">Institute Home</a></li><li><a href="https://am.is.mpg.de">Autonomous Motion</a></li><li></li><li><a href="https://ei.is.mpg.de">Empirical Inference</a></li><li><a href="https://hi.is.mpg.de">Haptic Intelligence</a></li><li></li><li></li><li><a href="https://mms.is.mpg.de">Modern Magnetic Systems</a></li><li><a href="https://ps.is.mpg.de">Perceiving Systems</a></li><li><a href="https://pi.is.mpg.de">Physical Intelligence</a></li><li></li><li></li><li><a href="https://sf.is.mpg.de">Social Foundations of Computation</a></li><li><a href="https://icm.is.mpg.de">Theory of Inhomogeneous Condensed Matter</a></li><li class='sperate-link-toc-item-top'><a href="https://avg.is.mpg.de">Autonomous Vision</a></li><li><a href="https://al.is.mpg.de">Autonomous Learning</a></li><li><a href="https://minibot.is.mpg.de">Bioinspired Autonomous Miniature Robots</a></li><li></li><li></li><li><a href="https://dlg.is.mpg.de">Dynamic Locomotion</a></li><li><a href="https://ev.is.mpg.de">Embodied Vision</a></li><li><a href="https://hml.is.mpg.de">Human Aspects of Machine Learning</a></li><li><a href="https://ics.is.mpg.de">Intelligent Control Systems</a></li><li><a href="https://lds.is.mpg.de">Learning and Dynamical Systems</a></li><li><a href="https://bio.is.mpg.de">Locomotion in Biorobotic and Somatic Systems</a></li><li><a href="https://pf.is.mpg.de">Micro, Nano, and Molecular Systems</a></li><li><a href="https://mg.is.mpg.de">Movement Generation and Control</a></li><li></li><li></li><li><a href="https://ncs.is.mpg.de">Neural Capture and Synthesis</a></li><li></li><li><a href="https://ld.is.mpg.de">Organizational Leadership and Diversity</a></li><li><a href="https://pio.is.mpg.de">Physics for Inference and Optimization</a></li><li><a href="https://plg.is.mpg.de">Probabilistic Learning Group</a></li><li><a href="https://pn.is.mpg.de">Probabilistic Numerics</a></li><li><a href="https://re.is.mpg.de">Rationality Enhancement</a></li><li><a href="https://robustml.is.mpg.de">Robust Machine Learning</a></li><li></li><li></li><li></li><li></li></ul></li><li class='dropdown'><a class="dropdown-toggle" href="/research">Research</a><ul class='dropdown-menu dropdown-menu-ps'><li class='sperate-link-toc-item-bottom'><a href="/research">Overview</a></li><li><a href="/research_fields/virtual-humans"> Modeling 3D Humans and Animals</a></li><li><a href="/research_fields/seeing-understanding-people">Human Pose, Shape, and Motion</a></li><li><a href="/research_fields/behavior-goals-action">Behavior, Action, and Language</a></li><li><a href="/research_fields/synthesizing-people">Synthesizing People</a></li><li><a href="/research_fields/medicine-and-psychology">Society, Medicine, and Psychology</a></li><li><a href="/research_fields/understanding-scenes">Scenes, Structure and Motion</a></li><li><a href="/research_fields/beyond-mocap">Beyond Mocap</a></li><li><a href="/research_fields/datasets-and-code">Datasets</a></li><li><a href="/research_fields/robot-perception-group">Robot Perception Group</a></li><li><a href="/research_fields/holistic-vision-group">Holistic Vision Group</a></li><li><a href="/research_fields/data-team">Data Team</a></li><li><a href="/research_fields/completed">Completed Projects</a></li></ul></li><li><a href="/publications">Publications</a></li><li><a href="/code">Code/Data</a></li><li><a href="/people">People</a></li><li class='dropdown'><a href='#' class='dropdown-toggle' data-toggle='dropdown'>News & Events</a><ul class='dropdown-menu dropdown-menu-ps'><li><a href="/news">News</a></li><li><a href="/events">Events</a></li><li><a href="/talks">Talks</a></li><li><a href="/awards">Awards</a></li></ul></li><li class='dropdown'><a class="dropdown-toggle" data-toggle="dropdown" href="/pages/facilities">Facilities</a><ul class='dropdown-menu dropdown-menu-ps'><li><a href="/pages/4d-capture">4D Scanner</a></li><li><a href="/pages/4d-dynamic-face-scanner">4D Face Scanner</a></li><li><a href="/pages/4d-foot-scanner">4D Foot Scanner</a></li><li><a href="/pages/4d-hand-scanner">4D Hand Scanner</a></li><li><a href="/pages/3dcapture">3D Scanner</a></li><li><a href="/pages/motion-capture">Vicon Motion Capture</a></li><li><a href="/pages/inertial-motion-capture">Inertial Motion Capture </a></li><li><a href="/pages/outdoor-aerial-motion-capture-system">Flying Motion Capture System</a></li><li><a href="/pages/video-capture">Multi-Camera Setup</a></li><li><a href="/pages/other-capture-technologies">Other Capture Technologies</a></li><li><a href="/pages/computing">Computing</a></li><li><a href="/pages/espresso">Espresso</a></li></ul></li><li><li class='dropdown'><a href='#' class='dropdown-toggle' data-toggle='dropdown'>Career</a><ul class='dropdown-menu dropdown-menu-ps'><li><a href="/why">Why MPI</a></li><li><a href="/jobs/">Current Jobs</a></li><li><a href="/jobs//phd">Ph.D Applicants</a></li><li><a href="/jobs//undergraduate">Undergraduate Interns</a></li><li class='sperate-link-toc-item-top'><a href="/jobs//participate">Trials <small class='text-muted-custom'>(Participants Wanted)</small></a></li><li class='sperate-link-toc-item-top'><a href="/jobs//art">Art <small class='text-muted-custom'>(Artist in Residence)</small></a></li></ul></li><li class='dropdown'><a class="dropdown-toggle" data-toggle="dropdown" href="/pages/about">About</a><ul class='dropdown-menu dropdown-menu-ps'><li><a href="/department">Department</a></li><li><a href="/principles">Principles</a></li><li><a href="/pages/startups">Startup Garage</a></li></ul></li><li><a href="/contact">Contact</a></li>
								</ul>
							</div>

				

			</div>
		</div>    
	</div>      
</div>
</nav>

	</header>


	<main class="wrapper">

		<!-- department hover header image (homepage) -->
		
		<!-- sliders -->

		<!-- search pages -->

		<!-- employee headers -->

		<!-- job headers -->

		<!-- construction header -->

		<!-- purchase styled header -->


		<!-- institute contact map -->

		<!-- content -->
			<div class="container custom-container">
				<!-- DEVISE -->
<!-- 
	3. Ensure you have flash messages in app/views/layouts/application.html.erb. For example:
		<p class="notice"></p>
		<p class="alert"></p>
-->






				

<div class="s-results">
  <div class="row">
    <div id="publicationFacetsDiv">
      <div class="col-md-3 hidden-xs related-search">


  
  <div class="row">

  
     <div class="col-md-12 col-sm-4"> 
      <h3>MPI Papers</h3>
      <ul class="list-unstyled">
        <li>

            <!-- DEFAULT ON (only show mpi papers by default) -->
            <!-- departments -->
              <span class="custom-highlights">
                <a href="/publications?mpi_papers%5B%5D=true">MPI Papers</a>
              </span>

        </li>
      </ul>
      <hr>
    </div>
  
  
    

      <div class="col-md-12 col-sm-4">    
        <h3>Publication Type</h3>
        <ul class="list-unstyled">
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Article">Article</a> 
              <small class="text-muted">(151)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Book">Book</a> 
              <small class="text-muted">(4)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Book+Chapter">Book Chapter</a> 
              <small class="text-muted">(24)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Conference+Paper">Conference Paper</a> 
              <small class="text-muted">(501)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=MPI+Year+Book">MPI Year Book</a> 
              <small class="text-muted">(2)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Manual">Manual</a> 
              <small class="text-muted">(1)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Master+Thesis">Master Thesis</a> 
              <small class="text-muted">(2)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Miscellaneous">Miscellaneous</a> 
              <small class="text-muted">(2)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Patent">Patent</a> 
              <small class="text-muted">(19)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Ph.D.+Thesis">Ph.D. Thesis</a> 
              <small class="text-muted">(17)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Proceedings">Proceedings</a> 
              <small class="text-muted">(1)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Technical+Report">Technical Report</a> 
              <small class="text-muted">(25)</small>
            </span>
        </li>
         <li>

            <span class="custom-highlights">
              <a href="/publications?publication_type%5B%5D=Unpublished">Unpublished</a> 
              <small class="text-muted">(1)</small>
            </span>
        </li>
      </ul>
      <hr>
    </div>  

    <div class="col-md-12 col-sm-4">
      <h3>Year</h3>
      <ul class="list-unstyled">
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2022">2022</a>
                  <small class="text-muted">(29)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2021">2021</a>
                  <small class="text-muted">(38)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2020">2020</a>
                  <small class="text-muted">(27)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2019">2019</a>
                  <small class="text-muted">(34)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2018">2018</a>
                  <small class="text-muted">(29)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2017">2017</a>
                  <small class="text-muted">(44)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2016">2016</a>
                  <small class="text-muted">(27)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2015">2015</a>
                  <small class="text-muted">(45)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2014">2014</a>
                  <small class="text-muted">(52)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2013">2013</a>
                  <small class="text-muted">(55)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2012">2012</a>
                  <small class="text-muted">(61)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2011">2011</a>
                  <small class="text-muted">(57)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2010">2010</a>
                  <small class="text-muted">(39)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2009">2009</a>
                  <small class="text-muted">(36)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2008">2008</a>
                  <small class="text-muted">(21)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2007">2007</a>
                  <small class="text-muted">(21)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2006">2006</a>
                  <small class="text-muted">(20)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2005">2005</a>
                  <small class="text-muted">(11)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2004">2004</a>
                  <small class="text-muted">(10)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2003">2003</a>
                  <small class="text-muted">(15)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2002">2002</a>
                  <small class="text-muted">(8)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2001">2001</a>
                  <small class="text-muted">(5)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=2000">2000</a>
                  <small class="text-muted">(7)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1999">1999</a>
                  <small class="text-muted">(5)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1998">1998</a>
                  <small class="text-muted">(12)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1997">1997</a>
                  <small class="text-muted">(7)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1996">1996</a>
                  <small class="text-muted">(7)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1995">1995</a>
                  <small class="text-muted">(8)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1994">1994</a>
                  <small class="text-muted">(6)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1993">1993</a>
                  <small class="text-muted">(4)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1992">1992</a>
                  <small class="text-muted">(3)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1991">1991</a>
                  <small class="text-muted">(2)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D=1990">1990</a>
                  <small class="text-muted">(2)</small>
                </span>
         </li>
          <li>
                <span class="custom-highlights">
                  <a href="/publications?year%5B%5D="></a>
                  <small class="text-muted">(3)</small>
                </span>
         </li>
     </ul>
     <hr>
   </div>   


 </div>   
     
</div>
    </div>
    <div class="col-md-9">
      <br />
      <form id="publication_search" action="/publications?query=" accept-charset="UTF-8" data-remote="true" method="get"><input name="utf8" type="hidden" value="&#x2713;" />



    <div class="input-group">
        <input type="text" name="query" id="query" class="form-control" autocomplete="off" placeholder="Search Publications ..." />
        <span class="input-group-btn">
            <button class="btn-u search-btn-u" type="submit"><i class="fa fa-search"></i></button>
        </span>
    </div>
    
</form>
      <hr />
      <div id="publicationsDiv">
        <div class="row">
	<div class="col-md-12">
		<span class="results-number custom-results-number pull-left">
			752 results

			<!-- IW-425 -->
			<!-- <small class="all-bib-export-button-text">()</small> -->
			<small class="all-bib-export-button-text">(<a target="_blank" href="/publications/get_bibtexfile_all?action=index&amp;controller=publications&amp;export_type=bibtex">View BibTeX file of all listed publications</a>)</small>

			</span>
			<span class="results-number pull-right">
				<div class="row ">
					<div class="col-md-12 viewSwitchButtonGroup">
						<div class="btn-group">
							<button id="detailViewButton" type="button" class="btn btn-default btn-sm">
								<i class="fa fa-th-list"></i>
							</button>
							<button id="listViewButton" type="button" class="btn btn-default btn-sm">
								<i class="fa fa-align-justify"></i>
							</button>
						</div>
					</div>
				</div>
			</span>
		</div>
	</div>

	<div id="publicationViewContainer" data-department="">



		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12 ">
					<div class="publication-year-container">
						<h4 class="year-h4-span "><span class="label label-default pull-right">2022</span></h4>
					</div>
				</div>
				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr-top" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c"><img class="img-responsive img-bordered t-img-bordered" alt="SCARF: Capturing and Animation of Body and Clothing from Monocular Video" src="/uploads/publication/image/27367/thumb_xl_teaser_2col.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c">SCARF: Capturing and Animation of Body and Clothing from Monocular Video</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/yfeng" >Feng, Y.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, Pollefeys, M., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>
							</p>


							In <em>SIGGRAPH Asia 2022 Conference Papers</em>,  pages: 9, SA’22, December 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27367">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27367" href="#abstractContent27367"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27367" class="panel-collapse collapse ">
											<div class="panel-body">
												We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://yfeng95.github.io/scarf/"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/yfeng95/SCARF"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2210.01868.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1145/3550469.3555423">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27367/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.: https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&amp;amp;title=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;summary=SCARF: Capturing and Animation of Body and Clothing from Monocular Video&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. %20https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;body=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="col-md-12 publication-container-list ">
						<div class="publication-year-container">
							<h4 class="year-h4-span "><span class="label label-default pull-right">2022</span></h4>
						</div>
					</div>
					<div class="margin-bottom-20"></div>
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/yfeng" >Feng, Y.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, Pollefeys, M., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>  
									
									<strong><a href =/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c>SCARF: Capturing and Animation of Body and Clothing from Monocular Video</a></strong> 
									

									In <em>SIGGRAPH Asia 2022 Conference Papers</em>,  pages: 9, SA’22, December 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://yfeng95.github.io/scarf/"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/yfeng95/SCARF"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2210.01868.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1145/3550469.3555423">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27367/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/dart2022"><img class="img-responsive img-bordered t-img-bordered" alt="{DART}: {A}rticulated {H}and {M}odel with {D}iverse {A}ccessories and {R}ich {T}extures" src="/uploads/publication/image/27270/thumb_xl_teaser.jpeg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/dart2022">DART: Articulated Hand Model with Diverse Accessories and Rich Textures</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Gao, D., <span class="default-link-ul"><a href="/person/yxiu" >Xiu, Y.</a></span>, Li, K., Yang, L., Wang, F., Zhang, P., Zhang, B., Lu, C., Tan, P.
							</p>


							<em>Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)</em>, November 2022 <small class='text-muted'>(conference)</small>

							<p>
								<div id="abstractAccordion27270">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27270" href="#abstractContent27270"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27270" class="panel-collapse collapse ">
											<div class="panel-body">
												Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision & graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic & lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation & surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://dart2022.github.io/"><i class="fa fa-github-square" ></i>  Home</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/DART2022/DARTset"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtube.com/embed/VvlUYe-9b7U"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27270/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dart2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.: https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dart2022&amp;amp;title=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;summary={DART}: {A}rticulated {H}and {M}odel with {D}iverse {A}ccessories and {R}ich {T}extures&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. %20https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/dart2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Gao, D., <span class="default-link-ul"><a href="/person/yxiu" >Xiu, Y.</a></span>, Li, K., Yang, L., Wang, F., Zhang, P., Zhang, B., Lu, C., Tan, P.  
									
									<strong><a href =/publications/dart2022>DART: Articulated Hand Model with Diverse Accessories and Rich Textures</a></strong> 
									

									<em>Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)</em>, November 2022 <small class='text-muted'>(conference)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://dart2022.github.io/"><i class="fa fa-github-square" ></i>  Home</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/DART2022/DARTset"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtube.com/embed/VvlUYe-9b7U"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27270/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/mica-eccv2022"><img class="img-responsive img-bordered t-img-bordered" alt="Towards Metrical Reconstruction of Human Faces" src="/uploads/publication/image/27243/thumb_xl_MICA.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/mica-eccv2022">Towards Metrical Reconstruction of Human Faces</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Zielonka, W., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/jthies" >Thies, J.</a></span>
							</p>


							In <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27243">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27243" href="#abstractContent27243"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27243" class="panel-collapse collapse ">
											<div class="panel-body">
												Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which
provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2204.06607.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://zielon.github.io/mica/"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/vzzEbvv08VA"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Zielon/MICA"><i class="fa fa-github-square" ></i>  code</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27243/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/mica-eccv2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which
provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.: https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/mica-eccv2022&amp;amp;title=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which
provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;summary=Towards Metrical Reconstruction of Human Faces&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which
provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. %20https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which
provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;body=https://ps.is.mpg.de/publications/mica-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Zielonka, W., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/jthies" >Thies, J.</a></span>  
									
									<strong><a href =/publications/mica-eccv2022>Towards Metrical Reconstruction of Human Faces</a></strong> 
									

									In <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2204.06607.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://zielon.github.io/mica/"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/vzzEbvv08VA"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Zielon/MICA"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27243/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/liu_iros_22"><img class="img-responsive img-bordered t-img-bordered" alt="Deep Residual Reinforcement Learning based Autonomous Blimp Control" src="/uploads/publication/image/27136/thumb_xl_blimp_rl_cover_new.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/liu_iros_22">Deep Residual Reinforcement Learning based Autonomous Blimp Control</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/yliu2" >Liu, Y. T.</a></span>, <span class="default-link-ul"><a href="/person/eprice" >Price, E.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>
							</p>


							<em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, IEEE, October 2022 <small class='text-muted'>(conference)</small>

							<p>
								<div id="abstractAccordion27136">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27136" href="#abstractContent27136"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27136" class="panel-collapse collapse ">
											<div class="panel-body">
												Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27136/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/liu_iros_22&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .: https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/liu_iros_22&amp;amp;title=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;summary=Deep Residual Reinforcement Learning based Autonomous Blimp Control&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . %20https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;body=https://ps.is.mpg.de/publications/liu_iros_22&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/yliu2" >Liu, Y. T.</a></span>, <span class="default-link-ul"><a href="/person/eprice" >Price, E.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>  
									
									<strong><a href =/publications/liu_iros_22>Deep Residual Reinforcement Learning based Autonomous Blimp Control</a></strong> 
									

									<em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, IEEE, October 2022 <small class='text-muted'>(conference)</small>

									<br />
									
									<p class="publication-button-p ">

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27136/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/trust-eccv2022"><img class="img-responsive img-bordered t-img-bordered" alt="Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation" src="/uploads/publication/image/27244/thumb_xl_TRUST.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/trust-eccv2022">Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/hfeng" >Feng, H.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/jtesch" >Tesch, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Abrevaya, V.
							</p>


							In <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27244">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27244" href="#abstractContent27244"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27244" class="panel-collapse collapse ">
											<div class="panel-body">
												Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene –as opposed to a cropped image of the face– contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-
of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2205.03962.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://trust.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/HavenFeng/TRUST"><i class="fa fa-github-square" ></i>  code</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27244/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/trust-eccv2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene –as opposed to a cropped image of the face– contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-
of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.: https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/trust-eccv2022&amp;amp;title=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene –as opposed to a cropped image of the face– contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-
of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;summary=Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene –as opposed to a cropped image of the face– contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-
of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene –as opposed to a cropped image of the face– contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-
of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/trust-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/hfeng" >Feng, H.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/jtesch" >Tesch, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Abrevaya, V.  
									
									<strong><a href =/publications/trust-eccv2022>Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation</a></strong> 
									

									In <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2205.03962.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://trust.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/HavenFeng/TRUST"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27244/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/learning_to_fit"><img class="img-responsive img-bordered t-img-bordered" alt="Learning to Fit Morphable Models" src="/uploads/publication/image/27333/thumb_xl_teaser_composition.jpg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/learning_to_fit">Learning to Fit Morphable Models</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/fbogo" >Bogo, F.</a></span>, Shen, J., Valentin, J.
							</p>


							In <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://vchoutas.github.io/learning_to_fit/"><i class="fa fa-github-square" ></i>  Project page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://www.dropbox.com/s/4cksaq8h4royt9r/Learning%20To%20Fit%20Morphable%20Models%20FHD%20HB.mp4?raw=1"><i class="fa fa-file-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/702/learning_to_fit.pdf"><i class="fa fa-file-pdf-o" ></i>  PDF</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/703/LearningToFit_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27333/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/learning_to_fit&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/learning_to_fit&amp;amp;title= &amp;amp;summary=Learning to Fit Morphable Models&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/learning_to_fit&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/fbogo" >Bogo, F.</a></span>, Shen, J., Valentin, J.  
									
									<strong><a href =/publications/learning_to_fit>Learning to Fit Morphable Models</a></strong> 
									

									In <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://vchoutas.github.io/learning_to_fit/"><i class="fa fa-github-square" ></i>  Project page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://www.dropbox.com/s/4cksaq8h4royt9r/Learning%20To%20Fit%20Morphable%20Models%20FHD%20HB.mp4?raw=1"><i class="fa fa-file-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/702/learning_to_fit.pdf"><i class="fa fa-file-pdf-o" ></i>  PDF</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/703/LearningToFit_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27333/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/supr"><img class="img-responsive img-bordered t-img-bordered" alt="{SUPR}: A Sparse Unified Part-Based Human Representation" src="/uploads/publication/image/27315/thumb_xl_FIG_1__1_.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/supr">SUPR: A Sparse Unified Part-Based Human Representation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/aosman" >Osman, A. A. A.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>European Conference on Computer Vision (ECCV) </em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27315">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27315" href="#abstractContent27315"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27315" class="panel-collapse collapse ">
											<div class="panel-body">
												Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts’ full range of motion. SUPR is publicly available for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://supr.is.tuebingen.mpg.de/"><i class="fa fa-file-o" ></i>  Project website</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/ahmedosman/SUPR"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/699/0570_source.pdf"><i class="fa fa-file-pdf-o" ></i>  Main Paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/700/0570-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supp. Mat. </a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/701/final_supr_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27315/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/supr&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts’ full range of motion. SUPR is publicly available for research purposes.: https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/supr&amp;amp;title=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts’ full range of motion. SUPR is publicly available for research purposes. &amp;amp;summary={SUPR}: A Sparse Unified Part-Based Human Representation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts’ full range of motion. SUPR is publicly available for research purposes. %20https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts’ full range of motion. SUPR is publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/supr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/aosman" >Osman, A. A. A.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/supr>SUPR: A Sparse Unified Part-Based Human Representation</a></strong> 
									

									In <em>European Conference on Computer Vision (ECCV) </em>, Springer International Publishing, October 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://supr.is.tuebingen.mpg.de/"><i class="fa fa-file-o" ></i>  Project website</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/ahmedosman/SUPR"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/699/0570_source.pdf"><i class="fa fa-file-pdf-o" ></i>  Main Paper</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/700/0570-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supp. Mat. </a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/701/final_supr_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27315/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/teach-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{TEACH}: {T}emporal {A}ction {C}omposition for 3D {H}umans" src="/uploads/publication/image/27247/thumb_xl_teach.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/teach-2022">TEACH: Temporal Action Composition for 3D Humans</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/nathanasiou" >Athanasiou, N.</a></span>, <span class="default-link-ul"><a href="/person/mpetrovich" >Petrovich, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/gvarol" >Varol, G.</a></span>
							</p>


							In <em>International Conference on 3D Vision (3DV)</em>, September 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27247">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27247" href="#abstractContent27247"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27247" class="panel-collapse collapse ">
											<div class="panel-body">
												Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for “TEmporal Action Compositions for Human motions”, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/athn-nik/teach"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2209.04066"><i class="fa fa-file-o" ></i>  paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://teach.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  website</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=ENr4GQO0RSc&amp;ab_channel=3DV2022"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27247/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/teach-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for “TEmporal Action Compositions for Human motions”, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.: https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/teach-2022&amp;amp;title=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for “TEmporal Action Compositions for Human motions”, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;summary={TEACH}: {T}emporal {A}ction {C}omposition for 3D {H}umans&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for “TEmporal Action Compositions for Human motions”, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for “TEmporal Action Compositions for Human motions”, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/teach-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/nathanasiou" >Athanasiou, N.</a></span>, <span class="default-link-ul"><a href="/person/mpetrovich" >Petrovich, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/gvarol" >Varol, G.</a></span>  
									
									<strong><a href =/publications/teach-2022>TEACH: Temporal Action Composition for 3D Humans</a></strong> 
									

									In <em>International Conference on 3D Vision (3DV)</em>, September 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/athn-nik/teach"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2209.04066"><i class="fa fa-file-o" ></i>  paper</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://teach.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  website</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=ENr4GQO0RSc&amp;ab_channel=3DV2022"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27247/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/intercap_gcpr2022"><img class="img-responsive img-bordered t-img-bordered" alt="{InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction" src="/uploads/publication/image/27278/thumb_xl_img.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/intercap_gcpr2022">InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">

							<p class="color-red">(Honorable Mention for Best Paper)</p>

							<p>
								<span class="default-link-ul"><a href="/person/yhuang2" >Huang, Y.</a></span>, <span class="default-link-ul"><a href="/person/otaheri" >Taheri, O.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>
							</p>


							In <em>German Conference on Pattern Recognition (DAGM)</em>, 13485, pages: 281-299, Springer, September 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27278">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27278" href="#abstractContent27278"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27278" class="panel-collapse collapse ">
											<div class="panel-body">
												Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/YinghaoHuang91/InterCap/tree/master"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://intercap.is.tue.mpg.de/download.php"><i class="fa fa-file-o" ></i>  Data</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/d5wHLDIqN6c"><i class="fa fa-file-video-o" ></i>  YouTube Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://intercap.is.tue.mpg.de/media/upload/main.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1007/978-3-031-16788-1_18">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27278/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.: https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/intercap_gcpr2022&amp;amp;title=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;summary={InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. %20https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/yhuang2" >Huang, Y.</a></span>, <span class="default-link-ul"><a href="/person/otaheri" >Taheri, O.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>  
									
									<strong><a href =/publications/intercap_gcpr2022>InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a></strong> 
									

									In <em>German Conference on Pattern Recognition (DAGM)</em>, 13485, pages: 281-299, Springer, September 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/YinghaoHuang91/InterCap/tree/master"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://intercap.is.tue.mpg.de/download.php"><i class="fa fa-file-o" ></i>  Data</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/d5wHLDIqN6c"><i class="fa fa-file-video-o" ></i>  YouTube Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://intercap.is.tue.mpg.de/media/upload/main.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1007/978-3-031-16788-1_18">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27278/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/skirt-3dv-2022"><img class="img-responsive img-bordered t-img-bordered" alt="Neural Point-based Shape Modeling of Humans in Challenging Clothing" src="/uploads/publication/image/27249/thumb_xl_SkiRT_small_teaser_website.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/skirt-3dv-2022">Neural Point-based Shape Modeling of Humans in Challenging Clothing</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/qma" >Ma, Q.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>
							</p>


							In <em>International Conference on 3D Vision (3DV) 2022</em>, September 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27249">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27249" href="#abstractContent27249"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27249" class="panel-collapse collapse ">
											<div class="panel-body">
												Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent “coarse shape” that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://qianlim.github.io/SkiRT"><i class="fa fa-github-square" ></i>  Project page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/qianlim/SkiRT"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2209.06814"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/695/SkiRT_main_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/696/SkiRT_supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supp</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/697/090_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://qianlim.github.io/SkiRT"><i class="fa fa-external-link" ></i>  link (url)</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27249/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent “coarse shape” that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.: https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/skirt-3dv-2022&amp;amp;title=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent “coarse shape” that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;summary=Neural Point-based Shape Modeling of Humans in Challenging Clothing&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent “coarse shape” that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. %20https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent “coarse shape” that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/qma" >Ma, Q.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>  
									
									<strong><a href =/publications/skirt-3dv-2022>Neural Point-based Shape Modeling of Humans in Challenging Clothing</a></strong> 
									

									In <em>International Conference on 3D Vision (3DV) 2022</em>, September 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://qianlim.github.io/SkiRT"><i class="fa fa-github-square" ></i>  Project page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/qianlim/SkiRT"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2209.06814"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/695/SkiRT_main_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/696/SkiRT_supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supp</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/697/090_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://qianlim.github.io/SkiRT"><i class="fa fa-external-link" ></i>  link (url)</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27249/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/wang2022reconstruction"><img class="img-responsive img-bordered t-img-bordered" alt="Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors" src="/uploads/publication/image/27340/thumb_xl_rhoi-overview.jpg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/wang2022reconstruction">Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Wang, X., Li, G., Kuo, Y., <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, Aksan, E., Hilliges, O.
							</p>


							In September 2022 <small class='text-muted'>(inproceedings)</small>

							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/rhoi/"><i class="fa fa-github-square" ></i>  Project Page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/YB1_xKlueUI"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2209.02485"><i class="fa fa-file-o" ></i>  Arxiv</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27340/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/wang2022reconstruction&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/wang2022reconstruction&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/wang2022reconstruction&amp;amp;title= &amp;amp;summary=Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/wang2022reconstruction&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/wang2022reconstruction&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Wang, X., Li, G., Kuo, Y., <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, Aksan, E., Hilliges, O.  
									
									<strong><a href =/publications/wang2022reconstruction>Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors</a></strong> 
									

									In September 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/rhoi/"><i class="fa fa-github-square" ></i>  Project Page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/YB1_xKlueUI"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2209.02485"><i class="fa fa-file-o" ></i>  Arxiv</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27340/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/invgan-gcpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{InvGAN}: Invertible {GANs}" src="/uploads/publication/image/27334/thumb_xl_invgan.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/invgan-gcpr-2022">InvGAN: Invertible GANs</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">

							<p class="color-red">(Best Paper Award)</p>

							<p>
								<span class="default-link-ul"><a href="/person/pghosh" >Ghosh, P.</a></span>, <span class="default-link-ul"><a href="/person/dzietlow" >Zietlow, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Davis, L. S., Hu, X.
							</p>


							In <em>German Conference on Pattern Recognition (GCPR)</em>,  pages: 3-19, Springer, September 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27334">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27334" href="#abstractContent27334"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27334" class="panel-collapse collapse ">
											<div class="panel-body">
												Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2112.04598.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1007/978-3-031-16788-1_1">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27334/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments.: https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/invgan-gcpr-2022&amp;amp;title=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. &amp;amp;summary={InvGAN}: Invertible {GANs}&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. %20https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. &amp;amp;body=https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/pghosh" >Ghosh, P.</a></span>, <span class="default-link-ul"><a href="/person/dzietlow" >Zietlow, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Davis, L. S., Hu, X.  
									
									<strong><a href =/publications/invgan-gcpr-2022>InvGAN: Invertible GANs</a></strong> 
									

									In <em>German Conference on Pattern Recognition (GCPR)</em>,  pages: 3-19, Springer, September 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2112.04598.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1007/978-3-031-16788-1_1">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27334/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/ziani2022tempclr"><img class="img-responsive img-bordered t-img-bordered" alt="TempCLR: Reconstructing Hands via Time-Coherent Contrastive Learning" src="/uploads/publication/image/27339/thumb_xl_thumbnail.jpg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/ziani2022tempclr">TempCLR: Reconstructing Hands via Time-Coherent Contrastive Learning</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Ziani, A., <span class="default-link-ul"><a href="/person/fzicong" >Fan, Z. (.</a></span>, <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, Christen, S., Hilliges, O.
							</p>


							In September 2022 <small class='text-muted'>(inproceedings)</small> <span class='label label-light'>Accepted</span>

							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/tempclr"><i class="fa fa-github-square" ></i>  Project Page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/eth-ait/tempclr"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://arxiv.org/abs/2209.00489"><i class="fa fa-file-o" ></i>  Arxiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/VSsKx8SnFio"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/tempclr"><i class="fa fa-external-link" ></i>  link (url)</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27339/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/ziani2022tempclr&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/ziani2022tempclr&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/ziani2022tempclr&amp;amp;title= &amp;amp;summary=TempCLR: Reconstructing Hands via Time-Coherent Contrastive Learning&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/ziani2022tempclr&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/ziani2022tempclr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Ziani, A., <span class="default-link-ul"><a href="/person/fzicong" >Fan, Z. (.</a></span>, <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, Christen, S., Hilliges, O.  
									
									<strong><a href =/publications/ziani2022tempclr>TempCLR: Reconstructing Hands via Time-Coherent Contrastive Learning</a></strong> 
									

									In September 2022 <small class='text-muted'>(inproceedings)</small> <span class='label label-light'>Accepted</span>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/tempclr"><i class="fa fa-github-square" ></i>  Project Page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/eth-ait/tempclr"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://arxiv.org/abs/2209.00489"><i class="fa fa-file-o" ></i>  Arxiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/VSsKx8SnFio"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/tempclr"><i class="fa fa-external-link" ></i>  link (url)</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27339/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/zheng-cvpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{I M} Avatar: Implicit Morphable Head Avatars from Videos" src="/uploads/publication/image/27076/thumb_xl_imavatarteaser.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/zheng-cvpr-2022">I M Avatar: Implicit Morphable Head Avatars from Videos</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/yzheng" >Zheng, Y.</a></span>, Abrevaya, V. F., Marcel C. Bühler, , <span class="default-link-ul"><a href="/person/xchen2" >Chen, X.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Hilliges, O.
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13545-13555, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27076">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27076" href="#abstractContent27076"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27076" class="panel-collapse collapse ">
											<div class="panel-body">
												Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2112.07471"><i class="fa fa-file-o" ></i>  paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://ait.ethz.ch/projects/2022/IMavatar/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/915baJNX-IU"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/zhengyuf/IMavatar"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://dataset.ait.ethz.ch/downloads/imaOsdfvRe/"><i class="fa fa-file-o" ></i>  synth. data</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27076/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.: https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/zheng-cvpr-2022&amp;amp;title=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. &amp;amp;summary={I M} Avatar: Implicit Morphable Head Avatars from Videos&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. %20https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. &amp;amp;body=https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/yzheng" >Zheng, Y.</a></span>, Abrevaya, V. F., Marcel C. Bühler, , <span class="default-link-ul"><a href="/person/xchen2" >Chen, X.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Hilliges, O.  
									
									<strong><a href =/publications/zheng-cvpr-2022>I M Avatar: Implicit Morphable Head Avatars from Videos</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13545-13555, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2112.07471"><i class="fa fa-file-o" ></i>  paper</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://ait.ethz.ch/projects/2022/IMavatar/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/915baJNX-IU"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/zhengyuf/IMavatar"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://dataset.ait.ethz.ch/downloads/imaOsdfvRe/"><i class="fa fa-file-o" ></i>  synth. data</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27076/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/xiu2022icon"><img class="img-responsive img-bordered t-img-bordered" alt="{ICON}: {I}mplicit {C}lothed humans {O}btained from {N}ormals" src="/uploads/publication/image/27033/thumb_xl_preview.jpeg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/xiu2022icon">ICON: Implicit Clothed humans Obtained from Normals</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/yxiu" >Xiu, Y.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13296-13306, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27033">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27033" href="#abstractContent27033"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27033" class="panel-collapse collapse ">
											<div class="panel-body">
												Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON ("Implicit Clothed humans Obtained from Normals"), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://icon.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Home</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/yuliangxiu/icon"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://huggingface.co/spaces/Yuliang/ICON"><i class="fa fa-file-o" ></i>  Demo</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/hZd6AYin2DE"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2112.09127"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/688/01209.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/689/01209-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat.</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/690/ICON-poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>


						<span>
							<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27033/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/xiu2022icon&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.: https://ps.is.mpg.de/publications/xiu2022icon&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/xiu2022icon&amp;amp;title=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. &amp;amp;summary={ICON}: {I}mplicit {C}lothed humans {O}btained from {N}ormals&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. %20https://ps.is.mpg.de/publications/xiu2022icon&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. &amp;amp;body=https://ps.is.mpg.de/publications/xiu2022icon&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/yxiu" >Xiu, Y.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/xiu2022icon>ICON: Implicit Clothed humans Obtained from Normals</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13296-13306, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://icon.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Home</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/yuliangxiu/icon"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://huggingface.co/spaces/Yuliang/ICON"><i class="fa fa-file-o" ></i>  Demo</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/hZd6AYin2DE"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2112.09127"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/688/01209.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/689/01209-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat.</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/690/ICON-poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>

										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27033/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/huang2022rich"><img class="img-responsive img-bordered t-img-bordered" alt="Capturing and Inferring Dense Full-Body Human-Scene Contact" src="/uploads/publication/image/27046/thumb_xl_Screen_Shot_2022-05-28_at_1.55.51_PM.jpg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/huang2022rich">Capturing and Inferring Dense Full-Body Human-Scene Contact</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/hyi" >Yi, H.</a></span>, <span class="default-link-ul"><a href="/person/mhoeschle" >Höschle, M.</a></span>, <span class="default-link-ul"><a href="/person/msafroshkin" >Safroshkin, M.</a></span>, <span class="default-link-ul"><a href="/person/talexiadis" >Alexiadis, T.</a></span>, <span class="default-link-ul"><a href="/person/senya" >Polikovsky, S.</a></span>, <span class="default-link-ul"><a href="/person/dscharstein" >Scharstein, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13274-13285, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27046">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27046" href="#abstractContent27046"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27046" class="panel-collapse collapse ">
											<div class="panel-body">
												Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for “Real scenes, Interaction, Contact and Humans.” RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://rich.is.tue.mpg.de"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2206.09553.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/paulchhuang/bstro"><i class="fa fa-github-square" ></i>  BSTRO code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/IbFc12L5Kc4"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>


						<span>
							<a data-toggle="tooltip" data-title="Capturing Contact" class="btn btn-default btn-xs" href="/research_projects/capturing-contact"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27046/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/huang2022rich&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for “Real scenes, Interaction, Contact and Humans.” RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.: https://ps.is.mpg.de/publications/huang2022rich&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/huang2022rich&amp;amp;title=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for “Real scenes, Interaction, Contact and Humans.” RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. &amp;amp;summary=Capturing and Inferring Dense Full-Body Human-Scene Contact&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for “Real scenes, Interaction, Contact and Humans.” RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/huang2022rich&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for “Real scenes, Interaction, Contact and Humans.” RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/huang2022rich&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/hyi" >Yi, H.</a></span>, <span class="default-link-ul"><a href="/person/mhoeschle" >Höschle, M.</a></span>, <span class="default-link-ul"><a href="/person/msafroshkin" >Safroshkin, M.</a></span>, <span class="default-link-ul"><a href="/person/talexiadis" >Alexiadis, T.</a></span>, <span class="default-link-ul"><a href="/person/senya" >Polikovsky, S.</a></span>, <span class="default-link-ul"><a href="/person/dscharstein" >Scharstein, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/huang2022rich>Capturing and Inferring Dense Full-Body Human-Scene Contact</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13274-13285, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://rich.is.tue.mpg.de"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2206.09553.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/paulchhuang/bstro"><i class="fa fa-github-square" ></i>  BSTRO code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/IbFc12L5Kc4"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		

										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Capturing Contact" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/capturing-contact"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27046/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/christen-cvpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{D-Grasp}: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions" src="/uploads/publication/image/27077/thumb_xl_dgrabteaser.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/christen-cvpr-2022">D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Christen, S., <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, Aksan, E., Hwangbo, J., Song, J., Hilliges, O.
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 20577-20586, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27077">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27077" href="#abstractContent27077"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27077" class="panel-collapse collapse ">
											<div class="panel-body">
												We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2112.03028.pdf"><i class="fa fa-file-pdf-o" ></i>  paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/d-grasp/"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/5OqbDq-pgLc"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/christsa/dgrasp"><i class="fa fa-github-square" ></i>  code</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27077/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/christen-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.: https://ps.is.mpg.de/publications/christen-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/christen-cvpr-2022&amp;amp;title=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. &amp;amp;summary={D-Grasp}: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. %20https://ps.is.mpg.de/publications/christen-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. &amp;amp;body=https://ps.is.mpg.de/publications/christen-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Christen, S., <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, Aksan, E., Hwangbo, J., Song, J., Hilliges, O.  
									
									<strong><a href =/publications/christen-cvpr-2022>D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 20577-20586, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2112.03028.pdf"><i class="fa fa-file-pdf-o" ></i>  paper</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.github.io/d-grasp/"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/5OqbDq-pgLc"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/christsa/dgrasp"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27077/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/rueeg-cvpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{BARC}: Learning to Regress {3D} Dog Shape from Images by Exploiting Breed Information" src="/uploads/publication/image/27073/thumb_xl_dogs_teaser2_copy_3.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/rueeg-cvpr-2022">BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/nrueegg" >Rueegg, N.</a></span>, <span class="default-link-ul"><a href="/person/szuffi" >Zuffi, S.</a></span>, Schindler, K., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 3876-3884, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27073">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27073" href="#abstractContent27073"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27073" class="panel-collapse collapse ">
											<div class="panel-body">
												Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/media/upload/barc_main.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/media/upload/barc_supmat-compressed-2.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat.</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/runa91/barc_release"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/media/upload/BARC_poster_v12.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/CSPbb1p5Hso"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://huggingface.co/spaces/runa91/barc_gradio"><i class="fa fa-file-o" ></i>  Demo</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27073/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.: https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&amp;amp;title=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. &amp;amp;summary={BARC}: Learning to Regress {3D} Dog Shape from Images by Exploiting Breed Information&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. %20https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. &amp;amp;body=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/nrueegg" >Rueegg, N.</a></span>, <span class="default-link-ul"><a href="/person/szuffi" >Zuffi, S.</a></span>, Schindler, K., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/rueeg-cvpr-2022>BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 3876-3884, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/media/upload/barc_main.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/media/upload/barc_supmat-compressed-2.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat.</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/runa91/barc_release"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://barc.is.tue.mpg.de/media/upload/BARC_poster_v12.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/CSPbb1p5Hso"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://huggingface.co/spaces/runa91/barc_gradio"><i class="fa fa-file-o" ></i>  Demo</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27073/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/khirodkar_ochmr_2022"><img class="img-responsive img-bordered t-img-bordered" alt="Occluded Human Mesh Recovery" src="/uploads/publication/image/26962/thumb_xl_teaser_ps.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/khirodkar_ochmr_2022">Occluded Human Mesh Recovery</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Khirodkar, R., <span class="default-link-ul"><a href="/person/stripathi" >Tripathi, S.</a></span>, Kitani, K.
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 1715-1725, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26962">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26962" href="#abstractContent26962"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26962" class="panel-collapse collapse ">
											<div class="panel-body">
												Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://rawalkhirodkar.github.io/ochmr"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2203.13349"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26962/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released.: https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&amp;amp;title=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. &amp;amp;summary=Occluded Human Mesh Recovery&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. %20https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. &amp;amp;body=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Khirodkar, R., <span class="default-link-ul"><a href="/person/stripathi" >Tripathi, S.</a></span>, Kitani, K.  
									
									<strong><a href =/publications/khirodkar_ochmr_2022>Occluded Human Mesh Recovery</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 1715-1725, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://rawalkhirodkar.github.io/ochmr"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2203.13349"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26962/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/keller-cvpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{OSSO}: Obtaining Skeletal Shape from Outside" src="/uploads/publication/image/26967/thumb_xl_renderpeople.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/keller-cvpr-2022">OSSO: Obtaining Skeletal Shape from Outside</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/mkeller2" >Keller, M.</a></span>, <span class="default-link-ul"><a href="/person/szuffi" >Zuffi, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/spujades" >Pujades, S.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 20492-20501, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26967">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26967" href="#abstractContent26967"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26967" class="panel-collapse collapse ">
											<div class="panel-body">
												We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://osso.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Project page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/684/06883.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/685/06883-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat.</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26967/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/keller-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.: https://ps.is.mpg.de/publications/keller-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/keller-cvpr-2022&amp;amp;title=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. &amp;amp;summary={OSSO}: Obtaining Skeletal Shape from Outside&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. %20https://ps.is.mpg.de/publications/keller-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. &amp;amp;body=https://ps.is.mpg.de/publications/keller-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/mkeller2" >Keller, M.</a></span>, <span class="default-link-ul"><a href="/person/szuffi" >Zuffi, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/spujades" >Pujades, S.</a></span>  
									
									<strong><a href =/publications/keller-cvpr-2022>OSSO: Obtaining Skeletal Shape from Outside</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 20492-20501, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://osso.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Project page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/684/06883.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/685/06883-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat.</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26967/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/price-ias-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Simulation and Control of Deformable Autonomous Airships in Turbulent Wind" src="/uploads/publication/image/24598/thumb_xl_blimp.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/price-ias-2021">Simulation and Control of Deformable Autonomous Airships in Turbulent Wind</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/eprice" >Price, E.</a></span>, <span class="default-link-ul"><a href="/person/yliu2" >Liu, Y. T.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>
							</p>


							In <em>Intelligent Autonomous Systems 16</em>,  pages: 608-626, Lecture Notes in Networks and Systems, 412, <span class='text-muted'>(Editors:  Ang Jr, Marcelo H. and Asama, Hajime and Lin, Wei and Foong, Shaohui)</span>, Springer, Cham, 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion24598">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion24598" href="#abstractContent24598"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent24598" class="panel-collapse collapse ">
											<div class="panel-body">
												Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.
Solutions for simulation and control of these vehicles are ubiquitous. This is
not the case for airships, a simulation of which needs to address unique
properties, i) dynamic deformation in response to aerodynamic and control
forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)
high variability in airship designs regarding placement, direction and
vectoring of thrusters and control surfaces. We present a flexible framework
for modeling, simulation and control of airships, based on the Robot operating
system (ROS), simulation environment (Gazebo) and commercial off the shelf
(COTS) electronics, both of which are open source. Based on simulated wind and
deformation, we predict substantial effects on controllability, verified in
real world flight experiments. All our code is shared as open source, for the
benefit of the community and to facilitate lighter-than-air vehicle (LTAV)
research. https://github.com/robot-perception-group/airship_simulation
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2012.15684.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2012.15684"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/robot-perception-group/airship_simulation"><i class="fa fa-github-square" ></i>  project/code</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1007/978-3-030-95892-3_46">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/24598/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/price-ias-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.
Solutions for simulation and control of these vehicles are ubiquitous. This is
not the case for airships, a simulation of which needs to address unique
properties, i) dynamic deformation in response to aerodynamic and control
forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)
high variability in airship designs regarding placement, direction and
vectoring of thrusters and control surfaces. We present a flexible framework
for modeling, simulation and control of airships, based on the Robot operating
system (ROS), simulation environment (Gazebo) and commercial off the shelf
(COTS) electronics, both of which are open source. Based on simulated wind and
deformation, we predict substantial effects on controllability, verified in
real world flight experiments. All our code is shared as open source, for the
benefit of the community and to facilitate lighter-than-air vehicle (LTAV)
research. https://github.com/robot-perception-group/airship_simulation: https://ps.is.mpg.de/publications/price-ias-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/price-ias-2021&amp;amp;title=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.
Solutions for simulation and control of these vehicles are ubiquitous. This is
not the case for airships, a simulation of which needs to address unique
properties, i) dynamic deformation in response to aerodynamic and control
forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)
high variability in airship designs regarding placement, direction and
vectoring of thrusters and control surfaces. We present a flexible framework
for modeling, simulation and control of airships, based on the Robot operating
system (ROS), simulation environment (Gazebo) and commercial off the shelf
(COTS) electronics, both of which are open source. Based on simulated wind and
deformation, we predict substantial effects on controllability, verified in
real world flight experiments. All our code is shared as open source, for the
benefit of the community and to facilitate lighter-than-air vehicle (LTAV)
research. https://github.com/robot-perception-group/airship_simulation &amp;amp;summary=Simulation and Control of Deformable Autonomous Airships in Turbulent Wind&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.
Solutions for simulation and control of these vehicles are ubiquitous. This is
not the case for airships, a simulation of which needs to address unique
properties, i) dynamic deformation in response to aerodynamic and control
forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)
high variability in airship designs regarding placement, direction and
vectoring of thrusters and control surfaces. We present a flexible framework
for modeling, simulation and control of airships, based on the Robot operating
system (ROS), simulation environment (Gazebo) and commercial off the shelf
(COTS) electronics, both of which are open source. Based on simulated wind and
deformation, we predict substantial effects on controllability, verified in
real world flight experiments. All our code is shared as open source, for the
benefit of the community and to facilitate lighter-than-air vehicle (LTAV)
research. https://github.com/robot-perception-group/airship_simulation %20https://ps.is.mpg.de/publications/price-ias-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.
Solutions for simulation and control of these vehicles are ubiquitous. This is
not the case for airships, a simulation of which needs to address unique
properties, i) dynamic deformation in response to aerodynamic and control
forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)
high variability in airship designs regarding placement, direction and
vectoring of thrusters and control surfaces. We present a flexible framework
for modeling, simulation and control of airships, based on the Robot operating
system (ROS), simulation environment (Gazebo) and commercial off the shelf
(COTS) electronics, both of which are open source. Based on simulated wind and
deformation, we predict substantial effects on controllability, verified in
real world flight experiments. All our code is shared as open source, for the
benefit of the community and to facilitate lighter-than-air vehicle (LTAV)
research. https://github.com/robot-perception-group/airship_simulation &amp;amp;body=https://ps.is.mpg.de/publications/price-ias-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/eprice" >Price, E.</a></span>, <span class="default-link-ul"><a href="/person/yliu2" >Liu, Y. T.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>  
									
									<strong><a href =/publications/price-ias-2021>Simulation and Control of Deformable Autonomous Airships in Turbulent Wind</a></strong> 
									

									In <em>Intelligent Autonomous Systems 16</em>,  pages: 608-626, Lecture Notes in Networks and Systems, 412, <span class='text-muted'>(Editors:  Ang Jr, Marcelo H. and Asama, Hajime and Lin, Wei and Foong, Shaohui)</span>, Springer, Cham, 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2012.15684.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2012.15684"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/robot-perception-group/airship_simulation"><i class="fa fa-github-square" ></i>  project/code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1007/978-3-030-95892-3_46">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/24598/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/shapy-2022"><img class="img-responsive img-bordered t-img-bordered" alt="Accurate 3D Body Shape Regression using Metric and Semantic Attributes" src="/uploads/publication/image/27067/thumb_xl_teaser_final_cropped.jpg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/shapy-2022">Accurate 3D Body Shape Regression using Metric and Semantic Attributes</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">

							<p class="color-red">(Best Paper Award Candidate)</p>

							<p>
								<span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/lmueller2" >Müller, L.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 2718-2728, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27067">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27067" href="#abstractContent27067"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27067" class="panel-collapse collapse ">
											<div class="panel-body">
												While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://shapy.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Home</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/muelea/shapy"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/7TzXrL4eV9g"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/691/00928.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/692/00928-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supplementary Material</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/693/Shapy_poster_compressed.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>


						<span>
							<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27067/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/shapy-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research.: https://ps.is.mpg.de/publications/shapy-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/shapy-2022&amp;amp;title=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. &amp;amp;summary=Accurate 3D Body Shape Regression using Metric and Semantic Attributes&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. %20https://ps.is.mpg.de/publications/shapy-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. &amp;amp;body=https://ps.is.mpg.de/publications/shapy-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/lmueller2" >Müller, L.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/shapy-2022>Accurate 3D Body Shape Regression using Metric and Semantic Attributes</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 2718-2728, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://shapy.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Home</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/muelea/shapy"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/7TzXrL4eV9g"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/691/00928.pdf"><i class="fa fa-file-pdf-o" ></i>  Paper</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/692/00928-supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supplementary Material</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/693/Shapy_poster_compressed.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>

										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27067/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/sun-cvpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="Putting People in their Place: Monocular Regression of {3D} People in Depth" src="/uploads/publication/image/27074/thumb_xl_ROMPteaser.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/sun-cvpr-2022">Putting People in their Place: Monocular Regression of 3D People in Depth</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Sun, Y., Liu, W., Bao, Q., Fu, Y., Mei, T., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13243-13252, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27074">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27074" href="#abstractContent27074"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27074" class="panel-collapse collapse ">
											<div class="panel-body">
												Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a "Relative Human" (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2112.08274"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arthur151.github.io/BEV/BEV.html"><i class="fa fa-github-square" ></i>  Project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Arthur151/ROMP"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Arthur151/Relative_Human"><i class="fa fa-github-square" ></i>  Data</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27074/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/sun-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.: https://ps.is.mpg.de/publications/sun-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/sun-cvpr-2022&amp;amp;title=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. &amp;amp;summary=Putting People in their Place: Monocular Regression of {3D} People in Depth&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. %20https://ps.is.mpg.de/publications/sun-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/sun-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Sun, Y., Liu, W., Bao, Q., Fu, Y., Mei, T., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/sun-cvpr-2022>Putting People in their Place: Monocular Regression of 3D People in Depth</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13243-13252, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2112.08274"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arthur151.github.io/BEV/BEV.html"><i class="fa fa-github-square" ></i>  Project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Arthur151/ROMP"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Arthur151/Relative_Human"><i class="fa fa-github-square" ></i>  Data</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27074/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/xu2022gdna"><img class="img-responsive img-bordered t-img-bordered" alt="{gDNA}: Towards Generative Detailed Neural Avatars" src="/uploads/publication/image/27036/thumb_xl_gDNA_thumbnail.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/xu2022gdna">gDNA: Towards Generative Detailed Neural Avatars</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/xchen2" >Chen, X.</a></span>, Jiang, T., Song, J., <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/ageiger" >Geiger, A.</a></span>, Hilliges, O.
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 20427-20437, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27036">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27036" href="#abstractContent27036"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27036" class="panel-collapse collapse ">
											<div class="panel-body">
												To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://xuchen-ethz.github.io/gdna/"><i class="fa fa-github-square" ></i>  Project page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://ait.ethz.ch/projects/2022/gdna/downloads/main.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=uOyoH7OO16I"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/xuchen-ethz/gdna"><i class="fa fa-github-square" ></i>  Code</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27036/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/xu2022gdna&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.: https://ps.is.mpg.de/publications/xu2022gdna&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/xu2022gdna&amp;amp;title=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. &amp;amp;summary={gDNA}: Towards Generative Detailed Neural Avatars&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. %20https://ps.is.mpg.de/publications/xu2022gdna&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. &amp;amp;body=https://ps.is.mpg.de/publications/xu2022gdna&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/xchen2" >Chen, X.</a></span>, Jiang, T., Song, J., <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/ageiger" >Geiger, A.</a></span>, Hilliges, O.  
									
									<strong><a href =/publications/xu2022gdna>gDNA: Towards Generative Detailed Neural Avatars</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 20427-20437, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://xuchen-ethz.github.io/gdna/"><i class="fa fa-github-square" ></i>  Project page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://ait.ethz.ch/projects/2022/gdna/downloads/main.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=uOyoH7OO16I"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/xuchen-ethz/gdna"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27036/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/yi_mover_2022"><img class="img-responsive img-bordered t-img-bordered" alt="Human-Aware Object Placement for Visual Environment Reconstruction" src="/uploads/publication/image/26938/thumb_xl_teaser.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/yi_mover_2022">Human-Aware Object Placement for Visual Environment Reconstruction</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/hyi" >Yi, H.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/mhassan" >Hassan, M.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/jthies" >Thies, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 3959-3970, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26938">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26938" href="#abstractContent26938"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26938" class="panel-collapse collapse ">
											<div class="panel-body">
												Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://mover.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2203.03609.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
						</span>


						<span>
							<a data-toggle="tooltip" data-title="Capturing Contact" class="btn btn-default btn-xs" href="/research_projects/capturing-contact"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26938/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi_mover_2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/yi_mover_2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi_mover_2022&amp;amp;title=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. &amp;amp;summary=Human-Aware Object Placement for Visual Environment Reconstruction&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/yi_mover_2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/yi_mover_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/hyi" >Yi, H.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/mhassan" >Hassan, M.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/jthies" >Thies, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/yi_mover_2022>Human-Aware Object Placement for Visual Environment Reconstruction</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 3959-3970, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://mover.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2203.03609.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
										</span>		

										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Capturing Contact" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/capturing-contact"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26938/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/emoca-cvpr-2022"><img class="img-responsive img-bordered t-img-bordered" alt="Emotion Driven Monocular Face Capture and Animation" src="/uploads/publication/image/26969/thumb_xl_EMOCA.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/emoca-cvpr-2022">Emotion Driven Monocular Face Capture and Animation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/rdanecek" >Daněček, R.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>
							</p>


							In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 20311-20322, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26969">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26969" href="#abstractContent26969"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26969" class="panel-collapse collapse ">
											<div class="panel-body">
												As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/radekd91/emoca"><i class="fa fa-github-square" ></i>  code </a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://emoca.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/686/EMOCA__CVPR22.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/687/EMOCA_sup_mat__CVPR2022.pdf"><i class="fa fa-file-pdf-o" ></i>  supplemental</a>
						</span>


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26969/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de.: https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/emoca-cvpr-2022&amp;amp;title=As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de. &amp;amp;summary=Emotion Driven Monocular Face Capture and Animation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/rdanecek" >Daněček, R.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>  
									
									<strong><a href =/publications/emoca-cvpr-2022>Emotion Driven Monocular Face Capture and Animation</a></strong> 
									

									In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 20311-20322, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/radekd91/emoca"><i class="fa fa-github-square" ></i>  code </a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://emoca.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/686/EMOCA__CVPR22.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/687/EMOCA_sup_mat__CVPR2022.pdf"><i class="fa fa-file-pdf-o" ></i>  supplemental</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26969/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/taheri-cvpr-2022a"><img class="img-responsive img-bordered t-img-bordered" alt="{GOAL}: Generating {4D} Whole-Body Motion for Hand-Object Grasping" src="/uploads/publication/image/27075/thumb_xl_teaser_copy.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/taheri-cvpr-2022a">GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/otaheri" >Taheri, O.</a></span>, <span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>
							</p>


							In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13263-13273, June 2022 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion27075">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion27075" href="#abstractContent27075"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent27075" class="panel-collapse collapse ">
											<div class="panel-body">
												Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/goal/GOAL_Taheri_CVPR2022.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/goal/GOAL_SuppMat_Taheri_CVPR2022.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://goal.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/A7b8DYovDZY"><i class="fa fa-file-video-o" ></i>  YouTube</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/otaheri/GOAL"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://goal.is.tue.mpg.de/download.php"><i class="fa fa-file-o" ></i>  Models</a>
						</span>


						<span>
							<a data-toggle="tooltip" data-title="Hands-Object Interaction" class="btn btn-default btn-xs" href="/research_projects/hands-in-action"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Capturing Contact" class="btn btn-default btn-xs" href="/research_projects/capturing-contact"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/27075/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.: https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&amp;amp;title=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. &amp;amp;summary={GOAL}: Generating {4D} Whole-Body Motion for Hand-Object Grasping&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. %20https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. &amp;amp;body=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/otaheri" >Taheri, O.</a></span>, <span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>  
									
									<strong><a href =/publications/taheri-cvpr-2022a>GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a></strong> 
									

									In <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 13263-13273, June 2022 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/goal/GOAL_Taheri_CVPR2022.pdf"><i class="fa fa-file-pdf-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/goal/GOAL_SuppMat_Taheri_CVPR2022.pdf"><i class="fa fa-file-pdf-o" ></i>  Sup. Mat</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://goal.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/A7b8DYovDZY"><i class="fa fa-file-video-o" ></i>  YouTube</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/otaheri/GOAL"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://goal.is.tue.mpg.de/download.php"><i class="fa fa-file-o" ></i>  Models</a>
										</span>		

										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Hands-Object Interaction" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/hands-in-action"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Capturing Contact" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/capturing-contact"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/27075/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/saini-irl-2022"><img class="img-responsive img-bordered t-img-bordered" alt="{AirPose}: Multi-View Fusion Network for Aerial {3D} Human Pose and Shape Estimation" src="/uploads/publication/image/26873/thumb_xl_NitinIRLAirPose.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/saini-irl-2022">AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/nsaini" >Saini, N.</a></span>, <span class="default-link-ul"><a href="/person/ebonetto" >Bonetto, E.</a></span>, <span class="default-link-ul"><a href="/person/eprice" >Price, E.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							<em>IEEE Robotics and Automation Letters</em>, 7(2):4805 - 4812, IEEE, April 2022, Also accepted and presented in the 2022 IEEE International Conference on Robotics and Automation (ICRA) <small class='text-muted'>(article)</small>

							<p>
								<div id="abstractAccordion26873">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26873" href="#abstractContent26873"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26873" class="panel-collapse collapse ">
											<div class="panel-body">
												In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://ieeexplore.ieee.org/document/9691814"><i class="fa fa-file-o" ></i>  paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/robot-perception-group/AirPose"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/Ss48ICeqvnQ"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9691814"><i class="fa fa-file-o" ></i>  pdf</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/LRA.2022.3145494">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26873/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/saini-irl-2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ.: https://ps.is.mpg.de/publications/saini-irl-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/saini-irl-2022&amp;amp;title=In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ. &amp;amp;summary={AirPose}: Multi-View Fusion Network for Aerial {3D} Human Pose and Shape Estimation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ. %20https://ps.is.mpg.de/publications/saini-irl-2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ. &amp;amp;body=https://ps.is.mpg.de/publications/saini-irl-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/nsaini" >Saini, N.</a></span>, <span class="default-link-ul"><a href="/person/ebonetto" >Bonetto, E.</a></span>, <span class="default-link-ul"><a href="/person/eprice" >Price, E.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/saini-irl-2022>AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation</a></strong> 
									

									<em>IEEE Robotics and Automation Letters</em>, 7(2):4805 - 4812, IEEE, April 2022, Also accepted and presented in the 2022 IEEE International Conference on Robotics and Automation (ICRA) <small class='text-muted'>(article)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://ieeexplore.ieee.org/document/9691814"><i class="fa fa-file-o" ></i>  paper</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/robot-perception-group/AirPose"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/Ss48ICeqvnQ"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9691814"><i class="fa fa-file-o" ></i>  pdf</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/LRA.2022.3145494">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26873/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/irotate2022"><img class="img-responsive img-bordered t-img-bordered" alt="{iRotate}: Active visual {SLAM} for omnidirectional robots" src="/uploads/publication/image/26976/thumb_xl_cover_lres.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/irotate2022">iRotate: Active visual SLAM for omnidirectional robots</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/ebonetto" >Bonetto, E.</a></span>, Goldschmid, P., <span class="default-link-ul"><a href="/person/mpabst" >Pabst, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>
							</p>


							<em>Robotics and Autonomous Systems</em>, Elsevier, 2022 <small class='text-muted'>(article)</small>

							<p>
								<div id="abstractAccordion26976">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26976" href="#abstractContent26976"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26976" class="panel-collapse collapse ">
											<div class="panel-body">
												In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot’s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot’s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels’ total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0921889022000550"><i class="fa fa-external-link" ></i>  link (url)</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1016/j.robot.2022.104102">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26976/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/irotate2022&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot’s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot’s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels’ total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation.: https://ps.is.mpg.de/publications/irotate2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/irotate2022&amp;amp;title=In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot’s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot’s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels’ total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation. &amp;amp;summary={iRotate}: Active visual {SLAM} for omnidirectional robots&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot’s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot’s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels’ total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation. %20https://ps.is.mpg.de/publications/irotate2022&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot’s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot’s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels’ total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation. &amp;amp;body=https://ps.is.mpg.de/publications/irotate2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/ebonetto" >Bonetto, E.</a></span>, Goldschmid, P., <span class="default-link-ul"><a href="/person/mpabst" >Pabst, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>  
									
									<strong><a href =/publications/irotate2022>iRotate: Active visual SLAM for omnidirectional robots</a></strong> 
									

									<em>Robotics and Autonomous Systems</em>, Elsevier, 2022 <small class='text-muted'>(article)</small>

									<br />
									
									<p class="publication-button-p ">

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0921889022000550"><i class="fa fa-external-link" ></i>  link (url)</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1016/j.robot.2022.104102">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26976/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12 publication-year-container-list-margin-top">
					<div class="publication-year-container">
						<h4 class="year-h4-span "><span class="label label-default pull-right">2021</span></h4>
					</div>
				</div>
				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr-top" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/pixie-3dv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Collaborative Regression of Expressive Bodies using Moderation" src="/uploads/publication/image/26352/thumb_xl_PIXIE.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/pixie-3dv-2021">Collaborative Regression of Expressive Bodies using Moderation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/yfeng" >Feng, Y.</a></span>, <span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							<em>2021 International Conference on 3D Vision (3DV 2021)</em>,  pages: 792-804, IEEE, Piscataway, NJ, December 2021 <small class='text-muted'>(conference)</small>

							<p>
								<div id="abstractAccordion26352">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26352" href="#abstractContent26352"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26352" class="panel-collapse collapse ">
											<div class="panel-body">
												Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X’s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer “gendered” 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2105.05301"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://pixie.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/667/PIXIE_3DV_CR.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/668/PIXIE_3DV_sup_mat.pdf"><i class="fa fa-file-pdf-o" ></i>  suppl</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/3DV53792.2021.00088">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Expressive Body Models" class="btn btn-default btn-xs" href="/research_projects/expressive-body-models"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26352/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/pixie-3dv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X’s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer “gendered” 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de.: https://ps.is.mpg.de/publications/pixie-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/pixie-3dv-2021&amp;amp;title=Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X’s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer “gendered” 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de. &amp;amp;summary=Collaborative Regression of Expressive Bodies using Moderation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X’s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer “gendered” 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/pixie-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X’s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer “gendered” 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/pixie-3dv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="col-md-12 publication-container-list publication-year-container-list-margin-top">
						<div class="publication-year-container">
							<h4 class="year-h4-span "><span class="label label-default pull-right">2021</span></h4>
						</div>
					</div>
					<div class="margin-bottom-20"></div>
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/yfeng" >Feng, Y.</a></span>, <span class="default-link-ul"><a href="/person/vchoutas" >Choutas, V.</a></span>, <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/dtzionas" >Tzionas, D.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/pixie-3dv-2021>Collaborative Regression of Expressive Bodies using Moderation</a></strong> 
									

									<em>2021 International Conference on 3D Vision (3DV 2021)</em>,  pages: 792-804, IEEE, Piscataway, NJ, December 2021 <small class='text-muted'>(conference)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2105.05301"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://pixie.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/667/PIXIE_3DV_CR.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/668/PIXIE_3DV_sup_mat.pdf"><i class="fa fa-file-pdf-o" ></i>  suppl</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/3DV53792.2021.00088">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Expressive Body Models" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/expressive-body-models"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26352/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/fan-3dv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation" src="/uploads/publication/image/26353/thumb_xl_hands3dv.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/fan-3dv-2021">Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/fzicong" >Fan, Z.</a></span>, Spurr, A., <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Hilliges, O.
							</p>


							<em>2021 International Conference on 3D Vision (3DV 2021)</em>,  pages: 1-10, IEEE, Piscataway, NJ, December 2021 <small class='text-muted'>(conference)</small>

							<p>
								<div id="abstractAccordion26353">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26353" href="#abstractContent26353"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26353" class="panel-collapse collapse ">
											<div class="panel-body">
												In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2107.00434"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://zc-alexfan.github.io/digit/"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/zc-alexfan/digit-interacting"><i class="fa fa-github-square" ></i>  code</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/3DV53792.2021.00011">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Hands-Object Interaction" class="btn btn-default btn-xs" href="/research_projects/hands-in-action"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26353/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/fan-3dv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes.: https://ps.is.mpg.de/publications/fan-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/fan-3dv-2021&amp;amp;title=In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes. &amp;amp;summary=Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes. %20https://ps.is.mpg.de/publications/fan-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/fan-3dv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/fzicong" >Fan, Z.</a></span>, Spurr, A., <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Hilliges, O.  
									
									<strong><a href =/publications/fan-3dv-2021>Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation</a></strong> 
									

									<em>2021 International Conference on 3D Vision (3DV 2021)</em>,  pages: 1-10, IEEE, Piscataway, NJ, December 2021 <small class='text-muted'>(conference)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2107.00434"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://zc-alexfan.github.io/digit/"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/zc-alexfan/digit-interacting"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/3DV53792.2021.00011">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Hands-Object Interaction" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/hands-in-action"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26353/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/wang2021neurips"><img class="img-responsive img-bordered t-img-bordered" alt="MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images" src="/uploads/publication/image/26258/thumb_xl_MetaAvatar.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/wang2021neurips">MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/swang" >Wang, S.</a></span>, Mihajlovic, M., <span class="default-link-ul"><a href="/person/qma" >Ma, Q.</a></span>, <span class="default-link-ul"><a href="/person/ageiger" >Geiger, A.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>
							</p>


							In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26258">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26258" href="#abstractContent26258"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26258" class="panel-collapse collapse ">
											<div class="panel-body">
												In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://neuralbodies.github.io/metavatar/"><i class="fa fa-github-square" ></i>  Project page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2106.11944"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>


						<span>
							<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Clothing Capture and Modeling" class="btn btn-default btn-xs" href="/research_projects/clothing"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26258/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/wang2021neurips&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.: https://ps.is.mpg.de/publications/wang2021neurips&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/wang2021neurips&amp;amp;title=In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. &amp;amp;summary=MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. %20https://ps.is.mpg.de/publications/wang2021neurips&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. &amp;amp;body=https://ps.is.mpg.de/publications/wang2021neurips&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/swang" >Wang, S.</a></span>, Mihajlovic, M., <span class="default-link-ul"><a href="/person/qma" >Ma, Q.</a></span>, <span class="default-link-ul"><a href="/person/ageiger" >Geiger, A.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>  
									
									<strong><a href =/publications/wang2021neurips>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</a></strong> 
									

									In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://neuralbodies.github.io/metavatar/"><i class="fa fa-github-square" ></i>  Project page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2106.11944"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		

										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="https://avg.is.mpg.de/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Clothing Capture and Modeling" class="btn btn-default btn-xs" href="https://avg.is.mpg.de/research_projects/clothing"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26258/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/foster-neuro-2021"><img class="img-responsive img-bordered t-img-bordered" alt="The neural coding of face and body orientation in occipitotemporal cortex" src="/uploads/publication/image/26583/thumb_xl_celiaNeuro.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/foster-neuro-2021">The neural coding of face and body orientation in occipitotemporal cortex</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/cfoster" >Foster, C.</a></span>, Zhao, M., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/abartels" >Bartels, A.</a></span>, Bülthoff, I.
							</p>


							<em>NeuroImage</em>,  pages: 1053-8119, December 2021 <small class='text-muted'>(article)</small>

							<p>
								<div id="abstractAccordion26583">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26583" href="#abstractContent26583"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26583" class="panel-collapse collapse ">
											<div class="panel-body">
												Face and body orientation convey important information for us to understand other people's actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants’ brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1053811921010557"><i class="fa fa-file-o" ></i>  paper</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.neuroimage.2021.118783">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26583/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/foster-neuro-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants’ brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex.: https://ps.is.mpg.de/publications/foster-neuro-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/foster-neuro-2021&amp;amp;title=Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants’ brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex. &amp;amp;summary=The neural coding of face and body orientation in occipitotemporal cortex&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants’ brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex. %20https://ps.is.mpg.de/publications/foster-neuro-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants’ brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex. &amp;amp;body=https://ps.is.mpg.de/publications/foster-neuro-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/cfoster" >Foster, C.</a></span>, Zhao, M., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/abartels" >Bartels, A.</a></span>, Bülthoff, I.  
									
									<strong><a href =/publications/foster-neuro-2021>The neural coding of face and body orientation in occipitotemporal cortex</a></strong> 
									

									<em>NeuroImage</em>,  pages: 1053-8119, December 2021 <small class='text-muted'>(article)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1053811921010557"><i class="fa fa-file-o" ></i>  paper</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.neuroimage.2021.118783">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26583/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/tpose_obesity_21"><img class="img-responsive img-bordered t-img-bordered" alt="A pose-independent method for accurate and precise body composition from 3D optical scans" src="/uploads/publication/image/26417/thumb_xl_tpose_obesity_21.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/tpose_obesity_21">A pose-independent method for accurate and precise body composition from 3D optical scans</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Wong, M. C., Ng, B. K., Tian, I., Sobhiyeh, S., Pagano, I., Dechenaud, M., Kennedy, S. F., Liu, Y. E., Kelly, N., Chow, D., Garber, A. K., Maskarinec, G., <span class="default-link-ul"><a href="/person/spujades" >Pujades, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Curless, B., Heymsfield, S. B., Shepherd, J. A.
							</p>


							<em>Obesity</em>, 29(11):1835-1847, Wiley, November 2021 <small class='text-muted'>(article)</small>

							<p>
								<div id="abstractAccordion26417">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26417" href="#abstractContent26417"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26417" class="panel-collapse collapse ">
											<div class="panel-body">
												Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.

Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).

Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.

Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.

											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://onlinelibrary.wiley.com/doi/10.1002/oby.23256"><i class="fa fa-file-o" ></i>  Wiley online adress</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1002/oby.23256">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Bodies in Medicine" class="btn btn-default btn-xs" href="/research_projects/medical-diagnosis"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26417/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tpose_obesity_21&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.

Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).

Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.

Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.
: https://ps.is.mpg.de/publications/tpose_obesity_21&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tpose_obesity_21&amp;amp;title=Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.

Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).

Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.

Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.
 &amp;amp;summary=A pose-independent method for accurate and precise body composition from 3D optical scans&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.

Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).

Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.

Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.
 %20https://ps.is.mpg.de/publications/tpose_obesity_21&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.

Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).

Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.

Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.
 &amp;amp;body=https://ps.is.mpg.de/publications/tpose_obesity_21&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Wong, M. C., Ng, B. K., Tian, I., Sobhiyeh, S., Pagano, I., Dechenaud, M., Kennedy, S. F., Liu, Y. E., Kelly, N., Chow, D., Garber, A. K., Maskarinec, G., <span class="default-link-ul"><a href="/person/spujades" >Pujades, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Curless, B., Heymsfield, S. B., Shepherd, J. A.  
									
									<strong><a href =/publications/tpose_obesity_21>A pose-independent method for accurate and precise body composition from 3D optical scans</a></strong> 
									

									<em>Obesity</em>, 29(11):1835-1847, Wiley, November 2021 <small class='text-muted'>(article)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://onlinelibrary.wiley.com/doi/10.1002/oby.23256"><i class="fa fa-file-o" ></i>  Wiley online adress</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/all_publications"><i class="fa fa-file-o" ></i>  Author Version</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1002/oby.23256">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Bodies in Medicine" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/medical-diagnosis"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26417/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/kalyan_fermiproblems_2021"><img class="img-responsive img-bordered t-img-bordered" alt="How much coffee was consumed during {EMNLP} 2019? Fermi Problems: A New Reasoning Challenge for {AI}" src="/uploads/publication/image/26154/thumb_xl_fermi_teaser_cropped.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/kalyan_fermiproblems_2021">How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Kalyan, A., Kumar, A., <span class="default-link-ul"><a href="/person/achandrasekaran" >Chandrasekaran, A.</a></span>, Sabharwal, A., Clark, P.
							</p>


							In <em>2021 Conference on Empirical Methods in Natural Language Processing - Proceedings of the Conference</em>,  pages: 7318-7328, <span class='text-muted'>(Editors:  Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Wen-tau Yih, Scott)</span>, Association for Computational Linguistics, Stroudsburg, PA, November 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26154">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26154" href="#abstractContent26154"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26154" class="panel-collapse collapse ">
											<div class="panel-body">
												Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, "How much would the sea level rise if all ice in the world melted?" FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2110.14207.pdf"><i class="fa fa-file-pdf-o" ></i>  paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://allenai.org/data/fermi"><i class="fa fa-file-o" ></i>  project webpage</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/allenai/fermi"><i class="fa fa-github-square" ></i>  data</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.18653/v1/2021.emnlp-main.582">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26154/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason.: https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&amp;amp;title=Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason. &amp;amp;summary=How much coffee was consumed during {EMNLP} 2019? Fermi Problems: A New Reasoning Challenge for {AI}&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason. %20https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason. &amp;amp;body=https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Kalyan, A., Kumar, A., <span class="default-link-ul"><a href="/person/achandrasekaran" >Chandrasekaran, A.</a></span>, Sabharwal, A., Clark, P.  
									
									<strong><a href =/publications/kalyan_fermiproblems_2021>How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI</a></strong> 
									

									In <em>2021 Conference on Empirical Methods in Natural Language Processing - Proceedings of the Conference</em>,  pages: 7318-7328, <span class='text-muted'>(Editors:  Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Wen-tau Yih, Scott)</span>, Association for Computational Linguistics, Stroudsburg, PA, November 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2110.14207.pdf"><i class="fa fa-file-pdf-o" ></i>  paper</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://allenai.org/data/fermi"><i class="fa fa-file-o" ></i>  project webpage</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/allenai/fermi"><i class="fa fa-github-square" ></i>  data</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.18653/v1/2021.emnlp-main.582">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26154/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/spice-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Learning Realistic Human Reposing using Cyclic Self-Supervision with {3D} Shape, Pose, and Appearance Consistency" src="/uploads/publication/image/26302/thumb_xl_Screenshot_from_2021-10-06_21-50-55.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/spice-iccv-2021">Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/ssanyal" >Sanyal, S.</a></span>, Vorobiov, A., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, Loper, M., Mohler, B., Davis, L., Romero, J., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11118-11127, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26302">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26302" href="#abstractContent26302"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26302" class="panel-collapse collapse ">
											<div class="panel-body">
												Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Sanyal_Learning_Realistic_Human_Reposing_Using_Cyclic_Self-Supervision_With_3D_Shape_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2110.05458v1"><i class="fa fa-file-o" ></i>  arxiv</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01095">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Neural Rendering" class="btn btn-default btn-xs" href="/research_projects/neural-rendering"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26302/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/spice-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only.: https://ps.is.mpg.de/publications/spice-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/spice-iccv-2021&amp;amp;title=Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only. &amp;amp;summary=Learning Realistic Human Reposing using Cyclic Self-Supervision with {3D} Shape, Pose, and Appearance Consistency&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only. %20https://ps.is.mpg.de/publications/spice-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only. &amp;amp;body=https://ps.is.mpg.de/publications/spice-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/ssanyal" >Sanyal, S.</a></span>, Vorobiov, A., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, Loper, M., Mohler, B., Davis, L., Romero, J., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/spice-iccv-2021>Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11118-11127, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Sanyal_Learning_Realistic_Human_Reposing_Using_Cyclic_Self-Supervision_With_3D_Shape_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2110.05458v1"><i class="fa fa-file-o" ></i>  arxiv</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01095">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Neural Rendering" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/neural-rendering"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26302/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/soma-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="{SOMA}: Solving Optical Marker-Based MoCap Automatically" src="/uploads/publication/image/26245/thumb_xl_teaser.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/soma-iccv-2021">SOMA: Solving Optical Marker-Based MoCap Automatically</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/nghorbani" >Ghorbani, N.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11097-11106, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26245">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26245" href="#abstractContent26245"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26245" class="panel-collapse collapse ">
											<div class="panel-body">
												Marker-based optical motion capture (mocap) is the “gold standard” method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. “labelling”. Given these labels, one can then “solve” for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/nghorbani/soma"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/soma/SOMA_ICCV21.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/soma/SOMA_Suppmat.pdf"><i class="fa fa-file-pdf-o" ></i>  suppl</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://arxiv.org/abs/2110.04431"><i class="fa fa-file-o" ></i>  arxiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://soma.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project website</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/BEFCqIefLA8"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/soma/SOMA_Poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01093">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Markers to Avatars" class="btn btn-default btn-xs" href="/research_projects/mosh"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26245/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/soma-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Marker-based optical motion capture (mocap) is the “gold standard” method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. “labelling”. Given these labels, one can then “solve” for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/soma-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/soma-iccv-2021&amp;amp;title=Marker-based optical motion capture (mocap) is the “gold standard” method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. “labelling”. Given these labels, one can then “solve” for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. &amp;amp;summary={SOMA}: Solving Optical Marker-Based MoCap Automatically&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Marker-based optical motion capture (mocap) is the “gold standard” method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. “labelling”. Given these labels, one can then “solve” for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/soma-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Marker-based optical motion capture (mocap) is the “gold standard” method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. “labelling”. Given these labels, one can then “solve” for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/soma-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/nghorbani" >Ghorbani, N.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/soma-iccv-2021>SOMA: Solving Optical Marker-Based MoCap Automatically</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11097-11106, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/nghorbani/soma"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/soma/SOMA_ICCV21.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/soma/SOMA_Suppmat.pdf"><i class="fa fa-file-pdf-o" ></i>  suppl</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://arxiv.org/abs/2110.04431"><i class="fa fa-file-o" ></i>  arxiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://soma.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project website</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/BEFCqIefLA8"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://download.is.tue.mpg.de/soma/SOMA_Poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01093">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Markers to Avatars" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/mosh"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26245/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/actor-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Action-Conditioned {3D} Human Motion Synthesis with Transformer {VAE}" src="/uploads/publication/image/26234/thumb_xl_ACTOR.jpeg" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/actor-iccv-2021">Action-Conditioned 3D Human Motion Synthesis with Transformer VAE</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/mpetrovich" >Petrovich, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/gvarol" >Varol, G.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 10965-10975, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26234">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26234" href="#abstractContent26234"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26234" class="panel-collapse collapse ">
											<div class="panel-body">
												We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2104.05670"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://imagine.enpc.fr/~petrovim/actor/"><i class="fa fa-file-o" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Mathux/ACTOR"><i class="fa fa-github-square" ></i>  code</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01080">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Language and Movement" class="btn btn-default btn-xs" href="/research_projects/language-and-movement"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Modeling Human Movement" class="btn btn-default btn-xs" href="/research_projects/modeling-human-movement"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26234/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/actor-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.: https://ps.is.mpg.de/publications/actor-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/actor-iccv-2021&amp;amp;title=We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page. &amp;amp;summary=Action-Conditioned {3D} Human Motion Synthesis with Transformer {VAE}&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page. %20https://ps.is.mpg.de/publications/actor-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page. &amp;amp;body=https://ps.is.mpg.de/publications/actor-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/mpetrovich" >Petrovich, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/gvarol" >Varol, G.</a></span>  
									
									<strong><a href =/publications/actor-iccv-2021>Action-Conditioned 3D Human Motion Synthesis with Transformer VAE</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 10965-10975, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2104.05670"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://imagine.enpc.fr/~petrovim/actor/"><i class="fa fa-file-o" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Mathux/ACTOR"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01080">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Language and Movement" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/language-and-movement"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Modeling Human Movement" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/modeling-human-movement"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26234/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/romp-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Monocular, One-Stage, Regression of Multiple {3D} People" src="/uploads/publication/image/26330/thumb_xl_ROMP2.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/romp-iccv-2021">Monocular, One-Stage, Regression of Multiple 3D People</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Sun, Y., Bao, Q., Liu, W., Fu, Y., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Mei, T.
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11159-11168, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26330">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26330" href="#abstractContent26330"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26330" class="panel-collapse collapse ">
											<div class="panel-body">
												This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Monocular_One-Stage_Regression_of_Multiple_3D_People_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Sun_Monocular_One-Stage_Regression_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://arxiv.org/abs/2008.12272"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Arthur151/ROMP"><i class="fa fa-github-square" ></i>  code</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01099">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26330/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/romp-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression.: https://ps.is.mpg.de/publications/romp-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/romp-iccv-2021&amp;amp;title=This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression. &amp;amp;summary=Monocular, One-Stage, Regression of Multiple {3D} People&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression. %20https://ps.is.mpg.de/publications/romp-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression. &amp;amp;body=https://ps.is.mpg.de/publications/romp-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Sun, Y., Bao, Q., Liu, W., Fu, Y., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Mei, T.  
									
									<strong><a href =/publications/romp-iccv-2021>Monocular, One-Stage, Regression of Multiple 3D People</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11159-11168, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Monocular_One-Stage_Regression_of_Multiple_3D_People_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Sun_Monocular_One-Stage_Regression_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://arxiv.org/abs/2008.12272"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/Arthur151/ROMP"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01099">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26330/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/kocabas_pare_2021"><img class="img-responsive img-bordered t-img-bordered" alt="{PARE}: Part Attention Regressor for {3D} Human Body Estimation" src="/uploads/publication/image/24865/thumb_xl_website_teaser_v3.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/kocabas_pare_2021">PARE: Part Attention Regressor for 3D Human Body Estimation</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, Hilliges, O., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11107-11117, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion24865">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion24865" href="#abstractContent24865"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent24865" class="panel-collapse collapse ">
											<div class="panel-body">
												Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  
To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE's  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kocabas_PARE_Part_Attention_Regressor_for_3D_Human_Body_Estimation_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kocabas_PARE_Part_Attention_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/mkocabas/PARE"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/3C9hdFajO3k"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2104.08527"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://pare.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project website</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://pare.is.tue.mpg.de/media/upload/pare_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01094">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/24865/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/kocabas_pare_2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  
To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks.: https://ps.is.mpg.de/publications/kocabas_pare_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/kocabas_pare_2021&amp;amp;title=Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  
To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. &amp;amp;summary={PARE}: Part Attention Regressor for {3D} Human Body Estimation&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  
To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. %20https://ps.is.mpg.de/publications/kocabas_pare_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  
To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. &amp;amp;body=https://ps.is.mpg.de/publications/kocabas_pare_2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, Hilliges, O., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/kocabas_pare_2021>PARE: Part Attention Regressor for 3D Human Body Estimation</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11107-11117, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kocabas_PARE_Part_Attention_Regressor_for_3D_Human_Body_Estimation_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kocabas_PARE_Part_Attention_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/mkocabas/PARE"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/3C9hdFajO3k"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2104.08527"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://pare.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project website</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://pare.is.tue.mpg.de/media/upload/pare_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01094">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/24865/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/samp"><img class="img-responsive img-bordered t-img-bordered" alt="Stochastic Scene-Aware Motion Prediction" src="/uploads/publication/image/24872/thumb_xl_samp3.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/samp">Stochastic Scene-Aware Motion Prediction</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/mhassan" >Hassan, M.</a></span>, Ceylan, D., Villegas, R., Saito, J., Yang, J., Zhou, Y., <span class="default-link-ul"><a href="/person/black" >Black, M.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11354-11364, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion24872">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion24872" href="#abstractContent24872"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent24872" class="panel-collapse collapse ">
											<div class="panel-body">
												A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  


											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://samp.is.tue.mpg.de"><i class="fa fa-file-o" ></i>  Project Page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/652/samp.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01118">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Modeling Human Movement" class="btn btn-default btn-xs" href="/research_projects/modeling-human-movement"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Putting People into Scenes" class="btn btn-default btn-xs" href="/research_projects/putting-people-into-scenes"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/24872/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/samp&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  

: https://ps.is.mpg.de/publications/samp&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/samp&amp;amp;title=A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  

 &amp;amp;summary=Stochastic Scene-Aware Motion Prediction&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  

 %20https://ps.is.mpg.de/publications/samp&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  

 &amp;amp;body=https://ps.is.mpg.de/publications/samp&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/mhassan" >Hassan, M.</a></span>, Ceylan, D., Villegas, R., Saito, J., Yang, J., Zhou, Y., <span class="default-link-ul"><a href="/person/black" >Black, M.</a></span>  
									
									<strong><a href =/publications/samp>Stochastic Scene-Aware Motion Prediction</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11354-11364, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://samp.is.tue.mpg.de"><i class="fa fa-file-o" ></i>  Project Page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/652/samp.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01118">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Modeling Human Movement" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/modeling-human-movement"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Putting People into Scenes" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/putting-people-into-scenes"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/24872/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/pop-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="The Power of Points for Modeling Humans in Clothing" src="/uploads/publication/image/26158/thumb_xl_POP2.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/pop-iccv-2021">The Power of Points for Modeling Humans in Clothing</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/qma" >Ma, Q.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 10954-10964, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26158">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26158" href="#abstractContent26158"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26158" class="panel-collapse collapse ">
											<div class="panel-body">
												Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along — the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://qianlim.github.io/POP.html"><i class="fa fa-github-square" ></i>  Project Page</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/qianlim/POP"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/JY5OI74yJ4w"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://pop.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Dataset</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2109.01137"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/655/POP_camera_ready.pdf"><i class="fa fa-file-pdf-o" ></i>  PDF</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/656/POP_supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supp.</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/658/POP_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01079">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Clothing Capture and Modeling" class="btn btn-default btn-xs" href="/research_projects/clothing"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26158/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/pop-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along — the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes.: https://ps.is.mpg.de/publications/pop-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/pop-iccv-2021&amp;amp;title=Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along — the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes. &amp;amp;summary=The Power of Points for Modeling Humans in Clothing&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along — the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes. %20https://ps.is.mpg.de/publications/pop-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along — the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/pop-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/qma" >Ma, Q.</a></span>, <span class="default-link-ul"><a href="/person/jyang" >Yang, J.</a></span>, <span class="default-link-ul"><a href="/person/stang" >Tang, S.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/pop-iccv-2021>The Power of Points for Modeling Humans in Clothing</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 10954-10964, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://qianlim.github.io/POP.html"><i class="fa fa-github-square" ></i>  Project Page</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/qianlim/POP"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/JY5OI74yJ4w"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://pop.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  Dataset</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2109.01137"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/655/POP_camera_ready.pdf"><i class="fa fa-file-pdf-o" ></i>  PDF</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/656/POP_supp.pdf"><i class="fa fa-file-pdf-o" ></i>  Supp.</a>
										</span>
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/658/POP_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  Poster</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01079">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Clothing Capture and Modeling" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/clothing"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26158/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/li_tofu_2021"><img class="img-responsive img-bordered t-img-bordered" alt="Topologically Consistent Multi-View Face Inference Using Volumetric Sampling" src="/uploads/publication/image/26285/thumb_xl_ToFu.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/li_tofu_2021">Topologically Consistent Multi-View Face Inference Using Volumetric Sampling</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/tli" >Li, T.</a></span>, Liu, S., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, Liu, J., Li, H., Zhao, Y.
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 3804-3814, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26285">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26285" href="#abstractContent26285"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26285" class="panel-collapse collapse ">
											<div class="panel-body">
												High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions.
In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topologically consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://tianyeli.github.io/tofu"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/659/paper.pdf"><i class="fa fa-file-pdf-o" ></i>  paper</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.00380">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Faces and Expressions" class="btn btn-default btn-xs" href="/research_projects/human-face-analysis"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26285/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/li_tofu_2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions.
In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topologically consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu.: https://ps.is.mpg.de/publications/li_tofu_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/li_tofu_2021&amp;amp;title=High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions.
In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topologically consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu. &amp;amp;summary=Topologically Consistent Multi-View Face Inference Using Volumetric Sampling&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions.
In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topologically consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu. %20https://ps.is.mpg.de/publications/li_tofu_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions.
In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topologically consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu. &amp;amp;body=https://ps.is.mpg.de/publications/li_tofu_2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/tli" >Li, T.</a></span>, Liu, S., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, Liu, J., Li, H., Zhao, Y.  
									
									<strong><a href =/publications/li_tofu_2021>Topologically Consistent Multi-View Face Inference Using Volumetric Sampling</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 3804-3814, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://tianyeli.github.io/tofu"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/uploads_file/attachment/attachment/659/paper.pdf"><i class="fa fa-file-pdf-o" ></i>  paper</a>
										</span>

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.00380">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Faces and Expressions" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/human-face-analysis"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26285/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/dsr-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Learning To Regress Bodies From Images Using Differentiable Semantic Rendering" src="/uploads/publication/image/26331/thumb_xl_DSR1.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/dsr-iccv-2021">Learning To Regress Bodies From Images Using Differentiable Semantic Rendering</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/sdwivedi" >Dwivedi, S. K.</a></span>, <span class="default-link-ul"><a href="/person/nathanasiou" >Athanasiou, N.</a></span>, <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11230-11239, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26331">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26331" href="#abstractContent26331"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26331" class="panel-collapse collapse ">
											<div class="panel-body">
												Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel “Differentiable Semantic Rendering (DSR)” loss. For Minimally-Clothed (MC) regions, we define the DSRMC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2110.03480.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Dwivedi_Learning_To_Regress_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/saidwivedi/DSR"><i class="fa fa-github-square" ></i>  code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://dsr.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project-website</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=SWAgEdGCFFo"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://dsr.is.tue.mpg.de/media/upload/DSR_ICCV2021_Poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01106">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26331/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dsr-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel “Differentiable Semantic Rendering (DSR)” loss. For Minimally-Clothed (MC) regions, we define the DSRMC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/: https://ps.is.mpg.de/publications/dsr-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dsr-iccv-2021&amp;amp;title=Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel “Differentiable Semantic Rendering (DSR)” loss. For Minimally-Clothed (MC) regions, we define the DSRMC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/ &amp;amp;summary=Learning To Regress Bodies From Images Using Differentiable Semantic Rendering&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel “Differentiable Semantic Rendering (DSR)” loss. For Minimally-Clothed (MC) regions, we define the DSRMC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/ %20https://ps.is.mpg.de/publications/dsr-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel “Differentiable Semantic Rendering (DSR)” loss. For Minimally-Clothed (MC) regions, we define the DSRMC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/ &amp;amp;body=https://ps.is.mpg.de/publications/dsr-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/sdwivedi" >Dwivedi, S. K.</a></span>, <span class="default-link-ul"><a href="/person/nathanasiou" >Athanasiou, N.</a></span>, <span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/dsr-iccv-2021>Learning To Regress Bodies From Images Using Differentiable Semantic Rendering</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11230-11239, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2110.03480.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Dwivedi_Learning_To_Regress_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/saidwivedi/DSR"><i class="fa fa-github-square" ></i>  code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://dsr.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project-website</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=SWAgEdGCFFo"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://dsr.is.tue.mpg.de/media/upload/DSR_ICCV2021_Poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01106">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26331/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/kocabas_spec_2021"><img class="img-responsive img-bordered t-img-bordered" alt="{SPEC}: Seeing People in the Wild with an Estimated Camera" src="/uploads/publication/image/24866/thumb_xl_webpage_teaser.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/kocabas_spec_2021">SPEC: Seeing People in the Wild with an Estimated Camera</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/jtesch" >Tesch, J.</a></span>, <span class="default-link-ul"><a href="/person/lmueller2" >Müller, L.</a></span>, Hilliges, O., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11015-11025, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion24866">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion24866" href="#abstractContent24866"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent24866" class="panel-collapse collapse ">
											<div class="panel-body">
												Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kocabas_SPEC_Seeing_People_in_the_Wild_With_an_Estimated_Camera_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kocabas_SPEC_Seeing_People_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2110.00620"><i class="fa fa-file-o" ></i>  arXiv</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/mkocabas/SPEC"><i class="fa fa-github-square" ></i>  code </a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/0nV6NoxbqUM"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://spec.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project website</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://spec.is.tue.mpg.de/media/upload/spec_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01085">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Optimizing Human Pose and Shape" class="btn btn-default btn-xs" href="/research_projects/optimizing-human-pose-and-shape"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/24866/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/kocabas_spec_2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies.: https://ps.is.mpg.de/publications/kocabas_spec_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/kocabas_spec_2021&amp;amp;title=Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. &amp;amp;summary={SPEC}: Seeing People in the Wild with an Estimated Camera&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. %20https://ps.is.mpg.de/publications/kocabas_spec_2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. &amp;amp;body=https://ps.is.mpg.de/publications/kocabas_spec_2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/mkocabas" >Kocabas, M.</a></span>, <span class="default-link-ul"><a href="/person/chuang2" >Huang, C. P.</a></span>, <span class="default-link-ul"><a href="/person/jtesch" >Tesch, J.</a></span>, <span class="default-link-ul"><a href="/person/lmueller2" >Müller, L.</a></span>, Hilliges, O., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>  
									
									<strong><a href =/publications/kocabas_spec_2021>SPEC: Seeing People in the Wild with an Estimated Camera</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11015-11025, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kocabas_SPEC_Seeing_People_in_the_Wild_With_an_Estimated_Camera_ICCV_2021_paper.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kocabas_SPEC_Seeing_People_ICCV_2021_supplemental.pdf"><i class="fa fa-file-pdf-o" ></i>  supp</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2110.00620"><i class="fa fa-file-o" ></i>  arXiv</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/mkocabas/SPEC"><i class="fa fa-github-square" ></i>  code </a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/0nV6NoxbqUM"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://spec.is.tue.mpg.de/"><i class="fa fa-file-o" ></i>  project website</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://spec.is.tue.mpg.de/media/upload/spec_poster.pdf"><i class="fa fa-file-pdf-o" ></i>  poster</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01085">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Regressing Humans" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/3d-pose-and-shape-from-images"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Optimizing Human Pose and Shape" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/optimizing-human-pose-and-shape"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/24866/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/chen-iccv-2021"><img class="img-responsive img-bordered t-img-bordered" alt="{SNARF}: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes" src="/uploads/publication/image/26236/thumb_xl_SNARF.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/chen-iccv-2021">SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/xchen2" >Chen, X.</a></span>, <span class="default-link-ul"><a href="/person/yzheng" >Zheng, Y.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Hilliges, O., <span class="default-link-ul"><a href="/person/ageiger" >Geiger, A.</a></span>
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11574-11584, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26236">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26236" href="#abstractContent26236"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26236" class="panel-collapse collapse ">
											<div class="panel-body">
												Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn.

To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations.

Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.


											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2104.03953.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://www.cvlibs.net/publications/Chen2021ICCV.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf 2</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://www.cvlibs.net/publications/Chen2021ICCV_supplementary.pdf"><i class="fa fa-file-pdf-o" ></i>  supplementary material</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://xuchen-ethz.github.io/snarf/"><i class="fa fa-github-square" ></i>  project</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.medium.com/animate-implicit-shapes-with-forward-skinning-c7ebbf355694"><i class="fa fa-file-o" ></i>  blog</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://autonomousvision.github.io/snarf/"><i class="fa fa-github-square" ></i>  blog 2</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/k5rdKC7gfFU"><i class="fa fa-file-video-o" ></i>  video</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://www.youtube.com/watch?v=Q99t36qGZUU"><i class="fa fa-file-video-o" ></i>  video 2</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/xuchen-ethz/snarf"><i class="fa fa-github-square" ></i>  code</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01139">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a data-toggle="tooltip" data-title="Clothing Capture and Modeling" class="btn btn-default btn-xs" href="/research_projects/clothing"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26236/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/chen-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn.

To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations.

Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.

: https://ps.is.mpg.de/publications/chen-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/chen-iccv-2021&amp;amp;title=Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn.

To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations.

Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.

 &amp;amp;summary={SNARF}: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn.

To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations.

Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.

 %20https://ps.is.mpg.de/publications/chen-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn.

To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations.

Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.

 &amp;amp;body=https://ps.is.mpg.de/publications/chen-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/xchen2" >Chen, X.</a></span>, <span class="default-link-ul"><a href="/person/yzheng" >Zheng, Y.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Hilliges, O., <span class="default-link-ul"><a href="/person/ageiger" >Geiger, A.</a></span>  
									
									<strong><a href =/publications/chen-iccv-2021>SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11574-11584, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/2104.03953.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://www.cvlibs.net/publications/Chen2021ICCV.pdf"><i class="fa fa-file-pdf-o" ></i>  pdf 2</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://www.cvlibs.net/publications/Chen2021ICCV_supplementary.pdf"><i class="fa fa-file-pdf-o" ></i>  supplementary material</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://xuchen-ethz.github.io/snarf/"><i class="fa fa-github-square" ></i>  project</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://eth-ait.medium.com/animate-implicit-shapes-with-forward-skinning-c7ebbf355694"><i class="fa fa-file-o" ></i>  blog</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://autonomousvision.github.io/snarf/"><i class="fa fa-github-square" ></i>  blog 2</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/k5rdKC7gfFU"><i class="fa fa-file-video-o" ></i>  video</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://www.youtube.com/watch?v=Q99t36qGZUU"><i class="fa fa-file-video-o" ></i>  video 2</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/xuchen-ethz/snarf"><i class="fa fa-github-square" ></i>  code</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.01139">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Implicit Representations" class="btn btn-default btn-xs" href="https://avg.is.mpg.de/research_projects/implicit-representations"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Clothing Capture and Modeling" class="btn btn-default btn-xs" href="https://avg.is.mpg.de/research_projects/clothing"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26236/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/adaclue-arxiv-2020"><img class="img-responsive img-bordered t-img-bordered" alt="Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings" src="/uploads/publication/image/24778/thumb_xl_adaclue_cropped_image.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/adaclue-arxiv-2020">Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Prabhu, V., <span class="default-link-ul"><a href="/person/achandrasekaran" >Chandrasekaran, A.</a></span>, Saenko, K., Hoffman, J.
							</p>


							In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 8485-8494, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion24778">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion24778" href="#abstractContent24778"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent24778" class="panel-collapse collapse ">
											<div class="panel-body">
												Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a subset that is maximally-informative via active learning (AL). In this work, we study the problem of AL under a domain shift. We empirically demonstrate how existing AL approaches based solely on model uncertainty or representative sampling are suboptimal for active domain adaptation. Our algorithm, Active Domain Adaptation via CLustering Uncertainty-weighted Embeddings (ADA-CLUE), i) identifies diverse datapoints for labeling that are both uncertain under the model and representative of unlabeled target data, and ii) leverages the available source and target data for adaptation by optimizing a semi-supervised adversarial entropy loss that is complimentary to our active sampling objective. On standard image classification benchmarks for domain adaptation, ADA-CLUE consistently performs as well or better than competing active adaptation, active learning, and domain adaptation methods across shift severities, model initializations, and labeling budgets.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2010.08666"><i class="fa fa-file-o" ></i>  paper</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://virajprabhu.github.io/adaclue-web/index.html"><i class="fa fa-github-square" ></i>  project webpage</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/virajprabhu/CLUE"><i class="fa fa-github-square" ></i>  Code</a>
						</span>
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/VGSiORxS7BY"><i class="fa fa-file-video-o" ></i>  Video</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.00839">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/24778/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/adaclue-arxiv-2020&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a subset that is maximally-informative via active learning (AL). In this work, we study the problem of AL under a domain shift. We empirically demonstrate how existing AL approaches based solely on model uncertainty or representative sampling are suboptimal for active domain adaptation. Our algorithm, Active Domain Adaptation via CLustering Uncertainty-weighted Embeddings (ADA-CLUE), i) identifies diverse datapoints for labeling that are both uncertain under the model and representative of unlabeled target data, and ii) leverages the available source and target data for adaptation by optimizing a semi-supervised adversarial entropy loss that is complimentary to our active sampling objective. On standard image classification benchmarks for domain adaptation, ADA-CLUE consistently performs as well or better than competing active adaptation, active learning, and domain adaptation methods across shift severities, model initializations, and labeling budgets.: https://ps.is.mpg.de/publications/adaclue-arxiv-2020&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/adaclue-arxiv-2020&amp;amp;title=Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a subset that is maximally-informative via active learning (AL). In this work, we study the problem of AL under a domain shift. We empirically demonstrate how existing AL approaches based solely on model uncertainty or representative sampling are suboptimal for active domain adaptation. Our algorithm, Active Domain Adaptation via CLustering Uncertainty-weighted Embeddings (ADA-CLUE), i) identifies diverse datapoints for labeling that are both uncertain under the model and representative of unlabeled target data, and ii) leverages the available source and target data for adaptation by optimizing a semi-supervised adversarial entropy loss that is complimentary to our active sampling objective. On standard image classification benchmarks for domain adaptation, ADA-CLUE consistently performs as well or better than competing active adaptation, active learning, and domain adaptation methods across shift severities, model initializations, and labeling budgets. &amp;amp;summary=Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a subset that is maximally-informative via active learning (AL). In this work, we study the problem of AL under a domain shift. We empirically demonstrate how existing AL approaches based solely on model uncertainty or representative sampling are suboptimal for active domain adaptation. Our algorithm, Active Domain Adaptation via CLustering Uncertainty-weighted Embeddings (ADA-CLUE), i) identifies diverse datapoints for labeling that are both uncertain under the model and representative of unlabeled target data, and ii) leverages the available source and target data for adaptation by optimizing a semi-supervised adversarial entropy loss that is complimentary to our active sampling objective. On standard image classification benchmarks for domain adaptation, ADA-CLUE consistently performs as well or better than competing active adaptation, active learning, and domain adaptation methods across shift severities, model initializations, and labeling budgets. %20https://ps.is.mpg.de/publications/adaclue-arxiv-2020&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a subset that is maximally-informative via active learning (AL). In this work, we study the problem of AL under a domain shift. We empirically demonstrate how existing AL approaches based solely on model uncertainty or representative sampling are suboptimal for active domain adaptation. Our algorithm, Active Domain Adaptation via CLustering Uncertainty-weighted Embeddings (ADA-CLUE), i) identifies diverse datapoints for labeling that are both uncertain under the model and representative of unlabeled target data, and ii) leverages the available source and target data for adaptation by optimizing a semi-supervised adversarial entropy loss that is complimentary to our active sampling objective. On standard image classification benchmarks for domain adaptation, ADA-CLUE consistently performs as well or better than competing active adaptation, active learning, and domain adaptation methods across shift severities, model initializations, and labeling budgets. &amp;amp;body=https://ps.is.mpg.de/publications/adaclue-arxiv-2020&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Prabhu, V., <span class="default-link-ul"><a href="/person/achandrasekaran" >Chandrasekaran, A.</a></span>, Saenko, K., Hoffman, J.  
									
									<strong><a href =/publications/adaclue-arxiv-2020>Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings</a></strong> 
									

									In <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 8485-8494, IEEE, Piscataway, NJ, October 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/2010.08666"><i class="fa fa-file-o" ></i>  paper</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://virajprabhu.github.io/adaclue-web/index.html"><i class="fa fa-github-square" ></i>  project webpage</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://github.com/virajprabhu/CLUE"><i class="fa fa-github-square" ></i>  Code</a>
										</span>		
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/VGSiORxS7BY"><i class="fa fa-file-video-o" ></i>  Video</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ICCV48922.2021.00839">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/24778/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/independentcambonetto"><img class="img-responsive img-bordered t-img-bordered" alt="Active Visual {SLAM} with Independently Rotating Camera" src="/uploads/publication/image/26150/thumb_xl_Untitled.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/independentcambonetto">Active Visual SLAM with Independently Rotating Camera</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/ebonetto" >Bonetto, E.</a></span>, Goldschmid, P., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>
							</p>


							In <em>2021 European Conference on Mobile Robots (ECMR 2021)</em>,  pages: 266-273, IEEE, Piscataway, NJ, 2021 <small class='text-muted'>(inproceedings)</small>

							<p>
								<div id="abstractAccordion26150">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26150" href="#abstractContent26150"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26150" class="panel-collapse collapse ">
											<div class="panel-body">
												In active Visual-SLAM (V-SLAM), a robot relies on the information retrieved by its cameras to control its own movements for autonomous mapping of the environment. Cameras are usually statically linked to the robot's body, limiting the extra degrees of freedom for visual information acquisition. In this work, we overcome the aforementioned problem by introducing and leveraging an independently rotating camera on the robot base. This enables us to continuously control the heading of the camera, obtaining the desired optimal orientation for active V-SLAM, without rotating the robot itself. However, this additional degree of freedom introduces additional estimation uncertainties, which need to be accounted for. We do this by extending our robot's state estimate to include the camera state and jointly estimate the uncertainties. We develop our method based on a state-of-the-art active V-SLAM approach for omnidirectional robots and evaluate it through rigorous simulation and real robot experiments. We obtain more accurate maps, with lower energy consumption, while maintaining the benefits of the active approach with respect to the baseline. We also demonstrate how our method easily generalizes to other non-omnidirectional robotic platforms, which was a limitation of the previous approach. Code and implementation details are provided as open-source.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://ieeexplore.ieee.org/document/9568791"><i class="fa fa-external-link" ></i>  link (url)</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ECMR50962.2021.9568791">DOI</a>
						</span>			

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26150/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/independentcambonetto&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In active Visual-SLAM (V-SLAM), a robot relies on the information retrieved by its cameras to control its own movements for autonomous mapping of the environment. Cameras are usually statically linked to the robot&amp;#39;s body, limiting the extra degrees of freedom for visual information acquisition. In this work, we overcome the aforementioned problem by introducing and leveraging an independently rotating camera on the robot base. This enables us to continuously control the heading of the camera, obtaining the desired optimal orientation for active V-SLAM, without rotating the robot itself. However, this additional degree of freedom introduces additional estimation uncertainties, which need to be accounted for. We do this by extending our robot&amp;#39;s state estimate to include the camera state and jointly estimate the uncertainties. We develop our method based on a state-of-the-art active V-SLAM approach for omnidirectional robots and evaluate it through rigorous simulation and real robot experiments. We obtain more accurate maps, with lower energy consumption, while maintaining the benefits of the active approach with respect to the baseline. We also demonstrate how our method easily generalizes to other non-omnidirectional robotic platforms, which was a limitation of the previous approach. Code and implementation details are provided as open-source.: https://ps.is.mpg.de/publications/independentcambonetto&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/independentcambonetto&amp;amp;title=In active Visual-SLAM (V-SLAM), a robot relies on the information retrieved by its cameras to control its own movements for autonomous mapping of the environment. Cameras are usually statically linked to the robot&amp;#39;s body, limiting the extra degrees of freedom for visual information acquisition. In this work, we overcome the aforementioned problem by introducing and leveraging an independently rotating camera on the robot base. This enables us to continuously control the heading of the camera, obtaining the desired optimal orientation for active V-SLAM, without rotating the robot itself. However, this additional degree of freedom introduces additional estimation uncertainties, which need to be accounted for. We do this by extending our robot&amp;#39;s state estimate to include the camera state and jointly estimate the uncertainties. We develop our method based on a state-of-the-art active V-SLAM approach for omnidirectional robots and evaluate it through rigorous simulation and real robot experiments. We obtain more accurate maps, with lower energy consumption, while maintaining the benefits of the active approach with respect to the baseline. We also demonstrate how our method easily generalizes to other non-omnidirectional robotic platforms, which was a limitation of the previous approach. Code and implementation details are provided as open-source. &amp;amp;summary=Active Visual {SLAM} with Independently Rotating Camera&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=In active Visual-SLAM (V-SLAM), a robot relies on the information retrieved by its cameras to control its own movements for autonomous mapping of the environment. Cameras are usually statically linked to the robot&amp;#39;s body, limiting the extra degrees of freedom for visual information acquisition. In this work, we overcome the aforementioned problem by introducing and leveraging an independently rotating camera on the robot base. This enables us to continuously control the heading of the camera, obtaining the desired optimal orientation for active V-SLAM, without rotating the robot itself. However, this additional degree of freedom introduces additional estimation uncertainties, which need to be accounted for. We do this by extending our robot&amp;#39;s state estimate to include the camera state and jointly estimate the uncertainties. We develop our method based on a state-of-the-art active V-SLAM approach for omnidirectional robots and evaluate it through rigorous simulation and real robot experiments. We obtain more accurate maps, with lower energy consumption, while maintaining the benefits of the active approach with respect to the baseline. We also demonstrate how our method easily generalizes to other non-omnidirectional robotic platforms, which was a limitation of the previous approach. Code and implementation details are provided as open-source. %20https://ps.is.mpg.de/publications/independentcambonetto&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=In active Visual-SLAM (V-SLAM), a robot relies on the information retrieved by its cameras to control its own movements for autonomous mapping of the environment. Cameras are usually statically linked to the robot&amp;#39;s body, limiting the extra degrees of freedom for visual information acquisition. In this work, we overcome the aforementioned problem by introducing and leveraging an independently rotating camera on the robot base. This enables us to continuously control the heading of the camera, obtaining the desired optimal orientation for active V-SLAM, without rotating the robot itself. However, this additional degree of freedom introduces additional estimation uncertainties, which need to be accounted for. We do this by extending our robot&amp;#39;s state estimate to include the camera state and jointly estimate the uncertainties. We develop our method based on a state-of-the-art active V-SLAM approach for omnidirectional robots and evaluate it through rigorous simulation and real robot experiments. We obtain more accurate maps, with lower energy consumption, while maintaining the benefits of the active approach with respect to the baseline. We also demonstrate how our method easily generalizes to other non-omnidirectional robotic platforms, which was a limitation of the previous approach. Code and implementation details are provided as open-source. &amp;amp;body=https://ps.is.mpg.de/publications/independentcambonetto&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/ebonetto" >Bonetto, E.</a></span>, Goldschmid, P., <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, <span class="default-link-ul"><a href="/person/aahmad" >Ahmad, A.</a></span>  
									
									<strong><a href =/publications/independentcambonetto>Active Visual SLAM with Independently Rotating Camera</a></strong> 
									

									In <em>2021 European Conference on Mobile Robots (ECMR 2021)</em>,  pages: 266-273, IEEE, Piscataway, NJ, 2021 <small class='text-muted'>(inproceedings)</small>

									<br />
									
									<p class="publication-button-p ">

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://ieeexplore.ieee.org/document/9568791"><i class="fa fa-external-link" ></i>  link (url)</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1109/ECMR50962.2021.9568791">DOI</a>
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26150/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/foster-hbm-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Separated and overlapping neural coding of face and body identity" src="/uploads/publication/image/24667/thumb_xl_celia.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/foster-hbm-2021">Separated and overlapping neural coding of face and body identity</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								<span class="default-link-ul"><a href="/person/cfoster" >Foster, C.</a></span>, Zhao, M., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Bartels, A., Bülthoff, I.
							</p>


							<em>Human Brain Mapping</em>, 42(13):4242-4260, September 2021 <small class='text-muted'>(article)</small>

							<p>
								<div id="abstractAccordion24667">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion24667" href="#abstractContent24667"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent24667" class="panel-collapse collapse ">
											<div class="panel-body">
												Recognising a person's identity often relies on face and body information, and is tolerant to changes in low-level visual input (e.g., viewpoint changes). Previous studies have suggested that face identity is disentangled from low-level visual input in the anterior face-responsive regions. It remains unclear which regions disentangle body identity from variations in viewpoint, and whether face and body identity are encoded separately or combined into a coherent person identity representation. We trained participants to recognise three identities, and then recorded their brain activity using fMRI while they viewed face and body images of these three identities from different viewpoints. Participants' task was to respond to either the stimulus identity or viewpoint. We found consistent decoding of body identity across viewpoint in the fusiform body area, right anterior temporal cortex, middle frontal gyrus and right insula. This finding demonstrates a similar function of fusiform and anterior temporal cortex for bodies as has previously been shown for faces, suggesting these regions may play a general role in extracting high-level identity information. Moreover, we could decode identity across fMRI activity evoked by faces and bodies in the early visual cortex, right inferior occipital cortex, right parahippocampal cortex and right superior parietal cortex, revealing a distributed network that encodes person identity abstractly. Lastly, identity decoding was consistently better when participants attended to identity, indicating that attention to identity enhances its neural representation. These results offer new insights into how the brain develops an abstract neural coding of person identity, shared by faces and bodies.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="https://onlinelibrary.wiley.com/doi/10.1002/hbm.25544"><i class="fa fa-file-o" ></i>  on-line</a>
						</span>

						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1002/hbm.25544">DOI</a>
						</span>			

						<span>
							<a data-toggle="tooltip" data-title="Body Perception" class="btn btn-default btn-xs" href="/research_projects/anorexia-and-body-shape"><i class='fa fa-external-link'></i> Project Page</a>
						</span>			
						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/24667/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/foster-hbm-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Recognising a person&amp;#39;s identity often relies on face and body information, and is tolerant to changes in low-level visual input (e.g., viewpoint changes). Previous studies have suggested that face identity is disentangled from low-level visual input in the anterior face-responsive regions. It remains unclear which regions disentangle body identity from variations in viewpoint, and whether face and body identity are encoded separately or combined into a coherent person identity representation. We trained participants to recognise three identities, and then recorded their brain activity using fMRI while they viewed face and body images of these three identities from different viewpoints. Participants&amp;#39; task was to respond to either the stimulus identity or viewpoint. We found consistent decoding of body identity across viewpoint in the fusiform body area, right anterior temporal cortex, middle frontal gyrus and right insula. This finding demonstrates a similar function of fusiform and anterior temporal cortex for bodies as has previously been shown for faces, suggesting these regions may play a general role in extracting high-level identity information. Moreover, we could decode identity across fMRI activity evoked by faces and bodies in the early visual cortex, right inferior occipital cortex, right parahippocampal cortex and right superior parietal cortex, revealing a distributed network that encodes person identity abstractly. Lastly, identity decoding was consistently better when participants attended to identity, indicating that attention to identity enhances its neural representation. These results offer new insights into how the brain develops an abstract neural coding of person identity, shared by faces and bodies.: https://ps.is.mpg.de/publications/foster-hbm-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/foster-hbm-2021&amp;amp;title=Recognising a person&amp;#39;s identity often relies on face and body information, and is tolerant to changes in low-level visual input (e.g., viewpoint changes). Previous studies have suggested that face identity is disentangled from low-level visual input in the anterior face-responsive regions. It remains unclear which regions disentangle body identity from variations in viewpoint, and whether face and body identity are encoded separately or combined into a coherent person identity representation. We trained participants to recognise three identities, and then recorded their brain activity using fMRI while they viewed face and body images of these three identities from different viewpoints. Participants&amp;#39; task was to respond to either the stimulus identity or viewpoint. We found consistent decoding of body identity across viewpoint in the fusiform body area, right anterior temporal cortex, middle frontal gyrus and right insula. This finding demonstrates a similar function of fusiform and anterior temporal cortex for bodies as has previously been shown for faces, suggesting these regions may play a general role in extracting high-level identity information. Moreover, we could decode identity across fMRI activity evoked by faces and bodies in the early visual cortex, right inferior occipital cortex, right parahippocampal cortex and right superior parietal cortex, revealing a distributed network that encodes person identity abstractly. Lastly, identity decoding was consistently better when participants attended to identity, indicating that attention to identity enhances its neural representation. These results offer new insights into how the brain develops an abstract neural coding of person identity, shared by faces and bodies. &amp;amp;summary=Separated and overlapping neural coding of face and body identity&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=Recognising a person&amp;#39;s identity often relies on face and body information, and is tolerant to changes in low-level visual input (e.g., viewpoint changes). Previous studies have suggested that face identity is disentangled from low-level visual input in the anterior face-responsive regions. It remains unclear which regions disentangle body identity from variations in viewpoint, and whether face and body identity are encoded separately or combined into a coherent person identity representation. We trained participants to recognise three identities, and then recorded their brain activity using fMRI while they viewed face and body images of these three identities from different viewpoints. Participants&amp;#39; task was to respond to either the stimulus identity or viewpoint. We found consistent decoding of body identity across viewpoint in the fusiform body area, right anterior temporal cortex, middle frontal gyrus and right insula. This finding demonstrates a similar function of fusiform and anterior temporal cortex for bodies as has previously been shown for faces, suggesting these regions may play a general role in extracting high-level identity information. Moreover, we could decode identity across fMRI activity evoked by faces and bodies in the early visual cortex, right inferior occipital cortex, right parahippocampal cortex and right superior parietal cortex, revealing a distributed network that encodes person identity abstractly. Lastly, identity decoding was consistently better when participants attended to identity, indicating that attention to identity enhances its neural representation. These results offer new insights into how the brain develops an abstract neural coding of person identity, shared by faces and bodies. %20https://ps.is.mpg.de/publications/foster-hbm-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=Recognising a person&amp;#39;s identity often relies on face and body information, and is tolerant to changes in low-level visual input (e.g., viewpoint changes). Previous studies have suggested that face identity is disentangled from low-level visual input in the anterior face-responsive regions. It remains unclear which regions disentangle body identity from variations in viewpoint, and whether face and body identity are encoded separately or combined into a coherent person identity representation. We trained participants to recognise three identities, and then recorded their brain activity using fMRI while they viewed face and body images of these three identities from different viewpoints. Participants&amp;#39; task was to respond to either the stimulus identity or viewpoint. We found consistent decoding of body identity across viewpoint in the fusiform body area, right anterior temporal cortex, middle frontal gyrus and right insula. This finding demonstrates a similar function of fusiform and anterior temporal cortex for bodies as has previously been shown for faces, suggesting these regions may play a general role in extracting high-level identity information. Moreover, we could decode identity across fMRI activity evoked by faces and bodies in the early visual cortex, right inferior occipital cortex, right parahippocampal cortex and right superior parietal cortex, revealing a distributed network that encodes person identity abstractly. Lastly, identity decoding was consistently better when participants attended to identity, indicating that attention to identity enhances its neural representation. These results offer new insights into how the brain develops an abstract neural coding of person identity, shared by faces and bodies. &amp;amp;body=https://ps.is.mpg.de/publications/foster-hbm-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									<span class="default-link-ul"><a href="/person/cfoster" >Foster, C.</a></span>, Zhao, M., <span class="default-link-ul"><a href="/person/tbolkart" >Bolkart, T.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M. J.</a></span>, Bartels, A., Bülthoff, I.  
									
									<strong><a href =/publications/foster-hbm-2021>Separated and overlapping neural coding of face and body identity</a></strong> 
									

									<em>Human Brain Mapping</em>, 42(13):4242-4260, September 2021 <small class='text-muted'>(article)</small>

									<br />
									
									<p class="publication-button-p ">
										
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="https://onlinelibrary.wiley.com/doi/10.1002/hbm.25544"><i class="fa fa-file-o" ></i>  on-line</a>
										</span>		

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="http://dx.doi.org/10.1002/hbm.25544">DOI</a>
										</span>			
										<span>
											
											<!-- IW-708 -->

												<a data-toggle="tooltip" data-title="Body Perception" class="btn btn-default btn-xs" href="https://ps.is.mpg.de/research_projects/anorexia-and-body-shape"><i class='fa fa-external-link'></i> Project Page</a>
											
										</span>			
										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/24667/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>




		<!-- DETAIL VIEW -->
		<div id="detailViewPublicationContainer" class="row detailViewPublicationContainer employeeDetailRow">

				<div class="col-md-12">
					<hr class="devider devider-dotted employee-seperator-hr" />
				</div>
				<div class="col-md-3">
					<!-- IW-123 -->
					<a href="/publications/smil-patent-2021"><img class="img-responsive img-bordered t-img-bordered" alt="Skinned multi-infant linear body model" src="/uploads/publication/image/26556/thumb_xl_smilpatent.png" /></a>
					<!-- <a href="/publications/">
						<img data-original="" data-original-width="500" data-original-height="365" witdth="190" height="104" class="lazyload img-responsive img-bordered t-img-bordered" title="" alt="" />
					</a> -->
				</div>
				<div class="col-md-9">
					<h5 class="publicationGridTitle">
						<a href="/publications/smil-patent-2021">Skinned multi-infant linear body model</a> <i class="fa fa-external-link"></i>
					</h5>

					<div class="row">
						<div class="col-md-12">


							<p>
								Hesse, N., <span class="default-link-ul"><a href="/person/spujades" >Pujades, S.</a></span>, <span class="default-link-ul"><a href="/person/jromero" >Romero, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M.</a></span>
							</p>


							 (US Patent 11,127,163, 2021), September 2021 <small class='text-muted'>(patent)</small>

							<p>
								<div id="abstractAccordion26556">
									<div>
										<div>
											<span class='text-muted-custom'>Abstract</span> <a data-toggle="collapse" data-parent="#abstractAccordion26556" href="#abstractContent26556"><i id="abstractIcon" class="fa fa-arrow-circle-right"></i></a>
										</div>
										<div id="abstractContent26556" class="panel-collapse collapse ">
											<div class="panel-body">
												A computer-implemented method for automatically obtaining pose and shape parameters of a human body. The method includes obtaining a sequence of digital 3D images of the body, recorded by at least one depth camera; automatically obtaining pose and shape parameters of the body, based on images of the sequence and a statistical body model; and outputting the pose and shape parameters. The body may be an infant body.
											</div>
										</div>
									</div>
								</div>
							</p>
							
						</div>
					</div>
					<p>
					</p>
					<p class="publication-button-p">


						<span>
							<a class="btn btn-default btn-xs" target="_blank" href="/publications/26556/get_bibtexfile?export_type=bibtex">[BibTex]</a>
						</span>	
						<span>
							<button type="button" class="btn btn-default btn-xs" data-html="true" data-container="body" data-toggle="popover" data-placement="top" data-content="<div style='width:800px'>&lt;!-- 
&lt;div class=&quot;share-contianer&quot;&gt; 

  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;
    &lt;li&gt;
      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/smil-patent-2021&quot; onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;
    
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - A computer-implemented method for automatically obtaining pose and shape parameters of a human body. The method includes obtaining a sequence of digital 3D images of the body, recorded by at least one depth camera; automatically obtaining pose and shape parameters of the body, based on images of the sequence and a statistical body model; and outputting the pose and shape parameters. The body may be an infant body.: https://ps.is.mpg.de/publications/smil-patent-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/smil-patent-2021&amp;amp;title=A computer-implemented method for automatically obtaining pose and shape parameters of a human body. The method includes obtaining a sequence of digital 3D images of the body, recorded by at least one depth camera; automatically obtaining pose and shape parameters of the body, based on images of the sequence and a statistical body model; and outputting the pose and shape parameters. The body may be an infant body. &amp;amp;summary=Skinned multi-infant linear body model&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href=&quot;https://plus.google.com/share?url=A computer-implemented method for automatically obtaining pose and shape parameters of a human body. The method includes obtaining a sequence of digital 3D images of the body, recorded by at least one depth camera; automatically obtaining pose and shape parameters of the body, based on images of the sequence and a statistical body model; and outputting the pose and shape parameters. The body may be an infant body. %20https://ps.is.mpg.de/publications/smil-patent-2021&quot;  onclick=&quot;popupCenter($(this).attr(&#39;href&#39;), &#39;&#39;, 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;a href=&quot;mailto:?subject=A computer-implemented method for automatically obtaining pose and shape parameters of a human body. The method includes obtaining a sequence of digital 3D images of the body, recorded by at least one depth camera; automatically obtaining pose and shape parameters of the body, based on images of the sequence and a statistical body model; and outputting the pose and shape parameters. The body may be an infant body. &amp;amp;body=https://ps.is.mpg.de/publications/smil-patent-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
 --&gt;


&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;
</div>" >
								<i class="fa fa-share-alt"></i> Share
							</button>
						</span>
					</p>
				</div>
			</div>

			<!-- LIST VIEW -->
			<div id="listViewPublicationContainer" class="listViewPublicationContainer hideSection">
					
					<div class="row">
						
						
						<div class="col-md-12">
							
							<div class="list-publication-container">
								
								<p class="default-link-ul publication-list-ul">

									Hesse, N., <span class="default-link-ul"><a href="/person/spujades" >Pujades, S.</a></span>, <span class="default-link-ul"><a href="/person/jromero" >Romero, J.</a></span>, <span class="default-link-ul"><a href="/person/black" >Black, M.</a></span>  
									
									<strong><a href =/publications/smil-patent-2021>Skinned multi-infant linear body model</a></strong> 
									

									 (US Patent 11,127,163, 2021), September 2021 <small class='text-muted'>(patent)</small>

									<br />
									
									<p class="publication-button-p ">

										<span>
											<a class="btn btn-default btn-xs" target="_blank" href="/publications/26556/get_bibtexfile?export_type=bibtex">[BibTex]</a>
										</span>	
										
									</p>
								</p>
								
							</div>
							
						</div>
					</div>
				</div>

			</div>

			<div id="" class="pagingContainer ">
				

					<ul class="pagination"><li class="previous_page disabled custom-pagination-diasbled custom-pagination-links"><a href="#">&#8592; Previous</a></li> <li class="active custom-pagination-links"><a href="/all_publications?page=1">1</a></li> <li class="custom-pagination-links"><a rel="next" href="/all_publications?page=2">2</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=3">3</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=4">4</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=5">5</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=6">6</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=7">7</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=8">8</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=9">9</a></li> <li class="disabled"><a>&hellip;</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=15">15</a></li> <li class="custom-pagination-links"><a href="/all_publications?page=16">16</a></li> <li class="next_page custom-pagination-links"><a rel="next" href="/all_publications?page=2">Next &#8594;</a></li></ul>
			</div>





      </div>
    </div>
  </div>
</div>
			</div>

		<!-- project - selected projects footer -->

		<!-- department hover footer image (homepage) -->
		
	</main>

	<!-- footer -->
		<footer>
			<div id="footer" class="footer-v1">
	<div class="footer custom-footer">
		<div class="container">
			<div class="row">

				
				<div class="col-md-3 md-margin-bottom-40 hidden-xs">
					<img width="240" height="50" class="footer-logo custom-footer-logo" src="/assets/header/header_logo_is-35f94c29a810a1541e01d2e275292633.png" alt="Header logo is" />
					<p>

								We combine research on computer vision, computer graphics, and machine learning to teach computers to see and understand humans and their behavior. A key goal is to learn digital humans.
					</p>

					<p></p>
				</div>
				

				<div class="col-md-3 md-margin-bottom-40 hidden-xs">
					<div class="posts">
						<div class="headline custom-headline-homepage-is"><h2>Latest News</h2></div>
						<ul class="list-unstyled latest-list">
							<li>
								<a href="/news/the-koenderink-prize-at-eccv-2022">The Koenderink Prize at ECCV 2022</a>
								<small>31 October 2022</small>
							</li>
							<li>
								<a href="/news/max-planck-spin-off-meshcapade-wins-new-startup-award">Max Planck spin-off Meshcapade wins new startup award </a>
								<small>28 March 2022</small>
							</li>
							<li>
								<a href="/news/better-decisions-more-control-best-paper-award-for-tubingen-researchers">Better decisions, more control: “Best Paper Award” for Tübingen researchers</a>
								<small>26 June 2021</small>
							</li>
							<li>
								<a href="/news/ellis-phd-program-call-for-applications-c6906bdc-f39a-4d22-aca1-f39c859f40fd">ELLIS PhD Program: Call for Applications</a>
								<small>10 September 2020</small>
							</li>
						</ul>
					</div>
				</div>

				<div class="col-md-3 md-margin-bottom-40 hidden-xs">
					<div class="headline custom-headline-homepage-is"><h2>Links</h2></div>

					<ul class="list-unstyled link-list margin-bottom-10">
						<li><a rel="noopener noreferrer" href="https://is.mpg.de">Institute Home</a><i class="fa fa-angle-right"></i></li>
						<li><a rel="noopener noreferrer" href="/departments/archived">Archived Departments &amp; Groups</a><i class="fa fa-angle-right"></i></li>
						<li><a target="_blank" rel="noopener noreferrer" href="https://anniversary.is.mpg.de/">100/10 Year Anniversary Website</a><i class="fa fa-angle-right"></i></li>
					</ul>
						
					<ul class="list-unstyled link-list margin-bottom-10">
						<li>
								<a target="_blank" rel="noopener noreferrer" href="https://www.mpg.de/en">About the Max Planck Society</a><i class="fa fa-angle-right"></i>
						</li>
					</ul>

					<ul class="list-unstyled link-list margin-bottom-20">
					<li><a target="_blank" rel="noopener noreferrer" href="http://cyber-valley.de">Cyber Valley</a><i class="fa fa-angle-right"></i></li>
					<li><a target="_blank" rel="noopener noreferrer" href="http://imprs.is.mpg.de">IMPRS-IS</a><i class="fa fa-angle-right"></i></li>
					<li><a target="_blank" rel="noopener noreferrer" href="http://learning-systems.org">Center for Learning Systems</a><i class="fa fa-angle-right"></i></li>
					<li><a target="_blank" rel="noopener noreferrer" href="https://www.cis.mpg.de/">Computer Science at Max Planck</a><i class="fa fa-angle-right"></i></li>
					<li><a rel="noopener noreferrer" href="/logos">Logos</a><i class="fa fa-angle-right"></i></li>
					<li></li>
				</ul>

					<!-- IW-594 -->
					<ul class="list-unstyled link-list margin-bottom-20">
						<li><a target="_blank" rel="noopener noreferrer" href="https://perceiving-systems.blog"><i class='fa fa-pencil footer-link-icon'></i> Blog</a><i class="fa fa-angle-right"></i></li>
						<li><a target="_blank" rel="noopener noreferrer" href="https://www.facebook.com/PerceivingSystems/"><i class='fa fa-twitter footer-link-icon'></i> FaceBook</a><i class="fa fa-angle-right"></i></li>
						<li><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/PerceivingSys"><i class='fa fa-twitter footer-link-icon'></i> Twitter</a><i class="fa fa-angle-right"></i></li>
						<li><a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/channel/UCqNJuPO0tyV6eWfYB7lcsvw"><i class='fa fa-youtube-play footer-link-icon'></i> YouTube</a><i class="fa fa-angle-right"></i></li>
						<li><a target="_blank" rel="noopener noreferrer" href="https://perceiving-systems.blog"><i class='fa fa-pencil footer-link-icon'></i> Blog</a><i class="fa fa-angle-right"></i></li>
					</ul>


				<ul class="list-unstyled link-list">
					<li><a target="_blank" rel="noopener noreferrer" href="https://atlas.is.localnet/confluence/">Confluence Wiki <small>(internal)</small></a><i class="fa fa-angle-right"></i></li>
				</ul>
			</div>



			<div class="col-md-3 map-img md-margin-bottom-40"> 
				<div class="headline custom-headline-homepage-is"><h2>Contact Us</h2></div>                     
				<address class="md-margin-bottom-40">

					<div class="margin-bottom-10">
						Max Planck Institute for Intelligent Systems <br />
					</div>
					<div class="margin-bottom-10">
						Max-Planck-Ring 4 <br />
						72076 Tübingen
					</div>
					<div class="margin-bottom-10">
						Heisenbergstr. 3<br />
						70569 Stuttgart
					</div>
					<div class="margin-bottom-10">
						Germany<br />
					</div>
					<div class="margin-bottom-10">
						 <script id="mail_to-uky9vqei">eval(decodeURIComponent('%76%61%72%20%73%63%72%69%70%74%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%67%65%74%45%6c%65%6d%65%6e%74%42%79%49%64%28%27%6d%61%69%6c%5f%74%6f%2d%75%6b%79%39%76%71%65%69%27%29%3b%76%61%72%20%61%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%45%6c%65%6d%65%6e%74%28%27%61%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%63%6c%61%73%73%27%2c%20%27%66%6f%6f%74%65%72%2d%6c%69%6e%6b%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%68%72%65%66%27%2c%20%27%6d%61%69%6c%74%6f%3a%69%6e%66%6f%40%69%73%2e%6d%70%67%2e%64%65%27%29%3b%61%2e%61%70%70%65%6e%64%43%68%69%6c%64%28%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%54%65%78%74%4e%6f%64%65%28%27%69%6e%66%6f%40%69%73%2e%6d%70%67%2e%64%65%27%29%29%3b%73%63%72%69%70%74%2e%70%61%72%65%6e%74%4e%6f%64%65%2e%69%6e%73%65%72%74%42%65%66%6f%72%65%28%61%2c%73%63%72%69%70%74%29%3b'))</script>
					</div>
				</address>
				<div class="fiiter-border">
					<p class="margin-top-10">
							For website questions and technical problems please contact:
					</p>
					<script id="mail_to-vs19nc0o">eval(decodeURIComponent('%76%61%72%20%73%63%72%69%70%74%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%67%65%74%45%6c%65%6d%65%6e%74%42%79%49%64%28%27%6d%61%69%6c%5f%74%6f%2d%76%73%31%39%6e%63%30%6f%27%29%3b%76%61%72%20%61%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%45%6c%65%6d%65%6e%74%28%27%61%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%63%6c%61%73%73%27%2c%20%27%66%6f%6f%74%65%72%2d%6c%69%6e%6b%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%68%72%65%66%27%2c%20%27%6d%61%69%6c%74%6f%3a%77%65%62%40%69%73%2e%6d%70%67%2e%64%65%27%29%3b%61%2e%61%70%70%65%6e%64%43%68%69%6c%64%28%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%54%65%78%74%4e%6f%64%65%28%27%77%65%62%40%69%73%2e%6d%70%67%2e%64%65%27%29%29%3b%73%63%72%69%70%74%2e%70%61%72%65%6e%74%4e%6f%64%65%2e%69%6e%73%65%72%74%42%65%66%6f%72%65%28%61%2c%73%63%72%69%70%74%29%3b'))</script>
				</div>
			</div>
		</div>
	</div> 
</div>
<div class="copyright custom-copyright">
	<div class="container">
		<div class="row">
			<div class="col-md-6">      
				<p> 
					&copy; 2022 Max-Planck-Gesellschaft - 
					<span class="custom-copyright-container">  
						<a href="/imprint">Imprint</a> | 
						<a href="/privacy-policy">Privacy Policy</a> | 
						<a href="/social-policy">Social Policy</a>
					</span>
				</p>
			</div>
			<div class="col-md-6">  

				<div class="sub-footer-btn-container">
					<a class="btn btn-footer btn-sm" href="/sign_in">Sign In</a>
				</div>
			
			</div>
		</div>
	</div> 
</div>	
		</footer>

	<div class="eupopup-container eupopup-container-bottom eupopup-color-default" style="display: none;" >
		<div class="eupopup-body">
				We use cookies to improve your website experience. <a href="https://is.mpg.de/privacy-policy">Find out more about our cookies and how to disable them</a>. By continuing, you consent to our use of cookies. <a href="#" id="eupopup-continuelink">Continue</a>
			
		</div>
		<a href="#" class="eupopup-closebutton hidden-xs hidden-sm hidden-md"><i class="fa fa-times-circle"></i></a>
	</div>

		<!-- Piwik -->
		<script>
			var _paq = _paq || [];
			_paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
			_paq.push(["setCookieDomain", "*.is.tuebingen.mpg.de"]);
			_paq.push(['trackPageView']);
			_paq.push(['enableLinkTracking']);
			(function() {
				var u=(("https:" == document.location.protocol) ? "https" : "http") + "://piwik.tuebingen.mpg.de/";
				_paq.push(['setTrackerUrl', u+'piwik.php']);
				_paq.push(['setSiteId', 14]);
				var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';
				g.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
			})();
		</script>
		<noscript><p><img src="http://piwik.tuebingen.mpg.de/piwik.php?idsite=14" style="border:0;" alt="" /></p></noscript>
		<!-- End Piwik Code -->

	<script src="/assets/application-9acad3c5ff05c9b78227a07a3729ddbc.js" data-turbolinks-track="true"></script>

</body>

</html>

