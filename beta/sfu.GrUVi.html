<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GrUVi - Publications</title>
  <meta name="description" content="GrUVi -- Publications.">
  <link rel="stylesheet" href="https://gruvi.cs.sfu.ca/css/main.css">
  <link rel="canonical" href="https://gruvi.cs.sfu.ca/publications/">
<link rel="shortcut icon" type ="image/x-icon" href="https://gruvi.cs.sfu.ca/images/favicon.ico">


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82472331-1', 'auto');
  ga('send', 'pageview');

</script>


</head>


  <body>

    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script>
// Hide Header on on scroll down
var didScroll;
var lastScrollTop = 0;
var delta = 50;
var navbarHeight = $('header').outerHeight();

$(window).scroll(function(event){
    didScroll = true;
});

setInterval(function() {
    if (didScroll) {
        hasScrolled();
        didScroll = false;
    }
}, 250);

function hasScrolled() {
    var st = $(this).scrollTop();
    
    // Make sure they scroll more than delta
    if((Math.abs(lastScrollTop - st) <= delta) || ($(window).width()>900))
        return;
    
    // If they scrolled down and are past the navbar, add class .nav-up.
    // This is necessary so you never see what is "behind" the navbar.
    if (st > lastScrollTop && st > navbarHeight){
        // Scroll Down
        $('#main-nav').fadeOut();
    } else {
        // Scroll Up
        if(st + $(window).height() < $(document).height()) {
            $('#main-nav').fadeIn();
        }
    }
    
    lastScrollTop = st;
}

$(window).resize(function() {

$('#main-nav').fadeIn();

});

</script>

<header class="nav-down">
<div id="main-nav" class="navbar navbar-default navbar-fixed-top" role="navigation">

  <div class="container-fluid backg b"">
	<div class="navbar-header navbar-right">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
   
   
    <a href="https://www.sfu.ca/">
      <img class="img_logo" src="https://gruvi.cs.sfu.ca/images/logopic/sfu_logo3.png" style="height: 55px;">
    </a>
    <a href="https://gruvi.cs.sfu.ca/">
      <img class="img_logo2" src="https://gruvi.cs.sfu.ca/images/logopic/gruvi_logo.png" style="height: 55px;">
    </a>
    
    
<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <p align="right" style="padding-right: 10px;">
                <a href="https://gruvi.cs.sfu.ca/" style="color:#FFFFFF; text-shadow: 2px 2px 3px #606060;"><strong>HOME</strong></a></br>
		<a href="https://gruvi.cs.sfu.ca/vc_workshop_2022" style="color:#FFFFFF; text-shadow: 2px 2px 3px #606060;"><strong>SFU VC WORKSHOP 2022</strong></a></br>
		<a href="https://gruvi.cs.sfu.ca/people" style="color:#FFFFFF; text-shadow: 2px 2px 3px #606060;"><strong>GRUVIERS</strong></a></br>
		<a href="https://gruvi.cs.sfu.ca/publications" style="color:#FFFFFF; text-shadow: 2px 2px 3px #606060;"><strong>PUBLICATIONS</strong></a></br>
		<a href="https://gruvi.cs.sfu.ca/about" style="color:#FFFFFF; text-shadow: 2px 2px 3px #606060;"><strong>ABOUT</strong></a>
             </p>
	</div>
</div>

    
	
	

</div> 
</div>
</header>



    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h2 id="publications">Publications</h2>

<h3 id="2021">2021</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen2021neural.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Neural Marching Cubes</pubtit>
      <p><em>Zhiqin Chen and Hao Zhang</em><br />
<em>In SIGGRAPH Asia (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2106.11272.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/gao2020tm.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>TM-NET: Deep Generative Networks for Textured Meshes</pubtit>
      <p><em>Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, and Hao Zhang</em><br />
<em>In SIGGRAPH Asia (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2010.06217.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/smith2018aerial.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Continuous Aerial Path Planning for 3D Urban Scene Reconstruction</pubtit>
      <p><em>Han Zhang, Yusong Yao, Ke Xie, Chi-Wing Fu, Hao Zhang, and Hui Huang</em><br />
<em>In SIGGRAPH Asia (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://vcc.tech/research/2021/DronePath">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/shabani2021extreme.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Extreme Structure from Motion for Indoor Panoramas without Visual Overlaps</pubtit>
      <p><em>Amin Shabani, Weilian Song, Hirochika Fujiki, Makoto Odamaki, and Yasutaka Furukawa</em><br />
<em>In ICCV (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://aminshabani.github.io/publications/extreme_sfm/pdfs/iccv2021_2088.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhang2021structured.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Structured Outdoor Architecture Reconstruction by Exploration and Classification</pubtit>
      <p><em>Fuyang Zhang, Sam Xu, Nelson Nauata, and Yasutaka Furukawa</em><br />
<em>In ICCV (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2108.07990.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/patel2021interpretation.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents</pubtit>
      <p><em>Shivansh Patel, Saim Wani, Unnat Jain, Alexander Schwing, Svetlana Lazebnik, Manolis Savva, Angel X. Chang</em><br />
<em>In ICCV (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2110.05769.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/fu20213d.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>3D-FRONT: 3D Furnished Rooms with Layout and Semantics</pubtit>
      <p><em>Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Cao Li, Zengqi Xun, Chengyue Sun, Yiyun Fei, Yu Zheng, Ying Li, Yi Liu, Peng Liu, Lin Ma, Le Weng, Xiaohang Hu, Xin Ma, Qian Qian, Rongfei Jia, Binqiang Zhao, and Hao Zhang</em><br />
<em>In ICCV (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2011.09127.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yin2021discovering.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Discovering Diverse Athletic Jumping Strategies</pubtit>
      <p><em>Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin</em><br />
<em>In SIGGRAPH (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/ma2021learning.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning and Exploring Motor Skills with Spacetime Bounds</pubtit>
      <p><em>Li-Ke Ma, Zeshi Yang, Xin Tong, Baining Guo, KangKang Yin</em><br />
<em>In Eurographics (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~kkyin/papers/STBounds.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/feitong_cvpr21.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>HumanGPS: Geodesic PreServing Feature for Dense Human Correspondence</pubtit>
      <p><em>Feitong Tan, Danhang Tang, Mingsong Dou, Kaiwen Guo, Rohit Pandey, Cem Keskin, Ruofei Du, Deqing Sun, Sofien Bouaziz, Sean Fanello, Ping Tan, Yinda Zhang</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2103.15573.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/shitao_cvpr21.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning Camera Localization via dense scene matching</pubtit>
      <p><em>Shitao Tang, Chengzhou Tang, Rui Huang, Siyu Zhu, Ping Tan</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2103.16792.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/riggable_cvpr21.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Riggable 3D Face Reconstruction via In-Network Optimization</pubtit>
      <p><em>Ziqian Bai, Zhaopeng Cui, Xiaoming Liu, Ping Tan</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2104.03493.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_D2IM_long.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>D2IM-Net: Learning Detail Disentangled Implicit Fields from Single Images</pubtit>
      <p><em>Manyi Li, Hao Zhang</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2012.06650.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/arxiv20_decorgan.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>DECOR-GAN: 3D Shape Detailization by Conditional Refinement</pubtit>
      <p><em>Zhiqin Chen, Vladimir Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Siddhartha Chaudhuri</em><br />
<em>In CVPR (oral presentation) (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2012.09159.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/scan2cap.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Scan2Cap: Context-aware Dense Captioning in RGB-D Scans</pubtit>
      <p><em>Dave Zhenyu Chen, Ali Gholami, Matthias Nießner, Angel X. Chang</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2012.02206.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/8-LayoutGMN.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>LayoutGMN: Neural Graph Matching for Structural Layout Similarity</pubtit>
      <p><em>Akshay Gadi Patil, Manyi Li, Matthew Fisher, Manolis Savva, Hao Zhang</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2012.06547.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/mirror3d.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Mirror3D: Depth Refinement for Mirror Surfaces</pubtit>
      <p><em>Jiaqi Tan, Weijie (Lewis) Lin, Angel X. Chang, Manolis Savva</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2106.06629.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cvpr2021-plan2cg.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Plan2Scene: Converting Floorplans to 3D Scenes</pubtit>
      <p><em>Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel Chang, and Manolis Savva</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2106.05375.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cvpr2021-hetero.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Heterogeneous Grid Convolution for Adaptive, Efficient, and Controllable Computation</pubtit>
      <p><em>Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www2.cs.sfu.ca/~furukawa/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cvpr2021-houseganpp.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>House-GAN++: Generative Adversarial Layout Refinement Networks</pubtit>
      <p><em>Nelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang, Hang Chu, Chin-Yi Cheng, and Yasutaka Furukawa</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2103.02574.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cvpr2021-roofgan.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Roof-GAN: Learning to Generate Roof Geometry and Relations for Residential Houses</pubtit>
      <p><em>Yiming Qian, Hao Zhang, and Yasutaka Furukawa</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2012.09340.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhai-cvpr21.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Hyper-LifelongGAN: Scalable Lifelong Learning for Image Conditioned Generation</pubtit>
      <p><em>Mengyao Zhai, Lei Chen, and Greg Mori</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www2.cs.sfu.ca/~mori/research/papers/zhai-cvpr21.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chang-cvpr21.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning Discriminative Prototypes with Dynamic Time Warping</pubtit>
      <p><em>Xiaobin Chang, Frederick Tung, and Greg Mori</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2103.09458.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/mrm.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging</pubtit>
      <p><em>S. Mahdi H. Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris, Yagiz Aksoy</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://yaksoy.github.io/research/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/PastedGraphic-2.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>End-to-End Rotation Averaging with Multi-Source Propagation</pubtit>
      <p><em>Luwei Yang, Heng Li, Jamal Rahim, Zhaopeng Cui, Ping Tan</em><br />
<em>In CVPR (2021)</em></p>

      <p></p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~pingtan/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2020">2020</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/Chen_ScanRefer.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language</pubtit>
      <p><em>Dave Zhenyu Chen, Angel X. Chang, Matthias Nießner</em><br />
<em>In ECCV (2020)</em></p>

      <p>This paper introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1912.08830.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://daveredrum.github.io/ScanRefer">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yimming_eccv20.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning Pairwise Inter-Plane Relations for Piecewise Planar Reconstruction</pubtit>
      <p><em>Yiming Qian and Yasutaka Furukawa</em><br />
<em>In ECCV (2020)</em></p>

      <p>This paper proposes a novel single-image piecewise planar reconstruction technique that infers and enforces inter-plane relationships. Our approach takes a planar reconstruction result from an existing system, then utilizes convolutional neural network (CNN) to (1) classify if two planes are orthogonal or parallel; and 2) infer if two planes are touching and, if so, where in the image. We formulate an optimization problem to refine plane parameters and employ a message passing neural network to refine plane segmentation masks by enforcing the inter-plane relations. Our qualitative and quantitative evaluations demonstrate the effectiveness of the proposed approach in terms of plane parameters and segmentation accuracy.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://drive.google.com/file/d/1OlUhc-P-NM9bDCTqK8CPCGff-xE1o3Vc/view?usp=sharing">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://yi-ming-qian.github.io/projects/interplane.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/housegan.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>House-GAN: Relational Generative Adversarial Networks for Graph-constrained House Layout Generation</pubtit>
      <p><em>Nelson Nauata, Kai-Hung Chang, Chin-Yi Cheng, Greg Mori, Yasutaka Furukawa</em><br />
<em>In ECCV (oral presentation) (2020)</em></p>

      <p>This paper proposes a novel graph-constrained generative adversarial network, whose generator and discriminator are built upon relational architecture. The main idea is to encode the constraint into the graph structure of its relational networks. We have demonstrated the proposed architecture for a new house layout generation problem, whose task is to take an architectural constraint as a graph (i.e., the number and types of rooms with their spatial adjacency) and produce a set of axis-aligned bounding boxes of rooms. We measure the quality of generated house layouts with the three metrics: the realism, the diversity, and the compatibility with the input graph constraint. Our qualitative and quantitative evaluations over 117,000 real floorplan images demonstrate that the proposed approach outperforms existing methods and baselines. We will publicly share all our code and data.  </p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/2003.06988">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://ennauata.github.io/housegan/page.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/vectorize-building.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Vectorizing World Buildings: Planar Graph Reconstruction by Primitive Detection and Relationship Inference</pubtit>
      <p><em>Nelson Nauata, Yasutaka Furukawa</em><br />
<em>In ECCV (2020)</em></p>

      <p>This paper tackles a 2D architecture vectorization problem, whose task is to infer an outdoor building architecture as a 2D planar graph from a single RGB image. We provide a new benchmark with ground-truth annotations for 2,001 complex buildings across the cities of Atlanta, Paris, and Las Vegas. We also propose a novel algorithm utilizing 1) convolutional neural networks (CNNs) that detects geometric primitives and infers their relationships and 2) an integer programming (IP) that assembles the information into a 2D planar graph. While being a trivial task for human vision, the inference of a graph structure with an arbitrary topology is still an open problem for computer vision. Qualitative and quantitative evaluations demonstrate that our algorithm makes significant improvements over the current state-of-the-art, towards an intelligent system at the level of human perception. We will share code and data.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1912.05135">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://ennauata.github.io/buildings2vec/page.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/lira_GANH.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>GANhopper: Multi-Hop GAN for Unsupervised Image-to-Image Translation</pubtit>
      <p><em>Wallace Lira, Johannes Merz, Daniel Ritchie, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In ECCV (2020)</em></p>

      <p>We introduce GANhopper, an unsupervised image-to-image translation network that transforms images gradually between two domains, through multiple hops. Instead of executing translation directly, we steer the translation by requiring the network to produce in-between images which resemble weighted hybrids between images from the two input domains. Our network is trained on unpaired images from the two domains only, without any in-between images. All hops are produced using a single generator along each direction. In addition to the standard cycle-consistency and adversarial losses, we introduce a new hybrid discriminator, which is trained to classify the intermediate images produced by the generator as weighted hybrids, with weights based on a predetermined hop count.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/2002.10102">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/wallacemplira/ganhopper">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/jin_DRKFD.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape Reconstruction</pubtit>
      <p><em>Jiongchao Jin, Akshay Gadi Patil, and Hao Zhang</em><br />
<em>In ECCV (2020)</em></p>

      <p>We advocate the use of differential visual shape metrics to train deep neural networks for 3D reconstruction. We introduce such a metric which compares two 3D shapes by measuring visual, image-space differences between multiview images differentiably rendered from the shapes. Furthermore, we develop a differentiable image-space distance based on mean-squared errors defined over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape metric can be easily plugged into various reconstruction networks, replacing the object-space distortion measures, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstruction results with better structural fidelity and visual quality.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1911.09204">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/fanbo_sapien.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SAPIEN: a SimulAted Part-based Interactive ENvironment</pubtit>
      <p><em>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas Guibas, Hao Su</em><br />
<em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11097-11107). (2020)</em></p>

      <p>Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate stateof-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/2003.08515.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://sapien.ucsd.edu/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/erik_dd_ppo.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames</pubtit>
      <p><em>Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra</em><br />
<em>In International Conference on Learning Representations (ICLR), 2020 (2020)</em></p>

      <p>We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ‘stale’), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim (Savva et al., 2019), DD-PPO exhibits near-linear scaling – achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) – over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially ‘solves’ the task – near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks – the analog of ‘ImageNet pre-training + task-specific fine-tuning’ for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1911.00357.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/facebookresearch/habitat-api">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/kadian_simulated_environments.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation</pubtit>
      <p><em>Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra</em><br />
<em>In Robotics and Automation Letters (RA-L) &amp; IROS (2020)</em></p>

      <p>Does progress in simulation translate to progress in robotics? Specifically, if method A outperforms method B in simulation, how likely is the trend to hold in reality on a robot? We examine this question for embodied (PointGoal) navigation – developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity, revealing surprising findings about prior work. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on a simulated agent and a physical robot. Habitat-to-Locobot transfer with HaPy involves just one line change in a config parameter, essentially treating reality as just another simulator! Second, we investigate sim2real predictivity of HabitatSim [1] for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify sim2real predictivity. Our analysis reveals several important findings. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), which suggests that performance improvements for this simulator-based challenge would not transfer well to a physical robot. We find that this gap is largely due to AI agents learning to ‘cheat’ by exploiting simulator imperfections – specifically, the way Habitat allows for ‘sliding’ along walls on collision. Essentially, the virtual robot is capable of cutting corners, leading to unrealistic shortcuts through parts of non-navigable space. Naturally, such exploits do not work in the real world where the robot stops on contact with walls. Our experiments show that it is possible to optimize simulation parameters to enable robots trained in imperfect simulators to generalize learned skills to reality (e.g. improving SRCCSucc from 0.18 to 0.844).</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1912.06321.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://abhiskk.github.io/sim2real">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/changan_crowd_navigation.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Relational Graph Learning for Crowd Navigation</pubtit>
      <p><em>Changan Chen, Sha Hu, Payam Nikdel, Greg Mori, Manolis Savva</em><br />
<em>In International Conference on Intelligent Robots and Systems (IROS) (2020)</em></p>

      <p>We present a relational graph learning approach for robotic crowd navigation using model-based deep reinforcement learning that plans actions by looking into the future. Our approach reasons about the relations between all agents based on their latent features and uses a Graph Convolutional Network to encode higher-order interactions in each agent’s state representation, which is subsequently leveraged for state prediction and value estimation. The ability to predict human motion allows us to perform multi-step lookahead planning, taking into account the temporal evolution of human crowds. We evaluate our approach against a state-of-the-art baseline for crowd navigation and ablations of our model to demonstrate that navigation with our approach is more efficient, results in fewer collisions, and avoids failure cases involving oscillatory and freezing behaviors.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1909.13165.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/ChanganVR/RelationalGraphLearning">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xu_tiling.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network</pubtit>
      <p><em>Hao Xu, Ka Hei Hui, Chi-Wing Fu, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH), Vol. 39, No. 4, 2020 (2020)</em></p>

      <p>We introduce the first neural optimization framework to solve a classical instance of the tiling problem. Namely, we seek a non-periodic tiling of an arbitrary 2D shape using one or more types of tiles: the tiles maximally fill the shape’s interior without overlaps or holes. To start, we reformulate tiling as a graph problem by modeling candidate tile locations in the target shape as graph nodes and connectivity between tile locations as edges. We build a graph convolutional neural network, coined TilinGNN, to progressively propagate and aggregate features over graph edges and predict tile placements. Our network is self-supervised and trained by maximizing the tiling coverage on target shapes, while avoiding overlaps and holes between the tiles. After training, TilinGNN has a running time that is roughly linear to the number of candidate tile locations, significantly outperforming traditional combinatorial search.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/hu_gtop.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Graph2Plan: Learning Floorplan Generation from Layout Graphs</pubtit>
      <p><em>Ruizhen Hu, Zeyu Huang, Yuhan Tang, Oliver van Kaick, Hao Zhang, and Hui Huang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH), Vol. 39, No. 4, 2020 (2020)</em></p>

      <p>We introduce a learning framework for automated floorplan generation which combines generative modeling using deep neural networks and user-in-the-loop designs to enable human users to provide sparse design constraints. Such constraints are represented by a layout graph. The core component of our learning framework is a deep neural network, Graph2Plan, which is trained on RPLAN, a large-scale dataset consisting of 80K annotated, human-designed floorplans. The network converts a layout graph, along with a building boundary, into a floorplan that fulfills both the layout and boundary constraints.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/2004.13204">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/guan_tvcg20_fame_small.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>FAME: 3D Shape Generation via Functionality-Aware Model Evolution</pubtit>
      <p><em>Yanran Guan, Han Liu, Kun Liu, Kangxue Yin, Ruizhen Hu, Oliver van Kaick, Yan Zhang, Ersin Yumer, Nathan Carr, Radomir Mech, and Hao Zhang</em><br />
<em>In IEEE Trans. on Visualization and Computer Graphics (TVCG), major revision, 2020 (2020)</em></p>

      <p>We introduce a modeling tool which can evolve a set of 3D objects in a functionality-aware manner. Our goal is for the evolution to generate large and diverse sets of plausible 3D objects for data augmentation, constrained modeling, as well as open-ended exploration to possibly inspire new designs. Starting with an initial population of 3D objects belonging to one or more functional categories, we evolve the shapes through part re-combination to produce generations of hybrids or crossbreeds between parents from the heterogeneous shape collection …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/2005.04464">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen_BSPNET.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>BSP-Net: Generating Compact Meshes via Binary Space Partitioning</pubtit>
      <p><em>Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang</em><br />
<em>In CVPR (oral presentation), 2020. Best Student Paper Award. (2020)</em></p>

      <p>Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-of-the-art methods while using much fewer primitives.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1911.06971">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://bsp-net.github.io/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/nelson_cvpr20.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction</pubtit>
      <p><em>Nelson Nauata * , Fuyang Zhang * , and Yasutaka Furukawa (* indicates equal contribution)</em><br />
<em>In CVPR 2020 (2020)</em></p>

      <p>This paper proposes a novel message passing neural (MPN) architecture Conv-MPN, which reconstructs an outdoor building as a planar graph from a single RGB image. Conv-MPN is specifically designed for cases where nodes of a graph have explicit spatial embedding. In our problem, nodes correspond to building edges in an image. Conv-MPN is different from MPN in that 1) the feature associated with a node is represented as a feature volume instead of a 1D vector; and 2) convolutions encode messages instead of fully connected layers. Conv-MPN learns to select a true subset of nodes (i.e., building edges) to reconstruct a building planar graph. Our qualitative and quantitative evaluations over 2,000 buildings show that Conv-MPN makes significant improvements over the existing fully neural solutions. We believe that the paper has a potential to open a new line of graph neural network research for structured geometry reconstruction.  </p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1912.01756">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>





 | 

<a target="_blank" href="https://zhangfuyang.github.io/conv-mpn/bib.txt">
   <img src="https://gruvi.cs.sfu.ca/images/icons/bibtex.jpg" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Bibtex</a>





 | 

<a target="_blank" href="https://zhangfuyang.github.io/conv-mpn/page.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhu_adacoseg.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</pubtit>
      <p><em>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas J. Guibas, and Hao Zhang</em><br />
<em>In CVPR (oral presentation), 2020 (2020)</em></p>

      <p>We introduce AdaSeg, a deep neural network architecture for adaptive co-segmentation of a set of 3D shapes represented as point clouds. Differently from the familiar single-instance segmentation problem, co-segmentation is intrinsically contextual: how a shape is segmented can vary depending on the set it is in. Hence, our network features an adaptive learning module to produce a consistent shape segmentation which adapts to a set.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1903.10297">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wu_PQNET.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes</pubtit>
      <p><em>Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Baoquan Chen</em><br />
<em>In CVPR, 2020 (2020)</em></p>

      <p>We introduce PQ-NET, a deep neural network which represents and generates 3D shapes via sequential part assembly. The input to our network is a 3D shape segmented into parts, where each part is first encoded into a feature representation using a part autoencoder. The core component of PQ-NET is a sequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part features into a latent vector of fixed size, and the decoder reconstructs the 3D shape, one part at a time, resulting in a sequential assembly. The latent space formed by the Seq2Seq encoder encodes both part structure and fine part geometry. The decoder can be adapted to perform several generative tasks including shape autoencoding, interpolation, novel shape generation, and single-view 3D reconstruction, where the generated shapes are all composed of meaningful parts.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1911.10949">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/egstar_2020.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning Generative Models of 3D Structures</pubtit>
      <p><em>Siddhartha Chaudhuri, Daniel Ritchie, Jiajun Wu, Kai Xu, and Hao Zhang</em><br />
<em>In Computer Graphics Forum (Eurographics STAR), 2020 (2020)</em></p>

      <p>3D models of objects and scenes are critical to many academic disciplines and industrial applications. Of particular interest is the emerging opportunity for 3D graphics to serve artificial intelligence: computer vision systems can benefit from synthetically- generated training data rendered from virtual 3D scenes, and robots can be trained to navigate in and interact with real-world environments by first acquiring skills in simulated ones. One of the most promising ways to achieve this is by learning and applying generative models of 3D content: computer programs that can synthesize new 3D shapes and scenes. To allow users to edit and manipulate the synthesized 3D content to achieve their goals, the generative model should also be structure-aware: it should express 3D shapes and scenes using abstractions that allow manipulation of their high-level structure. This state-of-the- art report surveys historical work and recent progress on learning structure-aware generative models of 3D shapes and scenes.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/egstar2020.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<h3 id="2019">2019</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/Kai_planit.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>PlanIT: Planning and Instantiating Indoor Scenes with Relation Graph and Spatial Prior Networks</pubtit>
      <p><em>Kai Wang, Yu-An Lin, Ben Weissmann, Angel X. Chang, Manolis Savva, Daniel Ritchie</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH), 38(4), pp.1-15. (2019)</em></p>

      <p>We present a new framework for interior scene synthesis that combines a high-level relation graph representation with spatial prior neural networks. We observe that prior work on scene synthesis is divided into two camps: object-oriented approaches (which reason about the set of objects in a scene and their configurations) and space-oriented approaches (which reason about what objects occupy what regions of space). Our insight is that the object-oriented paradigm excels at high-level planning of how a room should be laid out, while the space-oriented paradigm performs well at instantiating a layout by placing objects in precise spatial configurations. With this in mind, we present PlanIT, a layout-generation framework that divides the  problem into two distinct planning and instantiation phases. PlanIT repre- sents the “plan” for a scene via a relation graph, encoding objects as nodes  and spatial/semantic relationships between objects as edges. In the planning phase, it uses a deep graph convolutional generative model to synthesize relation graphs. In the instantiation phase, it uses image-based convolutional network modules to guide a search procedure that places objects into the scene in a manner consistent with the graph. By decomposing the problem in this way, PlanIT generates scenes of comparable quality to those generated by prior approaches (as judged by both people and learned classifiers), while also providing the modeling flexibility of the intermediate relationship graph representation. These graphs allow the system to support applications such as scene synthesis from a partial graph provided by a user.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://drive.google.com/file/d/1CJCM6EQyeUWwxdk6tl8cVxEIhV7s3DoA/view">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/brownvc/planit">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/julian_replica.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>The Replica Dataset: A Digital Replica of Indoor Spaces</pubtit>
      <p><em>Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, Richard Newcombe</em><br />
<em>In arXiv:1906.05797 (2019)</em></p>

      <p>We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, highresolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world – for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is ‘Habitatcompatible’, i.e. can be natively used with AI Habitat [24] for training and testing embodied agents.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1906.05797.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/facebookresearch/Replica-Dataset">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/manolis_habitat.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Habitat: A Platform for Embodied AI Research</pubtit>
      <p><em>Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra</em><br />
<em>In In Proceedings of the IEEE International Conference on Computer Vision (pp. 9339-9347). (2019)</em></p>

      <p>We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast – when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-toend development of embodied AI algorithms – defining tasks (e.g. navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or ‘merely’ impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works [20, 16] and find evidence for the opposite conclusion – that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} × {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1904.01201.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://aihabitat.org/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yifei_hierarchylayout.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout Prediction</pubtit>
      <p><em>Yifei Shi, Angel X. Chang, Zhelun Wu, Manolis Savva, Kai Xu</em><br />
<em>In In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1771-1780). (2019)</em></p>

      <p>Indoor scenes exhibit rich hierarchical structure in 3D object layouts. Many tasks in 3D scene understanding can benefit from reasoning jointly about the hierarchical context of a scene, and the identities of objects. We present a variational denoising recursive autoencoder (VDRAE) that generates and iteratively refines a hierarchical representation of 3D object layouts, interleaving bottom-up encoding for context aggregation and top-down decoding for propagation. We train our VDRAE on large-scale 3D scene datasets to predict both instance-level segmentations and a 3D object detections from an over-segmentation of an input point cloud. We show that our VDRAE improves object detection performance on real-world 3D point cloud datasets compared to baselines from prior work.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1903.03757.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.yifeishi.net/hierarchylayout.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/kaichun_partnet.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</pubtit>
      <p><em>Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas Guibas, Hao Su</em><br />
<em>In In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 909-918). (2019)</em></p>

      <p>We present PartNet: a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. We benchmark four state-ofthe-art 3D deep learning algorithms for fine-grained semantic segmentation and three baseline methods for hierarchical semantic segmentation. We also propose a novel method for part instance segmentation and demonstrate its superior performance over existing methods.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1812.02713.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://cs.stanford.edu/~kaichun/partnet/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/armen_scan2cad.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Scan2CAD: Learning CAD Model Alignment in RGB-D Scans</pubtit>
      <p><em>Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, Matthias Nießner</em><br />
<em>In In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2614-2623). (2019)</em></p>

      <p>We present Scan2CAD1 , a novel data-driven method that learns to align clean 3D CAD models from a shape database to the noisy and incomplete geometry of an RGBD scan. For a 3D reconstruction of an indoor scene, our method takes as input a set of CAD models, and predicts a 9DoF pose that aligns each model to the underlying scan geometry. To tackle this problem, we create a new scanto-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoint pairs between 14225 CAD models from ShapeNet and their counterpart objects in the scans. Our method selects a set of representative keypoints in a 3D scan for which we find correspondences to the CAD geometry. To this end, we design a novel 3D CNN architecture to learn a joint embedding between real and synthetic objects, and thus predict a correspondence heatmaps. Based on these correspondence heatmaps, we formulate a variational energy minimization that aligns a given set of CAD models to the reconstruction. We evaluate our approach on our newly introduced Scan2CAD benchmark where we outperform both handcrafted feature descriptor as well as state-of-the-art CNN based methods by 21.39%.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1811.11187.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://niessnerlab.org/projects/avetisyan2019scan2cad.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/justin_mimic_and_rephrase.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Mimic and Rephrase: Reflective Listening in Open-Ended Dialogue</pubtit>
      <p><em>Justin Dieter, Tian Wang, Gabor Angeli, Angel X. Chang, Arun Tejasvi Chaganty</em><br />
<em>In In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) (pp. 393-403). (2019)</em></p>

      <p>Reflective listening—demonstrating that you have heard your conversational partner—is key to effective communication. Expert human communicators often mimic and rephrase their conversational partner, e.g., when responding to sentimental stories or to questions they don’t know the answer to. We introduce a new task and an associated dataset wherein dialogue agents similarly mimic and rephrase a user’s request to communicate sympathy (I’m sorry to hear that) or lack of knowledge (I do not know that). We study what makes a rephrasal response good against a set of qualitative metrics. We then evaluate three models for generating responses: a syntax-aware rulebased system, a seq2seq LSTM neural models with attention (S2SA), and the same neural model augmented with a copy mechanism (S2SA+C). In a human evaluation, we find that S2SA+C and the rule-based system are comparable and approach human-generated response quality. In addition, experiences with a live deployment of S2SA+C in a customer support setting suggest that this generation task is a practical contribution to real world conversational agents.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.aclweb.org/anthology/K19-1037.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/square/MimicAndRephrase">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yin_LOGAN.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>LOGAN: Unpaired Shape Transform in Latent Overcomplete Space</pubtit>
      <p><em>Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 38, No. 6, Article 198, 2019 (2019)</em></p>

      <p>We introduce LOGAN, a deep neural network aimed at learning general-purpose shape transforms from unpaired domains. The network is trained on two sets of shapes, e.g., tables and chairs, while there is neither a pairing between shapes from the domains as supervision nor any point-wise correspondence between any shapes. Once trained, LOGAN takes a shape from one domain and transforms it into the other. Our network consists of an autoencoder to encode shapes from the two input domains into a common latent space, where the latent codes concatenate multi-scale shape features, resulting in an overcomplete representation. The translator is based on a latent generative adversarial network (GAN), where an adversarial loss enforces cross-domain translation while a feature preservation loss ensures that the right shape features are preserved for a natural shape transform.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1903.10170">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/kangxue/LOGAN">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/gao_SDM.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SDM-NET: Deep Generative Network for Structured Deformable Mesh</pubtit>
      <p><em>Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 38, No. 6, Article 243, 2019 (2019)</em></p>

      <p>We introduce SDM-NET, a deep generative neural network which produces structured deformable meshes. Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respect the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by deforming the box. The architecture of SDM-NET is that of a two-level variational autoencoder (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which jointly learns the part structure of a shape collection and the part geometries, ensuring a coherence between global shape structure and surface details.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1908.04520">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://geometrylearning.com/sdm-net/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xu_LEGO_long.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Computational LEGO Technic Design</pubtit>
      <p><em>Hao Xu, Ka Hei Hui, Chi-Wing Fu, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 38, No. 6, Article 196, 2019 (2019)</em></p>

      <p>We introduce a method to automatically compute LEGO Technic models from user input sketches, optionally with motion annotations. The generated models resemble the input sketches with coherently-connected bricks and simple layouts, while respecting the intended symmetry and mechanical properties expressed in the inputs. This complex computational assembly problem involves an immense search space, and a much richer brick set and connection mechanisms than regular LEGO. To address it, we first comprehensively model the brick properties and connection mechanisms, then formulate the construction requirements into an objective function, accounting for faithfulness to input sketch, model simplicity, and structural integrity. Next, we model the problem as a sketch cover, where we iteratively refine a random initial layout to cover the input sketch, while guided by the objective. At last, we provide a working system to analyze the balance, stress, and assemblability of the generated model.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/compute_technic/xu_siga19_paper.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://appsrv.cse.cuhk.edu.hk/~haoxu/projects/compute_technic/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yan_RPM.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud</pubtit>
      <p><em>Zhihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen, Oliver van Kaick, Hao Zhang, and Hui Huang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 38, No. 6, Article 240, 2019 (2019)</em></p>

      <p>We introduce RPM-Net, a deep learning-based approach which simultaneously infers movable parts and hallucinates their motions from a single, un-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel Recurrent Neural Network (RNN), composed of an encoder-decoder pair with interleaved Long Short-Term Memory (LSTM) components, which together predict a temporal sequence of point-wise displacements for the input shape. At the same time, the displacements allow the network to learn moveable parts, resulting in a motion-based shape segmentation. Recursive applications of RPM-Net on the obtained parts can predict finer-level part motions, resulting in a hierarchical object segmentation. Furthermore, we develop a separate network to estimate part mobilities, e.g., per part motion parameters, from the segmented motion sequence.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://vcc.szu.edu.cn/file/upload_file//image/research/att201911071109/RPM-Net_reduced.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://vcc.tech/research/2019/RPMNet">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen_BAENET.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>BAE-NET: Branched Autoencoder for Shape Co-Segmentation</pubtit>
      <p><em>Zhiqin Chen, Kangxue Yin, Matt Fisher, Siddhartha Chaudhuri, and Hao Zhang</em><br />
<em>In ICCV 2019 (2019)</em></p>

      <p>We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with all shapes in an input collection using a shape reconstruction loss, without ground-truth segmentations. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1903.11228">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/schor_partAE.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning to Generate the "Unseen" via Part Synthesis and Composition</pubtit>
      <p><em>Nadav Schor, Oren Katzier, Hao Zhang, and Daniel Cohen-Or</em><br />
<em>In ICCV 2019 (2019)</em></p>

      <p>Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to sample a rich variety of data from the entire target distribution, rather than only from the distribution of the training data. In other words, we would like the generative model to go beyond the observed training samples and learn to also generate “unseen” data. In our work, we present a generative neural network for shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1811.07441">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhu_cosegnet.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>CoSegNet: Deep Co-Segmentation of 3D Shapes with Group Consistency Loss</pubtit>
      <p><em>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas J. Guibas, and Hao Zhang</em><br />
<em>In arXiv (2019)</em></p>

      <p>We introduce CoSegNet, a deep neural network architecture for co-segmentation of a set of 3D shapes represented as point clouds. CoSegNet takes as input a set of unsegmented shapes, proposes per-shape parts, and then jointly optimizes the part labelings across the set subjected to a novel group consistency loss expressed via matrix rank estimates. The proposals are refined in each iteration by an auxiliary network that acts as a weak regularizing prior, pre-trained to denoise noisy, unlabeled parts from a large collection of segmented 3D shapes, where the part compositions within the same object category can be highly inconsistent. The output is a consistent part labeling for the input set, with each shape segmented into up to K (a user-specified hyperparameter) parts.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1903.10297">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen_cvpr19_IMGAN.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning Implicit Fields for Generative Shape Modeling</pubtit>
      <p><em>Zhiqin Chen and Hao Zhang</em><br />
<em>In CVPR 2019 (2019)</em></p>

      <p>We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. Our implicit field decoder is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1812.02822">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>





 | 

<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/haoz_paper_bib.html#imgan_cvpr19">
   <img src="https://gruvi.cs.sfu.ca/images/icons/bibtex.jpg" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Bibtex</a>





 | 

<a target="_blank" href="https://github.com/czq142857/implicit-decoder">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_tog18_grains.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>GRAINS: Generative Recursive Autoencoders for INdoor Scenes</pubtit>
      <p><em>Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics, to appear and be presented at SIGGRAPH 2019 (2019)</em></p>

      <p>We present a generative neural network which enables us to generate plausible 3D indoor scenes in large quantities and varieties, easily and highly efficiently. Our key observation is that indoor scene structures are inherently hierarchical. Hence, our network is not convolutional; it is a recursive neural network or RvNN. Using a dataset of annotated scene hierarchies, we train a variational recursive autoencoder, or RvNN-VAE, which performs scene object grouping during its encoding phase and scene generation during decoding.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1807.09193">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>





 | 

<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/haoz_paper_bib.html#grains_tog19">
   <img src="https://gruvi.cs.sfu.ca/images/icons/bibtex.jpg" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Bibtex</a>







]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/gan_gi19_album.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Qualitative Organization of Photo Collections via Quartet Analysis and Active Learning</pubtit>
      <p><em>Yuan Gan, Yan Zhang, and Hao Zhang</em><br />
<em>In Proc. of Graphics Interface, 2019 (2019)</em></p>

      <p>We introduce the use of qualitative analysis and active learning to photo album construction. Given a heterogeneous collection of pho- tos, we organize them into a hierarchical categorization tree (C-tree) based on qualitative analysis using quartets instead of relying on conventional, quantitative image similarity metrics. The main moti- vation is that in a heterogeneous collection, quantitative distances may become unreliable between dissimilar data and there is unlikely a single metric that is well applicable to all data.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xu_cvm19_mosaic.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Discernible Image Mosaic with Edge-Aware Adaptive Tiles</pubtit>
      <p><em>Pengfei Xu, Jiangqiang Ding, Hao Zhang, and Hui Huang</em><br />
<em>In Computational Visual Media (CVM), 2019 (2019)</em></p>

      <p>We present a novel method to produce discernible image mosaics, with relatively large image tiles replaced by images drawn from a database, to resemble a target image. Since visual edges strongly support content perception, we compose our mosaic via edge-aware photo retrieval to best preserve visual edges in the target image. Moreover, unlike most previous works which apply a pre-determined partition to an input image, our image mosaics are composed by adaptive tiles, whose sizes are determined based on the available images and an objective of maximizing resemblance to the target.</p>

      <p style="text-align: right;">

[





<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/haoz_paper_bib.html#mosaic_cvm19">
   <img src="https://gruvi.cs.sfu.ca/images/icons/bibtex.jpg" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Bibtex</a>





 | 

<a target="_blank" href="https://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<h3 id="2018">2018</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yi_branchGAN.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>BranchGAN: Branched Generative Adversarial Networks for Scale-Disentangled Learning and Synthesis of Images</pubtit>
      <p><em>Zili Yi, Zhiqin Chen, Hao Cai, Xin Huang, Minglun Gong, and Hao Zhang</em><br />
<em>In arXiv (2018)</em></p>

      <p>We introduce BranchGAN, a novel training method that enables unconditioned generative adversarial networks (GANs) to learn image manifolds at multiple scales. The key novel feature of BranchGAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network, is explicitly split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively “de-freeze” the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1803.08467.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/duxingren14/BranchGAN">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhu_siga18_scores.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SCORES: Shape Composition with Recursive Substructure Priors</pubtit>
      <p><em>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 37, No. 6 (2018)</em></p>

      <p>We introduce SCORES, a recursive neural network for shape composition. Our network takes as input sets of parts from two or more source 3D shapes and a rough initial placement of the parts. It outputs an optimized part structure for the composed shape, leading to high-quality geometry construction. A unique feature of our composition network is that it is not merely learning how to connect parts. Our goal is to produce a coherent and plausible 3D shape, despite large incompatibilities among the input parts. The network may significantly alter the geometry and structure of the input parts and synthesize a novel shape structure based on the inputs, while adding or removing parts to minimize a structure plausibility loss.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/abs/1809.05398">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://kevinkaixu.net/projects/scores.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/lira_siga18_wire.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Construction and Fabrication of Reversible Shape Transforms</pubtit>
      <p><em>Wallace Lira, Chi-Wing Fu, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 37, No. 6 (2018)</em></p>

      <p>We present a fully automatic method that finds a small number of machine fabricable wires with minimal overlap to reproduce a wire sculpture design as a 3D shape abstraction. Importantly, we consider non-planar wires, which can be fabricated by a wire bending machine, to enable efficient construction of complex 3D sculptures that cannot be achieved by previous works. We call our wires Eulerian wires, since they are as Eulerian as possible with small overlap to form the target design together.</p>

      <p style="text-align: right;">

[







<a target="_blank" href="http://gruvi.cs.sfu.ca/project/eulerianwires/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_siga18_riot.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Construction and Fabrication of Reversible Shape Transforms</pubtit>
      <p><em>Shuhua Li, Ali Mahdavi-Amiri, Ruizhen Hu, Han Liu, Changqing Zou, Oliver van Kaick, Xiuping Liu, Hui Huang, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 37, No. 6 (2018)</em></p>

      <p>We study a new and elegant instance of geometric dissection of 2D shapes: reversible hinged dissection, which corresponds to a dual transform between two shapes where one of them can be dissected in its interior and then inverted inside-out, with hinges on the shape boundary, to reproduce the other shape, and vice versa. We call such a transform reversible inside-out transform or RIOT. Since it is rare for two shapes to possess even a rough RIOT, let alone an exact one, we develop both a RIOT construction algorithm and a quick filtering mechanism to pick, from a shape collection, potential shape pairs that are likely to possess the transform. Our construction algorithm is fully automatic. It computes an approximate RIOT between two given input 2D shapes, whose boundaries can undergo slight deformations, while the filtering scheme picks good inputs for the construction.</p>

      <p style="text-align: right;">

[







<a target="_blank" href="https://sites.google.com/site/alimahdaviamiri/projects/reversible-shapes">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/ma_siga18_t2s.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Language-Driven Synthesis of 3D Scenes Using Scene Databases</pubtit>
      <p><em>Rui Ma, Akshay Gadi Patil (co-first author), Matt Fisher, Manyi Li, Soren Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong, Leonidas J. Guibas, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 37, No. 6 (2018)</em></p>

      <p>We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene databases. The advantage of natural language editing interfaces is strongest when performing semantic operations at the sub-scene level, acting on groups of objects. We learn how to manipulate these sub-scenes by analyzing existing 3D scenes. We perform edits by first parsing a natural language command from the user and trans- forming it into a semantic scene graph that is used to retrieve corresponding sub-scenes from the databases that match the command. We then augment this retrieved sub-scene by incorporating other objects that may be implied by the scene context. Finally, a new 3D scene is synthesized by aligning the augmented sub-scene with the user’s current scene, where new objects are spliced into the environment, possibly triggering appropriate adjustments to the existing scene arrangement.</p>

      <p style="text-align: right;">

[







<a target="_blank" href="http://www.sfu.ca/~agadipat/publications/2018/T2S/project_page.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen_siga18_pym.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SketchyScene: 3D Fabrication with Universal Building Blocks and Pyramidal Shells</pubtit>
      <p><em>Xuelin Chen, Honghua Li, Chi-Wing Fu, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia), Vol. 37, No. 6 (2018)</em></p>

      <p>We introduce a computational solution for cost-efficient 3D fabrication using universal building blocks. Our key idea is to employ a set of universal blocks, which can be massively prefabricated at a low cost, to quickly assemble and constitute a significant internal core of the target object, so that only the residual volume need to be 3D printed online. We further improve the fabrication efficiency by decomposing the residual volume into a small number of printing-friendly pyramidal pieces.</p>

      <p style="text-align: right;">

[







<a target="_blank" href="http://irc.cs.sdu.edu.cn/~xuelin/prefab/index.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zou_eccv18_sketch.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SketchyScene: Richly-Annotated Scene Sketches</pubtit>
      <p><em>Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengyi Gao, Baoquan Chen, and Hao Zhang</em><br />
<em>In ECCV (2018)</em></p>

      <p>We contribute the first large-scale dataset of scene sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. We demonstrate the potential impact of SketchyScene by training new computational models for semantic segmentation of scene sketches and showing how the new dataset enables several applications including image retrieval, sketch colorization, editing, and captioning, etc. We will release the complete crowdsourced dataset to the community.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zou_eccv18_sketch.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yin_sig18_p2p.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>P2P-NET: Bidirectional Point Displacement Net for Shape Transform</pubtit>
      <p><em>Kangxue Yin, Hui Huang, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH), Vol. 37, No. 4, Article 152 (2018)</em></p>

      <p>We introduce P2P-NET, a general-purpose deep neural network which learns geometric transformations between point-based shape representations from two domains, e.g., meso-skeletons and surfaces, partial and complete scans, etc. The architecture of the P2P-NET is that of a bi-directional point dis- placement network, which transforms a source point set to a prediction of the target point set with the same cardinality, and vice versa, by applying point-wise displacement vectors learned from data. P2P-NET is trained on paired shapes from the source and target domains, but without relying on point-to-point correspondences between the source and target point sets. The training loss combines two uni-directional geometric losses, each enforc- ing a shape-wise similarity between the predicted and the target point sets, and a cross-regularization term to encourage consistency between displace- ment vectors going in opposite directions.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/yin_sig18_p2p.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/hu_sig18_icon4.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Predictive and Generative Neural Networks for Object Functionality</pubtit>
      <p><em>Ruizhen Hu, Zhihao Yan, Jingwen Zhang, Oliver van Kaick, Ariel Shamir, Hao Zhang, and Hui Huang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH), Vol. 37, No. 4, Article 151 (2018)</em></p>

      <p>Humans can predict the functionality of an object even without any surroundings, since their knowledge and experience would allow them to “hallucinate” the interaction or usage scenarios involving the object. We develop predictive and generative deep convolutional neural networks to replicate this feat. Our networks are trained on a database of scene contexts, called interaction contexts, each consisting of a central object and one or more surrounding objects, that represent object functionalities. Given a 3D object in isolation, our functional similarity network (fSIM-NET), a variation of the triplet network, is trained to predict the functionality of the object by inferring functionality-revealing interaction contexts involving the object. fSIM-NET is complemented by a generative network (iGEN-NET) and a segmentation network (iSEG-NET). iGEN-NET takes a single voxelized 3D object and synthesizes a voxelized surround, i.e., the interaction context which visually demonstrates the object’s functionalities. iSEG-NET separates the interacting objects into different groups according to their interaction types.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/hu_sig18_icon4.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://vcc.szu.edu.cn/research/2018/ICON4/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhao_sig18_cnc.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>DSCarver: Decompose-and-Spiral-Carve for Subtractive Manufacturing</pubtit>
      <p><em>Haisen Zhao, Hao Zhang, Shiqing Xin, Yuanmin Deng, Changhe Tu, Wenping Wang, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH), Vol. 37, No. 4, Article 137 (2018)</em></p>

      <p>We present an automatic algorithm for subtractive manufacturing of freeform 3D objects using high-speed CNC machining. Our method decomposes the input object’s surface into a small number of patches each of which is fully accessible and machinable by the CNC machine, in continuous fashion, under a fixed drill-object setup configuration. This is achieved by covering the input surface using a minimum number of accessible regions and then extracting a set of machinable patches from each accessible region. For each patch obtained, we compute a continuous, space-filling, and iso-scallop tool path, in the form of connected Fermat spirals, which conforms to the patch boundary. Furthermore, we develop a novel method to control the spacing of Fermat spirals based on directional surface curvature and adapt the heat method to obtain iso-scallop carving.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zhao_sig18_cnc.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yin_tvcg18_intpair_small.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Sampling Approach to Generating Closely Interacting 3D Pose-pairs from 2D Annotations</pubtit>
      <p><em>Kangxue Yin, Hui Huang, Edmond S. L. Ho, Hao Wang, Taku Komura, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In IEEE Trans. on Visualization and Computer Graphics (TVCG), minor revision (2018)</em></p>

      <p>We introduce a data-driven method to generate a large number of plausible, closely interacting 3D human pose-pairs, for a given motion category, e.g., wrestling or salsa dance. With much difficulty in acquiring close interactions using 3D sensors, our approach utilizes abundant existing video data which cover many human activities. Instead of treating the data generation problem as one of reconstruction, we present a solution based on Markov Chain Monte Carlo (MCMC) sampling. Given a motion category and a set of video frames depicting the motion with the 2D pose-pair in each frame annotated, we start the sampling with one or few seed 3D pose-pairs which are manually created based on the target motion category. The initial set is then augmented by MCMC sampling around the seeds, via the Metropolis-Hastings algorithm and guided by a probability density function (PDF) that is defined by two terms to bias the sampling towards 3D pose-pairs that are physically valid and plausible for the motion category.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_gm18_csmetric.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Class-Sensitive Shape Dissimilarity Metric</pubtit>
      <p><em>Manyi Li, Noa Fish, Lili Cheng, Changhe Tu, Daniel Cohen-Or, Hao Zhang, and Baoquan Chen</em><br />
<em>In Graphical Models (2018)</em></p>

      <p>Shape dissimilarity is a fundamental problem with many applications such as shape exploration, retrieval, and classification. Given a collection of shapes, all existing methods develop a consistent global metric to compareand organize shapes. The global nature of the involved shape descriptors implies that overall shape appearanceis compared. These methods work well to distinguishshapes from different categories, but often fail for fine-grained classes within the same category. In this paper, we develop a dissimilarity metric for fine-grained classes by fusing together multiple distinctive metrics for different classes. The fused metric measures the dissimilarities among inter-class shapes by observing their unique traits.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/li_gm18_csmetric.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yi_arx18_bgan.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Branched Generative Adversarial Networks for Multi-Scale Image Manifold Learning</pubtit>
      <p><em>Zili Yi, Zhiqin Chen, Hao Zhang, Xin Huang, and Minglun Gong</em><br />
<em>In arXiv:1803.08467 (2018)</em></p>

      <p>Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://arxiv.org/pdf/1803.08467.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yu_tog18_style.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines</pubtit>
      <p><em>Fenggen Yu, Yan Zhang, Kai Xu, Ali Mahdavi-Amiri, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (2018)</em></p>

      <p>We present a semi-supervised co-analysis method for learning 3D shape styles from projected feature lines, achieving style patch localization with only weak supervision. Given a collection of 3D shapes spanning multiple object categories and styles, we perform style co-analysis over projected feature lines of each 3D shape and then backproject the learned style features onto the 3D shapes.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/yu_tog18_style.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2017">2017</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/hu_siga17_icon3.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning to Predict Part Mobility from a Single Static Snapshot</pubtit>
      <p><em>Ruizhen Hu, Wenchao Li, Oliver van Kaick, Ariel Shamir, Hao Zhang, and Hui Huang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia) (2017)</em></p>

      <p>We introduce a method for learning a model for the mobility of parts in 3D objects. Our method allows not only to understand the dynamic function- alities of one or more parts in a 3D object, but also to apply the mobility functions to static 3D models. Specifically, the learned part mobility model can predict mobilities for parts of a 3D object given in the form of a single static snapshot reflecting the spatial configuration of the object parts in 3D space, and transfer the mobility from relevant units in the training data … </p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/hu_siga17_icon3.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://vcc.szu.edu.cn/research/2017/ICON3/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zou_siga17_group.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning to Group Discrete Graphical Patterns</pubtit>
      <p><em>Zhaoliang Lun, Changqing Zou (joint first author), Haibin Huang, Evangelos Kalogerakis, Ping Tan, Marie-Paule Cani, and Hao Zhang</em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH Asia) (2017)</em></p>

      <p>We introduce a deep learning approach for grouping discrete patterns common in graphical designs. Our approach is based on a convolutional neural network architecture that learns a grouping measure defined over a pair of pattern elements. Motivated by perceptual grouping principles, the key feature of our network is the encoding of element shape, context, symmetries, and structural arrangements. These element properties are all jointly considered and appropriately weighted in our grouping measure … </p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zou_siga17_group.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://people.cs.umass.edu/~zlun/papers/PatternGrouping/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yi_arx17_dualgan.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>DualGAN:&#58; Unsupervised Dual Learning for Image-to-Image Translation</pubtit>
      <p><em>Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong</em><br />
<em>In Proc. of ICCV (2017)</em></p>

      <p>Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/yi_iccv17_dualGAN.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wr_cadg17_exq.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>ExquiMo:&#58; An Exquisite Corpse Tool for Co-creative 3D Shape Modeling</pubtit>
      <p><em>Warunika Ranaweera, Parmit Chilana, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In International Conference on Computer-Aided Design and Computer Graphics (CAD/Graphics) (2017)</em></p>

      <p>We introduce a shape modeling tool, ExquiMo, which is guided by the idea of improving the creativity of 3D shape designs through collaboration. Inspired by the game of Exquisite Corpse, our tool allocates distinct parts of a shape to multiple players who model the assigned parts in a sequence. Our approach is motivated by the understanding that effective surprise leads to creative outcomes. Hence, to maintain the surprise factor of the output, we conceal the previously modeled parts from the most recent player. Part designs from individual players are fused together to produce an often unexpected, hence creative, end result …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/rana_cg17_exquimo.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/time slice.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Time Slice Video Synthesis by Robust Video Alignment</pubtit>
      <p><em>Cui Z, Wang O, Tan P, Wang J. </em><br />
<em>In ACM Transactions on Graphics (SIGGRAPH 2017) (2017)</em></p>

      <p>We propose an easy-to-use and robust system for creating time slice videos from a wide variety of consumer videos. The main technical challenge we address is how to align videos taken at different times with substantially different appearances, in the presence of moving objects and moving cameras with slightly different trajectories.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://zhpcui.github.io/projects/timeslice/timeslice-authorversion.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://gruvi.cs.sfu.ca/project/timeslice/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/deformation driven correspondence.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Deformation-Driven Shape Correspondence via Shape Recognition</pubtit>
      <p><em>Zhu C, Yi R, Lira W, Alhashim I, Xu K, Zhang H. </em><br />
<em>In ACM Transactions on Graphics (SIGGRAPH 2017) (2017)</em></p>

      <p>Many approaches to shape comparison and recognition start by establishing a shape correspondence. We “turn the table” and show that quality shape correspondences can be obtained by performing many shape recognition tasks.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zhu_sig17_scsr_small.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/colocating style.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Co-Locating Style-Defining Elements on 3D Shapes</pubtit>
      <p><em>Hu R, Li W, van Kaick O, Huang H, Averkiou M, Cohen-Or D, Zhang H. </em><br />
<em>In ACM Transactions on Graphics (2017)</em></p>

      <p>We introduce a method for co-locating style-defining elements over a set of 3D shapes. Our goal is to translate high-level style descriptions, such as “Ming” or “European” for furniture models, into explicit and localized regions over the geometric models that characterize each style. For each style, the set of style-defining elements is defined as the union of all the elements that are able to discriminate the style. Another property of the style-defining elements is that they are frequently-occurring, reflecting shape characteristics that appear across multiple shapes of the same style.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/style_r.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/grass.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>GRASS&#58; Generative Recursive Autoencoders for Shape Structures</pubtit>
      <p><em>Li J, Xu K, Chaudhuri S, Yumer E, Zhang H, Guibas L. </em><br />
<em>In ACM Transactions on Graphics (SIGGRAPH 2017) (2017)</em></p>

      <p>We introduce a novel neural network architecture for encoding and synthesis of 3D shapes, particularly their structures. Our key insight is that 3D shapes are effectively characterized by their hierarchical organization of parts, which reflects fundamental intra-shape relationships such as adjacency and symmetry. We develop a recursive neural net (RvNN) based autoencoder to map a flat, unlabeled, arbitrary part layout to a compact code.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://kevinkaixu.net/papers/li_sig17_grass.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://kevinkaixu.net/projects/grass.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/polarimetric.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Polarimetric Multi-View Stereo</pubtit>
      <p><em>Cui Z, Gu J, Shi B, Tan P, Kautz J. </em><br />
<em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (2017)</em></p>

      <p>We propose polarimetric multi-view stereo, which combines per-pixel photometric information from polarization with epipolar constraints from multiple views for 3D reconstruction. Polarization reveals surface normal information, and is thus helpful to propagate depth to featureless regions. Polarimetric multi-view stereo is completely passive and can be applied outdoors in uncontrolled illumination, since the data capture can be done simply with either a polarizer or a polarization camera.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.gujinwei.org/pdfs/pmvs.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2016">2016</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/ma_siga16_action.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Action-Driven 3D Indoor Scene Evolution</pubtit>
      <p><em>Rui Ma, Honghua Li, Changqing Zou, Zicheng Liao, Xin Tong, and Hao Zhang</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 35, No. 6, Article 1736 (2016)</em></p>

      <p>We introduce a framework for action-driven evolution of 3D indoor scenes, where the goal is to simulate how scenes are altered by human actions, and specifically, by object placements necessitated by the actions. To this end, we develop an action model with each type of action combining information about one or more human poses, one or more object categories, and spatial configurations of object-object and object-human relations for the action. Importantly, all these pieces of information are learned from annotated photos.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/ma_siga16_action.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://maruitx.github.io/project/adise/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/ebLSRby6c_4" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_siga16tb.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Model-driven Sketch Reconstruction with Structure-oriented Retrieval</pubtit>
      <p><em>Lei Li, Zhe Huang, Changqing Zou, Chiew-Lan Tai, Rynson W.H. Lau, Hao Zhang, Ping Tan, and Hongbo Fu</em><br />
<em>In SIGGRAPH Asia Technical Brief (2016)</em></p>

      <p>We propose an interactive system that aims at lifting a 2D sketch into a 3D sketch with the help of existing models in shape collections. The key idea is to exploit part structure for shape retrieval and sketch reconstruction. We adopt sketch-based shape retrieval and develop a novel matching algorithm which considers structure in addition to traditional shape features.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/li_siga16tb_sketch.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zeinab_sgp16_scene.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning 3D Scene Synthesis from Annotated RGB-D Images</pubtit>
      <p><em>Zeinab Sadeghipour, Zicheng Liao, Ping Tan, and Hao Zhang</em><br />
<em>In Computer Graphics Forum (Special Issue of SGP), Vol. 35, No. 5, pp. 197-206 (2016)</em></p>

      <p>We present a data-driven method for synthesizing 3D indoor scenes by inserting objects progressively into an initial, possibly, empty scene. Instead of relying on few hundreds of hand-crafted 3D scenes, we take advantage of existing large-scale annotated RGB-D datasets, in particular, the SUN RGB-D database consisting of 10,000+ depth images of real scenes, to form the prior knowledge for our synthesis task. Our object insertion scheme follows a co-occurrence model and an arrangement model, both learned from the SUN dataset.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zeinab_sgp16_scene.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/hu_sig16_icon2.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning How Objects Function via Co-Analysis of Interactions</pubtit>
      <p><em>Ruizhen Hu, Oliver van Kaick, Bojian Wu, Hui Huang, Ariel Shamir, and Hao Zhang</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH), Vol. 35, No. 4, Article 47 (2016)</em></p>

      <p>We introduce a co-analysis method which learns a functionality model for an object category, e.g., strollers or backpacks. Like previous works on functionality, we analyze object-to-object interactions and intra-object properties and relations. Differently from previous works, our model goes beyond providing a functionalityoriented descriptor for a single object; it prototypes the functionality of a category of 3D objects by co-analyzing typical interactions involving objects from the category.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/hu_sig16_icon2.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://vcc.szu.edu.cn/research/2016/Icon2/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zou_sig16_calli.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Legible Compact Calligrams</pubtit>
      <p><em>Changqing Zou, Junjie Cao, Warunika Ranaweera, Ibraheem Alhashim, Ping Tan, Alla Sheffer, and Hao Zhang</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH), Vol. 35, No. 4, Article 122 (2016)</em></p>

      <p>A calligram is an arrangement of words or letters that creates a visual image, and a compact calligram fits one word into a 2D shape. We introduce a fully automatic method for the generation of legible compact calligrams which provides a balance between conveying the input shape, legibility, and aesthetics.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zou_sig16_calli.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhao_sig16_fermat.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Connected Fermat Spirals for Layered Fabrication</pubtit>
      <p><em>Haisen Zhao, Fanglin Gu, Qi-Xing Huang, Jorge Garcia, Yong Chen, Changhe Tu, Bedrich Benes, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH), Vol. 35, No. 4, Article 100 (2016)</em></p>

      <p>We develop a new kind of “space-filling” curves, connected Fermat spirals, and show their compelling properties as a tool path fill pattern for layered fabrication. Unlike classical space-filling curves such as the Peano or Hilbert curves, which constantly wind and bind to preserve locality, connected Fermat spirals are formed mostly by long, low-curvature paths. This geometric property, along with continuity, influences the quality and efficiency of layered fabrication.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zhao_sig16_fermat.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wan_tvc16_sparse.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Full and Partial Shape Similarity through Sparse Descriptor Reconstruction</pubtit>
      <p><em>Lili Wan, Changqing Zou, and Hao Zhang</em><br />
<em>In The Visual Computer (2016)</em></p>

      <p>We introduce a novel approach to measure similarity between two 3D shapes based on sparse reconstruction of shape descriptors. The main feature of our approach is its applicability to handle incomplete shapes. We characterize the shapes by learning a sparse dictionary from their local descriptors. The similarity between two shapes A and B is defined by the error incurred when reconstructing B’s descriptor set using the basis signals from Aâ€™s dictionary.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/wan_tvc16_sparse.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhang_tvc16_creative.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>From inspired modeling to creative modeling</pubtit>
      <p><em>Daniel Cohen-Or and Hao Zhang</em><br />
<em>In Visual Computer (invited paper), Vol. 32, No. 1 (2016)</em></p>

      <p>An intriguing and reoccurring question in many branches of computer science is whether machines can be creative, like humans. In this exploratory paper, we examine the problem from a computer graphics, and more specifically, geometric modeling, perspective. We focus our discussions on the weaker but still intriguing question: “Can machines assist or inspire humans in a creative endeavor for the generation of geometric forms?”</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zhang_tvc16_creative.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_16_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>MeshFlow&#58; Minimum Latency Online Video Stabilization</pubtit>
      <p><em>Shuaicheng Liu, Ping Tan, Lu Yuan, Jian Sun, Bing Zeng</em><br />
<em>In European Conference on Computer Vision (ECCV) (2016)</em></p>

      <p>Many existing video stabilization methods often stabilize videos off-line, i.e. as a postprocessing tool of pre-recorded videos. Some methods can stabilize videos online, but either require additional hardware sensors (e.g., gyroscope) or adopt a single parametric motion model (e.g., affine, homography) which is problematic to represent spatiallyvariant motions. In this paper, we propose a technique for online video stabilization with only one frame latency using a novel MeshFlow motion model. The MeshFlow is a spatial smooth sparse motion field with motion vectors only at the mesh vertexes.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.liushuaicheng.org/eccv2016/meshflow.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/lin_16_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>RepMatch&#58; Robust Feature Matching and Pose for Reconstructing Modern Cities</pubtit>
      <p><em>Wen-Yan Lin, Siying Liu, Nianjuan Jiang, Minh. N. Do, Ping Tan, Jianbo Lu</em><br />
<em>In European Conference on Computer Vision (ECCV) (2016)</em></p>

      <p>A perennial problem in recovering 3-D models from images is repeated structures common in modern cities. The problem can be traced to the feature matcher which needs to match less distinctive features (permitting wide-baselines and avoiding broken sequences), while simultaneously avoiding incorrect matching of ambiguous repeated features. To meet this need, we develop RepMatch.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.kind-of-works.com/papers/eccv_2016_repmatch.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>











 | 

<a target="_blank" href="https://gruvi.cs.sfu.ca/presentations/repmatch.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_ppt.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Slides</a>

]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/luwei_16_bmvc.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Attribute Recognition from Adaptive Parts</pubtit>
      <p><em>Luwei Yang, Ligeng Zhu, Yichen Wei, Shuang Liang, Ping Tan</em><br />
<em>In British Machine Vision Conference (BMVC) (2016)</em></p>

      <p>Previous part-based attribute recognition approaches perform part detection and attribute recognition in separate steps. The parts are not optimized for attribute recognitionand therefore could be sub-optimal. We present an end-to-end deep learning approach to overcome the limitation.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.bmva.org/bmvc/2016/papers/paper081/paper081.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zou_16_prletters.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>An example-based approach to 3D man-made object reconstruction from line drawings</pubtit>
      <p><em>Changqing Zou, Tianfan Xue, Xiaojiang Peng, Honghua Li, Baochang Zhang, Ping Tan, Jianzhuang Liu</em><br />
<em>In Pattern Recognition,Vol. 60, pp. 543–553 (2016)</em></p>

      <p>We propose an example-based approach for 3D man-made object reconstruction from single line drawings. Our approach can handle a wide range of 3D man-made objects including curved components.Comprehensive experiments show that the proposed approach outperforms previous work, especially for line drawings containing large degree of sketch errors.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.sciencedirect.com/science/article/pii/S0031320316301170">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/boxinshi_16_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Benchmark Dataset and Evaluation for Non-Lambertian and Uncalibrated Photometric Stereo</pubtit>
      <p><em>Boxin Shi, Zhe Wu, Zhipeng Mo, Dinglong Duan, Sai-Kit Yeung, Ping Tan</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2016)</em></p>

      <p>In this paper, we first survey and categorize existing methods using a photometric stereo taxonomy emphasizing on non-Lambertian and uncalibrated methods. We then introduce the ‘DiLiGenT’ photometric stereo image dataset with calibrated Directional Lightings, objects of General reflectance, and ‘ground Truth’ shapes (normals).</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_A_Benchmark_Dataset_CVPR_2016_paper.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/joyce_16_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Automatic Fence Segmentation in Videos of Dynamic Scenes</pubtit>
      <p><em>Renjiao Yi, Jue Wang, Ping Tan</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2016)</em></p>

      <p>We present a fully automatic approach to detect and segment fence-like occluders from a video clip. Unlike previous approaches that usually assume either static scenes or cameras, our method is capable of handling both dynamic scenes and moving cameras. Under a bottom-up framework, it first clusters pixels into coherent groups using color and motion features. These pixel groups are then analyzed in a fully connected graph, and labeled as either fence or non-fence using graph-cut optimization. Finally, we solve a dense Conditional Random Filed (CRF) constructed from multiple frames to enhance both spatial accuracy and temporal coherence of the segmentation</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.juew.org/publication/CVPR16-FenceSeg.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2015">2015</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/dd3dcorrespondence.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Deformation-Driven Topology-Varying 3D Shape Correspondence</pubtit>
      <p><em>Alhashim I, Xu K, Zhuang Y, Cao J, Simari P, Zhang H. </em><br />
<em>In ACM Transactions on Graphics (Proc. of SIGGRAPH Asia) (2015)</em></p>

      <p>We present a deformation-driven approach to topology-varying 3D shape correspondence. In this paradigm, the best correspondence between two shapes is the one that results in a minimal-energy, possibly topology-varying, deformation that transforms one shape to conform to the other while respecting the correspondence. Our deformation model, called GeoTopo transform, allows both geometric and topological operations such as part split, duplication, and merging, leading to fine-grained and piecewise continuous correspondence results. The key ingredient of our correspondence scheme is a deformation energy that penalizes geometric distortion, encourages structure preservation, and simultaneously allows topology changes. This is accomplished by connecting shape parts using structural rods, which behave similarly to virtual springs but simultaneously allow the encoding of energies arising from geometric, structural, and topological shape variations. Driven by the combined deformation energy, an optimal shape correspondence is obtained via a pruned beam search. We demonstrate our deformation-driven correspondence scheme on extensive sets of man-made models with rich geometric and topological variation and compare the results to state-of-the-art approaches.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://gruvi.cs.sfu.ca/project/geotopo/files/geotopo_SIGA2015_preprint_small.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://gruvi.cs.sfu.ca/project/geotopo/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/VqW2tWT5jEA" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/garment.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Garment Modeling with a Depth Camera</pubtit>
      <p><em>Chen X, Zhou B, Lu F, Wang L, Bi L, Tan P. </em><br />
<em>In ACM Transactions on Graphics (Proc. of SIGGRAPH Asia) (2015)</em></p>

      <p>We study the modeling of real garments and develop a system that is intuitive to use even for novice users. Our system includes garment component detectors and design attribute classifiers learned from a manually labeled garment image database. In the modeling time, we scan the garment with a Kinect and build a rough shape by KinectFusion from the raw RGBD sequence. The detectors and classifiers will identify garment components (e.g. collar, sleeve, pockets, belt, and buttons) and their design attributes (e.g. falbala collar or lapel collar, hubble-bubble sleeve or straight sleeve) from the RGB images. Our system also contains a 3D deformable template database for garment components. Once the components and their designs are determined, we choose appropriate templates, stitch them together, and fit them to the initial garment mesh generated by KinectFusion. Experiments on various different garment styles consistently generate high quality results.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://arts.buaa.edu.cn/papers/SA15-GarmentModeling-AuthorCopy.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/dapper.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Dapper&#58; Decompose-and-Pack for 3D Printing</pubtit>
      <p><em>Chen X, Zhang H, Lin J, Hu R, Lu L, Huang Q, Benes B, Cohen-Or D, Chen B. </em><br />
<em>In ACM Transactions on Graphics (Proc. of SIGGRAPH Asia) (2015)</em></p>

      <p>We pose the decompose-and-pack or DAP problem, which tightly combines shape decomposition and packing. While in general, DAP seeks to decompose an input shape into a small number of parts which can be efficiently packed, our focus is geared towards 3D printing. The goal is to optimally decompose-and-pack a 3D object into a printing volume to minimize support material, build time, and assembly cost. We present Dapper, a global optimization algorithm for the DAP problem which can be applied to both powder and FDM-based 3D printing. The solution search is top-down and iterative. Starting with a coarse decomposition of the input shape into few initial parts, we progressively pack a pile in the printing volume, by iteratively docking parts, possibly while introducing cuts, onto the pile. Exploration of the search space is via a prioritized and bounded beam search, with breadth and depth pruning guided by local and global DAP objectives.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/chen_siga15_dap.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cylinder.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Generalized Cylinder Decomposition</pubtit>
      <p><em>Zhou Y, Yin K, Huang H, Zhang H, Gong M, Cohen-Or D. </em><br />
<em>In ACM Transactions on Graphics (Proc. of SIGGRAPH Asia) (2015)</em></p>

      <p>Decomposing a complex shape into geometrically simple primitives is a fundamental problem in geometry processing. We are interested in a shape decomposition problem where the simple primitives sought are generalized cylinders, which are ubiquitous in both organic forms and man-made artifacts. We introduce a quantitative measure of cylindricity for a shape part and develop a cylindricity-driven optimization algorithm, with a global objective function, for generalized cylinder decomposition. As a measure of geometric simplicity and following the minimum description length principle, cylindricity is defined as the cost of representing a cylinder through skeletal and cross-section profile curves. Our decomposition algorithm progressively builds local to non-local cylinders, which form over-complete covers of the input shape. The over-completeness of the cylinder covers ensures a conservative buildup of the cylindrical parts, leaving the final decision on decomposition to global optimization. We solve the global optimization by finding an exact cover, which optimizes the global objective function.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.math.tau.ac.il/~dcor/articles/2015/generalized.cylinder.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









 | 

<a href="https://www.youtube.com/watch?v=eqvJeJ0ujMM" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/foldabilizing.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Foldabilizing Furniture</pubtit>
      <p><em>Li H, Hu R, Alhashim I, Zhang H. </em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH) (2015)</em></p>

      <p>We introduce the foldabilization problem for space-saving furniture design. Namely, given a 3D object representing a piece of furniture, our goal is to apply a minimum amount of modification to the object so that it can be folded to save space — the object is thus foldabilized. We focus on one instance of the problem where folding is with respect to a prescribed folding direction and allowed object modifications include hinge insertion and part shrinking.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://honghuali.github.io/projects/foldem/foldem_small.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://honghuali.github.io/projects/foldem/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/95dtci3g08s" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/icon.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Interaction Context (ICON)&#58; Towards a Geometric Functionality Descriptor</pubtit>
      <p><em>Hu R, Zhu C, van Kaick O, Liu L, Shamir A, Zhang H. </em><br />
<em>In ACM Transactions on Graphics (Special Issue of SIGGRAPH) (2015)</em></p>

      <p>We introduce a contextual descriptor which aims to provide a geometric description of the functionality of a 3D object in the context of a given scene. Differently from previous works, we do not regard functionality as an abstract label or represent it implicitly through an agent. Our descriptor, called interaction context or ICON for short, explicitly represents the geometry of object-to-object interactions. Our approach to object functionality analysis is based on the key premise that functionality should mainly be derived from interactions between objects and not objects in isolation. Specifically, ICON collects geometric and structural features to encode interactions between a central object in a 3D scene and its surrounding objects. These interactions are then grouped based on feature similarity, leading to a hierarchical structure.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://gigl.scs.carleton.ca/sites/default/files/oliver_van_kaick/icon.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://gigl.scs.carleton.ca/node/864">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://vimeo.com/148262152" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/defogging.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Simultaneous Video Defogging and Stereo Reconstruction (oral presentation)</pubtit>
      <p><em>Li Z, Tan P, Tan RT, Zou D, Zhou SZhiying, Cheong L-F. </em><br />
<em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)</em></p>

      <p>In our formulation, the depth cues from stereo matching and fog information reinforce each other, and produce superior results than conventional stereo or defogging algorithms. We first improve the photo-consistency term to explicitly model the appearance change due to the scattering effects. The prior matting Laplacian constraint on fog transmission imposes a detail preserving smoothness constraint on the scene depth. We further enforce the ordering consistency between scene depth and fog transmission at neighboring points.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_Simultaneous_Video_Defogging_2015_CVPR_paper.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









 | 

<a href="https://www.youtube.com/watch?v=uI_E8M86zS4" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/compaction.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Shape Compaction. New Perspectives in Shape</pubtit>
      <p><em>Li H, Zhang H. editors:: M. Breu, A. Bruckstein, P. Maragos, and S. Wuhrer, </em><br />
<em>In  (2015)</em></p>

      <p>We cover techniques designed for compaction of shape representations or shape configurations. The goal of compaction is to reduce storage space, a fundamental problem in many application domains.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/papers.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/distilled.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Distilled Collections from Textual Image Queries</pubtit>
      <p><em>Averbuch-Elor H, Wang Y, Qian Y, Gong M, Kopf J, Zhang H, Cohen-Or D. </em><br />
<em>In Computer Graphics Forum (Special Issue of Eurographics 2015) (2015)</em></p>

      <p>We present a distillation algorithm which operates on a large, unstructured, and noisy collection of internet images returned from an online object query. We introduce the notion of a distilled set, which is a clean, coherent, and structured subset of inlier images. In addition, the object of interest is properly segmented out throughout the distilled set. Our approach is unsupervised, built on a novel clustering scheme, and solves the distillation and object segmentation problems simultaneously.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/elor_eg15_distill.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/skeletonin.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Skeleton-Intrinsic Symmetrization of Shapes</pubtit>
      <p><em>Zheng Q, Hao Z, Huang H, Xu K, Zhang H, Cohen-Or D, Chen B. </em><br />
<em>In Computer Graphics Forum (Special Issue of Eurographics 2015) (2015)</em></p>

      <p>Enhancing the self-symmetry of a shape is of fundamental aesthetic virtue. In this paper, we are interested in recov- ering the aesthetics of intrinsic reflection symmetries, where an asymmetric shape is symmetrized while keeping its general pose and perceived dynamics. The key challenge to intrinsic symmetrization is that the input shape has only approximate reflection symmetries, possibly far from perfect. The main premise of our work is that curve skeletons provide a concise and effective shape abstraction for analyzing approximate intrinsic symmetries as well as symmetrization.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zheng_eg15_intsym.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/indirect.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Indirect Shape Analysis for 3D Shape Retrieval</pubtit>
      <p><em>Liu Z., Xie C., Bu S., Wang X., Zhang H.. </em><br />
<em>In Computer &amp; Graphics (Special Issue of SMI 2014) (2015)</em></p>

      <p>We introduce indirect shape analysis, or ISA, where a given shape is analyzed not based on geometric or topological features computed directly from the shape itself, but by studying how external agents interact with the shape. The potential benefits of ISA are two-fold. First, agent-object interactions often reveal an objectâ€™s function, which plays a key role in shape understanding. Second, compared to direct shape analysis, ISA, which utilizes pre-selected agents, is less affected by imperfections of, or inconsistencies between, the geometry or topology of the analyzed shapes.</p>

      <p style="text-align: right;">

[









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/linearglobal.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Linear Global Translation Estimation with Feature Tracks</pubtit>
      <p><em>Cui Z, Jiang N, Tang C, Tan P. </em><br />
<em>In British Machine Vision Conference (BMVC) (2015)</em></p>

      <p>This paper derives a novel linear position constraint for cameras seeing a common scene point, which leads to a direct linear method for global camera translation estimation. Unlike previous solutions, this method deals with collinear camera motion and weak image association at the same time. The final linear formulation does not involve the coordinates of scene points, which makes it efficient even for large scale data. We solve the linear equation based on $L_1$ norm, which makes our system more robust to outliers in essential matrices and feature correspondences. We experiment this method on both sequentially captured images and unordered Internet images. The experiments demonstrate its strength in robustness, accuracy, and efficiency.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.bmva.org/bmvc/2015/papers/paper046/paper046.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.bmva.org/bmvc/2015/papers/paper046/index.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/globalmotion.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Global Structure-from-Motion by Similarity Averaging</pubtit>
      <p><em>Cui Z, Tan P. </em><br />
<em>In IEEE International Conference on Computer Vision (ICCV) (2015)</em></p>

      <p>Global structure-from-motion (SfM) methods solve all cameras simultaneously from all available relative motions. It has better potential in both reconstruction accuracy and computation efficiency than incremental methods. However, global SfM is challenging, mainly because of two reasons. Firstly, translation averaging is difficult, since an essential matrix only tells the direction of relative translation. Secondly, it is also hard to filter out bad essential matrices due to feature matching failures. We propose to compute a sparse depth image at each camera to solve both problems. Depth images help to upgrade an essential matrix to a similarity transformation, which can determine the scale of relative translation. Thus, camera registration is formulated as a well-posed similarity averaging problem. Depth images also make the filtering of essential matrices simple and effective. In this way, translation averaging can be solved robustly in two convex L1 optimization problems, which reach the global optimum rapidly. We demonstrate this method in various examples including sequential data, Internet data, and ambiguous data with repetitive scene structures.</p>

      <p style="text-align: right;">

[









<a href="https://www.youtube.com/watch?v=UzXX9kbSrqg" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<h3 id="2014">2014</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/hu_siga14_pym_long.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Approximate Pyramidal Shape Decomposition</pubtit>
      <p><em>Ruizhen Hu, Honghua Li, Hao Zhang, and Daniel Cohen-Or</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 33, No. 6, Article 213 (2014)</em></p>

      <p>A shape is pyramidal if it has a flat base with the remaining boundary forming a height function over the base. Pyramidal shapes are optimal for molding, casting, and layered 3D printing. We introduce an algorithm for approximate pyramidal shape decomposition. The general exact pyramidal decomposition problem is NP-hard. We turn this problem into an NP-complete Exact Cover Problem which admits a practical solution … Our solution is equally applicable to 2D or 3D shapes, to shapes with polygonal or smooth boundaries, with or without holes …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/hu_siga14_pym.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/yin_siga14_morfit.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Morfit&#58; Interactive Surface Reconstruction from Incomplete Point Clouds with Curve-Driven Topology and Geometry Control</pubtit>
      <p><em>Kangxue Yin, Hui Huang, Hao Zhang, Minglun Gong, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 33, No. 6, Article 202 (2014)</em></p>

      <p>We present an interactive technique for surface reconstruction from incomplete and sparse scans of 3D objects possessing sharp features … We factor 3D editing by the user into two “orthogonal” interactions acting on skeletal and profile curves of the underlying shape, controlling its topology and geometric features, respectively. For surface completion, we introduce a novel skeleton-driven morph-to-fit, or morfit, scheme which reconstructs the shape as an ensemble of generalized cylinders. Morfit is a hybrid operator which optimally interpolates between adjacent curve profiles (the “morph”) and snaps the surface to input points (the “fit”) …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/yin_siga14_morfit.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://vcc.szu.edu.cn/research/2014/Morfit/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/alhashim_sig14_long.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Topology-Varying 3D Shape Creation via Structural Blending</pubtit>
      <p><em>Ibraheem Alhashim, Honghua Li, Kai Xu, Junjie Cao, Rui Ma, and Hao Zhang</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH), Vol. 33, No. 4, Article 158 (2014)</em></p>

      <p>We introduce an algorithm for generating novel 3D models via topology-varying shape blending. Given a source and a target shape, our method blends them topologically and geometrically, producing continuous series of in-betweens as new shape creations. The blending operations are defined on a spatio-structural graph composed of medial curves and sheets. Such a shape abstraction is structure-oriented, part-aware, and facilitates topology manipulations. Fundamental topological operations including split and merge are realized by allowing one-to-many correspondences between the source and the target ..</p>

      <p style="text-align: right;">

[







<a target="_blank" href="http://gruvi.cs.sfu.ca/project/topo/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/Xc4qf7v6a-w" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>





 | 

<a target="_blank" href="https://gruvi.cs.sfu.ca/presentations/topo_SIG2014_slides.pptx">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_ppt.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Slides</a>

]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xu_sig14_long.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Organizing Heterogeneous Scene Collections through Contextual Focal Points</pubtit>
      <p><em>Kai Xu, Rui Ma, Hao Zhang, Chenyang Zhu, Ariel Shamir, Daniel Cohen-Or, and Hui Huang</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH), Vol. 33, No. 4, Article 35 (2014)</em></p>

      <p>We introduce focal points for characterizing, comparing, and organizing collections of complex and heterogeneous data and apply the concepts and algorithms developed to collections of 3D indoor scenes. We represent each scene by a graph of its constituent objects and define focal points as representative substructures in a scene collection. To organize a heterogenous scene collection, we cluster the scenes based on a set of extracted focal points: scenes in a cluster are closely connected when viewed from the perspective of the representative focal points of that cluster … The problem of focal point extraction is intermixed with the problem of clustering groups of scenes based on their representative focal points. We present a co-analysis algorithm …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://kevinkaixu.net/papers/xu_sig14_focal.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://kevinkaixu.net/projects/focal.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>







 | 

<a target="_blank" href="https://gruvi.cs.sfu.ca/presentations/xu_sig14_focal.pptx">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_ppt.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Slides</a>

]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zou_cvpr14.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Sparse Dictionary Learning for Edit Propagation of High-resolution Images</pubtit>
      <p><em>Xiaowu Chen, Dongqing Zou, Jianwei Li, Xiaochun Cao, Qinping Zhao, and Hao Zhang</em><br />
<em>In Proceedings of IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2014)</em></p>

      <p>We introduce the use of sparse representation for edit propagation of high-resolution images or video. Previous approaches for edit propagation typically employ a global optimization over the whole set of image pixels, incurring a prohibitively high memory and time consumption for high-resolution images. Rather than propagating an edit pixel by pixel, we follow the principle of sparse representation to obtain a compact set of representative samples (or features) and perform edit propagation on the samples instead …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zou_cvpr14_sparse.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wang_gi14.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Spectral Global Intrinsic Symmetry Invariant Functions</pubtit>
      <p><em>Hui Wang, Patricio Simari, Zhixun Su, and Hao Zhang</em><br />
<em>In Proc. of Graphics Interface (2014)</em></p>

      <p>We introduce spectral Global Intrinsic Symmetry Invariant Functions (GISIFs), a class of GISIFs obtained via eigendecomposition of the Laplace-Beltrami operator on compact Riemannian manifolds. We discretize the spectral GISIFs for 2D manifolds approximated either by triangle meshes or point clouds. In contrast to GISIFs obtained from geodesic distances, our spectral GISIFs are robust to local topological changes. Additionally, for symmetry analysis our spectral GISIFs can be viewed as generalizations of the classical Heat (HKSs) and Wave Kernel Signatures (WKSs), and, as such, represent a more expressive and versatile class of functions …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/wang_gi14.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://huiw.weebly.com/spectralgisif.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>







 | 

<a target="_blank" href="https://gruvi.cs.sfu.ca/presentations/gi_2014_spectral_gisifs.pptx">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_ppt.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" /> 
Slides</a>

]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/boxinshi_14_3dv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Photometric Stereo using Internet Images</pubtit>
      <p><em>Boxin Shi, Kenji Inose, Yasuyuki Matsushita, Ping Tan, Sai-Kit Yeung, Katsushi Ikeuchi</em><br />
<em>In International Conference on 3D Vision (3DV) (2014)</em></p>

      <p>Photometric stereo using unorganized Internet images is very challenging, because the input images are captured under unknown general illuminations, with uncontrolled cameras. We propose to solve this difficult problem by a simple yet effective approach that makes use of a coarse shape prior. The shape prior…</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/3DV14.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://alumni.media.mit.edu/~shiboxin/project_pages/Shi_3DV14_Web.htm">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/5TMJ26BZGqI" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_14_siga.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>TrackCam&#58; 3D-aware Tracking Shots from Consumer Video</pubtit>
      <p><em>Shuaicheng Liu, Jue Wang, Sunghyun Cho, Ping Tan</em><br />
<em>In ACM Transaction on Graphics(TOG) and Proc. of SIGGRAPH Asia (2014)</em></p>

      <p>In this work we propose a system to generate realistic, 3D-aware tracking shots from consumer videos. We show how computer vision techniques such as segmentation and structure-from-motion can be used to lower the barrier and help novice users create high quality tracking shots that are physically plausible</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/3DV14.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wang_14_tip.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Video Tonal Stabilization via Color States Smoothing</pubtit>
      <p><em>Yinting Wang, Dacheng Tao, Xiang Li, Mingli Song, Jiajun Bu, Ping Tan</em><br />
<em>In IEEE Transaction on Image Processing (TIP), Vol 23, No. 11, pp. 4838–4849 (2014)</em></p>

      <p>We address the problem of removing video color tone jitter that is common in amateur videos recorded with handheld devices. To achieve this, we introduce color state to represent the exposure and white balance state of a frame</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/tip14.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhang_14_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>PanoContext&#58; A Whole-room 3D Context Model for Panoramic Scene Understanding</pubtit>
      <p><em>Yinda Zhang, Shuran Song, Ping Tan, Jianxiong Xiao</em><br />
<em>In European Conference on Computer Vision (ECCV) (2014)</em></p>

      <p>We address the problem of removing video color tone jitter that is common in amateur videos recorded with handheld devices. To achieve this, we introduce color state to represent the exposure and white balance state of a frame</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv14.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://panocontext.cs.princeton.edu/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/8I-v6gQXfD8" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/kunli_14_microairvehicle.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Off-board Visual Odometry and Control of an Ultralight Quadrotor MAV</pubtit>
      <p><em>Kun Li, Rui Huang, Swee King Phang, Shupeng Lai, Fei Wang, Ping Tan, Ben M. Chen, Tong Heng Lee</em><br />
<em>In International Micro Air Vehicle Conference and Competition (2014)</em></p>

      <p>We propose an approach to autonomously control a quadrotor micro aerial vehicle (MAV). With take-off weight of 50 g and 8-min flight endurance, the MAV platform codenamed ‘KayLion’ developed by the National University of Singapore (NUS) is able to perform autonomous flight with pre-planned path tracking</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/imav14.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_14_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SteadyFlow&#58; Spatially Smooth Optical Flow for Video Stabilization</pubtit>
      <p><em>Shuaicheng Liu, Lu Yuan, Ping Tan, Jian Sun</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2014)</em></p>

      <p>We propose a novel motion model, SteadyFlow, to represent the motion between neighboring video frames for stabilization. A SteadyFlow is a specific optical flow by enforcing strong spatial coherence, such that smoothing feature trajectories can be replaced by smoothing pixel profiles….</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/cvpr14_stabilization.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.liushuaicheng.org/CVPR2014/index.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://vimeo.com/156073058" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<h3 id="2013">2013</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/projectiveanalysis.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Projective Analysis for 3D Shape Segmentation</pubtit>
      <p><em>Wang Y, Gong M, Wang T, Cohen-Or D, Zhang H, Chen B. </em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia) (2013)</em></p>

      <p>We introduce projective analysis for semantic segmentation and labeling of 3D shapes. The analysis treats an input 3D shape as a collection of 2D projections, labels each projection by transferring knowledge from existing labeled images, and back-projects and fuses the labelings on the 3D shape. The image-space analysis involves matching projected binary images of 3D objects based on a novel bi-class Hausdorff distance.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/wang_siga13_psa.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/facadessymmetry.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Layered Analysis of Irregular Facades via Symmetry Maximization</pubtit>
      <p><em>Zhang H, Xu K, Jiang W, Lin J, Cohen-Or D, Chen B. </em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH) (2013)</em></p>

      <p>We present an algorithm for hierarchical and layered analysis of irregular facades, seeking a high-level understanding of facade structures. By introducing layering into the analysis, we no longer view a facade as a flat structure, but allow it to be structurally separated into depth layers, enabling more compact and natural interpretations of building facades.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/zhang_sig13_symax.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cohierarchical.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Co-Hierarchical Analysis of Shape Structures</pubtit>
      <p><em>van Kaick O, Xu K, Zhang H, Wang Y, Sun S, Shamir A, Cohen-Or D. </em><br />
<em>In ACM Trans. on Graphics (Proc. SIGGRAPH) (2013)</em></p>

      <p>We introduce an unsupervised co-hierarchical analysis of a set of shapes, aimed at discovering their hierarchical part structures and revealing relations between geometrically dissimilar yet functionally equivalent shape parts across the set. The core problem is that of representative co-selection. For each shape in the set, one representative hierarchy (tree) is selected from among many possible interpretations of the hierarchical structure of the shape. Collectively, the selected tree representatives maximize the within-cluster structural similarity among them. We develop an iterative algorithm for representative co-selection. At each step, a novel cluster-and-select scheme is applied to a set of candidate trees for all the shapes. The tree-to-tree distance for clustering caters to structural shape analysis by focusing on spatial arrangement of shape parts, rather than their geometric details. The final set of representative trees are unified to form a structural co-hierarchy. We demonstrate co-hierarchical analysis on families of man-made shapes exhibiting high degrees of geometric and finer-scale structural variabilities.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://people.scs.carleton.ca/~olivervankaick/pubs/conshier.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://people.scs.carleton.ca/~olivervankaick/conshier/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/l1medial.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>L1-Medial Skeleton of Point Cloud</pubtit>
      <p><em>Huang H, Wu S, Cohen-Or D, Gong M, Zhang H, Li G, Chen B. </em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH) (2013)</em></p>

      <p>We introduce L1-medial skeleton as a curve skeleton representation for 3D point cloud data. The L1-median is well-known as a robust global center of an arbitrary set of points. We make the key observation that adapting L1-medians locally to a point set representing a 3D shape gives rise to a one-dimensional structure, which can be seen as a localized center of the shape.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/huang_sig13_l1skel.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/quartet.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Qualitative Organization of Collections of Shapes via Quartet Analysis</pubtit>
      <p><em>Huang S-S, Shamir A, Shen C-H, Zhang H, Sheffer A, Hu S-M, Cohen-Or D. </em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH) (2013)</em></p>

      <p>We present a method for organizing a heterogeneous collection of 3D shapes for overview and exploration. Instead of relying on quantitative distances, which may become unreliable between dissimilar shapes, we introduce a qualitative analysis which utilizes multiple distance measures but only in cases where the measures can be reliably compared. Our analysis is based on the notion of quartets , each defined by two pairs of shapes, where the shapes in each pair are close to each other, but far apart from the shapes in the other pair. Combining the information from many quartets computed across a shape collection using several distance measures, we create a hierarchical structure we call categorization tree of the shape collection. This tree satisfies the topological (qualitative) constraints imposed by the quartets creating an effective organi- zation of the shapes.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.tau.ac.il/~dcor/articles/2013/Qualitative-Organization.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









 | 

<a href="https://www.youtube.com/watch?v=ABQEAcZGe-I" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/bilateral.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Bilateral Maps for Partial Matching</pubtit>
      <p><em>van Kaick O, Zhang H, Hamarneh G. </em><br />
<em>In Computer Graphics Forum. to appear (2013)</em></p>

      <p>We introduce the bilateral map, a local shape descriptor whose region of interest is defined by two feature points. Compared to the classical descriptor definition using a single point, the bilateral approach exploits the use of a second point to place more constraints on the selection of the spatial context for feature analysis. This leads to a descriptor where the shape of the region of interest adapts to the context of the two points, making it more refined for shape matching. In particular, we show that our new descriptor is more effective for partial matching, since potentially extraneous regions of the models are selectively ignored owing to the adaptive nature of the bilateral map.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/vanKaick_cgf13_bimap.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/evaluationseg.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>New Evaluation Metrics for Mesh Segmentation</pubtit>
      <p><em>Liu Z, Tang S, Bu S, Zhang H. </em><br />
<em>In Computer &amp; Graphics (Special issue of SMI) (2013)</em></p>

      <p>We propose two novel metrics to support comparison with multiple ground-truth segmentations, which are named Similarity Hamming Distance (SHD) and Adaptive Entropy Increment (AEI). SHD is based on partial similarity correspondences between automatic segmentation and ground-truth segmentations, and AEI measures entropy change when an automatic segmentation is added to a set of different ground-truth segmentations. A group of experiments demonstrates that the metrics are able to provide relatively higher discriminative power and stability when evaluating different hierarchical segmentations, and also provide an effective evaluation more consistent with human perception.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/liu_smi13_metric.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/structaware.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Structure-Aware Shape Processing</pubtit>
      <p><em>Mitra N, Wand M, Zhang H, Cohen-Or D, Bokeloh M. </em><br />
<em>In Eurographics State-of-the-art Report (STAR) (2013)</em></p>

      <p>We organize, summarize, and present the key concepts and methodological approaches towards efficient structure-aware shape processing. We discuss common models of structure, their implementation in terms of mathematical formalism and algorithms, and explain the key principles in the context of a number of state-of-the-art approaches. Further, we attempt to list the key open problems and challenges, both at the technical and at the conceptual level, to make it easier for new researchers to better explore and contribute to this topic. Our goal is to both give the practitioner an overview of available structure-aware shape processing techniques, as well as identify future research questions in this important, emerging, and fascinating research area.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://vovakim.com/data//papers/13_SIGACourse_StructAware.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/curvestyleanalysis.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Curve Style Analysis in a Set of Shapes</pubtit>
      <p><em>Li H, Zhang H, Wang Y, Cao J, Shamir A, Cohen-Or D. </em><br />
<em>In Computer Graphics Forum. :toappear (2013)</em></p>

      <p>The word “style” can be interpreted in so many different ways in so many different contexts. To provide a general analysis and understanding of styles is a highly challenging problem. We pose the open question “how to extract styles from geometric shapes?” and address one instance of the problem. Specifically, we present an unsupervised algorithm for identifying curve styles in a set of shapes. In our setting, a curve style is explicitly represented by a mode of curve features appearing along the 2D silhouettes of the shapes in the set. Unlike previous attempts, we do not rely on any preconceived conceptual characterizations, e.g., via specific shape descriptors, to define what is or is not a style. Our definition of styles is data-dependent ; it depends on the input set but we do not require computing a shape correspondence across the set. We provide an operational definition of curve styles which focuses on separating curve features that represent styles from curve features that are content-revealing. To this end, we develop a novel formulation and associated algorithm for style-content separation. The analysis is based on a feature-shape association matrix (FSM) whose rows correspond to modes of curve features, columns to shapes in the set, and each entry expresses the extent a feature mode is present in a shape.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/li_cgf13_style.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/edgeawareresampling.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Edge-Aware Point Set Resampling</pubtit>
      <p><em>Huang H, Wu S, Gong M, Cohen-Or D, Ascher U, Zhang H. </em><br />
<em>In ACM Trans. on Graphics (2013)</em></p>

      <p>Points acquired by laser scanners are not intrinsically equipped with normals, which are essential to surface reconstruction and point set rendering using surfels. Normal estimation is notoriously sensitive to noise. Near sharp features, the computation of noise-free normals becomes even more challenging due to the inherent undersampling problem at edge singularities. As a result, common edge-aware consolidation techniques such as bilateral smoothing may still produce erroneous normals near the edges. We propose a resampling approach to process a noisy and possibly outlier-ridden point set in an edge-aware manner. Our key idea is to first resample away from the edges so that reliable normals can be computed at the samples, and then based on reliable data, we progressively resample the point set while approaching the edge singularities. We demonstrate that our Edge-Aware Resampling (EAR) algorithm is capable of producing consolidated point sets with noise-free normals and clean preservation of sharp features. We also show that EAR leads to improved performance of edge-aware reconstruction methods and point set rendering techniques.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.ubc.ca/~ascher/papers/hwgcaz.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/globalregistration.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Global Linear Method for Camera Pose Registration</pubtit>
      <p><em>Jiang N, Cui Z, Tan P. </em><br />
<em>In IEEE International Conference on Computer Vision (ICCV) (2013)</em></p>

      <p>We present a linear method for global camera pose registration from pair wise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical `unbalanced scale’ problem in linear methods relying on pair wise translation direction constraints, i.e. an algebraic error, nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/iccv13_sfm.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2012">2012</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xu_siga12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Multi-Scale Partial Intrinsic Symmetry Detection</pubtit>
      <p><em>Kai Xu, Hao Zhang, Wei Jiang, Ramsay Dyer, Zhiquan Cheng, Ligang Liu, and Baoquan Chen</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 31, No. 6, Article 181 (2012)</em></p>

      <p>We present an algorithm for multi-scale partial intrinsic symmetry detection over 2D and 3D shapes, where the scale of a symmetric region is defined by intrinsic distances between symmetric points over the region. To identify prominent symmetric regions which overlap and vary in form and scale, we decouple scale extraction and symmetry extraction by performing two levels of clustering. First, significant symmetry scales are identified by clustering sample point pairs from an input shape …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/xu_siga12_msym.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://kevinkaixu.net/projects/msym.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_siga12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Stackabilization</pubtit>
      <p><em>Honghua Li, Ibraheem Alhashim, Hao Zhang, Ariel Shamir, and Daniel Cohen-Or</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 31, No. 6, Article 158 (2012)</em></p>

      <p>We introduce the geometric problem of stackabilization: how to geometrically modify a 3D object so that it is more amenable to stacking. Given a 3D object and a stacking direction, we define a measure of stackability, which is derived from the gap between the lower and upper envelopes of the object in a stacking configuration along the stacking direction. The main challenge in stackabilization lies in the desire to modify the object’s geometry only subtly so that the intended functionality and aesthetic appearance of the original object are not significantly affected …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/li_siga12_stack.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/huang_siga12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Field-Guided Registration for Feature-Conforming Shape Composition</pubtit>
      <p><em>Hui Huang, Minglun Gong, Daniel Cohen-Or, Yaobin Ouyang, Fuwen Tao, and Hao Zhang</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 31, No. 6, Article 179 (2012)</em></p>

      <p>We present an automatic shape composition method to fuse two shape parts which may not overlap and possibly contain sharp features, a scenario often encountered when modeling man-made objects. At the core of our method is a novel field-guided approach to automatically align two input parts in a feature-conforming manner. The key to our field-guided shape registration is a natural continuation of one part into the ambient field as a means to introduce an overlap with the distant part, which then allows a surface-to-field registration …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/huang_siga12_stitch.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wang_siga12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Active Co-Analysis of a Set of Shapes</pubtit>
      <p><em>Yunhai Wang, Shmulik Asafi, Oliver van Kaick, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH Asia), Vol. 31, No. 6, Article 165 (2012)</em></p>

      <p>We consider the use of a semi-supervised learning method where the user actively assists in the co-analysis by iteratively providing input that progressively constrains the system. We introduce a novel constrained clustering method based on a spring system which embeds elements to better respect their inter-distances in feature space together with the user given set of constraints. We also present an active learning method that suggests to the user where his input is likely to be the most effective in refining the results.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/wang_siga12_ssl.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/aghdaii_cag12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>5-6-7 Meshes&#58; Remeshing and Analysis</pubtit>
      <p><em>Nima Aghdaii, Hamid Younesy, and Hao Zhang</em><br />
<em>In Computer &amp; Graphics, extended version of GI’12 paper, Vol. 36, No. 8, pp. 1072-1083 (2012)</em></p>

      <p>We introduce a new type of meshes called 5-6-7 meshes, analyze their properties, and present a 5-6-7 remeshing algorithm. A 5-6-7 mesh is a closed triangle mesh where each vertex has valence 5, 6, or 7. We prove that it is always possible to convert an arbitrary mesh into a 5-6-7 mesh. We present a remeshing algorithm which converts a closed triangle mesh with arbitrary genus into a 5-6-7 mesh which a) closely approximates the original mesh geometrically, e.g., in terms of feature preservation, and b) has a comparable vertex count as the original mesh.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/aghdaii_cag12_567.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tag_sgp12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Mean Curvature Skeletons</pubtit>
      <p><em>Andrea Tagliassachi, Ibraheem Alhashim, Matt Olson, and Hao Zhang</em><br />
<em>In Computer Graphics Forum (Special Issue of SGP), Volume 31, Number 5, pp. 1735-1744 (2012)</em></p>

      <p>We formulate the skeletonization problem via mean curvature flow (MCF). While the classical application of MCF is surface fairing, we take advantage of its area-minimizing characteristic to drive the curvature flow towards the extreme so as to collapse the input mesh geometry and obtain a skeletal structure. By analyzing the differential characteristics of the flow, we reveal that MCF locally increases shape anisotropy. This justifies the use of curvature motion for skeleton computation, and leads to the generation of what we call “mean curvature skeletons” …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/tag_sgp12.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/sig12_civil.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Fit and Diverse&#58; Set Evolution for Inspiring 3D Shape Galleries</pubtit>
      <p><em>Kai Xu, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Trans. on Graphics (Special Issue of SIGGRAPH), Vol. 31, No. 4, pp. 57:1-57:10 (2012)</em></p>

      <p>We introduce set evolution as a means for creative 3D shape modeling, where an initial population of 3D models is evolved to produce generations of novel shapes. Part of the evolving set is presented to a user as a shape gallery to offer modeling suggestions. User preferences define the fitness for the evolution so that over time, the shape population will mainly consist of individuals with good fitness. However, to inspire the user’s creativity, we must also keep the evolving set diverse. Hence the evolution is ``fit and diverse’’ …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/xu_sig12_civil.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/aghdaii_gi12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>5-6-7 Meshes</pubtit>
      <p><em>Nima Aghdaii, Hamid Younesy, and Hao Zhang</em><br />
<em>In Proc. of Graphics Interface, pp. 27-34 (2012)</em></p>

      <p>A 5-6-7 mesh is a closed triangle mesh where each vertex has valence 5, 6, or 7. An intriguing question is whether it is always possible to convert an arbitrary mesh into a 5-6-7 mesh. In this paper, we answer the question in the positive. We present a 5-6-7 remeshing algorithm which converts any closed triangle mesh with arbitrary genus into a 5-6-7 mesh which a) closely approximates the original mesh geometrically, e.g., in terms of feature preservation, and b) has a comparable vertex count as the original mesh.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/aghdaii_gi12_567.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wang_gmp12.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Empirical Mode Decomposition on Surfaces</pubtit>
      <p><em>Hui Wang, Zhixun Su, Jinjie Cao, Ye Wang, and Hao Zhang</em><br />
<em>In Graphical Models (Special Issue of GMP), Vol. 74, No. 4, pp. 173-183 (2012)</em></p>

      <p>Empirical Mode Decomposition (EMD) is a powerful tool for the analysis of non-stationary and nonlinear signals, and has drawn a great deal of attention in various areas. In this paper, we generalize the classical EMD from Euclidean space to surfaces represented as triangular meshes. Inspired by the EMD, we also make a first step in using the extremal envelope method for feature-preserving smoothing.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/wang_gmp12.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/alhashim_tvc_detail.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Detail-Replicating Shape Stretching</pubtit>
      <p><em>Ibraheem Alhashim, Hao Zhang, and Ligang Liu</em><br />
<em>In the Visual Computer, Vol. 28, No. 12, pp. 1153-1166 (2012)</em></p>

      <p>We propose a simple and efficient method that helps create model variations by applying non-uniform stretching on 3D models with organic geometric details. The method replicates the geometric details and synthesizes extensions by adopting texture synthesis techniques on surface details.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/alhashim_tvc_detail_reduced.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









 | 

<a href="https://www.youtube.com/watch?v=VAXm7Wm-R7c" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhou_12_siga.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Image-based Clothes Animation for Virtual Fitting</pubtit>
      <p><em>Zhenglong Zhou, Bo Shu, Shaojie Zhuo, Xiaoming Deng, Ping Tan, Stephen Lin</em><br />
<em>In SIGGRAPH Asia Technique Briefs (2012)</em></p>

      <p>We propose an image-based approach for virtual clothes fitting, in which a user moves freely in front of a virtual mirror (i.e., video screen) that displays the user wearing a superimposed virtual garment.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/sigasia12_briefs.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Projects/Clothes/index.htm">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/5MNKQ9Gt1AU" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen_12_TOG.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Manifold Preserving Edit Propagation</pubtit>
      <p><em>Xiaowu Chen, Dongqing Zou, Qinping Zhao, Ping Tan</em><br />
<em>In ACM Transaction on Graphics(TOG) and Proc. of SIGGRAPH Asia (2012)</em></p>

      <p>We propose a novel edit propagation algorithm for interactive image and video manipulations. Our approach uses the locally linear embedding (LLE) to represent each pixel as a linear combination of its neighbors in a feature space</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/sigasia12.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/wang_12_icpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Detecting Discontinuities for Surface Reconstruction</pubtit>
      <p><em>Yinting Wang, Jiajun Bu, Na Li, Mingli Song, Ping Tan</em><br />
<em>In International Conference on Pattern Recognition (ICPR) (2012)</em></p>

      <p>A method is described for discontinuity detection in pictorial data. It computes at each point a planar approximation of the data and uses the statistics of the differences between the actual values and the approximations for detection of both steps and creases. The use of local statistical properties in the residuals provides…..</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.sciencedirect.com/science/article/pii/0734189X8590163X">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/chen_12_tvcg.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>PoseShop&#58; Human Image Database Construction and Personalized Content Synthesis</pubtit>
      <p><em>Tao Chen, Ping Tan, Li-Qian Ma, Ming-Ming Cheng, Ariel Shamir, Shi-Min Hu</em><br />
<em>In IEEE Transaction on Visualization and Computer Graphics (TVCG) (2012)</em></p>

      <p>We present PoseShop – a pipeline to construct segmented human image database with minimal manual intervention. By downloading, analyzing, and filtering massive amounts of human images….</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/tvcg12.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/Jiang_12_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>3D Reconstruction of Dynamic Scenes with Multiple Handheld Cameras</pubtit>
      <p><em>Hanqing Jiang, Haomin Liu, Ping Tan, Guofeng Zhang, Hujun Bao</em><br />
<em>In European Conference on Computer Vision (ECCV) (2012)</em></p>

      <p>We propose a novel dense depth estimation method which can automatically recover accurate and consistent depth maps from the synchronized video sequences taken by a few handheld cameras. Unlike fixed camera arrays…</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv12_stereo.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/joonlee_12_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Estimation of Intrinsic Image Sequences from Image+Depth Video</pubtit>
      <p><em>Kyong Joon Lee, Qi Zhao, Xin Tong, Minmin Gong, Shahram Izadi, Sang Uk Lee, Ping Tan, Stephen Lin</em><br />
<em>In European Conference on Computer Vision (ECCV) (2012)</em></p>

      <p>We present a technique for estimating intrinsic images from image+depth video, such as that acquired from a Kinect camera. Intrinsic image decomposition in this context has importance in applications like object modeling, in which surface colors need to be recovered without illumination effects. The proposed method is based on two new types of decomposition constraints</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv12_intrinsic.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/boxinshi_12_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Elevation Angle from Reflectance Monotonicity&#58; Photometric Stereo for General Isotropic Reectances</pubtit>
      <p><em>Boxin Shi, Ping Tan, Yasuyuki Matsushita, Katsushi Ikeuchi</em><br />
<em>In European Conference on Computer Vision (ECCV) (2012)</em></p>

      <p>This paper exploits the monotonicity of general isotropic re-flectances for estimating elevation angles of surface normal given the azimuth angles. With an assumption that the reflectance includes at least one lobe that is a monotonic function of the angle between the surface normal and half-vector (bisector of lighting and viewing directions), we prove that elevation angles can be uniquely</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv12_ps.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zou_12_tpami.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>CoSLAM&#58; Collaborative Visual SLAM in Dynamic Environments</pubtit>
      <p><em>Danping Zou, Ping Tan</em><br />
<em>In IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI) (2012)</em></p>

      <p>This paper studies the problem of vision-based simultaneous localization and mapping (SLAM) in dynamic environments with multiple cameras. We introduce inter-camera pose estimation and inter-camera mapping to deal with dynamic objects in the localization and mapping process.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/pami12_slam.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Projects/SLAM/index.htm">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/WRRAtbl_YlI" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/jiang_12_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Seeing Double Without Confusion&#58; Structure-from-Motion in Highly Ambiguous Scenes</pubtit>
      <p><em>Nianjuan Jiang, Ping Tan, Loong-Fah Cheong</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2012)</em></p>

      <p>We propose a novel optimization criteria based on the idea of ‘missing correspondences’. The global minimum of our optimization objective function is associated with the correct solution. We then design an ef- ficient algorithm for minimization, whose convergence to a local minimum is guaranteed.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/cvpr12_sfm.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/boxinshi_12_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Biquadratic Reflectance Model for Radiometric Image Analysis</pubtit>
      <p><em>Boxin Shi, Ping Tan, Yasuyuki Matsushita, Katsushi Ikeuchi.</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2012)</em></p>

      <p>We propose a compact biquadratic reflectance model to represent the reflectance of a broad class of materials precisely in the low-frequency domain. We validate our model by fitting to both existing parametric models and non-parametric measured data, and show that our model outperforms existing parametric diffuse models. We show applications of reflectometry using general diffuse surfaces and photometric stereo for general isotropic materials</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/cvpr12_biquadratic.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_12_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Video Stabilization with a Depth Camera</pubtit>
      <p><em>Shuaicheng Liu, YintingWang, Lu Yuan, Jiajun Bu, Ping Tan, Jian Sun</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2012)</em></p>

      <p>We propose to solve video stabilization with an additional depth sensor such as the Kinect camera. Though the depth image is noisy, incomplete and low resolution, it facilitates both camera motion estimation and frame warping, which makes the video stabilization a much well posed problem. The experiments demonstrate the effectiveness of our algorithm.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/cvpr12_stabilization.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.liushuaicheng.org/CVPR2012/index.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/H97LTmDCOrQ" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhao_12_tpami.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Closed-form Solution to Retinex with Non-local Texture Constraints</pubtit>
      <p><em>Qi Zhao, Ping Tan, Qiang Dai, Li Shen, Enhua Wu, Stephen Lin</em><br />
<em>In IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI) (2012)</em></p>

      <p>We propose a method for intrinsic image decomposition based on Retinex theory and texture analysis. While most previous methods approach this problem by analyzing local gradient properties, our technique additionally identifies distant pixels with the same reflectance through texture analysis, and uses these non-local reflectance constraints to significantly reduce ambiguity in decomposition</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/pami12_intrinsic.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2011">2011</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tuner.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Tuner&#58; Principled Parameter Finding for Image Segmentation Algorithms Using Visual Response Surface Exploration</pubtit>
      <p><em>Torsney-Weir T, Saad A, Moeller T, Hege H-C, Weber B, Verbavatz J-M. </em><br />
<em>In IEEE transactions on visualization and computer graphics (2011)</em></p>

      <p>In this paper we address the difficult problem of parameter-finding in image segmentation. We replace a tedious manual process that is often based on guess-work and luck by a principled approach that systematically explores the parameter space. Our core idea is the following two-stage technique: We start with a sparse sampling of the parameter space and apply a statistical model to estimate the response of the segmentation algorithm. The statistical model incorporates a model of uncertainty of the estimation which we use in conjunction with the actual estimate in (visually) guiding the user towards areas that need refinement by placing additional sample points. In the second stage the user navigates through the parameter space in order to determine areas where the response value (goodness of segmentation) is high. In our exploration we rely on existing ground-truth images in order to evaluate the “goodness” of an image segmentation technique. We evaluate its usefulness by demonstrating this technique on two image segmentation algorithms: a three parameter model to detect microtubules in electron tomograms and an eight parameter model to identify functional regions in dynamic Positron Emission Tomography scans.</p>

      <p style="text-align: right;">

[









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/unsupervisedcosegmentation.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Unsupervised Co-Segmentation of a Set of Shapes via Descriptor-Space Spectral Clustering</pubtit>
      <p><em>Sidi O, van Kaick O, Kleinman Y, Zhang H, Cohen-Or D. </em><br />
<em>In ACM Trans. on Graphics (Proc. of SIGGRAPH Asia) (2011)</em></p>

      <p>We introduce an algorithm for unsupervised co-segmentation of a set of shapes so as to reveal the semantic shape parts and establish their correspondence across the set. The input set may exhibit significant shape variability where the shapes do not admit proper spatial alignment and the corresponding parts in any pair of shapes may be geometrically dissimilar. Our algorithm can handle such challenging input sets since, first, we perform co-analysis in a descriptor space, where a combination of shape descriptors relates the parts independently of their pose, location, and cardinality. Secondly, we exploit a key enabling feature of the input set, namely, dissimilar parts may be “linked” through third-parties present in the set. The links are derived from the pairwise similarities between the parts’ descriptors. To reveal such linkages, which may manifest themselves as anisotropic and non-linear structures in the descriptor space, we perform spectral clustering with the aid of diffusion maps. We show that with our approach, we are able to co-segment sets of shapes that possess significant variability, achieving results that are close to those of a supervised approach.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.444.1075&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/surveycorrespondence.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Survey on Shape Correspondence</pubtit>
      <p><em>van Kaick O, Zhang H, Hamarneh G, Cohen-Or D. </em><br />
<em>In Computer Graphics Forum (2011)</em></p>

      <p>We review methods designed to compute correspondences between geometric shapes represented by triangle meshes, contours or point sets. This survey is motivated in part by recent developments in space–time registration, where one seeks a correspondence between non-rigid and time-varying surfaces, and semantic shape analysis, which underlines a recent trend to incorporate shape understanding into the analysis pipeline. Establishing a meaningful correspondence between shapes is often difficult because it generally requires an understanding of the structure of the shapes at both the local and global levels, and sometimes the functionality of the shape parts as well. Despite its inherent complexity, shape correspondence is a recurrent problem and an essential component of numerous geometry processing applications. In this survey, we discuss the different forms of the correspondence problem and review the main solution methods, aided by several classification criteria arising from the problem definition. The main categories of classification are defined in terms of the input and output representation, objective function and solution approach. We conclude the survey by discussing open problems and future perspectives</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/vanKaick_cgf11_survey.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/photoinspiredmodeling.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Photo-Inspired Model-Driven 3D Object Modeling</pubtit>
      <p><em>Xu K, Zheng H, Zhang H, Cohen-Or D, Liu L, Xiong Y. </em><br />
<em>In ACM Transactions on Graphics (SIGGRAPH 2011) (2011)</em></p>

      <p>We introduce an algorithm for 3D object modeling where the user draws creative inspiration from an object captured in a single photograph. Our method leverages the rich source of photographs for creative 3D modeling. However, with only a photo as a guide, creating a 3D model from scratch is a daunting task. We support the modeling process by utilizing an available set of 3D candidate models. Specifically, the user creates a digital 3D model as a geometric variation from a 3D candidate. Our modeling technique consists of two major steps. The first step is a user-guided image-space object segmentation to reveal the structure of the photographed object. The core step is the second one, in which a 3D candidate is automatically deformed to fit the photographed target under the guidance of silhouette correspondence. The set of candidate models have been pre-analyzed to possess useful high-level structural information, which is heavily utilized in both steps to compensate for the ill-posedness of the analysis and modeling problems based only on content in a single image. Equally important, the structural information is preserved by the geometric variation so that the final product is coherent with its inherited structural information readily usable for subsequent model refinement or processing.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.310.1880&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://kevinkaixu.net/projects/photo-inspired.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="http://kevinkaixu.net/videos/photo-inspired.mov" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/vase.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>VASE&#58; Volume-Aware Surface Evolution for Surface Reconstruction from Incomplete Point Clouds</pubtit>
      <p><em>Tagliasacchi A, Olson M, Zhang H, Hamarneh G, Cohen-Or D. </em><br />
<em>In Computer Graphics Forum (Proceedings of Symposium on Geometry Processing) (2011)</em></p>

      <p>Objects with many concavities are difficult to acquire using laser scanners. The highly concave areas are hard to access by a scanner due to occlusions by other components of the object. The resulting point scan typically suffers from large amounts of missing data. Methods that use surface-based priors rely on local surface estimates and perform well only when filling small holes. When the holes become large, the reconstruction problem becomes severely under-constrained, which necessitates the use of additional reconstruction priors. In this paper, we introduce weak volumetric priors which assume that the volume of a shape varies smoothly and that each point cloud sample is visible from outside the shape. Specifically, the union of view-rays given by the scanner implicitly carves the exterior volume, while volumetric smoothness regularizes the internal volume. We incorporate these priors into a surface evolution framework where a new energy term defined by volumetric smoothness is introduced to handle large amount of missing data. We demonstrate the effectiveness of our method on objects exhibiting deep concavities, and show its general applicability over a broader spectrum of geometric scenario.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://ai2-s2-pdfs.s3.amazonaws.com/9c5a/7f0354f38fb3fe07d3877dd94f031e9e24f0.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/pointsetsilhouettes.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Point Set Silhouettes via Local Reconstruction</pubtit>
      <p><em>Olson M, Dyer R, Zhang H, Sheffer A. </em><br />
<em>In Shape Modeling International (2011)</em></p>

      <p>We present an algorithm to compute the silhouette set of a point cloud. Previous methods extract point set silhouettes by thresholding point normals, which can lead to simultaneous over- and under-detection of silhouettes. We argue that additional information such as surface curvature is necessary to resolve these issues.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/olson_smi11_psil.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/symmetryhierarchy.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Symmetry Hierarchy of Man-Made Objects</pubtit>
      <p><em>Wang Y, Xu K, Li J, Zhang H, Shamir A, Liu L, Cheng Z, Xiong Y. </em><br />
<em>In Computer Graphics Forum (Special Issue of Eurographics 2011) (2011)</em></p>

      <p>We introduce symmetry hierarchy of man-made objects, a high-level structural representation of a 3D model providing a symmetry-induced, hierarchical organization of the model’s constituent parts. We show that symmetry hierarchy naturally implies a hierarchical segmentation that is more meaningful than those produced by local geometric considerations. We also develop an application of symmetry hierarchies for structural shape editing.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/wang_eg11_symh.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.computer-graphics.cn/~eric/symh.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/widthbounded.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Width-Bounded Geodesic Strips for Surface Tiling</pubtit>
      <p><em>Kahlert J, Olson M, Zhang H. </em><br />
<em>In The Visual Computer (2011)</em></p>

      <p>We present an algorithm for computing families of geodesic curves over an open mesh patch to partition the patch into strip-like segments. Specifically, the segments can be well approximated using strips obtained by trimming long, rectangular pieces of material possessing a prescribed width. We call this width-bounded geodesic strip tiling of a curved surface, a problem with practical applications such as the surfacing of curved roofs.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/kahlert_tvc10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/priorknowledge.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Prior Knowledge for Part Correspondence</pubtit>
      <p><em>van Kaick O., Tagliasacchi A., Sidi O., Zhang H., Cohen-Or D., Wolf L., Hamarneh G.. </em><br />
<em>In Computer Graphics Forum (Proc. Eurographics) (2011)</em></p>

      <p>We stipulate that in these cases, shape correspondence by humans involves recognition of the shape parts where prior knowledge on the parts would play a more dominant role than geometric similarity. We introduce an approach to part correspondence which incorporates prior knowledge imparted by a training set of pre-segmented, labeled models and combines the knowledge with content driven analysis based on geometric similarity between the matched shapes. First, the prior knowledge is learned from the training set in the form of per-label classifiers. Next, given two query shapes to be matched, we apply the classifiers to assign a probabilistic label to each shape face. Finally,by means of a joint labeling scheme, the probabilistic labels are used synergistically with pairwise assignments derived from geometric similarity to provide the resulting part correspondence.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/vanKaick_eg11_knowledge.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/sortfirstrendering.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Sort First Parallel Volume Rendering</pubtit>
      <p><em>Molomey B., Ament M., Weiskopf D., Möller T.</em><br />
<em>In IEEE Transactions on Visualization and Computer Graphics (2011)</em></p>

      <p>We demonstrate that sort first distributions are not only a viable method of performing data scalable parallel volume rendering, but more importan tly they allow for a range of rendering algorithms and techniques that are not efficient with sort last distributions. Several of these algorithms are discussed and two of them are implemented in a parallel environment: a new improved variant of early ray termination to speed up rendering when volumetric occlusion occurs and a volumetric shadowing technique that produces more realistic and infor mative images based on half angle slicing. Improved methods of distributing the computation of the load balancing and loading portions o f a subdivided data set are also presented. Our detailed test r esults for a typical GPU cluster with distributed memory show that our sort first rendering algorithm outperforms sort last rendering in many scenarios.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.vis.uni-stuttgart.de/~amentmo/docs/tvcg11_dvr.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/visualcomparability.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Visual Comparability of 3D Regular Sampling and Reconstruction</pubtit>
      <p><em>Meng T., Entezari A., Smith B., Weiskopf D., Möller T.</em><br />
<em>In IEEE Transactions on Visualization and Computer Graphics (2011)</em></p>

      <p>The Body-Centered Cubic (BCC) and Face-Centered Cubic (FCC) lattices have been analytically shown to be more efficient sampling lattices than the traditional Cartesian Cubic (CC) lattice, but there has been no estimate of their visual comparability. Two perceptual studies (each with N = 12 participants) compared the visual quality of images rendered from BCC and FCC lattices to images rendered from the CC lattice. Images were generated from two signals: the commonly used Marschner-Lobb synthetic function and a computed tomography scan of a fish tail. Observers found that BCC and FCC could produce images of comparable visual quality to CC, using 30-35 percent fewer samples. For the images used in our studies, the L(2) error metric shows high correlation with the judgement of human observers. Using the L(2) metric as a proxy, the results of the experiments appear to extend across a wide range of images and parameter choices.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://eprints.cs.univie.ac.at/4201/1/tvcg11_bccvisual.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/highqualitygradient.gif" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Toward High-Quality Gradient Estimation on Regular Lattices</pubtit>
      <p><em>Hossain Z., Alim U.R., Möller T.</em><br />
<em>In IEEE Transactions on Visualization and Computer Graphics (2011)</em></p>

      <p>We present two methods for accurate gradient estimation from scalar field data sampled on regular lattices. The first method is based on the multidimensional Taylor series expansion of the convolution sum and allows us to specify design criteria such as compactness and approximation power. The second method is based on a Hilbert space framework and provides a minimum error solution in the form of an orthogonal projection operating between two approximation spaces. Both methods lead to discrete filters, which can be combined with continuous reconstruction kernels to yield highly accurate estimators as compared to the current state of the art. We demonstrate the advantages of our methods in the context of volume rendering of data sampled on Cartesian and Body-Centered Cubic lattices. Our results show significant qualitative and quantitative improvements for both synthetic and real data, while incurring a moderate preprocessing and storage overhead.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://pdfs.semanticscholar.org/0d0b/e75479261a6c30a2a725f22ea2cff106d087.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2010">2010</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xu_siga10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Style-Content Separation by Anisotropic Part Scales</pubtit>
      <p><em>Kai Xu, Honghua Li, Hao Zhang, Daniel Cohen-Or, Yueshan Xiong, and Zhiquan Cheng</em><br />
<em>In ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2010), Volume 29, Number 6, pp. 184:1-184:10 (2010)</em></p>

      <p>We perform co-analysis of a set of man-made 3D objects to allow the creation of novel instances derived from the set. We analyze the objects at the part level and treat the anisotropic part scales as a shape style. The co-analysis then allows style transfer to synthesize new objects. The key to co-analysis is part correspondence, where a major challenge is the handling of large style variations and diverse geometric content in the shape set. We propose style-content separation as a means to address this challenge …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/xu_siga10_style.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://sites.google.com/site/kevinkaixu/publications/style">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/shalom_siga10_small.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Cone Carving for Surface Reconstruction</pubtit>
      <p><em>Shy Shalom, Ariel Shamir, Hao Zhang, and Daniel Cohen-Or</em><br />
<em>In ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2010), Volume 29, Number 6, Article 150 (2010)</em></p>

      <p>We present cone carving, a novel space carving technique towards topologically correct surface reconstruction from an incomplete scanned point cloud. The technique utilizes the point samples not only for local surface position estimation but also to obtain global visibility information under the assumption that each acquired point is visible from a point laying outside the shape. This enables associating each point with a generalized cone, called the visibility cone, that carves a portion of the outside ambient space of the shape from the inside out.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/shalom_siga10_cone.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/livny_siga10.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Automatic Reconstruction of Tree Skeletal Structures from Point Clouds</pubtit>
      <p><em>Yotam Livny, Feilong Yan, Matt Olson, Baoquan Chen, Hao Zhang, and Jihad El-Sana</em><br />
<em>In ACM Trans. on Graphics (Proceeding of SIGGRAPH Asia 2010), Volume 29, Number 6, Article 151 (2010)</em></p>

      <p>In this paper, we perform active laser scanning of real world vegetation and present an automatic approach that robustly reconstructs skeletal structures of trees, from which full geometry can be generated. The core of our method is a series of {\it global optimizations} that fit skeletal structures to the often sparse, incomplete, and noisy point data. A significant benefit of our approach is its ability to reconstruct multiple overlapping trees simultaneously without segmentation.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/livny_siga10_tree.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://web.siat.ac.cn/~vcc/publications/2010/tree_reconstruction/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/nan_sig10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>SmartBoxes for Interactive Urban Reconstruction</pubtit>
      <p><em>Liangliang Nan, Andrei Sharf, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen</em><br />
<em>In ACM Trans. on Graphics (Proceeding of SIGGRAPH 2010), Volume 29, Number 4, Article 93 (2010)</em></p>

      <p>We introduce an interactive tool which enables a user to quickly assemble an architectural model directly over a 3D point cloud acquired from large-scale scanning of an urban scene. The user loosely defines and manipulates simple building blocks, which we call SmartBoxes, over the point samples. These boxes quickly snap to their proper locations to conform to common architectural structures. The key idea is that the building blocks are smart …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/nan_sig10_highres.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://web.siat.ac.cn/~vcc/publications/2010/smartboxes/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/part_analogy_2.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Contextual Part Analogies in 3D Objects</pubtit>
      <p><em>Lior Shapira, Shy Shalom, Ariel Shamir, Daniel Cohen-Or, and Hao Zhang</em><br />
<em>In International Journal of Computer Vision, Vol. 89, No. 1-2, pp. 309-326 (2010)</em></p>

      <p>We address the problem of finding analogies between parts of 3D objects. By partitioning an object into meaningful parts and finding analogous parts in other objects, not necessarily of the same type, based on a contextual signature, many analysis and modeling tasks could be enhanced …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/ijcv10_analogy.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/cao_smi10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Point Cloud Skeletons via Laplacian-Based Contraction</pubtit>
      <p><em>Junjie Cao, Andrea Tagliasacchi, Matt Olson, Hao Zhang, and Zhixun Su</em><br />
<em>In Proc. of IEEE SMI, pp. 187-197 (2010)</em></p>

      <p>We present an algorithm for curve skeleton extraction via Laplacian-based contraction. Our algorithm can be applied to surfaces with boundaries, polygon soups, and point clouds. We develop a contraction operation that is designed to work on generalized discrete geometry data, particularly point clouds, via local Delaunay triangulation and topological thinning …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/cao_smi10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="https://github.com/ataiya/cloudcontr">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhang_cgf10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Spectral Mesh Processing</pubtit>
      <p><em>Hao Zhang, Oliver van Kaick, and Ramsay Dyer</em><br />
<em>In Computer Graphics Forum, Volume 29, Number 6, pp. 1865-1894 (2010)</em></p>

      <p>We provide the first comprehensive survey on spectral mesh processing. Spectral methods for mesh processing and analysis rely on eigenvalues, eigenvectors, or eigenspace projections derived from appropriately defined mesh operators to carry out desired tasks …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zhang_cgf10_spect_survey.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/vanKaick_ar10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Learning Fourier Descriptors for Computer-Aided Diagnosis of the Supraspinatus</pubtit>
      <p><em>Oliver van Kaick, Aaron Ward, Ghassan Hamarneh, Mark Schweitzer, and Hao Zhang</em><br />
<em>In Academic Radiology, Vol. 17, No. 8, pp. 1040-1049 (2010)</em></p>

      <p>Supraspinatus muscle disorders are frequent and debilitating, resulting in pain and a limited range of shoulder motion. The gold standard for diagnosis involves an invasive surgical procedure … we present a method to classify 3D shapes of the muscle into the relevant pathology groups, based on MRIs. The method learns the Fourier coefficients that best distinguish the different classes …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/vanKaick_ar10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/vanKaick_star10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Survey on Shape Correspondence</pubtit>
      <p><em>Oliver van Kaick, Hao Zhang, Ghassan Hamarneh, Daniel Cohen-Or</em><br />
<em>In Eurographics 2010 State-of-the-Art Report, TBA (2010)</em></p>

      <p>We present a review of the correspondence problem targeted towards the computer graphics audience. This survey is motivated by recent developments such as advances in the correspondence of non-rigid or isometric shapes and methods that extract semantic information from the shapes …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/vanKaick_egstar10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zheng_eg10.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Consensus Skeleton for Non-Rigid Space-Time Registration</pubtit>
      <p><em>Qian Zheng, Andrei Sharf, Andrea Tagliasacchi, Baoquan Chen, Hao Zhang, Alla Sheffer, Daniel Cohen-Or</em><br />
<em>In Computer Graphics Forum (Proceeding of Eurographics 2010), Volume 29, Number 2, pp. 635-644 (2010)</em></p>

      <p>We introduce the notion of consensus skeletons for non-rigid space-time registration of a deforming shape. Instead of basing the registration on point features, which are local and sensitive to noise, we adopt the curve skeleton of the shape as a global and descriptive feature for the task. Our method uses no template and only assumes that the skeletal structure of the captured shape remains largely consistent over time …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zheng_eg10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhou_10_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Ring-Light Photometric Stereo</pubtit>
      <p><em>Zhenglong Zhou, Ping Tan</em><br />
<em>In European Conference on Computer Vision (ECCV) (2010)</em></p>

      <p>We propose a novel algorithm for uncalibrated photometric stereo. While most of previous methods rely on various assumptions on scene properties, we exploit constraints in lighting configurations. We first derive an ambiguous reconstruction by requiring lights to lie on a view centered cone. This reconstruction is upgraded to Euclidean by constraints derived from lights of equal intensity and multiple view geometry.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/zhou_10_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Self-calibrating Photometric Stereo</pubtit>
      <p><em>Boxin Shi, Yasuyuki Matsushita, Yichen Wei, Chao Xu, Ping Tan</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2010)</em></p>

      <p>We present a self-calibrating photometric stereo method. From a set of images taken from a fixed viewpoint under different and unknown lighting conditions, our method automatically determines a radiometric response function and resolves the generalized bas-relief ambiguity for estimating accurate surface normals and albedos. We show that color and intensity profiles, which are obtained from registered pixels across images, serve as effective cues for addressing these two calibration problems.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv10.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2009">2009</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tangental.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Tangential Distance Field for Mesh Silhouette Analysis</pubtit>
      <p><em>Olson M, Zhang H. </em><br />
<em>In Computer Graphics Forum (2009)</em></p>

      <p>We consider a tangent-space representation of surfaces that maps each point on a surface to the tangent plane of the surface at that point. Such representations are known to facilitate the solution of several visibility problems, in particular, those involving silhouette analysis. In this paper, we introduce a novel class of distance fields for a given surface defined by its tangent planes. At each point in space, we assign a scalar value which is a weighted sum of distances to these tangent planes. We call the resulting scalar field a ‘tangential distance field’ (TDF). When applied to triangle mesh models, the tangent planes become supporting planes of the mesh triangles. The weighting scheme used to construct a TDF for a given mesh and the way the TDF is utilized can be closely tailored to a specific application. At the same time, the TDFs are continuous, lending themselves to standard optimization techniques such as greedy local search, thus leading to efficient algorithms.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/olson_zhang_cgf08.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/partialintrinsicsym.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Partial Intrinsic Reflectional Symmetry of 3D Shapes</pubtit>
      <p><em>Xu K, Zhang H, Tagliasacchi A, Liu L, Li G, Meng M, Xiong Y. </em><br />
<em>In ACM Trans. on Graphics, (Proceedings of SIGGRAPH Asia 2009) (2009)</em></p>

      <p>While many 3D objects exhibit various forms of global symmetries, prominent intrinsic symmetries which exist only on parts of an object are also well recognized. Such partial symmetries are often seen as more natural compared to a global one, especially on a composite shape. We introduce algorithms to extract partial intrinsic reflectional symmetries (PIRS) of a 3D shape. Given a closed 2-manifold mesh, we develop a voting scheme to obtain an intrinsic reflectional symmetry axis (IRSA) transform, which computes a scalar field over the mesh so as to accentuate prominent IRSAs of the shape. We then extract a set of explicit IRSA curves on the shape based on a refined measure of local reflectional symmetry support along a curve. The iterative refinement procedure combines IRSA-induced region growing and region-constrained symmetry support refinement to improve accuracy and address potential issues due to rotational symmetries in the shape. We show how the extracted IRSA curves can be incorporated into a conventional mesh segmentation scheme so that the implied symmetry cues can be utilized to obtain more meaningful results. We also demonstrate the use of IRSA curves for symmetry-driven part repair.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://kevinkaixu.net/papers/xu_siga09_pirs.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://kevinkaixu.net/projects/pirs.html">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="http://kevinkaixu.net/videos/pirs.mov" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/segbasedreg.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Segmentation-Based Regularization of Dynamic SPECT Reconstruction</pubtit>
      <p><em>Humphries T, Saad A, Celler A, Hamarneh G, Möller T, Trummer M. </em><br />
<em>In Proceedings of 2009 IEEE Nuclear Science Symposium and Medical Imaging Conference (2009)</em></p>

      <p>Dynamic SPECT reconstruction using a single slow camera rotation is a highly underdetermined problem, which requires the use of regularization techniques to obtain useful results. The dSPECT algorithm (Farncombe et al. 1999) provides temporal but not spatial regularization, resulting in poor contrast and low activity levels in organs of interest, due mostly to blurring. In this paper we incorporate a user-assisted segmentation algorithm (Saad et al. 2008) into the reconstruction process to improve the results. Following an initial reconstruction using the existing dSPECT technique, a user places seeds in the image to indicate regions of interest (ROIs). A random-walk based automatic segmentation algorithm then assigns every voxel in the image to one of the ROIs, based on its proximity to the seeds as well as the similarity between time activity curves (TACs). The user is then able to visualize the segmentation and improve it if necessary. Average TACs are extracted from each ROI and assigned to every voxel in the ROI, giving an image with a spatially uniform TAC in each ROI. This image is then used as initial input to a second run of dSPECT, in order to adjust the dynamic image to better fit the projection data. We test this approach with a digital phantom simulating the kinetics of Tc99m-DTPA in the renal system, including healthy and unhealthy behaviour. Summed TACs for each kidney and the bladder were calculated for the spatially regularized and non-regularized reconstructions, and compared to the true values. The TACs for the two kidneys were noticeably improved in every case, while TACs for the smaller bladder region were unchanged. Furthermore, in two cases where the segmentation was intentionally done incorrectly, the spatially regularized reconstructions were still as good as the non-regularized ones. In general, the segmentation-based regularization improves TAC quality within ROIs, as well as image contrast.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~hamarneh/ecopy/mic2009.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/curveskeleton.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Curve Skeleton Extraction from Incomplete Point Cloud</pubtit>
      <p><em>Tagliasacchi A, Zhang H, Cohen-Or D. </em><br />
<em>In ACM Transactions on Graphics, (Proceedings SIGGRAPH 2009) (2009)</em></p>

      <p>We present an algorithm for curve skeleton extraction from imperfect point clouds where large portions of the data may be missing. Our construction is primarily based on a novel notion of generalized rotational symmetry axis (ROSA) of an oriented point set. Specifically, given a subset S of oriented points, we introduce a variational definition for an oriented point that is most rotationally symmetric with respect to S. Our formulation effectively utilizes normal information to compensate for the missing data and leads to robust curve skeleton computation over regions of a shape that are generally cylindrical. We present an iterative algorithm via planar cuts to compute the ROSA of a point cloud. This is complemented by special handling of non-cylindrical joint regions to obtain a centered, topologically clean, and complete 1D skeleton. We demonstrate that quality curve skeletons can be extracted from a variety of shapes captured by incomplete point clouds. Finally, we show how our algorithm assists in shape completion under these challenges by developing a skeleton-driven point cloud completion scheme.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www-evasion.imag.fr/people/Franck.Hetroy/Teaching/ProjetsImage/2010/Bib/tagliasacchi_zhang_cohen-or-siggraph2009.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/expressioninsensitive.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Expression-Insensitive 3D Face Recognition using Sparse Representation</pubtit>
      <p><em>Li X, Jia T, Zhang H. </em><br />
<em>In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2009)</em></p>

      <p>We present a face recognition method based on sparse representation for recognizing 3D face meshes under expressions using low-level geometric features. First, to enable the application of the sparse representation framework, we develop a uniform remeshing scheme to establish a consistent sampling pattern across 3D faces.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/li_zhang_cvpr09.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/nonuniformbsplines.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>On Visualization and Reconstruction from Non-Uniform Point Sets using B-splines</pubtit>
      <p><em>Vuçini E, Möller T, Gröller E. </em><br />
<em>In Computer Graphics Forum (Proceedings of Eurographics/IEEE-VGTC Symposium on Visualization 2009 (EuroVis 2009)) (2009)</em></p>

      <p>We present a novel framework for the visualization and reconstruction from non-uniform point sets. We adopt a variational method for the reconstruction of 3D non-uniform data to a uniform grid of chosen resolution. We will extend this reconstruction to an efficient multi-resolution uniform representation of the underlying data. Our multi-resolution representation includes a traditional bottom-up approach and a novel top-down hierarchy for adaptive hierarchical reconstruction. Using a hybrid regularization functional we can improve the reconstruction results. Finally, we discuss further application scenarios and show rendering results to emphasize the effectiveness and quality of our proposed framework. By means of qualitative results and error comparisons we demonstrate superiority of our method compared to competing methods.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~torsten/Publications/Thesis/vucini.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/huepreserving.gif" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Hue-Preserving Color Blending</pubtit>
      <p><em>Chuang J, Weiskopf D, Möller T. </em><br />
<em>In EEE Transactions on Visualization and Computer Graphics (Proceedings Visualization / Information Visualization 2009) (2009)</em></p>

      <p>We propose a new perception-guided compositing operator for color blending. The operator maintains the same rules for achromatic compositing as standard operators (such as the over operator), but it modifies the computation of the chromatic channels. Chromatic compositing aims at preserving the hue of the input colors; color continuity is achieved by reducing the saturation of colors that are to change their hue value. The main benefit of hue preservation is that color can be used for proper visual labeling, even under the constraint of transparency rendering or image overlays. Therefore, the visualization of nominal data is improved. Hue-preserving blending can be used in any existing compositing algorithm, and it is particularly useful for volume rendering. The usefulness of hue-preserving blending and its visual characteristics are shown for several examples of volume visualization.</p>

      <p style="text-align: right;">

[









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/highqualityreconstruction.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>High-Quality Volumetric Reconstruction on Optimal Lattices for Computed Tomography</pubtit>
      <p><em>Finkbeiner B, Alim UR, Van De Ville D, Möller T. </em><br />
<em>In Computer Graphics Forum (Proceedings of Eurographics/IEEE-VGTC Symposium on Visualization (EuroVis 2009)) (2009)</em></p>

      <p>Within the context of emission tomography, we study volumetric reconstruction methods based on the Expectation Maximization (EM) algorithm. We show, for the first time, the equivalence of the standard implementation of the EM-based reconstruction with an implementation based on hardware-accelerated volume rendering for nearest-neighbor (NN) interpolation. This equivalence suggests that higher-order kernels should be used with caution and do not necessarily lead to better performance. We also show that the EM algorithm can easily be adapted for different lattices, the body-centered cubic (BCC) one in particular. For validation purposes, we use the 3D version of the Shepp-Logan synthetic phantom, for which we derive closed-form analytical expressions of the projection data.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~torsten/Publications/Papers/ev09-1.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/gabrielmeshes.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Gabriel meshes and Delaunay edge flips</pubtit>
      <p><em>Dyer R, Zhang H, Möller T. </em><br />
<em>In Proceedings of SIAM/ACM Conference on Geometric and Physical Modeling (2009)</em></p>

      <p>We undertake a study of the local properties of 2-Gabriel meshes: manifold triangle meshes each of whose faces has an open Euclidean diametric ball that contains no mesh vertices. We show that, under mild constraints on the dihedral angles, such meshes are Delaunay meshes: the open geodesic circumdisk of each face contains no mesh vertex. The analysis is done by means of the Delaunay edge flipping algorithm and it reveals the details of the distinction between these two mesh structures. In particular we observe that the obstructions which prohibit the existence of Gabriel meshes as homeomorphic representatives of smooth surfaces do not hinder the construction of Delaunay meshes.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/dyer_gpm09_delgab.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/featurealigned.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Feature-Aligned Shape Texturing</pubtit>
      <p><em>Xu K, Cohne-Or D, Ju T, Liu L, Zhang H, Zhou S, Xiong Y. </em><br />
<em>In ACM Transactions on Graphics, (Proceedings SIGGRAPH Asia 2009) (2009)</em></p>

      <p>The essence of a 3D shape can often be well captured by its salient feature curves. In this paper, we explore the use of salient curves in synthesizing intuitive, shape-revealing textures on surfaces. Our texture synthesis is guided by two principles: matching the direction of the texture patterns to those of the salient curves, and aligning the prominent feature lines in the texture to the salient curves exactly. We have observed that textures synthesized by these principles not only fit naturally to the surface geometry, but also visually reveal, even reinforce, the shape’s essential characteristics. We call these feature-aligned shape texturing. Our technique is fully automatic, and introduces two novel technical components in vector-field-guided texture synthesis: an algorithm that orients the salient curves on a surface for constrained vector field generation, and a feature-to-feature texture optimization.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/siga09_fast.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/consolidation.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Consolidation of Unorganized Point Clouds for Surface Reconstruction</pubtit>
      <p><em>Huang H, Li D, Zhang H, Ascher U, Cohen-Or D. </em><br />
<em>In ACM Transactions on Graphics, (Proceedings SIGGRAPH Asia 2009) (2009)</em></p>

      <p>We consolidate an unorganized point cloud with noise, outliers, non-uniformities, and in particular interference between close-by surface sheets as a preprocess to surface generation, focusing on reliable normal estimation. Our algorithm includes two new developments. First, a weighted locally optimal projection operator produces a set of denoised, outlier-free and evenly distributed particles over the original dense point cloud, so as to improve the reliability of local PCA for initial estimate of normals. Next, an iterative framework for robust normal estimation is introduced, where a priority-driven normal propagation scheme based on a new priority measure and an orientation-aware PCA work complementarily and iteratively to consolidate particle normals. The priority setting is reinforced with front stopping at thin surface features and normal flipping to enable robust handling of the close-by surface sheet problem. We demonstrate how a point cloud that is well-consolidated by our method steers conventional surface generation schemes towards a proper interpretation of the input data.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.ubc.ca/~ascher/papers/hlzac.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/onsampling.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>On sampling lattices with similarity scaling relationships</pubtit>
      <p><em>Bergner S., Van De Ville D., Blu T., Möller T.. </em><br />
<em>In Proc. of SAMPTA 2009 (2009)</em></p>

      <p>We provide a method for constructing regular sampling lattices in arbitrary dimensions together with an integer dilation matrix. Subsampling using this dilation matrix leads to a similarity-transformed version of the lattice with a chosen density reduction. These lattices are interesting candidates for multidimensional wavelet constructions with a limited number of subbands.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://hal.archives-ouvertes.fr/hal-00453440/document">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/partawaremetric.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Part-Aware Surface Metric for Shape Analysis</pubtit>
      <p><em>Liu R, Zhang H, Shamir A, Cohen-Or D. </em><br />
<em>In Computer Graphics Forum (Special Issue of Eurographics 2009) (2009)</em></p>

      <p>The main contribution of our work is to bring together these two fundamental concepts: shape parts and surface metric. Specifically, we develop a surface metric that is part-aware. To encode part information at a point on a shape, we model its volumetric context – called the volumetric shape image (VSI) – inside the shape’s enclosed volume, to capture relevant visibility information. We then define the part-aware metric by combining an appropriate VSI distance with geodesic distance and normal variation. We show how the volumetric view on part separation addresses certain limitations of the surface view, which relies on concavity measures over a surface as implied by the well-known minima rule. We demonstrate how the new metric can be effectively utilized in various applications including mesh segmentation, shape registration, part-aware sampling and shape retrieval.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.math.tau.ac.il/~dcor/articles/2009/A-Part-aware-Surface.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tangentialdis.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Tangential Distance Field for Mesh Silhouette Analysis</pubtit>
      <p><em>Olson M, Zhang H. </em><br />
<em>In Computer Graphics Forum (2009)</em></p>

      <p>We consider a tangent-space representation of surfaces that maps each point on a surface to the tangent plane of the surface at that point. Such representations are known to facilitate the solution of several visibility problems, in particular, those involving silhouette analysis. In this paper, we introduce a novel class of distance fields for a given surface defined by its tangent planes. At each point in space, we assign a scalar value which is a weighted sum of distances to these tangent planes. We call the resulting scalar field a ‘tangential distance field’ (TDF). When applied to triangle mesh models, the tangent planes become supporting planes of the mesh triangles. The weighting scheme used to construct a TDF for a given mesh and the way the TDF is utilized can be closely tailored to a specific application. At the same time, the TDFs are continuous, lending themselves to standard optimization techniques such as greedy local search, thus leading to efficient algorithms. In this paper, we use four applications to illustrate the benefit of using TDFs: multi-origin silhouette extraction in Hough space, silhouette-based view point selection, camera path planning and light source placement.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/olson_zhang_cgf08.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/latticeboltzman.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>The Lattice-Boltzmann Method on Optimal Sampling Lattices</pubtit>
      <p><em>Alim UR, Entezari A, Möller T. </em><br />
<em>In IEEE Transactions on Visualization and Computer Graphics (TVCG) (2009)</em></p>

      <p>We extend the single relaxation time lattice-Boltzmann method (LBM) to the 3D body-centered cubic (BCC) lattice. We show that the D3bQ15 lattice defined by a 15 neighborhood connectivity of the BCC lattice is not only capable of more accurately discretizing the velocity space of the continuous Boltzmann equation as compared to the D3Q15 Cartesian lattice, it also achieves a comparable spatial discretization with 30 percent less samples. We validate the accuracy of our proposed lattice by investigating its performance on the 3D lid-driven cavity flow problem and show that the D3bQ15 lattice offers significant cost savings while maintaining a comparable accuracy. We demonstrate the efficiency of our method and the impact on graphics and visualization techniques via the application of line-integral convolution on 2D slices as well as the extraction of streamlines of the 3D flow. We further study the benefits of our proposed lattice by applying it to the problem of simulating smoke and show that the D3bQ15 lattice yields more detail and turbulence at a reduced computational cost.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~torsten/Publications/Papers/tvcg09_lbm.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/illuminantspectra.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A tool to create illuminant and reflectance spectra for light-driven graphics and visualization</pubtit>
      <p><em>Bergner S., Drew M.S, Möller T.</em><br />
<em>In ACM Trans. Graph (2009)</em></p>

      <p>Full spectra allow the generation of a physically correct rendering of a scene under different lighting conditions. In this article we devise a tool to augment a palette of given lights and material reflectances with constructed spectra, yielding specified colors or spectral properties such as metamerism or objective color constancy. We utilize this to emphasize or hide parts of a scene by matching or differentiating colors under different illuminations. These color criteria are expressed as a quadratic programming problem, which may be solved with positivity constraints. Further, we characterize full spectra of lights, surfaces, and transmissive materials in an efficient linear subspace model by forming eigenvectors of sets of spectra and transform them to an intermediate space in which spectral interactions reduce to simple component-wise multiplications during rendering. The proposed method enhances the user’s freedom in designing photo-realistic scenes and helps in creating expressive visualizations. A key application of our technique is to use specific spectral lighting to scale the visual complexity of a scene by controlling visibility of texture details in surface graphics or material details in volume rendering.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~mark/ftp/Tog09/tog09.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2008">2008</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/dyer_sgp08.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Surface sampling and the intrinsic Voronoi diagram (Won the Best Paper Award)</pubtit>
      <p><em>Ramsay Dyer, Hao Zhang, and Torsten Moeller</em><br />
<em>In Computer Graphics Forum (Special Issue of SGP), Volume 27, Number 5, pp. 1431-1439 (2008)</em></p>

      <p>We develop adaptive sampling criteria which guarantee a topologically faithful mesh and demonstrate an improvement and simplification over earlier results, albeit restricted to 2D surfaces. These sampling criteria are based on the strong convexity radius and the injectivity radius …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/dyer_sgp08.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/sgp08_ddsc_2.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Deformation-Driven Shape Correspondence</pubtit>
      <p><em>Hao Zhang, Alla Sheffer, Daniel Cohen-Or, Qingnan Zhou, Oliver van Kaick, and Andrea Tagliasacchi</em><br />
<em>In Computer Graphics Forum (Special Issue of SGP), Volume 27, Number 5, pp. 1393-1402 (2008)</em></p>

      <p>We present an automatic feature correspondence algorithm capable of handling large, non-rigid shape variations, as well as partial matching … The search is deformation-driven, prioritized by a self-distortion energy measured on meshes deformed according to a given correspondence …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/zhang_sgp08.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.cs.ubc.ca/labs/imager/tr/2008/DeformationDriveShapeCorrespondence/">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_gi08.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Convex Hull Covering of Polygonal Scenes for Accurate Collision Detection in Games</pubtit>
      <p><em>Rong Liu, Hao Zhang, and James Busby</em><br />
<em>In Proc. of Graphics Interface 2008, pp. 203-210 (2008)</em></p>

      <p>We look at a particular instance of the convex decomposition problem which arises from real-world game development. Given a collection of polyhedral surfaces (possibly with boundaries, holes, and complex interior structures) that model the scene geometry in a game environment, we wish to find a small set of convex hulls …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/liu_zhang_gi08.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tan_08_tog.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Single Image Tree Modeling</pubtit>
      <p><em>Ping Tan, Tian Fang, Jianxiong Xiao, Peng Zhao, Long Quan</em><br />
<em>In ACM Transaction on Graphics(TOG) and Proc. of SIGGRAPH Asia (2008)</em></p>

      <p>In this paper, we introduce a simple sketching method to generate a realistic 3D tree model from a single image. The user draws at least two strokes in the tree image: the first crown stroke around the tree crown to mark up the leaf region, the second branch stroke from the tree root to mark up the main trunk, and possibly few other branch strokes for refinement. The method automatically generates a 3D tree model including branches and leaves. Branches are synthesized by a growth engine from a small library of elementary subtrees that are pre-defined or built on the fly from the recovered visible branches.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/sigasia08_tree.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Projects/ImageBasedModeling/index.htm">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/amD6_i3MVZM" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/xiao_08_tog.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Image-based Facade Modeling</pubtit>
      <p><em>Jianxiong Xiao, Tian Fang, Ping Tan, Peng Zhao, Eyal Ofek, Long Quan</em><br />
<em>In ACM Transaction on Graphics(TOG) and Proc. of SIGGRAPH Asia (2008)</em></p>

      <p>We propose in this paper a semi-automatic image-based approach to fac¸ade modeling that uses images captured along streets and relies on structure from motion to recover camera positions and point clouds automatically as the initial stage for modeling. We start by considering a building fac¸ade as a flat rectangular plane or a developable surface with an associated texture image composited from the multiple visible images</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/sigasia08_street.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>







 | 

<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Projects/ImageBasedModeling/index.htm">
   <img src="https://gruvi.cs.sfu.ca/images/icons/project_page.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Project Page</a>





 | 

<a href="https://youtu.be/amD6_i3MVZM" target="_blank">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_youtube.png" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Video</a>



]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tan_08_pami.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Subpixel Photometric Stereo</pubtit>
      <p><em>Ping Tan, Stephen Lin, Long Quan</em><br />
<em>In IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI) (2008)</em></p>

      <p>In this paper, we propose a method to recover subpixel surface geometry by studying the relationship between the subpixel geometry and the reflectance properties of a surface. We first describe a generalized physically-based reflectance model that relates the distribution of surface normals inside each pixel area to its reflectance function. The distribution of surface normals can be computed from the reflectance functions recorded in photometric stereo images. A convexity measure of subpixel geometry structure is also recovered at each pixel, through an analysis of brightness attenuation due to shadowing. Then, we use the recovered distribution of surface normals and the surface convexity to infer subpixel geometric structures on a surface of homogeneous material by spatially arranging the normals among pixels at a higher resolution than that of the input image</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/pami07.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tan_08_tvcg.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Filtering and Rendering of Resolution Dependent Reflectance Models</pubtit>
      <p><em>Ping Tan, Stephen Lin, Long Quan, Baining Guo, Heung-Yeung Shum</em><br />
<em>In IEEE Transaction on Visualization and Computer Graphics (TVCG) (2008)</em></p>

      <p>In this work, we propose to represent this resolution-dependent reflectance as a mixture of multiple conventional reflectance models, and present a framework for efficiently rendering the reflectance effects of such mixture models over different resolutions. To rapidly determine reflectance at runtime with respect to resolution, we record the mixture model parameters at multiple resolution levels in mipmaps, and propose a technique to minimize aliasing in the filtering of these mipmaps. This framework can be applied to several widely used parametric reflectance models and can be implemented in graphics hardware for real-time processing, using a presented hardware-accelerated technique for non-linear filtering of mixture model parameters.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/tvcg07.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/shen_08_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Intrinsic Image Decomposition with Non-Local Texture Cues</pubtit>
      <p><em>Li Shen, Ping Tan, Stephen Lin</em><br />
<em>In IEEE Conference on Computer Vision and Patten Recognition (CVPR) (2008)</em></p>

      <p>We present a method for decomposing an image into its intrinsic reflectance and shading components. Different from previous work, our method examines texture information to obtain constraints on reflectance among pixels that may be distant from one another in the image. We observe that distinct points with the same intensity-normalized texture configuration generally have the same reflectance value. The separation of shading and reflectance components should thus be performed in a manner that guarantees these non-local constraints</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/cvpr08.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2007">2007</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/A Mixing Board Interface for Graphics and Visualization Applications.1.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Mixing Board Interface for Graphics and Visualization Applications</pubtit>
      <p><em>Crider M, Bergner S, Smyth TN, Möller T, Tory MK, Kirkpatrick E., Weiskopf D. </em><br />
<em>In Graphics Interface 2007 (2007)</em></p>

      <p>We use a haptically enhanced mixing board with a video projector as an interface to various data visualization tasks. We report results of an expert review with four participants, qualitatively evaluating the board for three different applications: dynamic queries (abstract task), parallel coordinates interface (multi-dimensional combinatorial search), and ExoVis (3D spatial navigation). Our investigation sought to determine the strengths of this physical input given its capability to facilitate bimanual interaction, constraint maintenance, tight coupling of input and output, and other features.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.512.5194&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Non-Rigid Spectral Correspondence of Triangle Meshes</pubtit>
      <p><em>Jain V, Zhang H, van Kaick O. </em><br />
<em>In International Journal on Shape Modeling (2007)</em></p>

      <p>We present an algorithm for finding a meaningful vertex-to-vertex correspondence between two triangle meshes, which is designed to handle general non-rigid transformations. Our algorithm operates on embeddings of the two shapes in the spectral domain so as to normalize them with respect to uniform scaling and rigid-body transformation. Invariance to shape bending is achieved by relying on approximate geodesic point proximities on a mesh to capture its shape. To deal with moderate stretching, we first raise the issue of “eigenmode switching” and discuss heuristics to bring the eigenmodes to alignment. For additional non-rigid discrepancies in the spectral embeddings, we propose to use non-rigid alignment via thin-plate splines. This is combined with a refinement step based on geodesic proximities to improve dense correspondence. We show empirically that our algorithm outperforms previous spectral methods, as well as schemes that compute correspondence in the spatial domain via non-rigid iterative closest points or the use of local shape descriptors, e.g., 3D shape context. Finally, to speed up our algorithm, we examine the effect of using subsampling and Nystrom method.</p>

      <p style="text-align: right;">

[









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/Adapting Geometric Attributes for Expression-Invariant 3{D} Face Recognition..png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Adapting Geometric Attributes for Expression-Invariant 3{D} Face Recognition</pubtit>
      <p><em>Li X, Zhang H. </em><br />
<em>In Shape Modeling International (2007)</em></p>

      <p>We investigate the use of multiple intrinsic geometric attributes, including angles, geodesic distances, and curvatures, for 3D face recognition, where each face is represented by a triangle mesh, preprocessed to possess a uni- form connectivity. As invariance to facial expressions holds the key to improving recognition performance, we propose to train for the component-wise weights to be applied to each individual attribute, as well as the weights used to combine the attributes, in order to adapt to expression variations. Using the eigenface approach based on the training results and a nearest neighbor classifier, we report recognition results on the expression-rich GavabDB face database and the well-known Notre Dame FRGC 3D database. We also perform a cross validation between the two databases.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/li_zhang_smi07.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/designastraversal.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Design as traversal and consequences&#58; An exploration tool for experimental designs</pubtit>
      <p><em>Jennings C.G, Kirkpatrick A.E. </em><br />
<em>In In Proceedings of Graphics Interface 2007 (2007)</em></p>

      <p>We present a design space explorer for the space of experimental designs. For many design problems, design decisions are determined by the consequences of the design rather than its elemental parts. To support this need, the explorer is constructed to make the designer aware of design-level options, provide a structured context for design, and provide feedback on the consequences of design decisions. We argue that this approach encourages the designer to consider a wider variety of designs, which will lead to more effective designs overall. In a qualitative study, experiment designers using the explorer were found to consider a wider variety of designs and more designs overall than they reported considering in their normal practice.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://cgjennings.ca/papers/jennings_kirkpatrick_gi07.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/spectralshapebasedret.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A Spectral Approach to Shape-Based Retrieval of Articulated 3D Models</pubtit>
      <p><em>Jain V, Zhang H. </em><br />
<em>In Computer Aided Design (2007)</em></p>

      <p>We present an approach for robust shape retrieval from databases containing articulated 3D models. Each shape is represented by the eigenvectors of an appropriately defined affinity matrix, forming a spectral embedding which achieves normalization against rigid-body transformations, uniform scaling, and shape articulation (i.e., bending).</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/jain_zhang_cad07.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/evaluatingUIstack.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Evaluating user interfaces for stack mode viewing</pubtit>
      <p><em>M. Atkins S, Kirkpatrick AE, Knight A. </em><br />
<em>In In Proceedings of the SPIEMedical Imaging 2007. (2007)</em></p>

      <p>Three interaction techniques were evaluated for scrolling stack mode displays of volumetric data. Two used a scroll-wheel mouse: one used only the wheel, while another used a “click and drag” technique for fast scrolling, leaving the wheel for fine adjustments. The third technique used a Shuttle Xpress jog wheel. In a within-subjects design, nine radiologists searched stacked images for simulated hyper-intense regions on brain, knee, and thigh MR studies. Dependent measures were speed, accuracy, navigation path, and user preference.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~stella/papers/2007/spie.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/spectralembedcont.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Mesh Segmentation via Spectral Embedding and Contour Analysis</pubtit>
      <p><em>Liu R, Zhang H. </em><br />
<em>In Computer Graphics Forum (Special Issue of Eurographics 2007) (2007)</em></p>

      <p>We propose a mesh segmentation algorithm via recursive bisection where at each step, a sub-mesh embedded in 3D is first spectrally projected into the plane and then a contour is extracted from the planar embedding. We rely on two operators to compute the projection: the well-known graph Laplacian and a geometric operator designed to emphasize concavity.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.6311&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/delaunayconst.jpeg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Delaunay Mesh Construction</pubtit>
      <p><em>Dyer R, Zhang H, Möller T.. </em><br />
<em>In Proc. of Eurographics Symposium on Geometry Processing (2007)</em></p>

      <p>We present algorithms to produce Delaunay meshes from arbitrary triangle meshes by edge flipping and geometrypreserving refinement and prove their correctness. In particular we show that edge flipping serves to reduce mesh surface area, and that a poorly sampled input mesh may yield unflippable edges necessitating refinement to ensure a Delaunay mesh output. Multiresolution Delaunay meshes can be obtained via constrained mesh decimation. We further examine the usefulness of trading off the geometry-preserving feature of our algorithm with the ability to create fewer triangles. We demonstrate the performance of our algorithms through several experiments.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.641.6759&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/contourcorrespondence.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Contour Correspondence via Ant Colony Optimization</pubtit>
      <p><em>van Kaick O, Hamarneh G, Zhang H, Wighton P. </em><br />
<em>In Proc. Pacific Graphics (2007)</em></p>

      <p>We formulate contour correspondence as a Quadratic Assignment Problem (QAP), incorporating proximity information. By maintaining the neighborhood relation between points this way, we show that better matching results are obtained in practice. We propose the first Ant Colony Optimization (ACO) algorithm specifically aimed at solving the QAP-based shape correspondence problem.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~haoz/pubs/vanKaick_aco.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/nonrigidcorres.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Non-Rigid Spectral Correspondence of Triangle Meshes</pubtit>
      <p><em>Jain V, Zhang H, van Kaick O. </em><br />
<em>In International Journal on Shape Modeling (2007)</em></p>

      <p>We present an algorithm for finding a meaningful vertex-to-vertex correspondence between two triangle meshes, which is designed to handle general non-rigid transformations. Our algorithm operates on embeddings of the two shapes in the spectral domain so as to normalize them with respect to uniform scaling and rigid-body transformation. Invari-ance to shape bending is achieved by relying on approximate geodesic point proximities on a mesh to capture its shape.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://pdfs.semanticscholar.org/9368/a86aa592e3e0d481b4cc7bb37a1c5cca24cf.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/voronal.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Voronoi-Delaunay Duality and Delaunay Meshes</pubtit>
      <p><em>Dyer R, Zhang H, Möller T.. </em><br />
<em>In ACM Symposium on Solid and Physical Modeling (2007)</em></p>

      <p>We define a Delaunay mesh to be a manifold triangle mesh whose edges form an intrinsic Delaunay triangulation or iDT of its vertices, where the triangulated domain is the piecewise flat mesh surface. We show that meshes constructed from a smooth surface by taking an iDT or a restricted Delaunay triangulation, do not in general yield a Delaunay mesh.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.2492&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2006">2006</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_zhang_sgp06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Nonobtuse Remeshing and Decimation</pubtit>
      <p><em>John Li and Hao Zhang</em><br />
<em>In in Proc. of Symposium on Geometry Processing (SGP) 2006 (short paper), pp.235-238 (2006)</em></p>

      <p>We propose an algorithm for guaranteed nonobtuse remeshing and nonobtuse mesh decimation. Our strategy for the remeshing problem is to first convert an input mesh, using a modified Marching Cubes algorithm, into a rough approximate mesh that is guaranteed to be nonobtuse. We then apply iterative “deform-to-fit” …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/li_zhang_sgp06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/olson_zhang_eg06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Silhouette Extraction in Hough Space</pubtit>
      <p><em>Matt Olson and Hao Zhang</em><br />
<em>In Computer Graphics Forum (Special Issue on Eurographics 2006), Volume 25, Number 3, pp. 273-282 (2006)</em></p>

      <p>We present an efficient silhouette extractor for triangle meshes under perspective projection in the Hough space. The more favorable point distribution in Hough space allows us to obtain significant performance gains over the traditional dual-space based techniques …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/olson_zhang_eg06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/jain_zhang_gmp06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Shape-Based Retrieval of Articulated 3D Models Using Spectral Embedding</pubtit>
      <p><em>Varun Jain and Hao Zhang</em><br />
<em>In in Proceeding of Geometric Modeling and Processing 2006, pp. 295-308 (2006)</em></p>

      <p>We present a spectral approach for robust shape retrieval from databases containing articulated 3D shapes. We show absolute improvement in retrieval performance when conventional shape descriptors are used in the spectral domain on the McGill database of articulated 3D shapes. We also propose a simple eigenvalue-based descriptor …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/jain_zhang_gmp06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_zhang_gmp06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Spectral Sequencing based on Graph Distance</pubtit>
      <p><em>Rong Liu, Hao Zhang, and Oliver van Kaick</em><br />
<em>In in Proceeding of Geometric Modeling and Processing 2006 (poster paper), pp. 632-638 (2006)</em></p>

      <p>In this paper, we treat optimal mesh layout generation as a problem of preserving graph distances and propose to use the subdominant eigenvector of a kernel (affinity) matrix for sequencing …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/liu_zhang_gmp06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/liu_et_al_cgi06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Subsampling for Efficient Spectral Mesh Processing</pubtit>
      <p><em>Rong Liu, Varun Jain, and Hao Zhang</em><br />
<em>In in Proceeding of Computer Graphics International 2006, Lecture Notes in Computer Science 4035, H.-P. Seidel, T. Nishita, and Q. Peng, Eds., pp. 172-184 (2006)</em></p>

      <p>We apply Nystrom method, a sub-sampling and reconstruction technique, to speed up spectral mesh processing. We first relate this method to Kernel Principal Component Analysis (KPCA). This enables us to derive a novel measure in the form of a matrix trace, based soly on sampled data, to quantify the quality of Nystrom approximation …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/liu_cgi06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/jain_zhang_smi06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Robust 3D Shape Correspondence in the Spectral Domain</pubtit>
      <p><em>Varun Jain and Hao Zhang</em><br />
<em>In in Proceeding of International Conference on Shape Modeling and Applications (SMI) 2006, pp. 118-129 (2006)</em></p>

      <p>We present an algorithm for finding a meaningful correspondence between two 3D shapes given as triangle meshes. Our algorithm operates on embeddings of the two shapes in the spectral domain so as to normalize them with respect to uniform scaling, rigid-body transformation and shape bending …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/jain_zhang_smi06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/clements_zhang_smi06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Minimum Ratio Contours on Surface Meshes</pubtit>
      <p><em>Andrew Clements and Hao Zhang</em><br />
<em>In in Proceeding of International Conference on Shape Modeling and Applications (SMI) 2006, pp. 26-37 (2006)</em></p>

      <p>We present a novel approach for discretely optimizing contours on the surface of a triangle mesh. This is achieved through the use of a minimum ratio cycle (MRC) algorithm, where we compute a contour having the minimal ratio between a novel contour energy term and the length of the contour …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/clements_zhang_smi06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/li_zhang_crv06.jpg" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Expression-Invariant Face Recognition with Expression Classification</pubtit>
      <p><em>Xiaoxing Li, Greg Mori, and Hao Zhang</em><br />
<em>In in Proceeding of Canadian Conference on Computer and Robot Vision (CRV) 2006, pp. 77-83 (2006)</em></p>

      <p>Facial expression, which changes face geometry, usually has an adverse effect on the performance of a face recognition system. On the other hand, face geometry is a useful cue for recognition. Taking these into account, we utilize the idea of separating geometry and texture information in a face image …</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~haoz/pubs/li_crv06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/quan_06_tog.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Image-based Plant Modeling</pubtit>
      <p><em>Long Quan, Ping Tan, Gang Zeng, Lu Yuan, Jingdong Wang, Sing Bing Kang</em><br />
<em>In ACM Transaction on Graphics(TOG) and Proc. of SIGGRAPH (2006)</em></p>

      <p>In this paper, we propose a semi-automatic technique for modeling plants directly from images. Our image-based approach has the distinct advantage that the resulting model inherits the realistic shape and complexity of a real plant. We designed our modeling system to be interactive, automating the process of shape recovery while relying on the user to provide simple hints on segmentation. Segmentation is performed in both image and 3D spaces, allowing the user to easily visualize its effect immediately</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/siggraph06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tan_06_cvpr.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Separation of Highlight Reflections from Textured Surfaces</pubtit>
      <p><em>Ping Tan, Stephen Lin, Long Quan</em><br />
<em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2006)</em></p>

      <p>We present a method for separating highlight reflections on textured surfaces. In contrast to previous techniques that use diffuse color information from outside the highlight area to constrain the solution, the proposed method further capitalizes on the spatial distributions of colors to resolve ambiguities in separation that often arise in real images. For highlight pixels in which a clear-cut separation cannot be determined from color space analysis, we evaluate possible separation solutions based on their consistency with diffuse texture characteristics outside the highlight</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/cvpr06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/tan_06_eccv.PNG" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Resolution-Enhanced Photometric Stereo</pubtit>
      <p><em>Ping Tan, Stephen Lin, Long Quan</em><br />
<em>In European Conference on Computer Vision(ECCV) (2006)</em></p>

      <p>In this work, we propose a technique for resolution-enhanced photometric stereo, in which surface geometry is computed at a resolution higher than that of the input images. To achieve this goal, our method first utilizes a generalized reflectance model to recover the distribution of surface normals inside each pixel. This normal distribution is then used to infer sub-pixel structures on a surface of uniform material by spatially arranging the normals among pixels at a higher resolution according to a minimum description length criterion on 3D textons over the surface</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.cs.sfu.ca/~pingtan/Papers/eccv06.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<h3 id="2005">2005</h3>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/practitionalrendering.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A practical approach to Spectral Volume Rendering</pubtit>
      <p><em>Bergner S, Möller T, Tory M, Drew MS. </em><br />
<em>In IEEE Trans. on Vis. and Comp. Graphics (2005)</em></p>

      <p>To make a spectral representation of color practicable for volume rendering, a new low-dimensional subspace method is used to act as the carrier of spectral information. With that model, spectral light material interaction can be integrated into existing volume rendering methods at almost no penalty. In addition, slow rendering methods can profit from the new technique of postillumination-generating spectral images in real-time for arbitrary light spectra under a fixed viewpoint. Thus, the capability of spectral rendering to create distinct impressions of a scene under different lighting conditions is established as a method of real-time interaction.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://www.cs.sfu.ca/~mark/ftp/Tvcg05/tvcg05.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/robust2d.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Robust 2{D} Shape Correspondence using Geodesic Shape Context</pubtit>
      <p><em>Jain V, Zhang H. </em><br />
<em>In Proc. of Pacific Graphics (2005)</em></p>

      <p>We present a robust shape descriptor for points along a 2D contour, based on the curvature distribution collected over bins arranged geodesically along the contour. Convolution, binning and hysteresis thresholding of curvatures are applied to render the descriptor more robust against noise and non-rigid shape deformation. Once the shape descriptor is computed for every point or feature vertex of the two shapes to be matched, a one-to-one correspondence can be quickly established through best matching of the descriptors, aided by a proximity heuristic.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://pdfs.semanticscholar.org/0ad8/2b222a7af60c6defee815294d90d96697065.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/usingphysicbased.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Using the Physics-Based Rendering Toolkit (pbrt) for Medical Reconstruction</pubtit>
      <p><em>Bergner S, Dagenais E, Möller T, Celler A. </em><br />
<em>In IEEE Nuclear Science Symposium and Medical Imaging Conference Record (2005)</em></p>

      <p>In this paper we cast the problem of tomography in the realm of computer graphics. By using PBRT (physically based rendering toolkit) we create a scripting environment that simplifies the programming of tomography algorithms such as Maximum-Likelihood Expectation Maximization (ML-EM) or Simultaneous Algebraic Reconstruction Technique (SART, a deviant of ART).</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://eprints.cs.univie.ac.at/4977/1/2005_-_using_the_physical-based.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/genericframework.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>A generic software framework for the gpu volume rendering pipeline</pubtit>
      <p><em>Vollrath JE, Weiskopf D, Ertl T. </em><br />
<em>In In Proc. Vision, Modeling, and Visualization (2005)</em></p>

      <p>We use volume graphics for realistic image synthesis taking into account aspects of visual perception by means of real-time high dynamic range tone mapping. We propose a software architecture that embeds the volume rendering pipeline by using object-oriented design patterns, layers, and the concept of a shared application state.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9935&amp;rep=rep1&amp;type=pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/particleandtexture.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Particle and Texture Based Spatiotemporal Visualization of Time-Dependent Vector Fields</pubtit>
      <p><em>Weiskopf D, al et. </em><br />
<em>In  (2005)</em></p>

      <p>We propose a hybrid particle and texture based approach for the visualization of time-dependent vector fields. The underlying space-time framework builds a dense vector field representation in a two-step process: 1) particle-based forward integration of trajectories in spacetime for temporal coherence, and 2) texture-based convolution along another set of paths through the spacetime for spatially correlated patterns.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://pdfs.semanticscholar.org/b011/e267632ead118dc9ae10c0d62d16e417fa24.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/spatiotemporal.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Spatiotemporal-Chromatic Structure of Natural Scenes</pubtit>
      <p><em>Bergner S, Drew MS. </em><br />
<em>In Proc. of IEEE Int. Conf. on Img. Proc (2005)</em></p>

      <p>We investigate the implications of a unified spatiotemporal-chromatic basis for compression and reconstruction of image sequences. Different adaptive methods (PCA and ICA) are applied to generate basis functions. While typically such bases with spatial and temporal extent are investigated in terms of their correspondence to human visual perception, here we are interested in their applicability to multimedia encoding. The performance of the extracted spatiotemporal-chromatic patch bases is evaluated in terms of quality of reconstruction with respect to their potential for data compression. The results discussed here are intended to provide another path towards perceptually-based encoding of visual data by examining the interplay of chromatic features with spatiotemporal ones in data reduction.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="https://pdfs.semanticscholar.org/67d2/7161d19c442eddf0e4a7b4c904ffbee11f7a.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/einstein.gif" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Visualization in the Einstein Year 2005&#58; A Case Study on Explanatory and Illustrative Visualization of Relativity and Astrophysics</pubtit>
      <p><em>Weiskopf D, Borchers M, Ertl T, Falk M, Fechtig O, Frank R, Grave F, King A, Kraus U, Muller T et al.. </em><br />
<em>In Visualization Conference, IEEE (2005)</em></p>

      <p>In this application paper, we report on over fifteen years of experience with relativistic and astrophysical visualization, which has been culminating in a substantial engagement for visualization in the Einstein Year 2005 - the 100/sup th/ anniversary of Einstein’s publications on special relativity, the photoelectric effect, and Brownian motion. This paper focuses on explanatory and illustrative visualizations used to communicate aspects of the difficult theories of special and general relativity, their geometric structure, and of the related fields of cosmology and astrophysics. We discuss visualization strategies, motivated by physics education and didactics of mathematics, and describe what kind of visualization methods have proven to be useful for different types of media, such as still images in popular-science magazines, film contributions to TV shows, oral presentations, or interactive museum installations.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.vis.uni-stuttgart.de/uploads/tx_vispublications/vis05-weiskopf-relativity.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<div class="row">

  <div class="col-sm-12 clearfix">
    <div class="well clearfix">

      <p><img src="https://gruvi.cs.sfu.ca/images/pubpic/texturebasedvis.png" class="img-responsive" width="15%" style="float: left; min-width: 80px; min-height: 80px;" /></p>

      <pubtit>Texture-Based Visualization of Uncertainty in Flow Fields</pubtit>
      <p><em>Daniel RBotchen, Ertl T. </em><br />
<em>In In Proceedings of IEEE Visualization 2005 (2005)</em></p>

      <p>We present two novel texture-based techniques to visualize uncertainty in time-dependent 2D flow fields. Both methods use semi-Lagrangian texture advection to show flow direction by streaklines and convey uncertainty by blurring these streaklines. The first approach applies a cross advection perpendicular to the flow direction. The second method employs isotropic diffusion that can be implemented by Gaussian filtering. Both methods are derived from a generic filtering process that is incorporated into the traditional texture advection pipeline. Our visualization methods allow for a continuous change of the density of flow representation by adapting the density of particle injection. All methods can be mapped to efficient GPU implementations. Therefore, the user can interactively control all important characteristics of the system like particle density, error influence, or dye injection to create meaningful illustrations of the underlying uncertainty. Even though there are many sources of uncertainties, we focus on uncertainty that occurs during data acquisition. We demonstrate the usefulness of our methods for the example of real-world fluid flow data measured with the particle image velocimetry (PIV) technique. Furthermore, we compare these techniques with an adapted multi-frequency noise approach.</p>

      <p style="text-align: right;">

[


<a target="_blank" href="http://www.vis.uni-stuttgart.de/~weiskopf/publications/vis05_uncertainty.pdf">
   <img src="https://gruvi.cs.sfu.ca/images/icons/icon_pdf.gif" style="cursor: pointer; margin-bottom: 0px; margin-top: 0px; margin-right: 2px; border-radius:2%;" height="16" border="0" width="16" />
Paper</a>









]

</p>

    </div>
  </div>

</div>

<p>   </p>


</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-8">
			
		  <p>&copy 2017 GrUVi website implemented with <a href="https://jekyllrb.com">Jekyll</a>, based on <a href="http://www.allanlab.org/"> &copy Allan Lab's website</a></p>

		   <p>  </p><p>
            		  
            
		</div>
		<div class="col-sm-4">
GrUVi Lab, TASC-1,<br />
School of Computing Science<br />
Simon Fraser University<br />
8888 University Drive<br />
Burnaby, B.C<br />
Canada V5A 1S6<br />
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://gruvi.cs.sfu.ca/js/bootstrap.min.js"></script>
<script src="/js/trunk8.js" type="text/javascript"></script>


  </body>

</html>
