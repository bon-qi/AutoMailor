{
    "41": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zielon.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Wojciech Zielonka</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://sites.google.com/site/bolkartt/\" target=\"_blank\" rel=\"noopener noreferrer\">Timo Bolkart</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/mica/\">MICA: Towards Metrical Reconstruction of Human Faces</a></h2>\n\t\t\n\n\t\t<!--<p>Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely...</p>-->\n\t\t<p>Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context. Thus, we present MICA, a novel metrical face reconstruction method that combines face recognition with supervised face shape learning.</p>\n\t\t<span class=\"post-date\">2022, Jul 04&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2204.06607\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=vzzEbvv08VA\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/mica/zielonka2022mica.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "40": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/yawar_siddiqui/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Yawar Siddiqui</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://fangchangma.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Fangchang Ma</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://shanqi.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Qi Shan</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/texturify/\">Texturify: Generating Textures on 3D Shape Surfaces</a></h2>\n\t\t\n\n\t\t<!--<p>Texture cues on 3D objects are key to compelling visual representations, with the possibility to create high visual fidelity with inherent spatial consistency across different...</p>-->\n\t\t<p>Texturify learns to generate geometry-aware textures for untextured collections of 3D objects. Our method trains from only a collection of images and a collection of untextured shapes, which are both often available, without requiring any explicit 3D color supervision or shape-image correspondence. Textures are created directly on the surface of a given 3D shape, enabling generation of high-quality, compelling textured 3D shapes.</p>\n\t\t<span class=\"post-date\">2022, Jul 04&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2204.02411\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=M5OU_fiD3Jk\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/texturify/siddiqui2022texturify.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "39": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://hci.iwr.uni-heidelberg.de/vislearn/people\" target=\"_blank\" rel=\"noopener noreferrer\">Philip-William Grassal</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ncs.is.mpg.de/person/mprinzler\" target=\"_blank\" rel=\"noopener noreferrer\">Malte Prinzler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://titus-leistner.de/pages/about-me.html\" target=\"_blank\" rel=\"noopener noreferrer\">Titus Leistner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/\" target=\"_blank\" rel=\"noopener noreferrer\">Carsten Rother</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/neuralhead/\">Neural Head Avatars from Monocular RGB Videos</a></h2>\n\t\t\n\n\t\t<!--<p>We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be...</p>-->\n\t\t<p>We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar using a deep neural network. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture.</p>\n\t\t<span class=\"post-date\">2022, Mar 22&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2112.01554.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=S7LY1DtJCsI\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/neuralhead/grassal2022neuralhead.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "38": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://xyyhw.top/\" target=\"_blank\" rel=\"noopener noreferrer\">Hongwei Yi</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://is.mpg.de/person/chuang2\" target=\"_blank\" rel=\"noopener noreferrer\">Chun-Hao P. Huang</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ps.is.mpg.de/~dtzionas\" target=\"_blank\" rel=\"noopener noreferrer\">Dimitrios Tzionas</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ps.is.mpg.de/person/mkocabas\" target=\"_blank\" rel=\"noopener noreferrer\">Muhammed Kocabas</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ps.is.mpg.de/person/mhassan\" target=\"_blank\" rel=\"noopener noreferrer\">Mohamed Hassan</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html\" target=\"_blank\" rel=\"noopener noreferrer\">Siyu Tang</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ps.is.mpg.de/~black\" target=\"_blank\" rel=\"noopener noreferrer\">Michael J. Black</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/mover/\">Mover: Human-Aware Object Placement for Visual Environment Reconstruction</a></h2>\n\t\t\n\n\t\t<!--<p>Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information...</p>-->\n\t\t<p>We demonstrate that human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout.</p>\n\t\t<span class=\"post-date\">2022, Mar 22&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2203.03609.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=n_ejtZarRaM\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/mover/yi2022mover.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "37": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/dejan_azinovic/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Dejan Azinovic</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.ricardomartinbrualla.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Ricardo Martin-Brualla</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.danbgoldman.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Dan B Goldman</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/rgbdnerf/\">Neural RGB-D Surface Reconstruction</a></h2>\n\t\t\n\n\t\t<!--<p>In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance...</p>-->\n\t\t<p>We demonstrate how depth measurements can be incorporated into the neural radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone.</p>\n\t\t<span class=\"post-date\">2022, Mar 22&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://dazinovic.github.io/neural-rgbd-surface-reconstruction/\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=iWuSowPsC3g\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/rgbdnerf/azinovic2021rgbdnerf.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "36": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~atewari/\" target=\"_blank\" rel=\"noopener noreferrer\">Ayush Tewari</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://bmild.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ben Mildenhall</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://pratulsrinivasan.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Pratul_Srinivasan</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~tretschk/\" target=\"_blank\" rel=\"noopener noreferrer\">Edgar Tretschk</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://yifita.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Yifan Wanf</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://christophlassner.de/\" target=\"_blank\" rel=\"noopener noreferrer\">Christoph Lassner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://vsitzmann.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Vincent Sitzmann</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.ricardomartinbrualla.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Ricardo Martin-Brualla</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\"/>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.cs.cmu.edu/~tsimon/\" target=\"_blank\" rel=\"noopener noreferrer\">Tomas Simon</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://jonbarron.info/\" target=\"_blank\" rel=\"noopener noreferrer\">Jonathan T. Barron</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://stanford.edu/~gordonwz/\" target=\"_blank\" rel=\"noopener noreferrer\">Gordon Wetzstein</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~golyanik/\" target=\"_blank\" rel=\"noopener noreferrer\">Vladislav Golyanik</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/advancedneuralrenderingstar/\">Advances in Neural Rendering</a></h2>\n\t\t\n\n\t\t<!--<p>Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of...</p>-->\n\t\t<p>This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene.</p>\n\t\t<span class=\"post-date\">2022, Jan 01&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2111.05849.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/advancedneuralrenderingstar/tewari2021advances.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "35": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/aljaz_bozic/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Aljaz Bozic</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://pablorpalafox.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Pablo Palafox</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/transfusion/\">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</a></h2>\n\t\t\n\n\t\t<!--<p>We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that...</p>-->\n\t\t<p>We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation.</p>\n\t\t<span class=\"post-date\">2021, Jul 12&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2107.02191.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=LIpTKYfKSqw\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/transfusion/bozic2021transfusion.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "34": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/andrei_burov/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Andrei Burov</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/dsfn/\">Dynamic Surface Function Networks for Clothed Human Bodies</a></h2>\n\t\t\n\n\t\t<!--<p>We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model...</p>-->\n\t\t<p>We present a novel method for temporal coherent reconstruction and tracking of clothed humans using dynamic surface function networks which can be trained with a monocular RGB-D sequence.</p>\n\t\t<span class=\"post-date\">2021, Apr 12&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2104.03978.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=4wbSi9Sqdm4\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/dsfn/burov2021dsfn.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "33": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://pablorpalafox.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Pablo Palafox</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/aljaz_bozic/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Aljaz Bozic</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/npm/\">Neural Parametric Models for 3D Deformable Shapes</a></h2>\n\t\t\n\n\t\t<!--<p>Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the...</p>-->\n\t\t<p>We propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints.</p>\n\t\t<span class=\"post-date\">2021, Apr 12&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2104.00702.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=muZXXgkkMPY\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/npm/palafox2021npm.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "32": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/yawar_siddiqui/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Yawar Siddiqui</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://fangchangma.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Fangchang Ma</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://shanqi.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Qi Shan</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/retrievalfuse/\">RetrievalFuse: Neural 3D Scene Reconstruction with a Database</a></h2>\n\t\t\n\n\t\t<!--<p>3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In...</p>-->\n\t\t<p>In this paper, we introduce a new method that directly leverages scene geometry from the training database. It is able to reconstruct a high quality scene from pointcloud or low-res inputs using geometry patches from a database and an attention-based refinement.</p>\n\t\t<span class=\"post-date\">2021, Apr 12&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"http://arxiv.org/pdf/2104.00024\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=HbsUU0YODqE\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/retrievalfuse/siddiqui2021retrievalfuse.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "31": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/guy_gafni/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Guy Gafni</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/nerface/\">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</a></h2>\n\t\t\n\n\t\t<!--<p>We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a...</p>-->\n\t\t<p>We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup.</p>\n\t\t<span class=\"post-date\">2021, Mar 03&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2012.03065.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=sUjULZ0gDpU\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/nerface/gafni2020nerface.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "30": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/aljaz_bozic/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Aljaz Bozic</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://pablorpalafox.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Pablo Palafox</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/neuraldeformationgraphs/\">Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</a></h2>\n\t\t\n\n\t\t<!--<p>We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep...</p>-->\n\t\t<p>We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking.</p>\n\t\t<span class=\"post-date\">2021, Mar 03&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2012.01451.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=30HQk2Av6ds\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/neuraldeformationgraphs/bozic2020neuraldeformationgraphs.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "29": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/yawar_siddiqui/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Yawar Siddiqui</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://github.com/julienvalentin\" target=\"_blank\" rel=\"noopener noreferrer\">Julien Valentin</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/spsg/\">SPSG: Self-Supervised Photometric Scene Generation from RGB-D Scans</a></h2>\n\t\t\n\n\t\t<!--<p>We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to...</p>-->\n\t\t<p>We present a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion.</p>\n\t\t<span class=\"post-date\">2021, Mar 02&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2006.14660.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=1cj962m9zqo\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/spsg/dai2020spsg.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "28": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/davide_cozzolino.html\" target=\"_blank\" rel=\"noopener noreferrer\">Davide Cozzolino</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/andreas_roessler/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas R&#246;ssler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/verdoliva.html\" target=\"_blank\" rel=\"noopener noreferrer\">Luisa Verdoilva</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/spoc/\">SpoC: Spoofing Camera Fingerprints</a></h2>\n\t\t\n\n\t\t<!--<p>Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap &#8220;rich&#8221;...</p>-->\n\t\t<p>In this paper, we challenge forensic forgery detectors that are based on camera fingerprints (i.e., traces of the image capturing and processing pipeline) to gain insights into their vulnerabilities.</p>\n\t\t<span class=\"post-date\">2021, Jan 26&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1911.12069.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/spoc/cozzolino2021spoc.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "27": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/davide_cozzolino.html\" target=\"_blank\" rel=\"noopener noreferrer\">Davide Cozzolino</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/andreas_roessler/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas R&#246;ssler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/verdoliva.html\" target=\"_blank\" rel=\"noopener noreferrer\">Luisa Verdoilva</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/idreveal/\">ID-Reveal: Identity-aware DeepFake Video Detection</a></h2>\n\t\t\n\n\t\t<!--<p>State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question &#8216;is this video real or fake?&#8217;. Given that their training is...</p>-->\n\t\t<p>We introduce the DeepFake detection approach ID-Reveal, which is based on learned temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy.</p>\n\t\t<span class=\"post-date\">2021, Jan 01&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2012.02512.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=RsFxsOLvRdY\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/idreveal/cozzolino2020idreveal.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "26": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/aljaz_bozic/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Aljaz Bozic</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://pablorpalafox.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Pablo Palafox</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/neuraltracking/\">Neural Non-Rigid Tracking</a></h2>\n\t\t\n\n\t\t<!--<p>We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. Given two input RGB-D frames of a non-rigidly moving object, we...</p>-->\n\t\t<p>We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. By enabling gradient back-propagation through a non-rigid as-rigid-as-possible optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking.</p>\n\t\t<span class=\"post-date\">2020, Sep 29&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/abs/2006.13240\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=Kj_P-lHtWkU\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/neuraltracking/bozic2020neuraltracking.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "25": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://people.mpi-inf.mpg.de/~elgharib/\" target=\"_blank\" rel=\"noopener noreferrer\">Mohamed Elgharib</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://gvv.mpi-inf.mpg.de/GVV_Team.html\" target=\"_blank\" rel=\"noopener noreferrer\">Mohit Mendiratta</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~hpseidel/english.html\" target=\"_blank\" rel=\"noopener noreferrer\">Hans-Peter Seidel</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~atewari/\" target=\"_blank\" rel=\"noopener noreferrer\">Ayush Tewari</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~golyanik/\" target=\"_blank\" rel=\"noopener noreferrer\">Vladislav Golyanik</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/egochat/\">Egocentric Videoconferencing</a></h2>\n\t\t\n\n\t\t<!--<p>We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays...</p>-->\n\t\t<p>We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices.</p>\n\t\t<span class=\"post-date\">2020, Sep 28&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"http://gvv.mpi-inf.mpg.de/projects/EgoChat/data/Main.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t<span class=\"post-video-link\"><a href=\"http://gvv.mpi-inf.mpg.de/projects/EgoChat/data/Main.mp4\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/egochat/elgharib2020egochat.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "24": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.in.tum.de/cg/people/weiss/\" target=\"_blank\" rel=\"noopener noreferrer\">Sebastian Weiss</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\">Mustafa Isik</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.in.tum.de/cg/cover-page/\" target=\"_blank\" rel=\"noopener noreferrer\">R&#252;diger Westermann</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/learningadaptivesampling/\">Learning Adaptive Sampling and Reconstruction for Volume Visualization</a></h2>\n\t\t\n\n\t\t<!--<p>A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the...</p>-->\n\t\t<p>We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples.</p>\n\t\t<span class=\"post-date\">2020, Jul 22&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2007.10093.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/learningadaptivesampling/weiss2020learningadaptivesampling.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "23": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://hassanhaija.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Hassan Abu Alhaija</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://sivakm.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Siva Karthik Mustikovela</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.cvlibs.net/\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas Geiger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/\" target=\"_blank\" rel=\"noopener noreferrer\">Carsten Rother</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/intrinsicautoencoder/\">Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition</a></h2>\n\t\t\n\n\t\t<!--<p>Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation...</p>-->\n\t\t<p>We propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties.</p>\n\t\t<span class=\"post-date\">2020, Jun 23&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2006.16011.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/intrinsicautoencoder/alhaija2020intrinsicae.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "22": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~atewari/\" target=\"_blank\" rel=\"noopener noreferrer\">Ayush Tewari</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.ohadf.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Ohad Fried</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://vsitzmann.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Vincent Sitzmann</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\"/>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.kalyans.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Kalyan Sunkavalli</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.ricardomartinbrualla.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Ricardo Martin-Brualla</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.cs.cmu.edu/~tsimon/\" target=\"_blank\" rel=\"noopener noreferrer\">Tomas Simon</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://jsaragih.org/Home_Page.html\" target=\"_blank\" rel=\"noopener noreferrer\">Jason Saragih</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://research.google/people/106687/\" target=\"_blank\" rel=\"noopener noreferrer\">Rohit K Pandey</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.seanfanello.it/\" target=\"_blank\" rel=\"noopener noreferrer\">Sean Fanello</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://stanford.edu/~gordonwz/\" target=\"_blank\" rel=\"noopener noreferrer\">Gordon Wetzstein</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.csail.mit.edu/junyanz/\" target=\"_blank\" rel=\"noopener noreferrer\">Jun-Yan Zhu</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://graphics.stanford.edu/~maneesh/\" target=\"_blank\" rel=\"noopener noreferrer\">Maneesh Agrawala</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://research.adobe.com/person/eli-shechtman/\" target=\"_blank\" rel=\"noopener noreferrer\">Eli Shechtman</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.danbgoldman.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Dan B Goldman</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/neuralrenderingstar/\">State of the Art on Neural Rendering</a></h2>\n\t\t\n\n\t\t<!--<p>Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted...</p>-->\n\t\t<p>Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. This state-of-the-art report summarizes the recent trends and applications of neural rendering.</p>\n\t\t<span class=\"post-date\">2020, Apr 08&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2004.03805.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/neuralrenderingstar/tewari2020neuralrendering.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "21": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://stanford.edu/~jingweih/\" target=\"_blank\" rel=\"noopener noreferrer\">Jingwei Huang</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://angeladai.github.io\" target=\"_blank\" rel=\"noopener noreferrer\">Angela Dai</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://abhijitkundu.info/\" target=\"_blank\" rel=\"noopener noreferrer\">Abhijit Kundu</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.maxjiang.ml/\" target=\"_blank\" rel=\"noopener noreferrer\">Chiyu 'Max' Jiang</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://geometry.stanford.edu/member/guibas/\" target=\"_blank\" rel=\"noopener noreferrer\">Leonidas Guibas</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.cs.princeton.edu/~funk/\" target=\"_blank\" rel=\"noopener noreferrer\">Thomas Funkhouser</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/advtex/\">Adversarial Texture Optimization from RGB-D Scans</a></h2>\n\t\t\n\n\t\t<!--<p>Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera...</p>-->\n\t\t<p>We present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors.</p>\n\t\t<span class=\"post-date\">2020, Mar 19&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"http://stanford.edu/~jingweih/papers/advtex/supp/paper.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=52xlRn0ESek\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/advtex/huang2020advtex.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "20": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/ignor/\">Image-guided Neural Object Rendering</a></h2>\n\t\t\n\n\t\t<!--<p>We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to...</p>-->\n\t\t<p>We propose a new learning-based novel view synthesis approach for scanned objects that is trained based on a set of multi-view images, where we directly train a deep neural network to synthesize a view-dependent image of an object.</p>\n\t\t<span class=\"post-date\">2020, Jan 15&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1811.10720.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=S9yhekwyAiA\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/ignor/thies2020ignor.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "19": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://people.mpi-inf.mpg.de/~elgharib/\" target=\"_blank\" rel=\"noopener noreferrer\">Mohamed Elgharib</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~atewari/\" target=\"_blank\" rel=\"noopener noreferrer\">Ayush Tewari</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/neural-voice-puppetry/\">Neural Voice Puppetry: <br/> Audio-driven Facial Reenactment</a></h2>\n\t\t\n\n\t\t<!--<p>We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we...</p>-->\n\t\t<p>Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input.</p>\n\t\t<span class=\"post-date\">2020, Jan 08&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1912.05566.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=iFJuskGpWJw\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/neural-voice-puppetry/thies2020nvp.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "18": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/andreas_roessler/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas R&#246;ssler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/davide_cozzolino.html\" target=\"_blank\" rel=\"noopener noreferrer\">Davide Cozzolino</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/verdoliva.html\" target=\"_blank\" rel=\"noopener noreferrer\">Luisa Verdoilva</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.cs1.tf.fau.de/christian-riess/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Riess</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/faceforensics++/\">FaceForensics++: <br/> Learning to Detect Manipulated Facial Images</a></h2>\n\t\t\n\n\t\t<!--<p>The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society....</p>-->\n\t\t<p>In this paper, we examine the realism of state-of-the-art facial image manipulation methods, and how difficult it is to detect them - either automatically or by humans. In particular, we create a datasets that is focused on DeepFakes, Face2Face, FaceSwap, and Neural Textures as prominent representatives for facial manipulations.</p>\n\t\t<span class=\"post-date\">2019, Aug 26&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1901.08971.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=iwJocDEnL3E\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/faceforensics++/roessler2019faceforensicspp.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "17": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t<h2 class=\"post-title\"><a href=\"/posts/deferred-neural-rendering/\">Deferred Neural Rendering: <br/> Image Synthesis using Neural Textures</a></h2>\n\t\t\n\n\t\t<!--<p>The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we...</p>-->\n\t\t<p>Deferred Neural Rendering is a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable Neural Textures. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect.</p>\n\t\t<span class=\"post-date\">2019, Apr 28&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1904.12356.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=ofVgAEb1FiE\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/deferred-neural-rendering/thies2019neural.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "16": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://vsitzmann.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Vincent Sitzmann</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.cs.princeton.edu/~fheide/\" target=\"_blank\" rel=\"noopener noreferrer\">Felix Heide</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://stanford.edu/~gordonwz/\" target=\"_blank\" rel=\"noopener noreferrer\">Gordon Wetzstein</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/deepvoxels/\">DeepVoxels: Learning Persistent 3D Feature Embeddings</a></h2>\n\t\t\n\n\t\t<!--<p>In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To...</p>-->\n\t\t<p>In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D object without having to explicitly model its geometry.</p>\n\t\t<span class=\"post-date\">2019, Apr 11&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1812.01024.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=HM_WsZhoGXw\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/deepvoxels/sitzmann2019deepvoxels.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "15": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/acm-research-highlight/\">Research Highlight: Face2Face</a></h2>\n\t\t\n\n\t\t<!--<p>Face2Face is an approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video...</p>-->\n\t\t<p>Research highlight of the Face2Face approach featured on the cover of Communications of the ACM in January 2019. Face2Face is an approach for real-time facial reenactment of a monocular target video. The method had significant impact in the research community and far beyond; it won several wards, e.g., Siggraph ETech Best in Show Award, it was featured in countless media articles, e.g., NYT, WSJ, Spiegel, etc., and it had a massive reach on social media with millions of views.</p>\n\t\t<span class=\"post-date\">2019, Jan 01&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://dl.acm.org/citation.cfm?id=3301004.3292039&amp;coll=portal&amp;dl=ACM\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=KUjn6SrNbSo\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/acm-research-highlight/thies2018face.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "14": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/davide_cozzolino.html\" target=\"_blank\" rel=\"noopener noreferrer\">Davide Cozzolino</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/andreas_roessler/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas R&#246;ssler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.cs1.tf.fau.de/christian-riess/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Riess</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/verdoliva.html\" target=\"_blank\" rel=\"noopener noreferrer\">Luisa Verdoilva</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/forensictransfer/\">ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery Detection</a></h2>\n\t\t\n\n\t\t<!--<p>Distinguishing fakes from real images is becoming increasingly difficult as new sophisticated image manipulation approaches come out by the day. Convolutional neural networks (CNN) show...</p>-->\n\t\t<p>ForensicTransfer tackles two challenges in multimedia forensics. First, we devise a learning-based forensic detector which adapts well to new domains, i.e., novel manipulation methods. Second we handle scenarios where only a handful of fake examples are available during training.</p>\n\t\t<span class=\"post-date\">2018, Dec 06&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1812.02510.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/forensictransfer/cozzolino2018forensictransfer.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "13": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~hyeongwoo/\" target=\"_blank\" rel=\"noopener noreferrer\">Hyeongwoo Kim</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~pgarrido/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Pablo Garrido</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~atewari/\" target=\"_blank\" rel=\"noopener noreferrer\">Ayush Tewari</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~wxu/\" target=\"_blank\" rel=\"noopener noreferrer\">Weipeng Xu</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ptrckprz.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Patrick Perez</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://richardt.name/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Richardt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/deepvideo/\">Deep Video Portraits</a></h2>\n\t\t\n\n\t\t<!--<p>We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted...</p>-->\n\t\t<p>Our novel approach enables photo-realistic re-animation of portrait videos using only an input video. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor.</p>\n\t\t<span class=\"post-date\">2018, May 29&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1805.11714.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=qc5P2bvfl44\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/deepvideo/kim2018deepvideo.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "12": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/headon/\">HeadOn: Real-time Reenactment of Human Portrait Videos</a></h2>\n\t\t\n\n\t\t<!--<p>We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and...</p>-->\n\t\t<p>HeadOn is the first real-time reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. A novel reenactment algorithm employs this proxy to map the captured motion from the source to the target actor.</p>\n\t\t<span class=\"post-date\">2018, May 29&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1805.11729.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=UploR8HlEeo\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/headon/thies2018headon.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "11": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~hyeongwoo/\" target=\"_blank\" rel=\"noopener noreferrer\">Hyeongwoo Kim</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~atewari/\" target=\"_blank\" rel=\"noopener noreferrer\">Ayush Tewari</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://richardt.name/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Richardt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/inversefacenet/\">InverseFaceNet: Deep Monocular Inverse Face Rendering</a></h2>\n\t\t\n\n\t\t<!--<p>We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input...</p>-->\n\t\t<p>We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. This enables advanced real-time editing of facial imagery, such as appearance editing and relighting.</p>\n\t\t<span class=\"post-date\">2018, May 16&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/abs/1703.10956\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t<span class=\"post-video-link\"><a href=\"http://vimeo.com/262874743?dnt=1\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/inversefacenet/kim2018inversefacenet.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "10": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://la.disneyresearch.com/people/derek-bradley/\" target=\"_blank\" rel=\"noopener noreferrer\">Derek Bradley</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~pgarrido/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Pablo Garrido</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://la.disneyresearch.com/people/thabo-beeler/\" target=\"_blank\" rel=\"noopener noreferrer\">Thabo Beeler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://ptrckprz.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Patrick Perez</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/facestar/\">State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications</a></h2>\n\t\t\n\n\t\t<!--<p>The computer graphics and vision communities have dedicated long standing efforts in building computerized tools for reconstructing, tracking, and analyzing human faces based on visual...</p>-->\n\t\t<p>This report summarizes recent trends in monocular facial performance capture and discusses its applications, which range from performance-based animation to real-time facial reenactment. We focus on methods where the central task is to recover and track a three dimensional model of the human face using optimization-based reconstruction algorithms.</p>\n\t\t<span class=\"post-date\">2018, Apr 24&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"http://zollhoefer.com/papers/EG18_FaceSTAR/paper.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/facestar/zollhoefer2018facestar.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "9": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://niessnerlab.org/members/andreas_roessler/profile.html\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas R&#246;ssler</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/davide_cozzolino.html\" target=\"_blank\" rel=\"noopener noreferrer\">Davide Cozzolino</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www.grip.unina.it/people/userprofile/verdoliva.html\" target=\"_blank\" rel=\"noopener noreferrer\">Luisa Verdoilva</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.cs1.tf.fau.de/christian-riess/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Riess</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/faceforensics/\">FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces</a></h2>\n\t\t\n\n\t\t<!--<p>FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video...</p>-->\n\t\t<p>In this paper, we introduce FaceForensics, a large scale video dataset consisting of 1004 videos with more than 500000 frames, altered with Face2Face, that can be used for forgery detection and to train generative refinement methods.</p>\n\t\t<span class=\"post-date\">2018, Mar 24&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1803.09179.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=Tle7YaPkO_k\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/faceforensics/roessler2018faceforensics.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "8": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/facevr/\">FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in Virtual Reality</a></h2>\n\t\t\n\n\t\t<!--<p>We propose FaceVR, a novel image-based method that enables video teleconferencing in VR based on self-reenactment. State-of-the-art face tracking methods in the VR context are...</p>-->\n\t\t<p>We propose FaceVR, a novel image-based method that enables video teleconferencing in VR based on self-reenactment. The key component of FaceVR is a robust algorithm to perform real-time facial motion capture of an actor who is wearing a head-mounted display (HMD).</p>\n\t\t<span class=\"post-date\">2018, Mar 21&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/abs/1610.03151\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=ubOOiYoV3LA\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/facevr/thies2018facevr.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "7": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/dissertation/\">Dissertation: Face2Face - Facial Reenactment</a></h2>\n\t\t\n\n\t\t<!--<p>In this dissertation we show our advances in the field of 3D reconstruction of human faces using commodity hardware. Beside the reconstruction of the facial...</p>-->\n\t\t<p>This dissertation summarizes the work in the field of markerless motion tracking, face reconstruction and its applications. Especially, it shows real-time facial reenactment that enables the transfer of facial expressions from one video to another video.</p>\n\t\t<span class=\"post-date\">2017, Oct 16&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">2 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://diglib.eg.org/bitstream/handle/10.2312/2631994/dissertation_justus_thies.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/dissertation/thies2017diss.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "6": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/christian-siegl/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Siegl</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/vanessa-lange/\" target=\"_blank\" rel=\"noopener noreferrer\">Vanessa Lange</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/frank-bauer/\" target=\"_blank\" rel=\"noopener noreferrer\">Frank Bauer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/faceforge/\">FaceForge: Markerless Non-Rigid Face Multi-Projection Mapping</a></h2>\n\t\t\n\n\t\t<!--<p>Recent publications and art performances demonstrate amazing results using projection mapping. To our knowledge, there exists no multi-projection system that can project onto non-rigid target...</p>-->\n\t\t<p>In this paper, we introduce FaceForge, a multi-projection mapping system that is able to alter the appearance of a non-rigidly moving human face in real time.</p>\n\t\t<span class=\"post-date\">2017, Oct 10&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/1904.12356.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=zEza8E2R0Hc\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/faceforge/siegl2017faceforge.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "5": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/face2face/\">Face2Face: Real-time Face Capture and Reenactment of RGB Videos</a></h2>\n\t\t\n\n\t\t<!--<p>We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular...</p>-->\n\t\t<p>We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion.</p>\n\t\t<span class=\"post-date\">2016, Mar 23&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://arxiv.org/pdf/2007.14808.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=KUjn6SrNbSo\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/face2face/thies2016face.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "4": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www5.cs.fau.de/en/our-team/berger-martin/projects/\" target=\"_blank\" rel=\"noopener noreferrer\">Martin Berger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\">Kerstin M&#252;ller</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www5.cs.fau.de/en/our-team/aichert-andre\" target=\"_blank\" rel=\"noopener noreferrer\">Andre Aichert</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www5.cs.fau.de/en/our-team/unberath-mathias/projects/\" target=\"_blank\" rel=\"noopener noreferrer\">Mathias Unberath</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\">Jang-Hwan Choi</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\">Rebecca Fahrig</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"http://www5.cs.fau.de/en/our-team/maier-andreas\" target=\"_blank\" rel=\"noopener noreferrer\">Andreas Maier</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/knee-joint-alignment/\">Marker-free Motion Correction in Weight-Bearing Cone-Beam CT of the Knee Joint</a></h2>\n\t\t\n\n\t\t<!--<p>Weight-bearing imaging of the knee joint in a standing position poses additional requirements for the image reconstruction algorithm. In contrast to supine scans, patient motion...</p>-->\n\t\t<p>We present image-based motion estimation and compensation in weightbearing cone-beam computed tomography of the knee joint</p>\n\t\t<span class=\"post-date\">2016, Feb 22&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://www5.informatik.uni-erlangen.de/Forschung/Publikationen/2016/Berger16-MMC.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/knee-joint-alignment/berger2016.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "3": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/christian-siegl/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Siegl</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/matteo-colaianni/\" target=\"_blank\" rel=\"noopener noreferrer\">Matteo Colaianni</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/lucas-thies/\" target=\"_blank\" rel=\"noopener noreferrer\">Lucas Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://scholar.google.com/citations?user=hkCVqYkAAAAJ&amp;hl=de\" target=\"_blank\" rel=\"noopener noreferrer\">Shahram Izadi</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/frank-bauer/\" target=\"_blank\" rel=\"noopener noreferrer\">Frank Bauer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/projection-mapping/\">Real-Time Pixel Luminance Optimization for Dynamic Multi-Projection Mapping</a></h2>\n\t\t\n\n\t\t<!--<p>Using projection mapping enables us to bring virtual worlds into shared physical spaces. In this paper, we present a novel, adaptable and real-time projection mapping...</p>-->\n\t\t<p>Using projection mapping enables us to bring virtual worlds into shared physical spaces. In this paper, we present a novel, adaptable and real-time projection mapping system, which supports multiple projectors and high quality rendering of dynamic content on surfaces of complex geometrical shape. Our system allows for smooth blending across multiple projectors using a new optimization framework that simulates the diffuse direct light transport of the physical world to continuously adapt the color output of each projector pixel.</p>\n\t\t<span class=\"post-date\">2015, Sep 14&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://niessnerlab.org/papers/2015/10projectionmapping/siegl2015projectionmapping.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t<span class=\"post-video-link\"><a href=\"http://vimeo.com/139184967?dnt=1\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/projection-mapping/siegl2015projectionmapping.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "2": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://niessnerlab.org\" target=\"_blank\" rel=\"noopener noreferrer\">Matthias Nie&#223;ner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\">Levi Valgaerts</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://people.mpi-inf.mpg.de/~theobalt/\" target=\"_blank\" rel=\"noopener noreferrer\">Christian Theobalt</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/face-rgbd/\">Real-time Expression Transfer for Facial Reenactment</a></h2>\n\t\t\n\n\t\t<!--<p>We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video,...</p>-->\n\t\t<p>We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor.</p>\n\t\t<span class=\"post-date\">2015, Aug 27&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://vcai.mpi-inf.mpg.de/projects/MZ/Papers/SGASIA2015_RR/paper.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=pHbxLkQeOoI\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/face-rgbd/thies2015realtime.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
    "1": "<div class=\"post-content\">\n\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://zollhoefer.com\" target=\"_blank\" rel=\"noopener noreferrer\">Michael Zollh&#246;fer</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t<span class=\"post-author-bold\"><a href=\"/\">Justus Thies</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/matteo-colaianni/\" target=\"_blank\" rel=\"noopener noreferrer\">Matteo Colaianni</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/marc-stamminger/\" target=\"_blank\" rel=\"noopener noreferrer\">Marc Stamminger</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\t\t\t\t\n\t\t\t<span class=\"post-author\"><a href=\"https://www.lgdv.tf.fau.de/person/guenther-greiner/\" target=\"_blank\" rel=\"noopener noreferrer\">G&#252;nther Greiner</a>&#8195;</span>\n\t\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t<h2 class=\"post-title\"><a href=\"/posts/interactive-head-reconstruction/\">Interactive Model-based Reconstruction of the Human Head using an RGB-D Sensor</a></h2>\n\t\t\n\n\t\t<!--<p>We present a novel method for the interactive markerless reconstruction of human heads using a single commodity RGB-D sensor. Our entire reconstruction pipeline is implemented...</p>-->\n\t\t<p>We present a novel method for the interactive markerless reconstruction of human heads using a single commodity RGB&#8208;D sensor. Our entire reconstruction pipeline is implemented on the graphics processing unit and allows to obtain high&#8208;quality reconstructions of the human head using an interactive and intuitive reconstruction paradigm.</p>\n\t\t<span class=\"post-date\">2014, Apr 28&#160;&#160;&#160;&#8212;&#160;</span>\n\t\t<span class=\"post-words\">1 minute read&#160;&#160;&#160;&#160;</span>\n\n\t\t\n\t\t\t<span class=\"post-paper-link\"><a href=\"https://web.stanford.edu/~zollhoef/papers/CASA2014_Face/paper.pdf\" target=\"_blank\">[Paper]</a>&#160;</span>\n\t\t\t\n\t\t\n\t\t\t<span class=\"post-video-link\"><a href=\"https://www.youtube.com/watch?v=p7UIF1Mw5GI\" target=\"_blank\">[Video]</a>&#160;</span>\n\t\t\n\t\t\n\t\t\t<span class=\"post-paper-bibtex\"><a href=\"/posts/interactive-head-reconstruction/zollhoefer2014Head.bib\" target=\"_blank\">[Bibtex]</a>&#160;</span>\n\t\t\t\n\t\t</div>\n\t",
}