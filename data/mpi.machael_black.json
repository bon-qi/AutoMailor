{
    "50": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hassan-sig-2023\">Synthesizing Physical Character-scene Interactions</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, Guo, Y., Wang, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Fidler, S., Peng, X. B.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>SIGGRAPH Conf. Track</em>, August 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27602\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27602\" href=\"#abstractContent27602\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27602\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMovement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent's movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character's movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://xbpeng.github.io/projects/InterPhys/2023_InterPhys.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/q3hyQdaElQQ\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27602/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hassan-sig-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability.: https://ps.is.mpg.de/publications/hassan-sig-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hassan-sig-2023&amp;amp;title=Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability. &amp;amp;summary=Synthesizing Physical Character-scene Interactions&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability. %20https://ps.is.mpg.de/publications/hassan-sig-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability. &amp;amp;body=https://ps.is.mpg.de/publications/hassan-sig-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "49": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hood-cvpr-2023\">HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tGrigorev, A., Thomaszewski, B., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 16965-16974, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27608\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27608\" href=\"#abstractContent27608\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27608\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.07242\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dolorousrtur.github.io/hood/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/supplemental/Grigorev_HOOD_Hierarchical_Graphs_CVPR_2023_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27608/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hood-cvpr-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods.: https://ps.is.mpg.de/publications/hood-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hood-cvpr-2023&amp;amp;title=We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods. &amp;amp;summary={HOOD}: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods. %20https://ps.is.mpg.de/publications/hood-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods. &amp;amp;body=https://ps.is.mpg.de/publications/hood-cvpr-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "48": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/econ2023xiu\">ECON: Explicit Clothed humans Optimized via Normal integration</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Highlight Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, Cao, X., <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 512-523, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27519\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27519\" href=\"#abstractContent27519\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27519\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://econ.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.07422\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/Yuliang/ECON\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/YuliangXiu/ECON\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=5PEd_p90kS0\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno?usp=sharing\"><i class=\"fa fa-file-o\"/>  Colab</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://econ.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27519/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/econ2023xiu&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin.: https://ps.is.mpg.de/publications/econ2023xiu&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/econ2023xiu&amp;amp;title=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. &amp;amp;summary=ECON: Explicit Clothed humans Optimized via Normal integration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. %20https://ps.is.mpg.de/publications/econ2023xiu&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. &amp;amp;body=https://ps.is.mpg.de/publications/econ2023xiu&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "47": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/car2023liao\">High-Fidelity Clothed Avatar Reconstruction from a Single Image</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tLiao, T., Zhang, X., <span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, Liu, X., Qi, G., Zhang, Y., Wang, X., Zhu, X., Lei, Z.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8662-8672, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27646\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27646\" href=\"#abstractContent27646\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27646\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThis paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes. \n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/TingtingLiao/CAR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2304.03903\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tingtingliao.github.io/CAR/\"><i class=\"fa fa-github-square\"/>  Homepage</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/sVk6dAtC1kQ\"><i class=\"fa fa-file-video-o\"/>  Youtube</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tingtingliao.github.io/CAR/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27646/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/car2023liao&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes. : https://ps.is.mpg.de/publications/car2023liao&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/car2023liao&amp;amp;title=This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.  &amp;amp;summary=High-Fidelity Clothed Avatar Reconstruction from a Single Image&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.  %20https://ps.is.mpg.de/publications/car2023liao&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.  &amp;amp;body=https://ps.is.mpg.de/publications/car2023liao&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "46": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ruegg_2023_cvpr\">BITE: Beyond Priors for Improved Three-D Dog Pose Estimation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tR&#252;egg, N., <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8867-8876, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27609\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27609\" href=\"#abstractContent27609\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27609\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Ruegg_BITE_Beyond_Priors_for_Improved_Three-D_Dog_Pose_Estimation_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ruegg_BITE_Beyond_Priors_CVPR_2023_supplemental.zip\"><i class=\"fa fa-file-archive-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://bite.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27609/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de.: https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/ruegg_2023_cvpr&amp;amp;title=We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de. &amp;amp;summary={BITE}: Beyond Priors for Improved Three-{D} Dog Pose Estimation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "45": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/black_2023_cvpr\">BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Highlight Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ppatel\">Patel, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8726-8737, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27604\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27604\" href=\"#abstractContent27604\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27604\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://bedlam.is.tuebingen.mpg.de/media/upload/BEDLAM_CVPR2023.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://bedlam.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/html/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.html\"><i class=\"fa fa-file-o\"/>  CVF</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/pixelite1201/BEDLAM\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27604/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/black_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/black_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/black_2023_cvpr&amp;amp;title=We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. &amp;amp;summary={BEDLAM}: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/black_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/black_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "44": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/insta\">Instant Volumetric Head Avatars</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/wzielonka\">Zielonka, W.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27515\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27515\" href=\"#abstractContent27515\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27515\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2211.12499v2.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zielon.github.io/insta/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/HOgaeWTih7Q\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/INSTA\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/metrical-tracker\"><i class=\"fa fa-github-square\"/>  face tracker code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://keeper.mpdl.mpg.de/d/5ea4d2c300e9444a8b0b/\"><i class=\"fa fa-file-o\"/>  dataset</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27515/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/insta&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.: https://ps.is.mpg.de/publications/insta&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/insta&amp;amp;title=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. &amp;amp;summary=Instant Volumetric Head Avatars&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. %20https://ps.is.mpg.de/publications/insta&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. &amp;amp;body=https://ps.is.mpg.de/publications/insta&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "43": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/yi2022mime\">MIME: Human-Aware 3D Scene Generation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lhering\">Hering, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 12965-12976, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27611\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27611\" href=\"#abstractContent27611\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27611\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGenerating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mime.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.04360\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27611/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi2022mime&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de.: https://ps.is.mpg.de/publications/yi2022mime&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi2022mime&amp;amp;title=Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. &amp;amp;summary={MIME}: Human-Aware {3D} Scene Generation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/yi2022mime&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/yi2022mime&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "42": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/arctic\">ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/fzicong\">Fan, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Kaufmann, M., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 12943-12954, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27511\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27511\" href=\"#abstractContent27511\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27511\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arctic.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zc-alexfan/arctic\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/arctic/arctic_april_24.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2204.13662\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=bvMm8gfFbZ8\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arctic.is.tue.mpg.de\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27511/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/arctic&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC.: https://ps.is.mpg.de/publications/arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/arctic&amp;amp;title=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. &amp;amp;summary={ARCTIC}: A Dataset for Dexterous Bimanual Hand-Object Manipulation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. %20https://ps.is.mpg.de/publications/arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. &amp;amp;body=https://ps.is.mpg.de/publications/arctic&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "41": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/zheng_2023_cvpr\">PointAvatar: Deformable Point-Based Head Avatars From Videos</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yzheng\">Zheng, Y.</a></span>, Yifan, W., Wetzstein, G., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 21057-21067, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27605\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27605\" href=\"#abstractContent27605\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27605\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2212.08377.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zhengyuf.github.io/PointAvatar/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zhengyuf/pointavatar\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/wll_XtgpU7U\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27605/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.: https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/zheng_2023_cvpr&amp;amp;title=The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. &amp;amp;summary={PointAvatar}: Deformable Point-Based Head Avatars From Videos&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. %20https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. &amp;amp;body=https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "40": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/yi2023talkshow\">Generating Holistic 3D Human Motion from Speech</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, Liang, H., Liu, Y., Cao, Q., <span class=\"default-link-ul\"><a href=\"/person/ydwen\">Wen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, Tao, D., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 469-480, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27612\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27612\" href=\"#abstractContent27612\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27612\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThis work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://talkshow.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yhw-yhw/SHOW\"><i class=\"fa fa-github-square\"/>  SHOW code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yhw-yhw/TalkSHOW\"><i class=\"fa fa-github-square\"/>  TalkSHOW code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.04420\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"\"><i class=\"fa fa-file-o\"/>  </a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27612/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi2023talkshow&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de.: https://ps.is.mpg.de/publications/yi2023talkshow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi2023talkshow&amp;amp;title=This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de. &amp;amp;summary=Generating Holistic {3D} Human Motion from Speech&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/yi2023talkshow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/yi2023talkshow&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "39": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/sun-cvpr-2023\">TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSun, Y., Bao, Q., Liu, W., Mei, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8856-8866, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27643\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27643\" href=\"#abstractContent27643\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27643\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAlthough the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new \"maps\" to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/supplemental/Sun_TRACE_5D_Temporal_CVPR_2023_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.yusun.work/TRACE/TRACE.html\"><i class=\"fa fa-file-o\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/l8aLHDXWQRw\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27643/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/sun-cvpr-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes.: https://ps.is.mpg.de/publications/sun-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/sun-cvpr-2023&amp;amp;title=Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes. &amp;amp;summary={TRACE}: {5D} Temporal Regression of Avatars With Dynamic Cameras in {3D} Environments&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes. %20https://ps.is.mpg.de/publications/sun-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/sun-cvpr-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "38": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/forte23-cvpr-sgnify\">Reconstructing Signing Avatars from Video Using Linguistic Priors</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/Forte\">Forte, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/pkulits\">Kulits, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kjk\">Kuchenbecker, K. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 12791-12801, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27581\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27581\" href=\"#abstractContent27581\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27581\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tSign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify's 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27581/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de.: https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&amp;amp;title=Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de. &amp;amp;summary=Reconstructing Signing Avatars from Video Using Linguistic Priors&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "37": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tripathi2023arctic\">3D Human Pose Estimation via Intuitive Physics</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lmueller2\">M&#252;ller, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 4713-4725, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27512\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27512\" href=\"#abstractContent27512\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27512\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ipman.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://moyo.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Moyo Dataset</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ipman.is.tue.mpg.de\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27512/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tripathi2023arctic&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.: https://ps.is.mpg.de/publications/tripathi2023arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tripathi2023arctic&amp;amp;title=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. &amp;amp;summary={3D} Human Pose Estimation via Intuitive Physics&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. %20https://ps.is.mpg.de/publications/tripathi2023arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. &amp;amp;body=https://ps.is.mpg.de/publications/tripathi2023arctic&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "36": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hot2023chen\">Detecting Human-Object Contact in Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ychen2\">Chen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sdwivedi\">Dwivedi, S. K.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 17100-17110, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27520\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27520\" href=\"#abstractContent27520\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27520\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (\"Human-Object conTact\"), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://hot.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2303.03373\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yixchen/HOT\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27520/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hot2023chen&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability.: https://ps.is.mpg.de/publications/hot2023chen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hot2023chen&amp;amp;title=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. &amp;amp;summary=Detecting Human-Object Contact in Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. %20https://ps.is.mpg.de/publications/hot2023chen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. &amp;amp;body=https://ps.is.mpg.de/publications/hot2023chen&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "35": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tempeh\">Instant Multi-View Head Capture through Learnable Registration</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tli\">Li, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 768-779, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27582\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27582\" href=\"#abstractContent27582\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27582\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tExisting methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tempeh.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/AolpvKpmjEw\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/711/CVPR2023_Multiview_Face_Capture.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/712/CVPR2023_Multiview_Face_Capture_supmat.pdf\"><i class=\"fa fa-file-pdf-o\"/>  sup. mat.</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27582/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tempeh&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de.: https://ps.is.mpg.de/publications/tempeh&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tempeh&amp;amp;title=Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. &amp;amp;summary=Instant Multi-View Head Capture through Learnable Registration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/tempeh&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/tempeh&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "34": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dai_2023_cvpr\">SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tYudi, D., Yitai, L., Xiping, L., Chenglu, W., Lan, X., <span class=\"default-link-ul\"><a href=\"/person/hyi\">Hongwei, Y.</a></span>, Siqi, S., Yuexin, M., Cheng, W.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 682-692, CVF, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27645\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27645\" href=\"#abstractContent27645\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27645\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.lidarhumanmotion.net/sloper4d/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.lidarhumanmotion.net/data-sloper4d/\"><i class=\"fa fa-file-o\"/>  dataset</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/climbingdaily/SLOPER4D\"><i class=\"fa fa-github-square\"/>  codebase</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_SLOPER4D_A_Scene-Aware_Dataset_for_Global_4D_Human_Pose_Estimation_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2303.09095.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27645/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dai_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D.: https://ps.is.mpg.de/publications/dai_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dai_2023_cvpr&amp;amp;title=We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D. &amp;amp;summary=SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D. %20https://ps.is.mpg.de/publications/dai_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D. &amp;amp;body=https://ps.is.mpg.de/publications/dai_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "33": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870\">MeshDiffusion: Score-based Generative 3D Mesh Modeling</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/zliu2\">Liu, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Nowrouzezahrai, D., Paull, L., <span class=\"default-link-ul\"><a href=\"/person/wliu\">Liu, W.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Proceedings of the Eleventh International Conference on Learning Representations (ICLR)</em>, May 2023 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27571\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27571\" href=\"#abstractContent27571\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27571\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openreview.net/pdf?id=0cpM2ApF9p6\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27571/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.: https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&amp;amp;title=We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks. &amp;amp;summary=MeshDiffusion: Score-based Generative 3D Mesh Modeling&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks. %20https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks. &amp;amp;body=https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "32": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/chen-pami-2023\">Fast-SNARF: A Fast Deformer for Articulated Neural Fields</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, Jiang, T., Song, J., Rietmann, M., <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</em>,  pages: 1-15, April 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27599\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27599\" href=\"#abstractContent27599\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27599\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tNeural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.cvlibs.net/publications/Chen2023PAMI.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10112633\"><i class=\"fa fa-file-o\"/>  publisher site</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/xuchen-ethz/fast-snarf\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1109/TPAMI.2023.3271569\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27599/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/chen-pami-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans.: https://ps.is.mpg.de/publications/chen-pami-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/chen-pami-2023&amp;amp;title=Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans. &amp;amp;summary={Fast-SNARF}: A Fast Deformer for Articulated Neural Fields&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans. %20https://ps.is.mpg.de/publications/chen-pami-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans. &amp;amp;body=https://ps.is.mpg.de/publications/chen-pami-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "31": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/barc-ijcv-2023\">BARC: Breed-Augmented Regression Using Classification for 3D Dog Reconstruction from Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Int. J. of Comp. Vis. (IJCV)</em>, April 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27537\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27537\" href=\"#abstractContent27537\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27537\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/s11263-023-01780-3\"><i class=\"fa fa-file-o\"/>  On-line</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://trebuchet.public.springernature.app/get_content/2e120f6c-1f70-48d4-9629-9356f52e2230\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/s11263-023-01780-3\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27537/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/barc-ijcv-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/barc-ijcv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/barc-ijcv-2023&amp;amp;title=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. &amp;amp;summary={BARC}: Breed-Augmented Regression Using Classification for {3D} Dog Reconstruction from Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/barc-ijcv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/barc-ijcv-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "30": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/saini-ral-2023\">SmartMocap: Joint Estimation of Human and Camera Motion Using Uncalibrated RGB Cameras</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, 8(6):3206-3213, 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27600\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27600\" href=\"#abstractContent27600\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27600\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMarkerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10093047\"><i class=\"fa fa-file-o\"/>  publisher site</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2023.3264743\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27600/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/saini-ral-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera.: https://ps.is.mpg.de/publications/saini-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/saini-ral-2023&amp;amp;title=Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. &amp;amp;summary={SmartMocap}: Joint Estimation of Human and Camera Motion Using Uncalibrated {RGB} Cameras&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. %20https://ps.is.mpg.de/publications/saini-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. &amp;amp;body=https://ps.is.mpg.de/publications/saini-ral-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "29": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/price-ral-2023\">Viewpoint-Driven Formation Control of Airships for Cooperative Target Tracking</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, 8(6):3653-3660, 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27601\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27601\" href=\"#abstractContent27601\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27601\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tFor tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10092932\"><i class=\"fa fa-file-o\"/>  publisher's site</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2023.3264727\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27601/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/price-ral-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship.: https://ps.is.mpg.de/publications/price-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/price-ral-2023&amp;amp;title=For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship. &amp;amp;summary=Viewpoint-Driven Formation Control of Airships for Cooperative Target Tracking&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship. %20https://ps.is.mpg.de/publications/price-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship. &amp;amp;body=https://ps.is.mpg.de/publications/price-ral-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "28": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c\">SCARF: Capturing and Animation of Body and Clothing from Monocular Video</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, Pollefeys, M., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>SIGGRAPH Asia 2022 Conference Papers</em>,  pages: 9, SA&#8217;22, December 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27367\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27367\" href=\"#abstractContent27367\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27367\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://yfeng95.github.io/scarf/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yfeng95/SCARF\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2210.01868.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1145/3550469.3555423\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27367/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.: https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&amp;amp;title=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;summary={SCARF}: Capturing and Animation of Body and Clothing from Monocular Video&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. %20https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;body=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "27": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/choutas-thesis-2022\">Reconstructing Expressive 3D Humans from RGB Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tETH Zurich, Max Planck Institute for Intelligent Systems and ETH Zurich, December 2022 <small class=\"text-muted\">(thesis)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27431\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27431\" href=\"#abstractContent27431\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27431\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo interact with our environment, we need to adapt our body posture&#13;\nand grasp objects with our hands. During a conversation our facial expressions&#13;\nand hand gestures convey important non-verbal cues about our&#13;\nemotional state and intentions towards our fellow speakers. Thus, modeling&#13;\nand capturing 3D full-body shape and pose, hand articulation and facial&#13;\nexpressions are necessary to create realistic human avatars for augmented&#13;\nand virtual reality. This is a complex task, due to the large number of&#13;\ndegrees of freedom for articulation, body shape variance, occlusions from&#13;\nobjects and self-occlusions from body parts, e.g. crossing our hands, and&#13;\nsubject appearance. The community has thus far relied on expensive and&#13;\ncumbersome equipment, such as multi-view cameras or motion capture&#13;\nmarkers, to capture the 3D human body. While this approach is effective,&#13;\nit is limited to a small number of subjects and indoor scenarios. Using&#13;\nmonocular RGB cameras would greatly simplify the avatar creation process,&#13;\nthanks to their lower cost and ease of use. These advantages come at a price&#13;\nthough, since RGB capture methods need to deal with occlusions, perspective&#13;\nambiguity and large variations in subject appearance, in addition to&#13;\nall the challenges posed by full-body capture. In an attempt to simplify the&#13;\nproblem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;\nthe body, face and hands with distinct methods using part-specific&#13;\ndatasets and benchmarks. However, the hands and face constrain the body&#13;\nand vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;\netc.; the divide-and-conquer approach can not utilize this constraint.&#13;\nIn this thesis, we aim to reconstruct the full 3D human body, using only&#13;\nreadily accessible monocular RGB images. In a first step, we introduce a&#13;\nparametric 3D body model, called SMPL-X, that can represent full-body&#13;\nshape and pose, hand articulation and facial expression. Next, we present&#13;\nan iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;\n2D image keypoints. While SMPLify-X can produce plausible results if&#13;\nthe 2D observations are sufficiently reliable, it is slow and susceptible&#13;\nto initialization. To overcome these limitations, we introduce ExPose, a&#13;\nneural network regressor, that predicts SMPL-X parameters from an image&#13;\nusing body-driven attention, i.e. by zooming in on the hands and face,&#13;\nafter predicting the body. From the zoomed-in part images, dedicated&#13;\npart networks predict the hand and face parameters. ExPose combines&#13;\nthe independent body, hand, and face estimates by trusting them equally.&#13;\nThis approach though does not fully exploit the correlation between parts&#13;\nand fails in the presence of challenges such as occlusion or motion blur.&#13;\nThus, we need a better mechanism to aggregate information from the full&#13;\nbody and part images. PIXIE uses neural networks called moderators that&#13;\nlearn to fuse information from these two image sets before predicting the&#13;\nfinal part parameters. Overall, the addition of the hands and face leads to&#13;\nnoticeably more natural and expressive reconstructions.&#13;\nCreating high fidelity avatars from RGB images requires accurate estimation&#13;\nof 3D body shape. Although existing methods are effective at&#13;\npredicting body pose, they struggle with body shape. We identify the lack&#13;\nof proper training data as the cause. To overcome this obstacle, we propose&#13;\nto collect internet images from fashion models websites, together with&#13;\nanthropometric measurements. At the same time, we ask human annotators&#13;\nto rate images and meshes according to a pre-defined set of linguistic attributes.&#13;\nWe then define mappings between measurements, linguistic shape&#13;\nattributes and 3D body shape. Equipped with these mappings, we train a&#13;\nneural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;\nfrom a single RGB image. We observe that existing 3D shape benchmarks&#13;\nlack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;\nbenchmark, Human Bodies in the Wild (HBW), which contains images of&#13;\nhumans and their corresponding 3D ground-truth body shape. SHAPY&#13;\nshows how we can overcome the lack of in-the-wild images with 3D shape&#13;\nannotations through easy-to-obtain anthropometric measurements and linguistic&#13;\nshape attributes.&#13;\nRegressors that estimate 3D model parameters are robust and accurate,&#13;\nbut often fail to tightly fit the observations. Optimization-based approaches&#13;\ntightly fit the data, by minimizing an energy function composed of a data&#13;\nterm that penalizes deviations from the observations and priors that encode&#13;\nour knowledge of the problem. Finding the balance between these terms&#13;\nand implementing a performant version of the solver is a time-consuming&#13;\nand non-trivial task. Machine-learned continuous optimizers combine the&#13;\nbenefits of both regression and optimization approaches. They learn the&#13;\npriors directly from data, avoiding the need for hand-crafted heuristics and&#13;\nloss term balancing, and benefit from optimized neural network frameworks&#13;\nfor fast inference. Inspired from the classic Levenberg-Marquardt&#13;\nalgorithm, we propose a neural optimizer that outperforms classic optimization,&#13;\nregression and hybrid optimization-regression approaches. Our&#13;\nproposed update rule uses a weighted combination of gradient descent&#13;\nand a network-predicted update. To show the versatility of the proposed&#13;\nmethod, we apply it on three other problems, namely full body estimation&#13;\nfrom (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;\ndevice and (iii) face tracking from dense 2D landmarks. Our method can&#13;\neasily be applied to new model fitting problems and offers a competitive&#13;\nalternative to well-tuned traditional model fitting pipelines, both in terms&#13;\nof accuracy and speed.&#13;\nTo summarize, we propose a new and richer representation of the human&#13;\nbody, SMPL-X, that is able to jointly model the 3D human body pose&#13;\nand shape, facial expressions and hand articulation. We propose methods,&#13;\nSMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;\nmonocular RGB images, progressively improving the accuracy and realism&#13;\nof the predictions. To further improve reconstruction fidelity, we demonstrate&#13;\nhow we can use easy-to-collect internet data and human annotations&#13;\nto overcome the lack of 3D shape data and train a model, SHAPY, that&#13;\npredicts accurate 3D body shape from a single RGB image. Finally, we&#13;\npropose a flexible learnable update rule for parametric human model fitting&#13;\nthat outperforms both classic optimization and neural network approaches.&#13;\nThis approach is easily applicable to a variety of problems, unlocking new&#13;\napplications in AR/VR scenarios.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.research-collection.ethz.ch/handle/20.500.11850/590562\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27431/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/choutas-thesis-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios.: https://ps.is.mpg.de/publications/choutas-thesis-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/choutas-thesis-2022&amp;amp;title=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. &amp;amp;summary=Reconstructing Expressive {3D} Humans from {RGB} Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. %20https://ps.is.mpg.de/publications/choutas-thesis-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. &amp;amp;body=https://ps.is.mpg.de/publications/choutas-thesis-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "26": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/immvirtreal_sbehrens_2022\">How immersive virtual reality can become a key tool to advance research and psychotherapy of eating and weight disorders</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/smoelbert\">Behrens, S. C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, Keizer, A., Giel, K. E.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Frontiers in Psychiatry</em>, 13, November 2022 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27490\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27490\" href=\"#abstractContent27490\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27490\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tImmersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.3389/fpsyt.2022.1011620\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27490/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed.: https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&amp;amp;title=Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed. &amp;amp;summary=How immersive virtual reality can become a key tool to advance research and psychotherapy of eating and weight disorders&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed. %20https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed. &amp;amp;body=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "25": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dart2022\">DART: Articulated Hand Model with Diverse Accessories and Rich Textures</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tGao, D., <span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, Li, K., Yang, L., Wang, F., Zhang, P., Zhang, B., Lu, C., Tan, P.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)</em>, November 2022 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27270\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27270\" href=\"#abstractContent27270\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27270\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dart2022.github.io/\"><i class=\"fa fa-github-square\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/DART2022/DARTset\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtube.com/embed/VvlUYe-9b7U\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27270/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dart2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.: https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dart2022&amp;amp;title=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;summary={DART}: {A}rticulated {H}and {M}odel with {D}iverse {A}ccessories and {R}ich {T}extures&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. %20https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/dart2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "24": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/mica-eccv2022\">Towards Metrical Reconstruction of Human Faces</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/wzielonka\">Zielonka, W.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 13, pages: 250-269, Lecture Notes in Computer Science, 13673, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27243\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27243\" href=\"#abstractContent27243\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27243\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tFace reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;\nprovides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2204.06607.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zielon.github.io/mica/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/vzzEbvv08VA\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/MICA\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-19778-9_15\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27243/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/mica-eccv2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.: https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/mica-eccv2022&amp;amp;title=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;summary=Towards Metrical Reconstruction of Human Faces&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. %20https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;body=https://ps.is.mpg.de/publications/mica-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "23": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/liu_iros_22\">Deep Residual Reinforcement Learning based Autonomous Blimp Control</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yliu2\">Liu, Y. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)</em>,  pages: 12566-12573, IEEE, Piscataway, NJ, October 2022 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27136\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27136\" href=\"#abstractContent27136\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27136\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tBlimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/IROS47612.2022.9981182\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27136/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/liu_iros_22&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .: https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/liu_iros_22&amp;amp;title=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;summary=Deep Residual Reinforcement Learning based Autonomous Blimp Control&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . %20https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;body=https://ps.is.mpg.de/publications/liu_iros_22&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "22": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/trust-eccv2022\">Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hfeng\">Feng, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Abrevaya, V. F.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 13, pages: 72-90, Lecture Notes in Computer Science, 13673, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27244\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27244\" href=\"#abstractContent27244\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27244\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tVirtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;\nof-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2205.03962.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://trust.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/HavenFeng/TRUST\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-19778-9_5\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27244/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/trust-eccv2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.: https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/trust-eccv2022&amp;amp;title=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;summary=Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/trust-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "21": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/learning_to_fit\">Learning to Fit Morphable Models</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fbogo\">Bogo, F.</a></span>, Shen, J., Valentin, J.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 6, pages: 160-179, Lecture Notes in Computer Science, 13666, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://vchoutas.github.io/learning_to_fit/\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.dropbox.com/s/4cksaq8h4royt9r/Learning%20To%20Fit%20Morphable%20Models%20FHD%20HB.mp4?raw=1\"><i class=\"fa fa-file-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/702/learning_to_fit.pdf\"><i class=\"fa fa-file-pdf-o\"/>  PDF</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/703/LearningToFit_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-20068-7_10\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27333/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/learning_to_fit&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/learning_to_fit&amp;amp;title= &amp;amp;summary=Learning to Fit Morphable Models&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/learning_to_fit&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "20": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/supr\">SUPR: A Sparse Unified Part-Based Human Representation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aosman\">Osman, A. A. A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 2, pages: 568-585, Lecture Notes in Computer Science, 13662, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27315\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27315\" href=\"#abstractContent27315\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27315\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tStatistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://supr.is.tuebingen.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/ahmedosman/SUPR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/699/0570_source.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Main Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/700/0570-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supp. Mat. </a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/701/final_supr_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-20086-1_33\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27315/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/supr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes.: https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/supr&amp;amp;title=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. &amp;amp;summary={SUPR}: A Sparse Unified Part-Based Human Representation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. %20https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/supr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "19": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/intercap_gcpr2022\">InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Honorable Mention for Best Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yhuang2\">Huang, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Pattern Recognition</em>,  pages: 281-299, Lecture Notes in Computer Science, 13485, <span class=\"text-muted\">(Editors: Andres, Bj&#246;rn and Bernard, Florian and Cremers, Daniel and Frintrop, Simone and Goldl&#252;cke, Bastian and Ihrke, Ivo)</span>, Springer, Cham, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27278\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27278\" href=\"#abstractContent27278\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27278\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/YinghaoHuang91/InterCap/tree/master\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://intercap.is.tue.mpg.de/download.php\"><i class=\"fa fa-file-o\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/d5wHLDIqN6c\"><i class=\"fa fa-file-video-o\"/>  YouTube Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://intercap.is.tue.mpg.de/media/upload/main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-16788-1_18\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27278/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.: https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/intercap_gcpr2022&amp;amp;title=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;summary={InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. %20https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "18": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/teach-2022\">TEACH: Temporal Action Composition for 3D Humans</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nathanasiou\">Athanasiou, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 International Conference on 3D Vision (3DV 2022)</em>,  pages: 414-423, IEEE, Piscataway, NJ, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27247\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27247\" href=\"#abstractContent27247\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27247\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGiven a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/athn-nik/teach\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.04066\"><i class=\"fa fa-file-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://teach.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=ENr4GQO0RSc&amp;ab_channel=3DV2022\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV57658.2022.00053\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27247/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/teach-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.: https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/teach-2022&amp;amp;title=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;summary={TEACH}: {T}emporal {A}ction {C}omposition for 3{D} {H}umans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/teach-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "17": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/skirt-3dv-2022\">Neural Point-based Shape Modeling of Humans in Challenging Clothing</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/qma\">Ma, Q.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 International Conference on 3D Vision (3DV 2022)</em>,  pages: 679-689, IEEE, Piscataway, NJ, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27249\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27249\" href=\"#abstractContent27249\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27249\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tParametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://qianlim.github.io/SkiRT\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/qianlim/SkiRT\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.06814\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/695/SkiRT_main_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/696/SkiRT_supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/697/090_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://qianlim.github.io/SkiRT\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV57658.2022.00078\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27249/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.: https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/skirt-3dv-2022&amp;amp;title=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;summary=Neural Point-based Shape Modeling of Humans in Challenging Clothing&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. %20https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "16": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/wang2022reconstruction\">Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tWang, X., Li, G., Kuo, Y., <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Aksan, E., Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 International Conference on 3D Vision (3DV 2022)</em>,  pages: 353-362, IEEE, Piscataway, NJ, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27340\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27340\" href=\"#abstractContent27340\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27340\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;\narising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;\nof humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;\na large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/rhoi/\"><i class=\"fa fa-github-square\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/YB1_xKlueUI\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.02485\"><i class=\"fa fa-file-o\"/>  Arxiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV57658.2022.00047\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27340/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/wang2022reconstruction&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.: https://ps.is.mpg.de/publications/wang2022reconstruction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/wang2022reconstruction&amp;amp;title=We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories. &amp;amp;summary=Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories. %20https://ps.is.mpg.de/publications/wang2022reconstruction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories. &amp;amp;body=https://ps.is.mpg.de/publications/wang2022reconstruction&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "15": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/invgan-gcpr-2022\">InvGAN: Invertible GANs</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Best Paper Award)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/pghosh\">Ghosh, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dzietlow\">Zietlow, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Davis, L. S., Hu, X.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Pattern Recognition</em>,  pages: 3-19, Lecture Notes in Computer Science, 13485, <span class=\"text-muted\">(Editors: Andres, Bj&#246;rn and Bernard, Florian and Cremers, Daniel and Frintrop, Simone and Goldl&#252;cke, Bastian and Ihrke, Ivo)</span>, Springer, Cham, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27334\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27334\" href=\"#abstractContent27334\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27334\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGeneration of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2112.04598.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-16788-1_1\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27334/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments.: https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/invgan-gcpr-2022&amp;amp;title=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. &amp;amp;summary={InvGAN}: Invertible {GANs}&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. %20https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. &amp;amp;body=https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "14": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ziani2022tempclr\">TempCLR: Reconstructing Hands via Time-Coherent Contrastive Learning</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tZiani, A., <span class=\"default-link-ul\"><a href=\"/person/fzicong\">Fan, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Christen, S., Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 International Conference on 3D Vision (3DV 2022)</em>,  pages: 627-636, IEEE, Piscataway, NJ, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27339\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27339\" href=\"#abstractContent27339\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27339\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/tempclr\"><i class=\"fa fa-github-square\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eth-ait/tempclr\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://arxiv.org/abs/2209.00489\"><i class=\"fa fa-file-o\"/>  Arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/VSsKx8SnFio\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/tempclr\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV57658.2022.00073\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27339/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/ziani2022tempclr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively.: https://ps.is.mpg.de/publications/ziani2022tempclr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/ziani2022tempclr&amp;amp;title=We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively. &amp;amp;summary={TempCLR}: Reconstructing Hands via Time-Coherent Contrastive Learning&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively. %20https://ps.is.mpg.de/publications/ziani2022tempclr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively. &amp;amp;body=https://ps.is.mpg.de/publications/ziani2022tempclr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "13": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/xiu2022icon\">ICON: Implicit Clothed humans Obtained from Normals</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13286-13296 , IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27033\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27033\" href=\"#abstractContent27033\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27033\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tCurrent methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://icon.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yuliangxiu/icon\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/Yuliang/ICON\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/hZd6AYin2DE\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2112.09127\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/688/01209.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/689/01209-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat.</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/690/ICON-poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01294\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Implicit Representations\" class=\"btn btn-default btn-xs\" href=\"/research_projects/implicit-representations\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27033/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/xiu2022icon&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.: https://ps.is.mpg.de/publications/xiu2022icon&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/xiu2022icon&amp;amp;title=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. &amp;amp;summary={ICON}: {I}mplicit {C}lothed humans {O}btained from {N}ormals&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. %20https://ps.is.mpg.de/publications/xiu2022icon&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. &amp;amp;body=https://ps.is.mpg.de/publications/xiu2022icon&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "12": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/zheng-cvpr-2022\">I M Avatar: Implicit Morphable Head Avatars from Videos</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yzheng\">Zheng, Y.</a></span>, Abrevaya, V. F., Marcel C. B&#252;hler, , <span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13535-13545, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27076\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27076\" href=\"#abstractContent27076\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27076\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTraditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2112.07471\"><i class=\"fa fa-file-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ait.ethz.ch/projects/2022/IMavatar/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/915baJNX-IU\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zhengyuf/IMavatar\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dataset.ait.ethz.ch/downloads/imaOsdfvRe/\"><i class=\"fa fa-file-o\"/>  synth. data</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01318\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27076/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.: https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/zheng-cvpr-2022&amp;amp;title=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. &amp;amp;summary={I M} Avatar: Implicit Morphable Head Avatars from Videos&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. %20https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. &amp;amp;body=https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "11": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/huang2022rich\">Capturing and Inferring Dense Full-Body Human-Scene Contact</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mhoeschle\">H&#246;schle, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/msafroshkin\">Safroshkin, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/talexiadis\">Alexiadis, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/senya\">Polikovsky, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dscharstein\">Scharstein, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13264-13275, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27046\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27046\" href=\"#abstractContent27046\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27046\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tInferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://rich.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2206.09553.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/paulchhuang/bstro\"><i class=\"fa fa-github-square\"/>  BSTRO code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/IbFc12L5Kc4\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01292\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-contact\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27046/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/huang2022rich&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.: https://ps.is.mpg.de/publications/huang2022rich&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/huang2022rich&amp;amp;title=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. &amp;amp;summary=Capturing and Inferring Dense Full-Body Human-Scene Contact&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/huang2022rich&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/huang2022rich&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "10": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/christen-cvpr-2022\">D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tChristen, S., <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Aksan, E., Hwangbo, J., Song, J., Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 20545-20554, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27077\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27077\" href=\"#abstractContent27077\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27077\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2112.03028.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/d-grasp/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/5OqbDq-pgLc\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/christsa/dgrasp\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01992\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27077/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/christen-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.: https://ps.is.mpg.de/publications/christen-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/christen-cvpr-2022&amp;amp;title=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. &amp;amp;summary={D-Grasp}: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. %20https://ps.is.mpg.de/publications/christen-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. &amp;amp;body=https://ps.is.mpg.de/publications/christen-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "9": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/price-ias-2021\">Simulation and Control of Deformable Autonomous Airships in Turbulent Wind</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yliu2\">Liu, Y. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Intelligent Autonomous Systems 16</em>,  pages: 608-626, Lecture Notes in Networks and Systems, 412, <span class=\"text-muted\">(Editors:  Ang Jr, Marcelo H. and Asama, Hajime and Lin, Wei and Foong, Shaohui)</span>, Springer, Cham, 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion24598\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion24598\" href=\"#abstractContent24598\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent24598\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAbstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;\nSolutions for simulation and control of these vehicles are ubiquitous. This is&#13;\nnot the case for airships, a simulation of which needs to address unique&#13;\nproperties, i) dynamic deformation in response to aerodynamic and control&#13;\nforces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;\nhigh variability in airship designs regarding placement, direction and&#13;\nvectoring of thrusters and control surfaces. We present a flexible framework&#13;\nfor modeling, simulation and control of airships, based on the Robot operating&#13;\nsystem (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;\n(COTS) electronics, both of which are open source. Based on simulated wind and&#13;\ndeformation, we predict substantial effects on controllability, verified in&#13;\nreal world flight experiments. All our code is shared as open source, for the&#13;\nbenefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;\nresearch. https://github.com/robot-perception-group/airship_simulation\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2012.15684.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2012.15684\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/robot-perception-group/airship_simulation\"><i class=\"fa fa-github-square\"/>  project/code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-030-95892-3_46\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/24598/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/price-ias-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation: https://ps.is.mpg.de/publications/price-ias-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/price-ias-2021&amp;amp;title=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation &amp;amp;summary=Simulation and Control of Deformable Autonomous Airships in Turbulent Wind&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation %20https://ps.is.mpg.de/publications/price-ias-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation &amp;amp;body=https://ps.is.mpg.de/publications/price-ias-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "8": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/keller-cvpr-2022\">OSSO: Obtaining Skeletal Shape from Outside</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkeller2\">Keller, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 20460-20469, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26967\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26967\" href=\"#abstractContent26967\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26967\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://osso.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/684/06883.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/685/06883-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat.</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01984\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26967/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/keller-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.: https://ps.is.mpg.de/publications/keller-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/keller-cvpr-2022&amp;amp;title=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. &amp;amp;summary={OSSO}: Obtaining Skeletal Shape from Outside&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. %20https://ps.is.mpg.de/publications/keller-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. &amp;amp;body=https://ps.is.mpg.de/publications/keller-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "7": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/khirodkar_ochmr_2022\">Occluded Human Mesh Recovery</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tKhirodkar, R., <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, Kitani, K.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 1705-1715, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26962\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26962\" href=\"#abstractContent26962\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26962\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTop-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://rawalkhirodkar.github.io/ochmr\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2203.13349\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00176\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26962/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released.: https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&amp;amp;title=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. &amp;amp;summary=Occluded Human Mesh Recovery&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. %20https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. &amp;amp;body=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "6": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/rueeg-cvpr-2022\">BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 3866-3874, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27073\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27073\" href=\"#abstractContent27073\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27073\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tOur goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/media/upload/barc_main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/media/upload/barc_supmat-compressed-2.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat.</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/runa91/barc_release\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/media/upload/BARC_poster_v12.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/CSPbb1p5Hso\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/runa91/barc_gradio\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00385\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27073/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.: https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&amp;amp;title=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. &amp;amp;summary={BARC}: Learning to Regress {3D} Dog Shape from Images by Exploiting Breed Information&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. %20https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. &amp;amp;body=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "5": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/shapy-2022\">Accurate 3D Body Shape Regression using Metric and Semantic Attributes</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Best Paper Award Candidate)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lmueller2\">M&#252;ller, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 2708-2718, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27067\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27067\" href=\"#abstractContent27067\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27067\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWhile methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://shapy.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/muelea/shapy\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/7TzXrL4eV9g\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/691/00928.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/692/00928-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supplementary Material</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/693/Shapy_poster_compressed.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00274\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Regressing Humans\" class=\"btn btn-default btn-xs\" href=\"/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27067/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/shapy-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research.: https://ps.is.mpg.de/publications/shapy-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/shapy-2022&amp;amp;title=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. &amp;amp;summary=Accurate 3D Body Shape Regression using Metric and Semantic Attributes&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. %20https://ps.is.mpg.de/publications/shapy-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. &amp;amp;body=https://ps.is.mpg.de/publications/shapy-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "4": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/xu2022gdna\">gDNA: Towards Generative Detailed Neural Avatars</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, Jiang, T., Song, J., <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 204395-20405, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27036\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27036\" href=\"#abstractContent27036\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27036\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://xuchen-ethz.github.io/gdna/\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ait.ethz.ch/projects/2022/gdna/downloads/main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=uOyoH7OO16I\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/xuchen-ethz/gdna\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01978\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27036/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/xu2022gdna&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.: https://ps.is.mpg.de/publications/xu2022gdna&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/xu2022gdna&amp;amp;title=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. &amp;amp;summary={gDNA}: Towards Generative Detailed Neural Avatars&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. %20https://ps.is.mpg.de/publications/xu2022gdna&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. &amp;amp;body=https://ps.is.mpg.de/publications/xu2022gdna&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "3": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/sun-cvpr-2022\">Putting People in their Place: Monocular Regression of 3D People in Depth</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSun, Y., Liu, W., Bao, Q., Fu, Y., Mei, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13233-13242, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27074\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27074\" href=\"#abstractContent27074\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27074\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGiven an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a \"Relative Human\" (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2112.08274\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arthur151.github.io/BEV/BEV.html\"><i class=\"fa fa-github-square\"/>  Project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Arthur151/ROMP\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Arthur151/Relative_Human\"><i class=\"fa fa-github-square\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2022/supplemental/Sun_Putting_People_in_CVPR_2022_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup Mat</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01289\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27074/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/sun-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.: https://ps.is.mpg.de/publications/sun-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/sun-cvpr-2022&amp;amp;title=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. &amp;amp;summary=Putting People in their Place: Monocular Regression of {3D} People in Depth&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. %20https://ps.is.mpg.de/publications/sun-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/sun-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "2": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/yi_mover_2022\">Human-Aware Object Placement for Visual Environment Reconstruction</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 3949-3960, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26938\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26938\" href=\"#abstractContent26938\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26938\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mover.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2203.03609.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00393\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-contact\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26938/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi_mover_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/yi_mover_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi_mover_2022&amp;amp;title=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. &amp;amp;summary=Human-Aware Object Placement for Visual Environment Reconstruction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/yi_mover_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/yi_mover_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "1": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/taheri-cvpr-2022a\">GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13253-13263, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27075\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27075\" href=\"#abstractContent27075\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27075\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGenerating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/goal/GOAL_Taheri_CVPR2022.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/goal/GOAL_SuppMat_Taheri_CVPR2022.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://goal.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/A7b8DYovDZY\"><i class=\"fa fa-file-video-o\"/>  YouTube</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/otaheri/GOAL\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://goal.is.tue.mpg.de/download.php\"><i class=\"fa fa-file-o\"/>  Models</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01291\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Hands-Object Interaction\" class=\"btn btn-default btn-xs\" href=\"/research_projects/hands-in-action\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-contact\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27075/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.: https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&amp;amp;title=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. &amp;amp;summary={GOAL}: Generating {4D} Whole-Body Motion for Hand-Object Grasping&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. %20https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. &amp;amp;body=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
}