{
    "50": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/bharadwaj2023flare-d8a77013-848c-43da-b54a-d9b85cefec38\">FLARE: Fast learning of Animatable and Relightable Mesh Avatars</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/sbharadwaj\">Bharadwaj, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yzheng\">Zheng, Y.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Abrevaya, V. F.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>ACM Transactions on Graphics</em>, 42, pages: 15, December 2023 <small class=\"text-muted\">(article)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27766\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27766\" href=\"#abstractContent27766\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27766\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tOur goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://drive.google.com/file/d/1iiON_x9d2E4erP2n-2A4JQ-piHr5kdQ3/view?usp=sharing\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://flare.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1145/3618401\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27766/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/bharadwaj2023flare-d8a77013-848c-43da-b54a-d9b85cefec38&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.: https://ps.is.mpg.de/publications/bharadwaj2023flare-d8a77013-848c-43da-b54a-d9b85cefec38&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/bharadwaj2023flare-d8a77013-848c-43da-b54a-d9b85cefec38&amp;amp;title=Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches. &amp;amp;summary={FLARE}: Fast learning of Animatable and Relightable Mesh Avatars&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches. %20https://ps.is.mpg.de/publications/bharadwaj2023flare-d8a77013-848c-43da-b54a-d9b85cefec38&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches. &amp;amp;body=https://ps.is.mpg.de/publications/bharadwaj2023flare-d8a77013-848c-43da-b54a-d9b85cefec38&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "49": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/emote\">Emotional Speech-Driven Animation with Content-Emotion Disentanglement</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/rdanecek\">Dan&#283;&#269;ek, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kchhatre\">Chhatre, K.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ydwen\">Wen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn ACM, December 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27758\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27758\" href=\"#abstractContent27758\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27758\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2306.08990\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://emote.is.tue.mpg.de/index.html\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1145/3610548.3618183\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27758/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/emote&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control.: https://ps.is.mpg.de/publications/emote&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/emote&amp;amp;title=To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control. &amp;amp;summary=Emotional Speech-Driven Animation with Content-Emotion Disentanglement&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control. %20https://ps.is.mpg.de/publications/emote&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control. &amp;amp;body=https://ps.is.mpg.de/publications/emote&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "48": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/keller_skel\">From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkeller2\">Keller, M.</a></span>, Werling, K., <span class=\"default-link-ul\"><a href=\"/person/sshin\">Shin, S.</a></span>, Delp, S., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, Liu, C. K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Transaction on Graphics</em>, 42, December 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27760\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27760\" href=\"#abstractContent27760\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27760\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGreat progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. &#13;\n&#13;\nWe build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to &#8220;upgrade\" existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained \n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://skel.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/skel/main_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27760/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/keller_skel&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. &#13;&#10;&#13;&#10;We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to &#8220;upgrade&amp;quot; existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained : https://ps.is.mpg.de/publications/keller_skel&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/keller_skel&amp;amp;title=Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. &#13;&#10;&#13;&#10;We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to &#8220;upgrade&amp;quot; existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained  &amp;amp;summary=From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. &#13;&#10;&#13;&#10;We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to &#8220;upgrade&amp;quot; existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained  %20https://ps.is.mpg.de/publications/keller_skel&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. &#13;&#10;&#13;&#10;We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to &#8220;upgrade&amp;quot; existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained  &amp;amp;body=https://ps.is.mpg.de/publications/keller_skel&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "47": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/humerusplate\">Optimizing the 3D Plate Shape for Proximal Humerus Fractures</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkeller2\">Keller, M.</a></span>, Krall, M., Smith, J., Clement, H., Kerner, A. M., Gradischar, A., Sch&#228;fer, &#220;., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Weinberg, A., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>,  pages: 487-496, Springer, October 2023 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27759\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27759\" href=\"#abstractContent27759\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27759\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo treat bone fractures, implant manufacturers produce 2D anatomically contoured plates. Unfortunately, existing plates only fit a limited segment of the population and/or require manual bending during surgery. Patient-specific implants would provide major benefits such as reducing surgery time and improving treatment outcomes but they are still rare in clinical practice.&#13;\n&#13;\nIn this work, we propose a patient-specific design for the long helical 2D PHILOS (Proximal Humeral Internal Locking System) plate, used to treat humerus shaft fractures. Our method automatically creates a custom plate from a CT scan of a patient's bone. We start by designing an optimal plate on a template bone and, with an anatomy-aware registration method, we transfer this optimal design to any bone. In addition, for an arbitrary bone, our method assesses if a given plate is fit for surgery by automatically positioning it on the bone. We use this process to generate a compact set of plate shapes capable of fitting the bones within a given population.&#13;\n&#13;\nThis plate set can be pre-printed in advance and readily available, removing the fabrication time between the fracture occurrence and the surgery. Extensive experiments on ex-vivo arms and 3D-printed bones show that the generated plate shapes (personalized and plate-set) faithfully match the individual bone anatomy and are suitable for clinical practice.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://humerusplate.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/MarilynKeller/HumerusPlate\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://humerusplate.is.tue.mpg.de/media/upload/paper2383.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/humerusplate/plate_poster_miccai.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-43990-2_46\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27759/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/humerusplate&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To treat bone fractures, implant manufacturers produce 2D anatomically contoured plates. Unfortunately, existing plates only fit a limited segment of the population and/or require manual bending during surgery. Patient-specific implants would provide major benefits such as reducing surgery time and improving treatment outcomes but they are still rare in clinical practice.&#13;&#10;&#13;&#10;In this work, we propose a patient-specific design for the long helical 2D PHILOS (Proximal Humeral Internal Locking System) plate, used to treat humerus shaft fractures. Our method automatically creates a custom plate from a CT scan of a patient&amp;#39;s bone. We start by designing an optimal plate on a template bone and, with an anatomy-aware registration method, we transfer this optimal design to any bone. In addition, for an arbitrary bone, our method assesses if a given plate is fit for surgery by automatically positioning it on the bone. We use this process to generate a compact set of plate shapes capable of fitting the bones within a given population.&#13;&#10;&#13;&#10;This plate set can be pre-printed in advance and readily available, removing the fabrication time between the fracture occurrence and the surgery. Extensive experiments on ex-vivo arms and 3D-printed bones show that the generated plate shapes (personalized and plate-set) faithfully match the individual bone anatomy and are suitable for clinical practice.: https://ps.is.mpg.de/publications/humerusplate&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/humerusplate&amp;amp;title=To treat bone fractures, implant manufacturers produce 2D anatomically contoured plates. Unfortunately, existing plates only fit a limited segment of the population and/or require manual bending during surgery. Patient-specific implants would provide major benefits such as reducing surgery time and improving treatment outcomes but they are still rare in clinical practice.&#13;&#10;&#13;&#10;In this work, we propose a patient-specific design for the long helical 2D PHILOS (Proximal Humeral Internal Locking System) plate, used to treat humerus shaft fractures. Our method automatically creates a custom plate from a CT scan of a patient&amp;#39;s bone. We start by designing an optimal plate on a template bone and, with an anatomy-aware registration method, we transfer this optimal design to any bone. In addition, for an arbitrary bone, our method assesses if a given plate is fit for surgery by automatically positioning it on the bone. We use this process to generate a compact set of plate shapes capable of fitting the bones within a given population.&#13;&#10;&#13;&#10;This plate set can be pre-printed in advance and readily available, removing the fabrication time between the fracture occurrence and the surgery. Extensive experiments on ex-vivo arms and 3D-printed bones show that the generated plate shapes (personalized and plate-set) faithfully match the individual bone anatomy and are suitable for clinical practice. &amp;amp;summary=Optimizing the 3D Plate Shape for Proximal Humerus Fractures&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To treat bone fractures, implant manufacturers produce 2D anatomically contoured plates. Unfortunately, existing plates only fit a limited segment of the population and/or require manual bending during surgery. Patient-specific implants would provide major benefits such as reducing surgery time and improving treatment outcomes but they are still rare in clinical practice.&#13;&#10;&#13;&#10;In this work, we propose a patient-specific design for the long helical 2D PHILOS (Proximal Humeral Internal Locking System) plate, used to treat humerus shaft fractures. Our method automatically creates a custom plate from a CT scan of a patient&amp;#39;s bone. We start by designing an optimal plate on a template bone and, with an anatomy-aware registration method, we transfer this optimal design to any bone. In addition, for an arbitrary bone, our method assesses if a given plate is fit for surgery by automatically positioning it on the bone. We use this process to generate a compact set of plate shapes capable of fitting the bones within a given population.&#13;&#10;&#13;&#10;This plate set can be pre-printed in advance and readily available, removing the fabrication time between the fracture occurrence and the surgery. Extensive experiments on ex-vivo arms and 3D-printed bones show that the generated plate shapes (personalized and plate-set) faithfully match the individual bone anatomy and are suitable for clinical practice. %20https://ps.is.mpg.de/publications/humerusplate&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To treat bone fractures, implant manufacturers produce 2D anatomically contoured plates. Unfortunately, existing plates only fit a limited segment of the population and/or require manual bending during surgery. Patient-specific implants would provide major benefits such as reducing surgery time and improving treatment outcomes but they are still rare in clinical practice.&#13;&#10;&#13;&#10;In this work, we propose a patient-specific design for the long helical 2D PHILOS (Proximal Humeral Internal Locking System) plate, used to treat humerus shaft fractures. Our method automatically creates a custom plate from a CT scan of a patient&amp;#39;s bone. We start by designing an optimal plate on a template bone and, with an anatomy-aware registration method, we transfer this optimal design to any bone. In addition, for an arbitrary bone, our method assesses if a given plate is fit for surgery by automatically positioning it on the bone. We use this process to generate a compact set of plate shapes capable of fitting the bones within a given population.&#13;&#10;&#13;&#10;This plate set can be pre-printed in advance and readily available, removing the fabrication time between the fracture occurrence and the surgery. Extensive experiments on ex-vivo arms and 3D-printed bones show that the generated plate shapes (personalized and plate-set) faithfully match the individual bone anatomy and are suitable for clinical practice. &amp;amp;body=https://ps.is.mpg.de/publications/humerusplate&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "46": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tripathi2023deco\">DECO: Dense Estimation of 3D Human-Scene Contact in the Wild</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Oral presentation)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, Chatterjee, A., <span class=\"default-link-ul\"><a href=\"/person/jpassy\">Passy, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27761\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27761\" href=\"#abstractContent27761\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27761\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tUnderstanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available at https://deco.is.tue.mpg.de/login.php. \n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://deco.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=o7MLobqAFTQ&amp;feature=youtu.be\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.dropbox.com/scl/fi/kvhpfnkvga2pt19ayko8u/ICCV2023_DECO_Poster_v2.pptx?rlkey=ihbf3fi6u9j0ha9x1gfk2cwd0&amp;dl=0\"><i class=\"fa fa-file-powerpoint-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/sha2nkt/deco\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://deco.is.tue.mpg.de/download.php\"><i class=\"fa fa-file-o\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://deco.is.tue.mpg.de/login.php\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27761/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tripathi2023deco&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Understanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available at https://deco.is.tue.mpg.de/login.php. : https://ps.is.mpg.de/publications/tripathi2023deco&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tripathi2023deco&amp;amp;title=Understanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available at https://deco.is.tue.mpg.de/login.php.  &amp;amp;summary={DECO}: Dense Estimation of 3D Human-Scene Contact in the Wild&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Understanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available at https://deco.is.tue.mpg.de/login.php.  %20https://ps.is.mpg.de/publications/tripathi2023deco&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Understanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available at https://deco.is.tue.mpg.de/login.php.  &amp;amp;body=https://ps.is.mpg.de/publications/tripathi2023deco&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "45": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/sinc-iccv-2023-96f4db44-d7e5-4adc-a93f-4fb8987ec6da\">SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nathanasiou\">Athanasiou, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27700\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27700\" href=\"#abstractContent27700\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27700\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tOur goal is to synthesize 3D human motions given tex-&#13;\ntual inputs describing multiple simultaneous actions, for&#13;\nexample &#8216;waving hand&#8217; while &#8216;walking&#8217; at the same time.&#13;\nWe refer to generating such simultaneous movements as&#13;\nperforming &#8216;spatial compositions&#8217;. In contrast to &#8216;tempo-&#13;\nral compositions&#8217; that seek to transition from one action&#13;\nto another in a sequence, spatial compositing requires un-&#13;\nderstanding which body parts are involved with which ac-&#13;\ntion. Motivated by the observation that the correspondence&#13;\nbetween actions and body parts is encoded in powerful&#13;\nlanguage models, we extract this knowledge by prompting&#13;\nGPT-3 with text such as &#8220;what parts of the body are mov-&#13;\ning when someone is doing the action <action name=\"\">?&#8221;.&#13;\nGiven this action-part mapping, we automatically create&#13;\nnew training data by artificially combining body parts from&#13;\nmultiple text-motion pairs together. We extend previous&#13;\nwork on text-to-motions synthesis to train on spatial com-&#13;\npositions, and introduce SINC (&#8220;SImultaneous actioN Com-&#13;\npositions for 3D human motions&#8221;). We experimentally val-&#13;\nidate that our additional GPT-guided data helps to better&#13;\nlearn compositionality compared to training only on exist-&#13;\ning real data of simultaneous actions, which is limited in&#13;\nquantity. Our models and code will be publicly available.\n\t\t\t\t\t\t\t\t\t\t\t</action></div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://sinc.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/athn-nik/sinc\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2304.10417\"><i class=\"fa fa-file-o\"/>  paper-arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=uwUriDnKTLI\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27700/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/sinc-iccv-2023-96f4db44-d7e5-4adc-a93f-4fb8987ec6da&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Our goal is to synthesize 3D human motions given tex-&#13;&#10;tual inputs describing multiple simultaneous actions, for&#13;&#10;example &#8216;waving hand&#8217; while &#8216;walking&#8217; at the same time.&#13;&#10;We refer to generating such simultaneous movements as&#13;&#10;performing &#8216;spatial compositions&#8217;. In contrast to &#8216;tempo-&#13;&#10;ral compositions&#8217; that seek to transition from one action&#13;&#10;to another in a sequence, spatial compositing requires un-&#13;&#10;derstanding which body parts are involved with which ac-&#13;&#10;tion. Motivated by the observation that the correspondence&#13;&#10;between actions and body parts is encoded in powerful&#13;&#10;language models, we extract this knowledge by prompting&#13;&#10;GPT-3 with text such as &#8220;what parts of the body are mov-&#13;&#10;ing when someone is doing the action &amp;lt;action name&amp;gt;?&#8221;.&#13;&#10;Given this action-part mapping, we automatically create&#13;&#10;new training data by artificially combining body parts from&#13;&#10;multiple text-motion pairs together. We extend previous&#13;&#10;work on text-to-motions synthesis to train on spatial com-&#13;&#10;positions, and introduce SINC (&#8220;SImultaneous actioN Com-&#13;&#10;positions for 3D human motions&#8221;). We experimentally val-&#13;&#10;idate that our additional GPT-guided data helps to better&#13;&#10;learn compositionality compared to training only on exist-&#13;&#10;ing real data of simultaneous actions, which is limited in&#13;&#10;quantity. Our models and code will be publicly available.: https://ps.is.mpg.de/publications/sinc-iccv-2023-96f4db44-d7e5-4adc-a93f-4fb8987ec6da&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/sinc-iccv-2023-96f4db44-d7e5-4adc-a93f-4fb8987ec6da&amp;amp;title=Our goal is to synthesize 3D human motions given tex-&#13;&#10;tual inputs describing multiple simultaneous actions, for&#13;&#10;example &#8216;waving hand&#8217; while &#8216;walking&#8217; at the same time.&#13;&#10;We refer to generating such simultaneous movements as&#13;&#10;performing &#8216;spatial compositions&#8217;. In contrast to &#8216;tempo-&#13;&#10;ral compositions&#8217; that seek to transition from one action&#13;&#10;to another in a sequence, spatial compositing requires un-&#13;&#10;derstanding which body parts are involved with which ac-&#13;&#10;tion. Motivated by the observation that the correspondence&#13;&#10;between actions and body parts is encoded in powerful&#13;&#10;language models, we extract this knowledge by prompting&#13;&#10;GPT-3 with text such as &#8220;what parts of the body are mov-&#13;&#10;ing when someone is doing the action &amp;lt;action name&amp;gt;?&#8221;.&#13;&#10;Given this action-part mapping, we automatically create&#13;&#10;new training data by artificially combining body parts from&#13;&#10;multiple text-motion pairs together. We extend previous&#13;&#10;work on text-to-motions synthesis to train on spatial com-&#13;&#10;positions, and introduce SINC (&#8220;SImultaneous actioN Com-&#13;&#10;positions for 3D human motions&#8221;). We experimentally val-&#13;&#10;idate that our additional GPT-guided data helps to better&#13;&#10;learn compositionality compared to training only on exist-&#13;&#10;ing real data of simultaneous actions, which is limited in&#13;&#10;quantity. Our models and code will be publicly available. &amp;amp;summary={SINC}: Spatial Composition of {3D} Human Motions for Simultaneous Action Generation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Our goal is to synthesize 3D human motions given tex-&#13;&#10;tual inputs describing multiple simultaneous actions, for&#13;&#10;example &#8216;waving hand&#8217; while &#8216;walking&#8217; at the same time.&#13;&#10;We refer to generating such simultaneous movements as&#13;&#10;performing &#8216;spatial compositions&#8217;. In contrast to &#8216;tempo-&#13;&#10;ral compositions&#8217; that seek to transition from one action&#13;&#10;to another in a sequence, spatial compositing requires un-&#13;&#10;derstanding which body parts are involved with which ac-&#13;&#10;tion. Motivated by the observation that the correspondence&#13;&#10;between actions and body parts is encoded in powerful&#13;&#10;language models, we extract this knowledge by prompting&#13;&#10;GPT-3 with text such as &#8220;what parts of the body are mov-&#13;&#10;ing when someone is doing the action &amp;lt;action name&amp;gt;?&#8221;.&#13;&#10;Given this action-part mapping, we automatically create&#13;&#10;new training data by artificially combining body parts from&#13;&#10;multiple text-motion pairs together. We extend previous&#13;&#10;work on text-to-motions synthesis to train on spatial com-&#13;&#10;positions, and introduce SINC (&#8220;SImultaneous actioN Com-&#13;&#10;positions for 3D human motions&#8221;). We experimentally val-&#13;&#10;idate that our additional GPT-guided data helps to better&#13;&#10;learn compositionality compared to training only on exist-&#13;&#10;ing real data of simultaneous actions, which is limited in&#13;&#10;quantity. Our models and code will be publicly available. %20https://ps.is.mpg.de/publications/sinc-iccv-2023-96f4db44-d7e5-4adc-a93f-4fb8987ec6da&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Our goal is to synthesize 3D human motions given tex-&#13;&#10;tual inputs describing multiple simultaneous actions, for&#13;&#10;example &#8216;waving hand&#8217; while &#8216;walking&#8217; at the same time.&#13;&#10;We refer to generating such simultaneous movements as&#13;&#10;performing &#8216;spatial compositions&#8217;. In contrast to &#8216;tempo-&#13;&#10;ral compositions&#8217; that seek to transition from one action&#13;&#10;to another in a sequence, spatial compositing requires un-&#13;&#10;derstanding which body parts are involved with which ac-&#13;&#10;tion. Motivated by the observation that the correspondence&#13;&#10;between actions and body parts is encoded in powerful&#13;&#10;language models, we extract this knowledge by prompting&#13;&#10;GPT-3 with text such as &#8220;what parts of the body are mov-&#13;&#10;ing when someone is doing the action &amp;lt;action name&amp;gt;?&#8221;.&#13;&#10;Given this action-part mapping, we automatically create&#13;&#10;new training data by artificially combining body parts from&#13;&#10;multiple text-motion pairs together. We extend previous&#13;&#10;work on text-to-motions synthesis to train on spatial com-&#13;&#10;positions, and introduce SINC (&#8220;SImultaneous actioN Com-&#13;&#10;positions for 3D human motions&#8221;). We experimentally val-&#13;&#10;idate that our additional GPT-guided data helps to better&#13;&#10;learn compositionality compared to training only on exist-&#13;&#10;ing real data of simultaneous actions, which is limited in&#13;&#10;quantity. Our models and code will be publicly available. &amp;amp;body=https://ps.is.mpg.de/publications/sinc-iccv-2023-96f4db44-d7e5-4adc-a93f-4fb8987ec6da&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "44": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/arteq-iccv-2023\">Generalizing Neural Human Fitting to Unseen Poses With Articulated SE(3) Equivariance</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Oral)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hfeng\">Feng, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/pkulits\">Kulits, P.</a></span>, Liu, S., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Abrevaya, V. F.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">To be published</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27717\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27717\" href=\"#abstractContent27717\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27717\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the problem of fitting a parametric human body model (SMPL) to point cloud data. Optimization based methods require careful initialization and are prone to becoming trapped in local optima. Learning-based methods address this but do not generalize well when the input pose is far from those seen during training. For rigid point clouds, remarkable generalization has been achieved by leveraging SE(3)-equivariant networks, but these methods do not work on articulated objects. In this work we extend this idea to human bodies and propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for SMPL model estimation from point clouds. Specifically, we learn a part detection network by leveraging local SO(3) invariance, and regress shape and pose using articulated SE(3) shape-invariant and pose-equivariant networks, all trained end-to-end. Our novel pose regression module leverages the permutation-equivariant property of self-attention layers to preserve rotational equivariance. Experimental results show that ArtEq generalizes to poses not seen during training, outperforming state-of-the-art methods by ~44%in terms of body reconstruction accuracy, without requiring an optimization refinement step. Furthermore, ArtEq is three orders of magnitude faster during inference than prior work and has 97.3% fewer parameters. The code and model are available for research purposes at https://arteq.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2304.10528\"><i class=\"fa fa-file-o\"/>  arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arteq.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arteq.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27717/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/arteq-iccv-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of fitting a parametric human body model (SMPL) to point cloud data. Optimization based methods require careful initialization and are prone to becoming trapped in local optima. Learning-based methods address this but do not generalize well when the input pose is far from those seen during training. For rigid point clouds, remarkable generalization has been achieved by leveraging SE(3)-equivariant networks, but these methods do not work on articulated objects. In this work we extend this idea to human bodies and propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for SMPL model estimation from point clouds. Specifically, we learn a part detection network by leveraging local SO(3) invariance, and regress shape and pose using articulated SE(3) shape-invariant and pose-equivariant networks, all trained end-to-end. Our novel pose regression module leverages the permutation-equivariant property of self-attention layers to preserve rotational equivariance. Experimental results show that ArtEq generalizes to poses not seen during training, outperforming state-of-the-art methods by ~44%in terms of body reconstruction accuracy, without requiring an optimization refinement step. Furthermore, ArtEq is three orders of magnitude faster during inference than prior work and has 97.3% fewer parameters. The code and model are available for research purposes at https://arteq.is.tue.mpg.de.: https://ps.is.mpg.de/publications/arteq-iccv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/arteq-iccv-2023&amp;amp;title=We address the problem of fitting a parametric human body model (SMPL) to point cloud data. Optimization based methods require careful initialization and are prone to becoming trapped in local optima. Learning-based methods address this but do not generalize well when the input pose is far from those seen during training. For rigid point clouds, remarkable generalization has been achieved by leveraging SE(3)-equivariant networks, but these methods do not work on articulated objects. In this work we extend this idea to human bodies and propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for SMPL model estimation from point clouds. Specifically, we learn a part detection network by leveraging local SO(3) invariance, and regress shape and pose using articulated SE(3) shape-invariant and pose-equivariant networks, all trained end-to-end. Our novel pose regression module leverages the permutation-equivariant property of self-attention layers to preserve rotational equivariance. Experimental results show that ArtEq generalizes to poses not seen during training, outperforming state-of-the-art methods by ~44%in terms of body reconstruction accuracy, without requiring an optimization refinement step. Furthermore, ArtEq is three orders of magnitude faster during inference than prior work and has 97.3% fewer parameters. The code and model are available for research purposes at https://arteq.is.tue.mpg.de. &amp;amp;summary=Generalizing Neural Human Fitting to Unseen Poses With Articulated {SE}(3) Equivariance&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of fitting a parametric human body model (SMPL) to point cloud data. Optimization based methods require careful initialization and are prone to becoming trapped in local optima. Learning-based methods address this but do not generalize well when the input pose is far from those seen during training. For rigid point clouds, remarkable generalization has been achieved by leveraging SE(3)-equivariant networks, but these methods do not work on articulated objects. In this work we extend this idea to human bodies and propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for SMPL model estimation from point clouds. Specifically, we learn a part detection network by leveraging local SO(3) invariance, and regress shape and pose using articulated SE(3) shape-invariant and pose-equivariant networks, all trained end-to-end. Our novel pose regression module leverages the permutation-equivariant property of self-attention layers to preserve rotational equivariance. Experimental results show that ArtEq generalizes to poses not seen during training, outperforming state-of-the-art methods by ~44%in terms of body reconstruction accuracy, without requiring an optimization refinement step. Furthermore, ArtEq is three orders of magnitude faster during inference than prior work and has 97.3% fewer parameters. The code and model are available for research purposes at https://arteq.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/arteq-iccv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We address the problem of fitting a parametric human body model (SMPL) to point cloud data. Optimization based methods require careful initialization and are prone to becoming trapped in local optima. Learning-based methods address this but do not generalize well when the input pose is far from those seen during training. For rigid point clouds, remarkable generalization has been achieved by leveraging SE(3)-equivariant networks, but these methods do not work on articulated objects. In this work we extend this idea to human bodies and propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for SMPL model estimation from point clouds. Specifically, we learn a part detection network by leveraging local SO(3) invariance, and regress shape and pose using articulated SE(3) shape-invariant and pose-equivariant networks, all trained end-to-end. Our novel pose regression module leverages the permutation-equivariant property of self-attention layers to preserve rotational equivariance. Experimental results show that ArtEq generalizes to poses not seen during training, outperforming state-of-the-art methods by ~44%in terms of body reconstruction accuracy, without requiring an optimization refinement step. Furthermore, ArtEq is three orders of magnitude faster during inference than prior work and has 97.3% fewer parameters. The code and model are available for research purposes at https://arteq.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/arteq-iccv-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "43": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dif2023\">D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tYang, X., Luo, Y., <span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, Wang, W., Xu, H., Fan, Z.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27736\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27736\" href=\"#abstractContent27736\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27736\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tRealistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple \"value to distribution\" transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/psyai-net/D-IF_release\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://yxt7979.github.io/idf/\"><i class=\"fa fa-github-square\"/>  Homepage</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/psyai-net/D-IF_release\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27736/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dif2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple &amp;quot;value to distribution&amp;quot; transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs.: https://ps.is.mpg.de/publications/dif2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dif2023&amp;amp;title=Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple &amp;quot;value to distribution&amp;quot; transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs. &amp;amp;summary=D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple &amp;quot;value to distribution&amp;quot; transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs. %20https://ps.is.mpg.de/publications/dif2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple &amp;quot;value to distribution&amp;quot; transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs. &amp;amp;body=https://ps.is.mpg.de/publications/dif2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "42": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ag3d-iccv-2023\">AG3D: Learning to Generate 3D Avatars from 2D Image Collections</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tDong, Z., <span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">J.Black, M.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27762\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27762\" href=\"#abstractContent27762\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27762\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWhile progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses.&#13;\n&#13;\nIn this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps.&#13;\n&#13;\nWe experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zj-dong.github.io/AG3D/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zj-dong.github.io/AG3D/assets/paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zj-dong/AG3D\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/niP1YhJXEBE\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27762/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/ag3d-iccv-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses.&#13;&#10;&#13;&#10;In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps.&#13;&#10;&#13;&#10;We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.: https://ps.is.mpg.de/publications/ag3d-iccv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/ag3d-iccv-2023&amp;amp;title=While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses.&#13;&#10;&#13;&#10;In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps.&#13;&#10;&#13;&#10;We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies. &amp;amp;summary={AG3D}: Learning to Generate {3D} Avatars from {2D} Image Collections&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses.&#13;&#10;&#13;&#10;In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps.&#13;&#10;&#13;&#10;We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies. %20https://ps.is.mpg.de/publications/ag3d-iccv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses.&#13;&#10;&#13;&#10;In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps.&#13;&#10;&#13;&#10;We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies. &amp;amp;body=https://ps.is.mpg.de/publications/ag3d-iccv-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "41": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tmr\">TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27770\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27770\" href=\"#abstractContent27770\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27770\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mathis.petrovich.fr/tmr\"><i class=\"fa fa-file-o\"/>  website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Mathux/TMR\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2305.00976\"><i class=\"fa fa-file-o\"/>  paper-arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=FK0RukgDEtM\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mathis.petrovich.fr/tmr\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27770/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tmr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available.: https://ps.is.mpg.de/publications/tmr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tmr&amp;amp;title=In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available. &amp;amp;summary=TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available. %20https://ps.is.mpg.de/publications/tmr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available. &amp;amp;body=https://ps.is.mpg.de/publications/tmr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "40": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/simple2023wen\">Pairwise Similarity Learning is SimPLE</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ydwen\">Wen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/wliu\">Liu, W.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, Raj, B., Singh, R., Weller, A., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/bs\">Sch&#246;lkopf, B.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>, October 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27768\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27768\" href=\"#abstractContent27768\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27768\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://simple.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27768/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/simple2023wen&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods.: https://ps.is.mpg.de/publications/simple2023wen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/simple2023wen&amp;amp;title=In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. &amp;amp;summary=Pairwise Similarity Learning is SimPLE&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. %20https://ps.is.mpg.de/publications/simple2023wen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. &amp;amp;body=https://ps.is.mpg.de/publications/simple2023wen&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "39": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/bonetto2023ecmrzebras\">Synthetic Data-Based Detection of Zebras in Drone Imagery</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ebonetto\">Bonetto, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2023 European Conference on Mobile Robots (ECMR)</em>,  pages: 1-8, IEEE, September 2023 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27663\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27663\" href=\"#abstractContent27663\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27663\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tNowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at https://eliabntt.github.io/grade-rr.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eliabntt/GRADE-RR\"><i class=\"fa fa-github-square\"/>  Generation code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://grade.is.mpg.de\"><i class=\"fa fa-file-o\"/>  Data and models</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/718/2305.00432.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10256293\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ECMR59166.2023.10256293\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27663/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/bonetto2023ecmrzebras&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at https://eliabntt.github.io/grade-rr.: https://ps.is.mpg.de/publications/bonetto2023ecmrzebras&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/bonetto2023ecmrzebras&amp;amp;title=Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at https://eliabntt.github.io/grade-rr. &amp;amp;summary=Synthetic Data-Based Detection of Zebras in Drone Imagery&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at https://eliabntt.github.io/grade-rr. %20https://ps.is.mpg.de/publications/bonetto2023ecmrzebras&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at https://eliabntt.github.io/grade-rr. &amp;amp;body=https://ps.is.mpg.de/publications/bonetto2023ecmrzebras&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "38": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hassan-sig-2023\">Synthesizing Physical Character-scene Interactions</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, Guo, Y., Wang, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Fidler, S., Peng, X. B.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>SIGGRAPH Conf. Track</em>, August 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27602\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27602\" href=\"#abstractContent27602\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27602\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMovement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent's movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character's movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://xbpeng.github.io/projects/InterPhys/2023_InterPhys.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/q3hyQdaElQQ\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27602/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hassan-sig-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability.: https://ps.is.mpg.de/publications/hassan-sig-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hassan-sig-2023&amp;amp;title=Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability. &amp;amp;summary=Synthesizing Physical Character-scene Interactions&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability. %20https://ps.is.mpg.de/publications/hassan-sig-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Movement is how people interact with and affect their environment. For realistic virtual character animation, it is necessary to realistically synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent&amp;#39;s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interaction require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method is able to learn natural scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character&amp;#39;s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. Using an efficient approach to randomize the training objects and their placements during training enables our method to generalize beyond the objects and scenarios in the training dataset, producing natural character-scene interactions despite wide variation in object shape and placement. The approach takes physics-based character motion generation a step closer to broad applicability. &amp;amp;body=https://ps.is.mpg.de/publications/hassan-sig-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "37": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/econ2023xiu\">ECON: Explicit Clothed humans Optimized via Normal integration</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Highlight Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, Cao, X., <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 512-523, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27519\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27519\" href=\"#abstractContent27519\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27519\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://econ.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.07422\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/Yuliang/ECON\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/YuliangXiu/ECON\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=5PEd_p90kS0\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno?usp=sharing\"><i class=\"fa fa-file-o\"/>  Colab</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://econ.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27519/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/econ2023xiu&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin.: https://ps.is.mpg.de/publications/econ2023xiu&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/econ2023xiu&amp;amp;title=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. &amp;amp;summary=ECON: Explicit Clothed humans Optimized via Normal integration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. %20https://ps.is.mpg.de/publications/econ2023xiu&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. &amp;amp;body=https://ps.is.mpg.de/publications/econ2023xiu&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "36": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hood-cvpr-2023\">HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tGrigorev, A., Thomaszewski, B., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 16965-16974, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27608\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27608\" href=\"#abstractContent27608\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27608\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.07242\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dolorousrtur.github.io/hood/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/supplemental/Grigorev_HOOD_Hierarchical_Graphs_CVPR_2023_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27608/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hood-cvpr-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods.: https://ps.is.mpg.de/publications/hood-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hood-cvpr-2023&amp;amp;title=We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods. &amp;amp;summary={HOOD}: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods. %20https://ps.is.mpg.de/publications/hood-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods. &amp;amp;body=https://ps.is.mpg.de/publications/hood-cvpr-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "35": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/car2023liao\">High-Fidelity Clothed Avatar Reconstruction from a Single Image</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tLiao, T., Zhang, X., <span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, Liu, X., Qi, G., Zhang, Y., Wang, X., Zhu, X., Lei, Z.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8662-8672, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27646\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27646\" href=\"#abstractContent27646\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27646\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThis paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes. \n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/TingtingLiao/CAR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2304.03903\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tingtingliao.github.io/CAR/\"><i class=\"fa fa-github-square\"/>  Homepage</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/sVk6dAtC1kQ\"><i class=\"fa fa-file-video-o\"/>  Youtube</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tingtingliao.github.io/CAR/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27646/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/car2023liao&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes. : https://ps.is.mpg.de/publications/car2023liao&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/car2023liao&amp;amp;title=This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.  &amp;amp;summary=High-Fidelity Clothed Avatar Reconstruction from a Single Image&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.  %20https://ps.is.mpg.de/publications/car2023liao&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.  &amp;amp;body=https://ps.is.mpg.de/publications/car2023liao&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "34": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/insta\">Instant Volumetric Head Avatars</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/wzielonka\">Zielonka, W.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27515\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27515\" href=\"#abstractContent27515\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27515\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2211.12499v2.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zielon.github.io/insta/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/HOgaeWTih7Q\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/INSTA\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/metrical-tracker\"><i class=\"fa fa-github-square\"/>  face tracker code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://keeper.mpdl.mpg.de/d/5ea4d2c300e9444a8b0b/\"><i class=\"fa fa-file-o\"/>  dataset</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27515/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/insta&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.: https://ps.is.mpg.de/publications/insta&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/insta&amp;amp;title=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. &amp;amp;summary=Instant Volumetric Head Avatars&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. %20https://ps.is.mpg.de/publications/insta&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. &amp;amp;body=https://ps.is.mpg.de/publications/insta&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "33": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/black_2023_cvpr\">BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Highlight Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ppatel\">Patel, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8726-8737, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27604\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27604\" href=\"#abstractContent27604\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27604\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://bedlam.is.tuebingen.mpg.de/media/upload/BEDLAM_CVPR2023.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://bedlam.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/html/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.html\"><i class=\"fa fa-file-o\"/>  CVF</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/pixelite1201/BEDLAM\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27604/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/black_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/black_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/black_2023_cvpr&amp;amp;title=We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. &amp;amp;summary={BEDLAM}: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/black_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/black_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "32": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ruegg_2023_cvpr\">BITE: Beyond Priors for Improved Three-D Dog Pose Estimation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tR&#252;egg, N., <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8867-8876, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27609\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27609\" href=\"#abstractContent27609\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27609\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Ruegg_BITE_Beyond_Priors_for_Improved_Three-D_Dog_Pose_Estimation_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ruegg_BITE_Beyond_Priors_CVPR_2023_supplemental.zip\"><i class=\"fa fa-file-archive-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://bite.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27609/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de.: https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/ruegg_2023_cvpr&amp;amp;title=We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de. &amp;amp;summary={BITE}: Beyond Priors for Improved Three-{D} Dog Pose Estimation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/ruegg_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "31": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/yi2022mime\">MIME: Human-Aware 3D Scene Generation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lhering\">Hering, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 12965-12976, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27611\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27611\" href=\"#abstractContent27611\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27611\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGenerating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mime.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.04360\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27611/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi2022mime&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de.: https://ps.is.mpg.de/publications/yi2022mime&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi2022mime&amp;amp;title=Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. &amp;amp;summary={MIME}: Human-Aware {3D} Scene Generation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/yi2022mime&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a &#8220;scanner&#8221; of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/yi2022mime&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "30": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/bonetto2023learningfromgrade\">Learning from synthetic data generated with GRADE</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ebonetto\">Bonetto, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/cxu\">Xu, C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>ICRA 2023 Pretraining for Robotics (PT4R) Workshop</em>, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27661\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27661\" href=\"#abstractContent27661\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27661\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tRecently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. Simulations for most robotics applications are obtained in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, first introduced in~\\cite{GRADE}. GRADE supports full simulation control, ROS integration, realistic physics, while being in an engine that produces high visual fidelity images and ground truth data. We use GRADE to generate a dataset focused on indoor dynamic scenes with people and flying objects. Using this, we evaluate the performance of YOLO and Mask R-CNN on the tasks of segmenting and detecting people. Our results provide evidence that using data generated with GRADE can improve the model performance when used for a pre-training step. We also show that, even training using only synthetic data, can generalize well to real-world images in the same application domain such as the ones from the TUM-RGBD dataset. The code, results, trained models, and the generated data are provided as open-source at https://eliabntt.github.io/grade-rr.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eliabntt/GRADE-RR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eliabntt/GRADE_data\"><i class=\"fa fa-github-square\"/>  Data and network models</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/716/2305.04282.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openreview.net/forum?id=SUIOuV2y-Ce\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27661/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/bonetto2023learningfromgrade&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. Simulations for most robotics applications are obtained in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, first introduced in~\\cite{GRADE}. GRADE supports full simulation control, ROS integration, realistic physics, while being in an engine that produces high visual fidelity images and ground truth data. We use GRADE to generate a dataset focused on indoor dynamic scenes with people and flying objects. Using this, we evaluate the performance of YOLO and Mask R-CNN on the tasks of segmenting and detecting people. Our results provide evidence that using data generated with GRADE can improve the model performance when used for a pre-training step. We also show that, even training using only synthetic data, can generalize well to real-world images in the same application domain such as the ones from the TUM-RGBD dataset. The code, results, trained models, and the generated data are provided as open-source at https://eliabntt.github.io/grade-rr.: https://ps.is.mpg.de/publications/bonetto2023learningfromgrade&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/bonetto2023learningfromgrade&amp;amp;title=Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. Simulations for most robotics applications are obtained in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, first introduced in~\\cite{GRADE}. GRADE supports full simulation control, ROS integration, realistic physics, while being in an engine that produces high visual fidelity images and ground truth data. We use GRADE to generate a dataset focused on indoor dynamic scenes with people and flying objects. Using this, we evaluate the performance of YOLO and Mask R-CNN on the tasks of segmenting and detecting people. Our results provide evidence that using data generated with GRADE can improve the model performance when used for a pre-training step. We also show that, even training using only synthetic data, can generalize well to real-world images in the same application domain such as the ones from the TUM-RGBD dataset. The code, results, trained models, and the generated data are provided as open-source at https://eliabntt.github.io/grade-rr. &amp;amp;summary=Learning from synthetic data generated with GRADE&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. Simulations for most robotics applications are obtained in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, first introduced in~\\cite{GRADE}. GRADE supports full simulation control, ROS integration, realistic physics, while being in an engine that produces high visual fidelity images and ground truth data. We use GRADE to generate a dataset focused on indoor dynamic scenes with people and flying objects. Using this, we evaluate the performance of YOLO and Mask R-CNN on the tasks of segmenting and detecting people. Our results provide evidence that using data generated with GRADE can improve the model performance when used for a pre-training step. We also show that, even training using only synthetic data, can generalize well to real-world images in the same application domain such as the ones from the TUM-RGBD dataset. The code, results, trained models, and the generated data are provided as open-source at https://eliabntt.github.io/grade-rr. %20https://ps.is.mpg.de/publications/bonetto2023learningfromgrade&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. Simulations for most robotics applications are obtained in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, first introduced in~\\cite{GRADE}. GRADE supports full simulation control, ROS integration, realistic physics, while being in an engine that produces high visual fidelity images and ground truth data. We use GRADE to generate a dataset focused on indoor dynamic scenes with people and flying objects. Using this, we evaluate the performance of YOLO and Mask R-CNN on the tasks of segmenting and detecting people. Our results provide evidence that using data generated with GRADE can improve the model performance when used for a pre-training step. We also show that, even training using only synthetic data, can generalize well to real-world images in the same application domain such as the ones from the TUM-RGBD dataset. The code, results, trained models, and the generated data are provided as open-source at https://eliabntt.github.io/grade-rr. &amp;amp;body=https://ps.is.mpg.de/publications/bonetto2023learningfromgrade&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "29": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/arctic\">ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/fzicong\">Fan, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Kaufmann, M., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 12943-12954, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27511\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27511\" href=\"#abstractContent27511\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27511\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arctic.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zc-alexfan/arctic\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/arctic/arctic_april_24.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2204.13662\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=bvMm8gfFbZ8\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arctic.is.tue.mpg.de\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27511/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/arctic&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC.: https://ps.is.mpg.de/publications/arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/arctic&amp;amp;title=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. &amp;amp;summary={ARCTIC}: A Dataset for Dexterous Bimanual Hand-Object Manipulation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. %20https://ps.is.mpg.de/publications/arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. &amp;amp;body=https://ps.is.mpg.de/publications/arctic&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "28": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/zheng_2023_cvpr\">PointAvatar: Deformable Point-Based Head Avatars From Videos</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yzheng\">Zheng, Y.</a></span>, Yifan, W., Wetzstein, G., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 21057-21067, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27605\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27605\" href=\"#abstractContent27605\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27605\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2212.08377.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zhengyuf.github.io/PointAvatar/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zhengyuf/pointavatar\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/wll_XtgpU7U\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27605/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.: https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/zheng_2023_cvpr&amp;amp;title=The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. &amp;amp;summary={PointAvatar}: Deformable Point-Based Head Avatars From Videos&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. %20https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. &amp;amp;body=https://ps.is.mpg.de/publications/zheng_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "27": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/yi2023talkshow\">Generating Holistic 3D Human Motion from Speech</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, Liang, H., Liu, Y., Cao, Q., <span class=\"default-link-ul\"><a href=\"/person/ydwen\">Wen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, Tao, D., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 469-480, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27612\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27612\" href=\"#abstractContent27612\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27612\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThis work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://talkshow.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yhw-yhw/SHOW\"><i class=\"fa fa-github-square\"/>  SHOW code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yhw-yhw/TalkSHOW\"><i class=\"fa fa-github-square\"/>  TalkSHOW code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.04420\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"\"><i class=\"fa fa-file-o\"/>  </a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27612/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi2023talkshow&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de.: https://ps.is.mpg.de/publications/yi2023talkshow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi2023talkshow&amp;amp;title=This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de. &amp;amp;summary=Generating Holistic {3D} Human Motion from Speech&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/yi2023talkshow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/yi2023talkshow&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "26": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/sun-cvpr-2023\">TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSun, Y., Bao, Q., Liu, W., Mei, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 8856-8866, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27643\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27643\" href=\"#abstractContent27643\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27643\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAlthough the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new \"maps\" to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/supplemental/Sun_TRACE_5D_Temporal_CVPR_2023_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.yusun.work/TRACE/TRACE.html\"><i class=\"fa fa-file-o\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/l8aLHDXWQRw\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27643/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/sun-cvpr-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes.: https://ps.is.mpg.de/publications/sun-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/sun-cvpr-2023&amp;amp;title=Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes. &amp;amp;summary={TRACE}: {5D} Temporal Regression of Avatars With Dynamic Cameras in {3D} Environments&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes. %20https://ps.is.mpg.de/publications/sun-cvpr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new &amp;quot;maps&amp;quot; to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/sun-cvpr-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "25": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/bonetto2023dynamicslam\">Simulation of Dynamic Environments for SLAM</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ebonetto\">Bonetto, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/cxu\">Xu, C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>ICRA 2023 Workshop on the Active Methods in Autonomous Navigation</em>, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27662\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27662\" href=\"#abstractContent27662\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27662\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tSimulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information  like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rr .\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eliabntt/GRADE-RR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/robot-perception-group/GRADE_tools\"><i class=\"fa fa-github-square\"/>  Evaluation code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eliabntt/GRADE_data\"><i class=\"fa fa-github-square\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/717/2305.04286.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://robotics.pme.duth.gr/workshop_active2/wp-content/uploads/2023/05/01.-Simulation-of-Dynamic-Environments-for-SLAM.pdf%7D%7D\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27662/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/bonetto2023dynamicslam&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Simulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information  like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rr .: https://ps.is.mpg.de/publications/bonetto2023dynamicslam&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/bonetto2023dynamicslam&amp;amp;title=Simulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information  like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rr . &amp;amp;summary=Simulation of Dynamic Environments for SLAM&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Simulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information  like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rr . %20https://ps.is.mpg.de/publications/bonetto2023dynamicslam&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Simulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information  like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rr . &amp;amp;body=https://ps.is.mpg.de/publications/bonetto2023dynamicslam&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "24": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/behrens-vr-2023\">Virtual Reality Exposure to a Healthy Weight Body Is a Promising Adjunct Treatment for Anorexia Nervosa</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/smoelbert\">Behrens, S. C.</a></span>, Tesch, J., Sun, P. J., Starke, S., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Schneider, H., Pruccoli, J., Zipfel, S., Giel, K. E.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Psychotherapy and Psychosomatics</em>, 92(3):170-179, June 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27667\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27667\" href=\"#abstractContent27667\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27667\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIntroduction/Objective: Treatment results of anorexia nervosa (AN) are modest, with fear of weight gain being a strong predictor of treatment outcome and relapse. Here, we present a virtual reality (VR) setup for exposure to healthy weight and evaluate its potential as an adjunct treatment for AN. Methods: In two studies, we investigate VR experience and clinical effects of VR exposure to higher weight in 20 women with high weight concern or shape concern and in 20 women with AN. Results: In study 1, 90% of participants (18/20) reported symptoms of high arousal but verbalized low to medium levels of fear. Study 2 demonstrated that VR exposure to healthy weight induced high arousal in patients with AN and yielded a trend that four sessions of exposure improved fear of weight gain. Explorative analyses revealed three clusters of individual reactions to exposure, which need further exploration. Conclusions: VR exposure is a well-accepted and powerful tool for evoking fear of weight gain in patients with AN. We observed a statistical trend that repeated virtual exposure to healthy weight improved fear of weight gain with large effect sizes. Further studies are needed to determine the mechanisms and differential effects.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://karger.com/pps/article/92/3/170/844027/Virtual-Reality-Exposure-to-a-Healthy-Weight-Body\"><i class=\"fa fa-file-o\"/>  on-line</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1159/000530932\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27667/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/behrens-vr-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Introduction/Objective: Treatment results of anorexia nervosa (AN) are modest, with fear of weight gain being a strong predictor of treatment outcome and relapse. Here, we present a virtual reality (VR) setup for exposure to healthy weight and evaluate its potential as an adjunct treatment for AN. Methods: In two studies, we investigate VR experience and clinical effects of VR exposure to higher weight in 20 women with high weight concern or shape concern and in 20 women with AN. Results: In study 1, 90% of participants (18/20) reported symptoms of high arousal but verbalized low to medium levels of fear. Study 2 demonstrated that VR exposure to healthy weight induced high arousal in patients with AN and yielded a trend that four sessions of exposure improved fear of weight gain. Explorative analyses revealed three clusters of individual reactions to exposure, which need further exploration. Conclusions: VR exposure is a well-accepted and powerful tool for evoking fear of weight gain in patients with AN. We observed a statistical trend that repeated virtual exposure to healthy weight improved fear of weight gain with large effect sizes. Further studies are needed to determine the mechanisms and differential effects.: https://ps.is.mpg.de/publications/behrens-vr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/behrens-vr-2023&amp;amp;title=Introduction/Objective: Treatment results of anorexia nervosa (AN) are modest, with fear of weight gain being a strong predictor of treatment outcome and relapse. Here, we present a virtual reality (VR) setup for exposure to healthy weight and evaluate its potential as an adjunct treatment for AN. Methods: In two studies, we investigate VR experience and clinical effects of VR exposure to higher weight in 20 women with high weight concern or shape concern and in 20 women with AN. Results: In study 1, 90% of participants (18/20) reported symptoms of high arousal but verbalized low to medium levels of fear. Study 2 demonstrated that VR exposure to healthy weight induced high arousal in patients with AN and yielded a trend that four sessions of exposure improved fear of weight gain. Explorative analyses revealed three clusters of individual reactions to exposure, which need further exploration. Conclusions: VR exposure is a well-accepted and powerful tool for evoking fear of weight gain in patients with AN. We observed a statistical trend that repeated virtual exposure to healthy weight improved fear of weight gain with large effect sizes. Further studies are needed to determine the mechanisms and differential effects. &amp;amp;summary=Virtual Reality Exposure to a Healthy Weight Body Is a Promising Adjunct Treatment for Anorexia Nervosa&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Introduction/Objective: Treatment results of anorexia nervosa (AN) are modest, with fear of weight gain being a strong predictor of treatment outcome and relapse. Here, we present a virtual reality (VR) setup for exposure to healthy weight and evaluate its potential as an adjunct treatment for AN. Methods: In two studies, we investigate VR experience and clinical effects of VR exposure to higher weight in 20 women with high weight concern or shape concern and in 20 women with AN. Results: In study 1, 90% of participants (18/20) reported symptoms of high arousal but verbalized low to medium levels of fear. Study 2 demonstrated that VR exposure to healthy weight induced high arousal in patients with AN and yielded a trend that four sessions of exposure improved fear of weight gain. Explorative analyses revealed three clusters of individual reactions to exposure, which need further exploration. Conclusions: VR exposure is a well-accepted and powerful tool for evoking fear of weight gain in patients with AN. We observed a statistical trend that repeated virtual exposure to healthy weight improved fear of weight gain with large effect sizes. Further studies are needed to determine the mechanisms and differential effects. %20https://ps.is.mpg.de/publications/behrens-vr-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Introduction/Objective: Treatment results of anorexia nervosa (AN) are modest, with fear of weight gain being a strong predictor of treatment outcome and relapse. Here, we present a virtual reality (VR) setup for exposure to healthy weight and evaluate its potential as an adjunct treatment for AN. Methods: In two studies, we investigate VR experience and clinical effects of VR exposure to higher weight in 20 women with high weight concern or shape concern and in 20 women with AN. Results: In study 1, 90% of participants (18/20) reported symptoms of high arousal but verbalized low to medium levels of fear. Study 2 demonstrated that VR exposure to healthy weight induced high arousal in patients with AN and yielded a trend that four sessions of exposure improved fear of weight gain. Explorative analyses revealed three clusters of individual reactions to exposure, which need further exploration. Conclusions: VR exposure is a well-accepted and powerful tool for evoking fear of weight gain in patients with AN. We observed a statistical trend that repeated virtual exposure to healthy weight improved fear of weight gain with large effect sizes. Further studies are needed to determine the mechanisms and differential effects. &amp;amp;body=https://ps.is.mpg.de/publications/behrens-vr-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "23": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tripathi2023arctic\">3D Human Pose Estimation via Intuitive Physics</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lmueller2\">M&#252;ller, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 4713-4725, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27512\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27512\" href=\"#abstractContent27512\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27512\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ipman.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://moyo.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Moyo Dataset</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ipman.is.tue.mpg.de\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27512/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tripathi2023arctic&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.: https://ps.is.mpg.de/publications/tripathi2023arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tripathi2023arctic&amp;amp;title=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. &amp;amp;summary={3D} Human Pose Estimation via Intuitive Physics&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. %20https://ps.is.mpg.de/publications/tripathi2023arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. &amp;amp;body=https://ps.is.mpg.de/publications/tripathi2023arctic&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "22": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/forte23-cvpr-sgnify\">Reconstructing Signing Avatars from Video Using Linguistic Priors</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/Forte\">Forte, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/pkulits\">Kulits, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kjk\">Kuchenbecker, K. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 12791-12801, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27581\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27581\" href=\"#abstractContent27581\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27581\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tSign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify's 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27581/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de.: https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&amp;amp;title=Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de. &amp;amp;summary=Reconstructing Signing Avatars from Video Using Linguistic Priors&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify&amp;#39;s 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/forte23-cvpr-sgnify&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "21": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hot2023chen\">Detecting Human-Object Contact in Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ychen2\">Chen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sdwivedi\">Dwivedi, S. K.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 17100-17110, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27520\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27520\" href=\"#abstractContent27520\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27520\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (\"Human-Object conTact\"), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://hot.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2303.03373\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yixchen/HOT\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27520/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hot2023chen&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability.: https://ps.is.mpg.de/publications/hot2023chen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hot2023chen&amp;amp;title=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. &amp;amp;summary=Detecting Human-Object Contact in Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. %20https://ps.is.mpg.de/publications/hot2023chen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. &amp;amp;body=https://ps.is.mpg.de/publications/hot2023chen&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "20": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tempeh\">Instant Multi-View Head Capture through Learnable Registration</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tli\">Li, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 768-779, June 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27582\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27582\" href=\"#abstractContent27582\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27582\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tExisting methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tempeh.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/AolpvKpmjEw\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/711/CVPR2023_Multiview_Face_Capture.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/712/CVPR2023_Multiview_Face_Capture_supmat.pdf\"><i class=\"fa fa-file-pdf-o\"/>  sup. mat.</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/713/TEMPEH_CVPR_poster_final.png\"><i class=\"fa fa-file-image-o\"/>  poster</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27582/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tempeh&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de.: https://ps.is.mpg.de/publications/tempeh&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tempeh&amp;amp;title=Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. &amp;amp;summary=Instant Multi-View Head Capture through Learnable Registration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/tempeh&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow, and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans&#8217; surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/tempeh&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "19": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dai_2023_cvpr\">SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tYudi, D., Yitai, L., Xiping, L., Chenglu, W., Lan, X., <span class=\"default-link-ul\"><a href=\"/person/hyi\">Hongwei, Y.</a></span>, Siqi, S., Yuexin, M., Cheng, W.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 682-692, CVF, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27645\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27645\" href=\"#abstractContent27645\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27645\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.lidarhumanmotion.net/sloper4d/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.lidarhumanmotion.net/data-sloper4d/\"><i class=\"fa fa-file-o\"/>  dataset</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/climbingdaily/SLOPER4D\"><i class=\"fa fa-github-square\"/>  codebase</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_SLOPER4D_A_Scene-Aware_Dataset_for_Global_4D_Human_Pose_Estimation_CVPR_2023_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2303.09095.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27645/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dai_2023_cvpr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D.: https://ps.is.mpg.de/publications/dai_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dai_2023_cvpr&amp;amp;title=We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D. &amp;amp;summary=SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D. %20https://ps.is.mpg.de/publications/dai_2023_cvpr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects&#8217; activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100K LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released https://github.com/climbingdaily/SLOPER4D. &amp;amp;body=https://ps.is.mpg.de/publications/dai_2023_cvpr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "18": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870\">MeshDiffusion: Score-based Generative 3D Mesh Modeling</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/zliu2\">Liu, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Nowrouzezahrai, D., Paull, L., <span class=\"default-link-ul\"><a href=\"/person/wliu\">Liu, W.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Proceedings of the Eleventh International Conference on Learning Representations (ICLR)</em>, May 2023 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27571\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27571\" href=\"#abstractContent27571\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27571\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openreview.net/pdf?id=0cpM2ApF9p6\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27571/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.: https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&amp;amp;title=We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks. &amp;amp;summary=MeshDiffusion: Score-based Generative 3D Mesh Modeling&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks. %20https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks. &amp;amp;body=https://ps.is.mpg.de/publications/liuetal23-17c6dde4-cfd9-436f-b8e6-afe70b58b870&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "17": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/chen-pami-2023\">Fast-SNARF: A Fast Deformer for Articulated Neural Fields</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, Jiang, T., Song, J., Rietmann, M., <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</em>,  pages: 1-15, April 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27599\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27599\" href=\"#abstractContent27599\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27599\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tNeural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.cvlibs.net/publications/Chen2023PAMI.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10112633\"><i class=\"fa fa-file-o\"/>  publisher site</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/xuchen-ethz/fast-snarf\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1109/TPAMI.2023.3271569\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27599/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/chen-pami-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans.: https://ps.is.mpg.de/publications/chen-pami-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/chen-pami-2023&amp;amp;title=Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans. &amp;amp;summary={Fast-SNARF}: A Fast Deformer for Articulated Neural Fields&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans. %20https://ps.is.mpg.de/publications/chen-pami-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of 150&#215; . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans. &amp;amp;body=https://ps.is.mpg.de/publications/chen-pami-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "16": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/barc-ijcv-2023\">BARC: Breed-Augmented Regression Using Classification for 3D Dog Reconstruction from Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Int. J. of Comp. Vis. (IJCV)</em>, April 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27537\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27537\" href=\"#abstractContent27537\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27537\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/s11263-023-01780-3\"><i class=\"fa fa-file-o\"/>  On-line</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://trebuchet.public.springernature.app/get_content/2e120f6c-1f70-48d4-9629-9356f52e2230\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/s11263-023-01780-3\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27537/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/barc-ijcv-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/barc-ijcv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/barc-ijcv-2023&amp;amp;title=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. &amp;amp;summary={BARC}: Breed-Augmented Regression Using Classification for {3D} Dog Reconstruction from Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/barc-ijcv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/barc-ijcv-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "15": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/saini-ral-2023\">SmartMocap: Joint Estimation of Human and Camera Motion Using Uncalibrated RGB Cameras</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, 8(6):3206-3213, 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27600\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27600\" href=\"#abstractContent27600\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27600\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMarkerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10093047\"><i class=\"fa fa-file-o\"/>  publisher site</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2023.3264743\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27600/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/saini-ral-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera.: https://ps.is.mpg.de/publications/saini-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/saini-ral-2023&amp;amp;title=Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. &amp;amp;summary={SmartMocap}: Joint Estimation of Human and Camera Motion Using Uncalibrated {RGB} Cameras&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. %20https://ps.is.mpg.de/publications/saini-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences (~1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. &amp;amp;body=https://ps.is.mpg.de/publications/saini-ral-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "14": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/price-ral-2023\">Viewpoint-Driven Formation Control of Airships for Cooperative Target Tracking</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, 8(6):3653-3660, 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27601\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27601\" href=\"#abstractContent27601\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27601\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tFor tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/10092932\"><i class=\"fa fa-file-o\"/>  publisher's site</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2023.3264727\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27601/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/price-ral-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship.: https://ps.is.mpg.de/publications/price-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/price-ral-2023&amp;amp;title=For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship. &amp;amp;summary=Viewpoint-Driven Formation Control of Airships for Cooperative Target Tracking&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship. %20https://ps.is.mpg.de/publications/price-ral-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=For tracking and motion capture (MoCap) of animals in their natural habitat, a formation of safe and silent aerial platforms, such as airships with on-board cameras, is well suited. In our prior work we derived formation properties for optimal MoCap, which include maintaining constant angular separation between observers w.r.t. the subject, threshold distance to it and keeping it centered in the camera view. Unlike multi-rotors, airships have non-holonomic constrains and are affected by ambient wind. Their orientation and flight direction are also tightly coupled. Therefore a control scheme for multicopters that assumes independence of motion direction and orientation is not applicable. In this letter, we address this problem by first exploiting a periodic relationship between the airspeed of an airship and its distance to the subject. We use it to derive analytical and numeric solutions that satisfy the formation properties for optimal MoCap. Based on this, we develop an MPC-based formation controller. We perform theoretical analysis of our solution, boundary conditions of its applicability, extensive simulation experiments and a real world demonstration of our control method with an unmanned airship. &amp;amp;body=https://ps.is.mpg.de/publications/price-ral-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "13": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c\">SCARF: Capturing and Animation of Body and Clothing from Monocular Video</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, Pollefeys, M., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>SIGGRAPH Asia 2022 Conference Papers</em>,  pages: 9, SA&#8217;22, December 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27367\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27367\" href=\"#abstractContent27367\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27367\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://yfeng95.github.io/scarf/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yfeng95/SCARF\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2210.01868.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1145/3550469.3555423\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27367/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.: https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&amp;amp;title=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;summary={SCARF}: Capturing and Animation of Body and Clothing from Monocular Video&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. %20https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;body=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "12": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/choutas-thesis-2022\">Reconstructing Expressive 3D Humans from RGB Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tETH Zurich, Max Planck Institute for Intelligent Systems and ETH Zurich, December 2022 <small class=\"text-muted\">(thesis)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27431\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27431\" href=\"#abstractContent27431\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27431\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo interact with our environment, we need to adapt our body posture&#13;\nand grasp objects with our hands. During a conversation our facial expressions&#13;\nand hand gestures convey important non-verbal cues about our&#13;\nemotional state and intentions towards our fellow speakers. Thus, modeling&#13;\nand capturing 3D full-body shape and pose, hand articulation and facial&#13;\nexpressions are necessary to create realistic human avatars for augmented&#13;\nand virtual reality. This is a complex task, due to the large number of&#13;\ndegrees of freedom for articulation, body shape variance, occlusions from&#13;\nobjects and self-occlusions from body parts, e.g. crossing our hands, and&#13;\nsubject appearance. The community has thus far relied on expensive and&#13;\ncumbersome equipment, such as multi-view cameras or motion capture&#13;\nmarkers, to capture the 3D human body. While this approach is effective,&#13;\nit is limited to a small number of subjects and indoor scenarios. Using&#13;\nmonocular RGB cameras would greatly simplify the avatar creation process,&#13;\nthanks to their lower cost and ease of use. These advantages come at a price&#13;\nthough, since RGB capture methods need to deal with occlusions, perspective&#13;\nambiguity and large variations in subject appearance, in addition to&#13;\nall the challenges posed by full-body capture. In an attempt to simplify the&#13;\nproblem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;\nthe body, face and hands with distinct methods using part-specific&#13;\ndatasets and benchmarks. However, the hands and face constrain the body&#13;\nand vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;\netc.; the divide-and-conquer approach can not utilize this constraint.&#13;\nIn this thesis, we aim to reconstruct the full 3D human body, using only&#13;\nreadily accessible monocular RGB images. In a first step, we introduce a&#13;\nparametric 3D body model, called SMPL-X, that can represent full-body&#13;\nshape and pose, hand articulation and facial expression. Next, we present&#13;\nan iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;\n2D image keypoints. While SMPLify-X can produce plausible results if&#13;\nthe 2D observations are sufficiently reliable, it is slow and susceptible&#13;\nto initialization. To overcome these limitations, we introduce ExPose, a&#13;\nneural network regressor, that predicts SMPL-X parameters from an image&#13;\nusing body-driven attention, i.e. by zooming in on the hands and face,&#13;\nafter predicting the body. From the zoomed-in part images, dedicated&#13;\npart networks predict the hand and face parameters. ExPose combines&#13;\nthe independent body, hand, and face estimates by trusting them equally.&#13;\nThis approach though does not fully exploit the correlation between parts&#13;\nand fails in the presence of challenges such as occlusion or motion blur.&#13;\nThus, we need a better mechanism to aggregate information from the full&#13;\nbody and part images. PIXIE uses neural networks called moderators that&#13;\nlearn to fuse information from these two image sets before predicting the&#13;\nfinal part parameters. Overall, the addition of the hands and face leads to&#13;\nnoticeably more natural and expressive reconstructions.&#13;\nCreating high fidelity avatars from RGB images requires accurate estimation&#13;\nof 3D body shape. Although existing methods are effective at&#13;\npredicting body pose, they struggle with body shape. We identify the lack&#13;\nof proper training data as the cause. To overcome this obstacle, we propose&#13;\nto collect internet images from fashion models websites, together with&#13;\nanthropometric measurements. At the same time, we ask human annotators&#13;\nto rate images and meshes according to a pre-defined set of linguistic attributes.&#13;\nWe then define mappings between measurements, linguistic shape&#13;\nattributes and 3D body shape. Equipped with these mappings, we train a&#13;\nneural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;\nfrom a single RGB image. We observe that existing 3D shape benchmarks&#13;\nlack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;\nbenchmark, Human Bodies in the Wild (HBW), which contains images of&#13;\nhumans and their corresponding 3D ground-truth body shape. SHAPY&#13;\nshows how we can overcome the lack of in-the-wild images with 3D shape&#13;\nannotations through easy-to-obtain anthropometric measurements and linguistic&#13;\nshape attributes.&#13;\nRegressors that estimate 3D model parameters are robust and accurate,&#13;\nbut often fail to tightly fit the observations. Optimization-based approaches&#13;\ntightly fit the data, by minimizing an energy function composed of a data&#13;\nterm that penalizes deviations from the observations and priors that encode&#13;\nour knowledge of the problem. Finding the balance between these terms&#13;\nand implementing a performant version of the solver is a time-consuming&#13;\nand non-trivial task. Machine-learned continuous optimizers combine the&#13;\nbenefits of both regression and optimization approaches. They learn the&#13;\npriors directly from data, avoiding the need for hand-crafted heuristics and&#13;\nloss term balancing, and benefit from optimized neural network frameworks&#13;\nfor fast inference. Inspired from the classic Levenberg-Marquardt&#13;\nalgorithm, we propose a neural optimizer that outperforms classic optimization,&#13;\nregression and hybrid optimization-regression approaches. Our&#13;\nproposed update rule uses a weighted combination of gradient descent&#13;\nand a network-predicted update. To show the versatility of the proposed&#13;\nmethod, we apply it on three other problems, namely full body estimation&#13;\nfrom (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;\ndevice and (iii) face tracking from dense 2D landmarks. Our method can&#13;\neasily be applied to new model fitting problems and offers a competitive&#13;\nalternative to well-tuned traditional model fitting pipelines, both in terms&#13;\nof accuracy and speed.&#13;\nTo summarize, we propose a new and richer representation of the human&#13;\nbody, SMPL-X, that is able to jointly model the 3D human body pose&#13;\nand shape, facial expressions and hand articulation. We propose methods,&#13;\nSMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;\nmonocular RGB images, progressively improving the accuracy and realism&#13;\nof the predictions. To further improve reconstruction fidelity, we demonstrate&#13;\nhow we can use easy-to-collect internet data and human annotations&#13;\nto overcome the lack of 3D shape data and train a model, SHAPY, that&#13;\npredicts accurate 3D body shape from a single RGB image. Finally, we&#13;\npropose a flexible learnable update rule for parametric human model fitting&#13;\nthat outperforms both classic optimization and neural network approaches.&#13;\nThis approach is easily applicable to a variety of problems, unlocking new&#13;\napplications in AR/VR scenarios.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.research-collection.ethz.ch/handle/20.500.11850/590562\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27431/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/choutas-thesis-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios.: https://ps.is.mpg.de/publications/choutas-thesis-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/choutas-thesis-2022&amp;amp;title=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. &amp;amp;summary=Reconstructing Expressive {3D} Humans from {RGB} Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. %20https://ps.is.mpg.de/publications/choutas-thesis-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. &amp;amp;body=https://ps.is.mpg.de/publications/choutas-thesis-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "11": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/immvirtreal_sbehrens_2022\">How immersive virtual reality can become a key tool to advance research and psychotherapy of eating and weight disorders</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/smoelbert\">Behrens, S. C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, Keizer, A., Giel, K. E.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Frontiers in Psychiatry</em>, 13, pages: 1011620, November 2022 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27490\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27490\" href=\"#abstractContent27490\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27490\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tImmersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.3389/fpsyt.2022.1011620\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27490/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed.: https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&amp;amp;title=Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed. &amp;amp;summary=How immersive virtual reality can become a key tool to advance research and psychotherapy of eating and weight disorders&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed. %20https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Immersive virtual reality technology (VR) still waits for its wide dissemination in research and psychotherapy of eating and weight disorders. Given the comparably high efforts in producing a VR setup, we outline that the technology&#8217;s breakthrough needs tailored exploitation of specific features of VR and user-centered design of setups. In this paper, we introduce VR hardware and review the specific properties of immersive VR versus real-world setups providing examples how they improved existing setups. We then summarize current approaches to make VR a tool for psychotherapy of eating and weight disorders and introduce user-centered design of VR environments as a solution to support their further development. Overall, we argue that exploitation of the specific properties of VR can substantially improve existing approaches for research and therapy of eating and weight disorders. To produce more than pilot setups, iterative development of VR setups within a user-centered design approach is needed. &amp;amp;body=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "10": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dart2022\">DART: Articulated Hand Model with Diverse Accessories and Rich Textures</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tGao*, D., <span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu*, Y.</a></span>, Li*, K., Yang*, L., Wang, F., Zhang, P., Zhang, B., Lu, C., Tan, P.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)</em>, November 2022 <small class=\"text-muted\">(conference)</small> <span class=\"label label-light\">In press</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27270\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27270\" href=\"#abstractContent27270\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27270\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dart2022.github.io/\"><i class=\"fa fa-github-square\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/DART2022/DARTset\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtube.com/embed/VvlUYe-9b7U\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27270/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dart2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.: https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dart2022&amp;amp;title=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;summary={DART}: {A}rticulated {H}and {M}odel with {D}iverse {A}ccessories and {R}ich {T}extures&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. %20https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/dart2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "9": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/mica-eccv2022\">Towards Metrical Reconstruction of Human Faces</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/wzielonka\">Zielonka, W.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 13, pages: 250-269, Lecture Notes in Computer Science, 13673, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27243\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27243\" href=\"#abstractContent27243\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27243\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tFace reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;\nprovides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2204.06607.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zielon.github.io/mica/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/vzzEbvv08VA\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/MICA\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-19778-9_15\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27243/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/mica-eccv2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.: https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/mica-eccv2022&amp;amp;title=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;summary=Towards Metrical Reconstruction of Human Faces&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. %20https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;body=https://ps.is.mpg.de/publications/mica-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "8": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/liu_iros_22\">Deep Residual Reinforcement Learning based Autonomous Blimp Control</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yliu2\">Liu, Y. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)</em>,  pages: 12566-12573, IEEE, Piscataway, NJ, October 2022 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27136\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27136\" href=\"#abstractContent27136\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27136\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tBlimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/IROS47612.2022.9981182\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27136/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/liu_iros_22&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .: https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/liu_iros_22&amp;amp;title=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;summary=Deep Residual Reinforcement Learning based Autonomous Blimp Control&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . %20https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;body=https://ps.is.mpg.de/publications/liu_iros_22&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "7": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/trust-eccv2022\">Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hfeng\">Feng, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Abrevaya, V. F.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 13, pages: 72-90, Lecture Notes in Computer Science, 13673, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27244\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27244\" href=\"#abstractContent27244\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27244\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tVirtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;\nof-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2205.03962.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://trust.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/HavenFeng/TRUST\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-19778-9_5\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27244/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/trust-eccv2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.: https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/trust-eccv2022&amp;amp;title=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;summary=Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/trust-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "6": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/learning_to_fit\">Learning to Fit Morphable Models</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fbogo\">Bogo, F.</a></span>, Shen, J., Valentin, J.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 6, pages: 160-179, Lecture Notes in Computer Science, 13666, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://vchoutas.github.io/learning_to_fit/\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.dropbox.com/s/4cksaq8h4royt9r/Learning%20To%20Fit%20Morphable%20Models%20FHD%20HB.mp4?raw=1\"><i class=\"fa fa-file-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/702/learning_to_fit.pdf\"><i class=\"fa fa-file-pdf-o\"/>  PDF</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/703/LearningToFit_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-20068-7_10\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27333/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/learning_to_fit&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/learning_to_fit&amp;amp;title= &amp;amp;summary=Learning to Fit Morphable Models&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/learning_to_fit&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "5": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/supr\">SUPR: A Sparse Unified Part-Based Human Representation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aosman\">Osman, A. A. A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision &#8211; ECCV 2022</em>, 2, pages: 568-585, Lecture Notes in Computer Science, 13662, <span class=\"text-muted\">(Editors: Avidan, Shai and Brostow, Gabriel and Ciss&#233;, Moustapha and Farinella, Giovanni Maria and Hassner, Tal)</span>, Springer, Cham, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27315\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27315\" href=\"#abstractContent27315\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27315\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tStatistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://supr.is.tuebingen.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/ahmedosman/SUPR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/699/0570_source.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Main Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/700/0570-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supp. Mat. </a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/701/final_supr_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-20086-1_33\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27315/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/supr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes.: https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/supr&amp;amp;title=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. &amp;amp;summary={SUPR}: A Sparse Unified Part-Based Human Representation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. %20https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/supr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "4": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/temos\">TEMOS: Generating diverse human motions from textual descriptions</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27769\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27769\" href=\"#abstractContent27769\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27769\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mathis.petrovich.fr/temos\"><i class=\"fa fa-file-o\"/>  website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Mathux/TEMOS\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2204.14109\"><i class=\"fa fa-file-o\"/>  paper-arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=07dQiKK17aQ\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mathis.petrovich.fr/temos\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27769/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/temos&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage.: https://ps.is.mpg.de/publications/temos&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/temos&amp;amp;title=We address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage. &amp;amp;summary=TEMOS: Generating diverse human motions from textual descriptions&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage. %20https://ps.is.mpg.de/publications/temos&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage. &amp;amp;body=https://ps.is.mpg.de/publications/temos&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "3": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/teach-2022\">TEACH: Temporal Action Composition for 3D Humans</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nathanasiou\">Athanasiou, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn  pages: 414-423, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27247\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27247\" href=\"#abstractContent27247\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27247\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGiven a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/athn-nik/teach\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.04066\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://teach.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=ENr4GQO0RSc&amp;ab_channel=3DV2022\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/abstract/document/10044438\"><i class=\"fa fa-file-o\"/>  camera-ready</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV57658.2022.00053\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27247/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/teach-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.: https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/teach-2022&amp;amp;title=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;summary={TEACH}: {T}emporal {A}ction {C}omposition for 3{D} {H}umans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/teach-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "2": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/intercap_gcpr2022\">InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Honorable Mention for Best Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yhuang2\">Huang, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Pattern Recognition</em>,  pages: 281-299, Lecture Notes in Computer Science, 13485, <span class=\"text-muted\">(Editors: Andres, Bj&#246;rn and Bernard, Florian and Cremers, Daniel and Frintrop, Simone and Goldl&#252;cke, Bastian and Ihrke, Ivo)</span>, Springer, Cham, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27278\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27278\" href=\"#abstractContent27278\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27278\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/YinghaoHuang91/InterCap/tree/master\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://intercap.is.tue.mpg.de/download.php\"><i class=\"fa fa-file-o\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/d5wHLDIqN6c\"><i class=\"fa fa-file-video-o\"/>  YouTube Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://intercap.is.tue.mpg.de/media/upload/main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-16788-1_18\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27278/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.: https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/intercap_gcpr2022&amp;amp;title=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;summary={InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. %20https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "1": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/skirt-3dv-2022\">Neural Point-based Shape Modeling of Humans in Challenging Clothing</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/qma\">Ma, Q.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 International Conference on 3D Vision (3DV 2022)</em>,  pages: 679-689, IEEE, Piscataway, NJ, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27249\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27249\" href=\"#abstractContent27249\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27249\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tParametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://qianlim.github.io/SkiRT\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/qianlim/SkiRT\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.06814\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/695/SkiRT_main_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/696/SkiRT_supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/697/090_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://qianlim.github.io/SkiRT\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV57658.2022.00078\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27249/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.: https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/skirt-3dv-2022&amp;amp;title=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;summary=Neural Point-based Shape Modeling of Humans in Challenging Clothing&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. %20https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
}