{
    "50": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/econ2023xiu\">ECON: Explicit Clothed humans Optimized via Normal integration</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Highlight)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, Cao, X., <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27519\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27519\" href=\"#abstractContent27519\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27519\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://econ.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2212.07422\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/Yuliang/ECON\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/YuliangXiu/ECON\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=j5hw4tsWpoY\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno?usp=sharing\"><i class=\"fa fa-file-o\"/>  Colab</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://econ.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27519/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/econ2023xiu&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin.: https://ps.is.mpg.de/publications/econ2023xiu&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/econ2023xiu&amp;amp;title=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. &amp;amp;summary=ECON: Explicit Clothed humans Optimized via Normal integration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. %20https://ps.is.mpg.de/publications/econ2023xiu&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry but produce disembodied limbs or degenerate shapes for unseen poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit and explicit methods. To this end, we make two key observations:(1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a &#8220;canvas&#8221; for stitching together detailed surface patches. ECON infers high-fidelity 3D humans even in loose clothes and challenging poses, while having realistic faces and fingers. This goes beyond previous methods. Quantitative, evaluation of the CAPE and Renderpeople datasets shows that ECON is more accurate than the state of the art. Perceptual studies also show that ECON&#8217;s perceived realism is better by a large margin. &amp;amp;body=https://ps.is.mpg.de/publications/econ2023xiu&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "49": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/arctic\">ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/fzicong\">Fan, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Kaufmann, M., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27511\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27511\" href=\"#abstractContent27511\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27511\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arctic.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zc-alexfan/arctic\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/arctic/arctic_april_24.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2204.13662\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=bvMm8gfFbZ8\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arctic.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27511/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/arctic&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC.: https://ps.is.mpg.de/publications/arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/arctic&amp;amp;title=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. &amp;amp;summary={ARCTIC}: A Dataset for Dexterous Bimanual Hand-Object Manipulation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. %20https://ps.is.mpg.de/publications/arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. &amp;amp;body=https://ps.is.mpg.de/publications/arctic&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "48": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tripathi2023arctic\">3D Human Pose Estimation via Intuitive Physics</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lmueller2\">M&#252;ller, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27512\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27512\" href=\"#abstractContent27512\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27512\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ipman.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://moyo.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Moyo Dataset</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ipman.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27512/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tripathi2023arctic&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.: https://ps.is.mpg.de/publications/tripathi2023arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tripathi2023arctic&amp;amp;title=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. &amp;amp;summary=3D Human Pose Estimation via Intuitive Physics&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. %20https://ps.is.mpg.de/publications/tripathi2023arctic&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body&#8217;s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a &#8220;stable&#8221; configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research. &amp;amp;body=https://ps.is.mpg.de/publications/tripathi2023arctic&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "47": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hot2023chen\">Detecting Human-Object Contact in Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ychen2\">Chen, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sdwivedi\">Dwivedi, S. K.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>, June 2023 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27520\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27520\" href=\"#abstractContent27520\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27520\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (\"Human-Object conTact\"), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://hot.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2303.03373\"><i class=\"fa fa-file-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yixchen/HOT\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27520/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/hot2023chen&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability.: https://ps.is.mpg.de/publications/hot2023chen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/hot2023chen&amp;amp;title=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. &amp;amp;summary=Detecting Human-Object Contact in Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. %20https://ps.is.mpg.de/publications/hot2023chen&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (&amp;quot;Human-Object conTact&amp;quot;), a new dataset of human-object contacts for images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons for the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. &amp;amp;body=https://ps.is.mpg.de/publications/hot2023chen&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "46": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/barc-ijcv-2023\">BARC: Breed-Augmented Regression Using Classification for 3D Dog Reconstruction from Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Int. J. of Comp. Vis. (IJCV)</em>, April 2023 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27537\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27537\" href=\"#abstractContent27537\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27537\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/s11263-023-01780-3\"><i class=\"fa fa-file-o\"/>  On-line</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://trebuchet.public.springernature.app/get_content/2e120f6c-1f70-48d4-9629-9356f52e2230\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/s11263-023-01780-3\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27537/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/barc-ijcv-2023&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/barc-ijcv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/barc-ijcv-2023&amp;amp;title=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. &amp;amp;summary={BARC}: Breed-Augmented Regression Using Classification for {3D} Dog Reconstruction from Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/barc-ijcv-2023&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/barc-ijcv-2023&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "45": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/insta\">Instant Volumetric Head Avatars</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/wzielonka\">Zielonka, W.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>, 2023 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27515\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27515\" href=\"#abstractContent27515\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27515\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2211.12499v2.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zielon.github.io/insta/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/HOgaeWTih7Q\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/INSTA\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/metrical-tracker\"><i class=\"fa fa-github-square\"/>  face tracker code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://keeper.mpdl.mpg.de/d/5ea4d2c300e9444a8b0b/\"><i class=\"fa fa-file-o\"/>  dataset</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27515/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/insta&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.: https://ps.is.mpg.de/publications/insta&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/insta&amp;amp;title=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. &amp;amp;summary=Instant Volumetric Head Avatars&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. %20https://ps.is.mpg.de/publications/insta&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present Instant Volumetric Head Avatars (INSTA),a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. &amp;amp;body=https://ps.is.mpg.de/publications/insta&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "44": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c\">SCARF: Capturing and Animation of Body and Clothing from Monocular Video</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, Pollefeys, M., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>SIGGRAPH Asia 2022 Conference Papers</em>,  pages: 9, SA&#8217;22, December 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27367\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27367\" href=\"#abstractContent27367\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27367\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://yfeng95.github.io/scarf/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yfeng95/SCARF\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2210.01868.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1145/3550469.3555423\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27367/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects.: https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&amp;amp;title=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;summary={SCARF}: Capturing and Animation of Body and Clothing from Monocular Video&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. %20https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. &amp;amp;body=https://ps.is.mpg.de/publications/scarf-ac995f96-7046-4145-9d80-6cf84bc1d53c&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "43": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/choutas-thesis-2022\">Reconstructing Expressive 3D Humans from RGB Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tETH Zurich, Max Planck Institute for Intelligent Systems and ETH Zurich, December 2022 <small class=\"text-muted\">(thesis)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27431\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27431\" href=\"#abstractContent27431\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27431\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo interact with our environment, we need to adapt our body posture&#13;\nand grasp objects with our hands. During a conversation our facial expressions&#13;\nand hand gestures convey important non-verbal cues about our&#13;\nemotional state and intentions towards our fellow speakers. Thus, modeling&#13;\nand capturing 3D full-body shape and pose, hand articulation and facial&#13;\nexpressions are necessary to create realistic human avatars for augmented&#13;\nand virtual reality. This is a complex task, due to the large number of&#13;\ndegrees of freedom for articulation, body shape variance, occlusions from&#13;\nobjects and self-occlusions from body parts, e.g. crossing our hands, and&#13;\nsubject appearance. The community has thus far relied on expensive and&#13;\ncumbersome equipment, such as multi-view cameras or motion capture&#13;\nmarkers, to capture the 3D human body. While this approach is effective,&#13;\nit is limited to a small number of subjects and indoor scenarios. Using&#13;\nmonocular RGB cameras would greatly simplify the avatar creation process,&#13;\nthanks to their lower cost and ease of use. These advantages come at a price&#13;\nthough, since RGB capture methods need to deal with occlusions, perspective&#13;\nambiguity and large variations in subject appearance, in addition to&#13;\nall the challenges posed by full-body capture. In an attempt to simplify the&#13;\nproblem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;\nthe body, face and hands with distinct methods using part-specific&#13;\ndatasets and benchmarks. However, the hands and face constrain the body&#13;\nand vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;\netc.; the divide-and-conquer approach can not utilize this constraint.&#13;\nIn this thesis, we aim to reconstruct the full 3D human body, using only&#13;\nreadily accessible monocular RGB images. In a first step, we introduce a&#13;\nparametric 3D body model, called SMPL-X, that can represent full-body&#13;\nshape and pose, hand articulation and facial expression. Next, we present&#13;\nan iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;\n2D image keypoints. While SMPLify-X can produce plausible results if&#13;\nthe 2D observations are sufficiently reliable, it is slow and susceptible&#13;\nto initialization. To overcome these limitations, we introduce ExPose, a&#13;\nneural network regressor, that predicts SMPL-X parameters from an image&#13;\nusing body-driven attention, i.e. by zooming in on the hands and face,&#13;\nafter predicting the body. From the zoomed-in part images, dedicated&#13;\npart networks predict the hand and face parameters. ExPose combines&#13;\nthe independent body, hand, and face estimates by trusting them equally.&#13;\nThis approach though does not fully exploit the correlation between parts&#13;\nand fails in the presence of challenges such as occlusion or motion blur.&#13;\nThus, we need a better mechanism to aggregate information from the full&#13;\nbody and part images. PIXIE uses neural networks called moderators that&#13;\nlearn to fuse information from these two image sets before predicting the&#13;\nfinal part parameters. Overall, the addition of the hands and face leads to&#13;\nnoticeably more natural and expressive reconstructions.&#13;\nCreating high fidelity avatars from RGB images requires accurate estimation&#13;\nof 3D body shape. Although existing methods are effective at&#13;\npredicting body pose, they struggle with body shape. We identify the lack&#13;\nof proper training data as the cause. To overcome this obstacle, we propose&#13;\nto collect internet images from fashion models websites, together with&#13;\nanthropometric measurements. At the same time, we ask human annotators&#13;\nto rate images and meshes according to a pre-defined set of linguistic attributes.&#13;\nWe then define mappings between measurements, linguistic shape&#13;\nattributes and 3D body shape. Equipped with these mappings, we train a&#13;\nneural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;\nfrom a single RGB image. We observe that existing 3D shape benchmarks&#13;\nlack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;\nbenchmark, Human Bodies in the Wild (HBW), which contains images of&#13;\nhumans and their corresponding 3D ground-truth body shape. SHAPY&#13;\nshows how we can overcome the lack of in-the-wild images with 3D shape&#13;\nannotations through easy-to-obtain anthropometric measurements and linguistic&#13;\nshape attributes.&#13;\nRegressors that estimate 3D model parameters are robust and accurate,&#13;\nbut often fail to tightly fit the observations. Optimization-based approaches&#13;\ntightly fit the data, by minimizing an energy function composed of a data&#13;\nterm that penalizes deviations from the observations and priors that encode&#13;\nour knowledge of the problem. Finding the balance between these terms&#13;\nand implementing a performant version of the solver is a time-consuming&#13;\nand non-trivial task. Machine-learned continuous optimizers combine the&#13;\nbenefits of both regression and optimization approaches. They learn the&#13;\npriors directly from data, avoiding the need for hand-crafted heuristics and&#13;\nloss term balancing, and benefit from optimized neural network frameworks&#13;\nfor fast inference. Inspired from the classic Levenberg-Marquardt&#13;\nalgorithm, we propose a neural optimizer that outperforms classic optimization,&#13;\nregression and hybrid optimization-regression approaches. Our&#13;\nproposed update rule uses a weighted combination of gradient descent&#13;\nand a network-predicted update. To show the versatility of the proposed&#13;\nmethod, we apply it on three other problems, namely full body estimation&#13;\nfrom (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;\ndevice and (iii) face tracking from dense 2D landmarks. Our method can&#13;\neasily be applied to new model fitting problems and offers a competitive&#13;\nalternative to well-tuned traditional model fitting pipelines, both in terms&#13;\nof accuracy and speed.&#13;\nTo summarize, we propose a new and richer representation of the human&#13;\nbody, SMPL-X, that is able to jointly model the 3D human body pose&#13;\nand shape, facial expressions and hand articulation. We propose methods,&#13;\nSMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;\nmonocular RGB images, progressively improving the accuracy and realism&#13;\nof the predictions. To further improve reconstruction fidelity, we demonstrate&#13;\nhow we can use easy-to-collect internet data and human annotations&#13;\nto overcome the lack of 3D shape data and train a model, SHAPY, that&#13;\npredicts accurate 3D body shape from a single RGB image. Finally, we&#13;\npropose a flexible learnable update rule for parametric human model fitting&#13;\nthat outperforms both classic optimization and neural network approaches.&#13;\nThis approach is easily applicable to a variety of problems, unlocking new&#13;\napplications in AR/VR scenarios.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.research-collection.ethz.ch/handle/20.500.11850/590562\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27431/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/choutas-thesis-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios.: https://ps.is.mpg.de/publications/choutas-thesis-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/choutas-thesis-2022&amp;amp;title=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. &amp;amp;summary=Reconstructing Expressive {3D} Humans from {RGB} Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. %20https://ps.is.mpg.de/publications/choutas-thesis-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To interact with our environment, we need to adapt our body posture&#13;&#10;and grasp objects with our hands. During a conversation our facial expressions&#13;&#10;and hand gestures convey important non-verbal cues about our&#13;&#10;emotional state and intentions towards our fellow speakers. Thus, modeling&#13;&#10;and capturing 3D full-body shape and pose, hand articulation and facial&#13;&#10;expressions are necessary to create realistic human avatars for augmented&#13;&#10;and virtual reality. This is a complex task, due to the large number of&#13;&#10;degrees of freedom for articulation, body shape variance, occlusions from&#13;&#10;objects and self-occlusions from body parts, e.g. crossing our hands, and&#13;&#10;subject appearance. The community has thus far relied on expensive and&#13;&#10;cumbersome equipment, such as multi-view cameras or motion capture&#13;&#10;markers, to capture the 3D human body. While this approach is effective,&#13;&#10;it is limited to a small number of subjects and indoor scenarios. Using&#13;&#10;monocular RGB cameras would greatly simplify the avatar creation process,&#13;&#10;thanks to their lower cost and ease of use. These advantages come at a price&#13;&#10;though, since RGB capture methods need to deal with occlusions, perspective&#13;&#10;ambiguity and large variations in subject appearance, in addition to&#13;&#10;all the challenges posed by full-body capture. In an attempt to simplify the&#13;&#10;problem, researchers generally adopt a divide-and-conquer strategy, estimating&#13;&#10;the body, face and hands with distinct methods using part-specific&#13;&#10;datasets and benchmarks. However, the hands and face constrain the body&#13;&#10;and vice-versa, e.g. the position of the wrist depends on the elbow, shoulder,&#13;&#10;etc.; the divide-and-conquer approach can not utilize this constraint.&#13;&#10;In this thesis, we aim to reconstruct the full 3D human body, using only&#13;&#10;readily accessible monocular RGB images. In a first step, we introduce a&#13;&#10;parametric 3D body model, called SMPL-X, that can represent full-body&#13;&#10;shape and pose, hand articulation and facial expression. Next, we present&#13;&#10;an iterative optimization method, named SMPLify-X, that fits SMPL-X to&#13;&#10;2D image keypoints. While SMPLify-X can produce plausible results if&#13;&#10;the 2D observations are sufficiently reliable, it is slow and susceptible&#13;&#10;to initialization. To overcome these limitations, we introduce ExPose, a&#13;&#10;neural network regressor, that predicts SMPL-X parameters from an image&#13;&#10;using body-driven attention, i.e. by zooming in on the hands and face,&#13;&#10;after predicting the body. From the zoomed-in part images, dedicated&#13;&#10;part networks predict the hand and face parameters. ExPose combines&#13;&#10;the independent body, hand, and face estimates by trusting them equally.&#13;&#10;This approach though does not fully exploit the correlation between parts&#13;&#10;and fails in the presence of challenges such as occlusion or motion blur.&#13;&#10;Thus, we need a better mechanism to aggregate information from the full&#13;&#10;body and part images. PIXIE uses neural networks called moderators that&#13;&#10;learn to fuse information from these two image sets before predicting the&#13;&#10;final part parameters. Overall, the addition of the hands and face leads to&#13;&#10;noticeably more natural and expressive reconstructions.&#13;&#10;Creating high fidelity avatars from RGB images requires accurate estimation&#13;&#10;of 3D body shape. Although existing methods are effective at&#13;&#10;predicting body pose, they struggle with body shape. We identify the lack&#13;&#10;of proper training data as the cause. To overcome this obstacle, we propose&#13;&#10;to collect internet images from fashion models websites, together with&#13;&#10;anthropometric measurements. At the same time, we ask human annotators&#13;&#10;to rate images and meshes according to a pre-defined set of linguistic attributes.&#13;&#10;We then define mappings between measurements, linguistic shape&#13;&#10;attributes and 3D body shape. Equipped with these mappings, we train a&#13;&#10;neural network regressor, SHAPY, that predicts accurate 3D body shapes&#13;&#10;from a single RGB image. We observe that existing 3D shape benchmarks&#13;&#10;lack subject variety and/or ground-truth shape. Thus, we introduce a new&#13;&#10;benchmark, Human Bodies in the Wild (HBW), which contains images of&#13;&#10;humans and their corresponding 3D ground-truth body shape. SHAPY&#13;&#10;shows how we can overcome the lack of in-the-wild images with 3D shape&#13;&#10;annotations through easy-to-obtain anthropometric measurements and linguistic&#13;&#10;shape attributes.&#13;&#10;Regressors that estimate 3D model parameters are robust and accurate,&#13;&#10;but often fail to tightly fit the observations. Optimization-based approaches&#13;&#10;tightly fit the data, by minimizing an energy function composed of a data&#13;&#10;term that penalizes deviations from the observations and priors that encode&#13;&#10;our knowledge of the problem. Finding the balance between these terms&#13;&#10;and implementing a performant version of the solver is a time-consuming&#13;&#10;and non-trivial task. Machine-learned continuous optimizers combine the&#13;&#10;benefits of both regression and optimization approaches. They learn the&#13;&#10;priors directly from data, avoiding the need for hand-crafted heuristics and&#13;&#10;loss term balancing, and benefit from optimized neural network frameworks&#13;&#10;for fast inference. Inspired from the classic Levenberg-Marquardt&#13;&#10;algorithm, we propose a neural optimizer that outperforms classic optimization,&#13;&#10;regression and hybrid optimization-regression approaches. Our&#13;&#10;proposed update rule uses a weighted combination of gradient descent&#13;&#10;and a network-predicted update. To show the versatility of the proposed&#13;&#10;method, we apply it on three other problems, namely full body estimation&#13;&#10;from (i) 2D keypoints, (ii) head and hand location from a head-mounted&#13;&#10;device and (iii) face tracking from dense 2D landmarks. Our method can&#13;&#10;easily be applied to new model fitting problems and offers a competitive&#13;&#10;alternative to well-tuned traditional model fitting pipelines, both in terms&#13;&#10;of accuracy and speed.&#13;&#10;To summarize, we propose a new and richer representation of the human&#13;&#10;body, SMPL-X, that is able to jointly model the 3D human body pose&#13;&#10;and shape, facial expressions and hand articulation. We propose methods,&#13;&#10;SMPLify-X, ExPose and PIXIE that estimate SMPL-X parameters from&#13;&#10;monocular RGB images, progressively improving the accuracy and realism&#13;&#10;of the predictions. To further improve reconstruction fidelity, we demonstrate&#13;&#10;how we can use easy-to-collect internet data and human annotations&#13;&#10;to overcome the lack of 3D shape data and train a model, SHAPY, that&#13;&#10;predicts accurate 3D body shape from a single RGB image. Finally, we&#13;&#10;propose a flexible learnable update rule for parametric human model fitting&#13;&#10;that outperforms both classic optimization and neural network approaches.&#13;&#10;This approach is easily applicable to a variety of problems, unlocking new&#13;&#10;applications in AR/VR scenarios. &amp;amp;body=https://ps.is.mpg.de/publications/choutas-thesis-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "42": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/immvirtreal_sbehrens_2022\">How immersive virtual reality can become a key tool to advance research and psychotherapy of eating and weight disorders</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/smoelbert\">Behrens, S. C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, Keizer, A., Giel, K. E.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Frontiers in Psychiatry</em>, 13, November 2022 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.3389/fpsyt.2022.1011620\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27490/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&amp;amp;title= &amp;amp;summary=How immersive virtual reality can become a key tool to advance research and psychotherapy of eating and weight disorders&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/immvirtreal_sbehrens_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "41": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dart2022\">DART: Articulated Hand Model with Diverse Accessories and Rich Textures</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tGao, D., <span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, Li, K., Yang, L., Wang, F., Zhang, P., Zhang, B., Lu, C., Tan, P.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)</em>, November 2022 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27270\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27270\" href=\"#abstractContent27270\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27270\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dart2022.github.io/\"><i class=\"fa fa-github-square\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/DART2022/DARTset\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtube.com/embed/VvlUYe-9b7U\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27270/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/dart2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes.: https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/dart2022&amp;amp;title=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;summary={DART}: {A}rticulated {H}and {M}odel with {D}iverse {A}ccessories and {R}ich {T}extures&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. %20https://ps.is.mpg.de/publications/dart2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision &amp;amp; graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic &amp;amp; lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART. DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover different kinds of blemishes, make-ups, and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g. pose, camera, background, lighting, and DART textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called DARTset. Experiments demonstrate its superiority in generalization and diversity. As a great complement to existing datasets, DARTset could boost hand pose estimation &amp;amp; surface reconstruction tasks. DART and Unity software will be publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/dart2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "40": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/mica-eccv2022\">Towards Metrical Reconstruction of Human Faces</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tZielonka, W., <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27243\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27243\" href=\"#abstractContent27243\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27243\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tFace reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;\nprovides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2204.06607.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zielon.github.io/mica/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/vzzEbvv08VA\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Zielon/MICA\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27243/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/mica-eccv2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/.: https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/mica-eccv2022&amp;amp;title=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;summary=Towards Metrical Reconstruction of Human Faces&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. %20https://ps.is.mpg.de/publications/mica-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which&#13;&#10;provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively). Project website: https://zielon.github.io/mica/. &amp;amp;body=https://ps.is.mpg.de/publications/mica-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "39": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/liu_iros_22\">Deep Residual Reinforcement Learning based Autonomous Blimp Control</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yliu2\">Liu, Y. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, IEEE, October 2022 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27136\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27136\" href=\"#abstractContent27136\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27136\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tBlimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27136/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/liu_iros_22&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL .: https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/liu_iros_22&amp;amp;title=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;summary=Deep Residual Reinforcement Learning based Autonomous Blimp Control&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . %20https://ps.is.mpg.de/publications/liu_iros_22&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL . &amp;amp;body=https://ps.is.mpg.de/publications/liu_iros_22&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "38": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/learning_to_fit\">Learning to Fit Morphable Models</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fbogo\">Bogo, F.</a></span>, Shen, J., Valentin, J.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://vchoutas.github.io/learning_to_fit/\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.dropbox.com/s/4cksaq8h4royt9r/Learning%20To%20Fit%20Morphable%20Models%20FHD%20HB.mp4?raw=1\"><i class=\"fa fa-file-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/702/learning_to_fit.pdf\"><i class=\"fa fa-file-pdf-o\"/>  PDF</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/703/LearningToFit_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27333/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/learning_to_fit&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/learning_to_fit&amp;amp;title= &amp;amp;summary=Learning to Fit Morphable Models&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/learning_to_fit&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/learning_to_fit&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "37": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/trust-eccv2022\">Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hfeng\">Feng, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Abrevaya, V. F.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Springer International Publishing, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27244\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27244\" href=\"#abstractContent27244\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27244\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tVirtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;\nof-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2205.03962.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://trust.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/HavenFeng/TRUST\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27244/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/trust-eccv2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de.: https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/trust-eccv2022&amp;amp;title=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;summary=Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/trust-eccv2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Virtual facial avatars will play an increasingly important role in immersive communication, games and the metaverse, and it is therefore critical that they be inclusive. This requires accurate recovery of the albedo, regardless of age, sex, or ethnicity. While significant progress has been made on estimating 3D facial geometry, appearance estimation has received less attention. The task is fundamentally ambiguous because the observed color is a function of albedo and lighting, both of which are unknown. We find that current methods are biased towards light skin tones due to (1) strongly biased priors that prefer lighter pigmentation and (2) algorithmic solutions that disregard the light/albedo ambiguity. To address this, we propose a new evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation and, hence, fairness. Specifically, we create the first facial albedo evaluation benchmark where subjects are balanced in terms of skin color, and measure accuracy using the Individual Typology Angle (ITA) metric. We then address the light/albedo ambiguity by building on a key observation: the image of the full scene &#8211;as opposed to a cropped image of the face&#8211; contains important information about lighting that can be used for disambiguation. TRUST regresses facial albedo by conditioning on both the face region and a global illumination signal obtained from the scene image. Our experimental results show significant improvement compared to state-&#13;&#10;of-the-art methods on albedo estimation, both in terms of accuracy and fairness. The evaluation benchmark and code are available for research purposes at https://trust.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/trust-eccv2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "36": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/supr\">SUPR: A Sparse Unified Part-Based Human Representation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aosman\">Osman, A. A. A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV) </em>, Springer International Publishing, October 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27315\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27315\" href=\"#abstractContent27315\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27315\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tStatistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://supr.is.tuebingen.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/ahmedosman/SUPR\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/699/0570_source.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Main Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/700/0570-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supp. Mat. </a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/701/final_supr_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27315/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/supr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes.: https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/supr&amp;amp;title=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. &amp;amp;summary={SUPR}: A Sparse Unified Part-Based Human Representation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. %20https://ps.is.mpg.de/publications/supr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important in- formation about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet de- form due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts&#8217; full range of motion. SUPR is publicly available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/supr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "35": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/intercap_gcpr2022\">InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Honorable Mention for Best Paper)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yhuang2\">Huang, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (DAGM)</em>, 13485, pages: 281-299, Springer, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27278\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27278\" href=\"#abstractContent27278\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27278\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/YinghaoHuang91/InterCap/tree/master\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://intercap.is.tue.mpg.de/download.php\"><i class=\"fa fa-file-o\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/d5wHLDIqN6c\"><i class=\"fa fa-file-video-o\"/>  YouTube Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://intercap.is.tue.mpg.de/media/upload/main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-16788-1_18\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27278/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes.: https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/intercap_gcpr2022&amp;amp;title=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;summary={InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. %20https://ps.is.mpg.de/publications/intercap_gcpr2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are areavailable for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/intercap_gcpr2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "34": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/teach-2022\">TEACH: Temporal Action Composition for 3D Humans</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nathanasiou\">Athanasiou, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on 3D Vision (3DV)</em>, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27247\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27247\" href=\"#abstractContent27247\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27247\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGiven a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/athn-nik/teach\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.04066\"><i class=\"fa fa-file-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://teach.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=ENr4GQO0RSc&amp;ab_channel=3DV2022\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27247/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/teach-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.: https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/teach-2022&amp;amp;title=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;summary={TEACH}: {T}emporal {A}ction {C}omposition for 3{D} {H}umans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/teach-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#8220;TEmporal Action Compositions for Human motions&#8221;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/teach-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "33": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/skirt-3dv-2022\">Neural Point-based Shape Modeling of Humans in Challenging Clothing</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/qma\">Ma, Q.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on 3D Vision (3DV) 2022</em>, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27249\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27249\" href=\"#abstractContent27249\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27249\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tParametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://qianlim.github.io/SkiRT\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/qianlim/SkiRT\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.06814\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/695/SkiRT_main_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/696/SkiRT_supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/697/090_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://qianlim.github.io/SkiRT\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27249/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes.: https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/skirt-3dv-2022&amp;amp;title=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;summary=Neural Point-based Shape Modeling of Humans in Challenging Clothing&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. %20https://ps.is.mpg.de/publications/skirt-3dv-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to cloth- ing because they have a fixed mesh topology and resolution. To address this limitation, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose- independent &#8220;coarse shape&#8221; that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person- specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/skirt-3dv-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "32": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/wang2022reconstruction\">Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tWang, X., Li, G., Kuo, Y., <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Aksan, E., Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on 3D Vision (3DV)</em>, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27340\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27340\" href=\"#abstractContent27340\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27340\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;\narising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;\nof humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;\na large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/rhoi/\"><i class=\"fa fa-github-square\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/YB1_xKlueUI\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2209.02485\"><i class=\"fa fa-file-o\"/>  Arxiv</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27340/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/wang2022reconstruction&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.: https://ps.is.mpg.de/publications/wang2022reconstruction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/wang2022reconstruction&amp;amp;title=We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories. &amp;amp;summary=Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories. %20https://ps.is.mpg.de/publications/wang2022reconstruction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities&#13;&#10;arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements&#13;&#10;of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on&#13;&#10;a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories. &amp;amp;body=https://ps.is.mpg.de/publications/wang2022reconstruction&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "31": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/invgan-gcpr-2022\">InvGAN: Invertible GANs</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Best Paper Award)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/pghosh\">Ghosh, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dzietlow\">Zietlow, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Davis, L. S., Hu, X.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>,  pages: 3-19, Springer, September 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27334\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27334\" href=\"#abstractContent27334\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27334\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGeneration of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2112.04598.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-031-16788-1_1\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27334/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments.: https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/invgan-gcpr-2022&amp;amp;title=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. &amp;amp;summary={InvGAN}: Invertible {GANs}&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. %20https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Generation of photo-realistic images, semantic editing and representation learning are only a few of many applications of high-resolution generative models. Recent progress in GANs have established them as an excellent choice for such tasks. However, since they do not provide an inference model, downstream tasks such as classification cannot be easily applied on real images using the GAN latent space. Despite numerous efforts to train an inference model or design an iterative method to invert a pre-trained generator, previous methods are dataset (e.g. human face images) and architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend to novel datasets or architectures. We propose a general framework that is agnostic to architecture and datasets. Our key insight is that, by training the inference and the generative model together, we allow them to adapt to each other and to converge to a better quality model. Our InvGAN, short for Invertible GAN, successfully embeds real images in the latent space of a high quality generative model. This allows us to perform image inpainting, merging, interpolation and online data augmentation. We demonstrate this with extensive qualitative and quantitative experiments. &amp;amp;body=https://ps.is.mpg.de/publications/invgan-gcpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "30": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ziani2022tempclr\">TempCLR: Reconstructing Hands via Time-Coherent Contrastive Learning</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tZiani, A., <span class=\"default-link-ul\"><a href=\"/person/fzicong\">Fan, Z.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Christen, S., Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on 3D Vision (3DV)</em>, September 2022 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27339\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27339\" href=\"#abstractContent27339\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27339\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/tempclr\"><i class=\"fa fa-github-square\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eth-ait/tempclr\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://arxiv.org/abs/2209.00489\"><i class=\"fa fa-file-o\"/>  Arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/VSsKx8SnFio\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/tempclr\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27339/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/ziani2022tempclr&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively.: https://ps.is.mpg.de/publications/ziani2022tempclr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/ziani2022tempclr&amp;amp;title=We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively. &amp;amp;summary={TempCLR}: Reconstructing Hands via Time-Coherent Contrastive Learning&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively. %20https://ps.is.mpg.de/publications/ziani2022tempclr&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We introduce TempCLR, a new time-coherent contrastive learning approach for the structured regression task of 3D hand reconstruction. Unlike previous time-contrastive methods for hand pose estimation, our framework considers temporal consistency in its augmentation scheme, and accounts for the differences of hand poses along the temporal direction. Our data-driven method leverages unlabelled videos and a standard CNN, without relying on synthetic data, pseudo-labels, or specialized architectures. Our approach improves the performance of fully-supervised hand reconstruction methods by 15.9% and 7.6% in PA-V2V on the HO-3D and FreiHAND datasets respectively, thus establishing new state-of-the-art performance. Finally, we demonstrate that our approach produces smoother hand reconstructions through time, and is more robust to heavy occlusions compared to the previous state-of-the-art which we show quantitatively and qualitatively. &amp;amp;body=https://ps.is.mpg.de/publications/ziani2022tempclr&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "29": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/xiu2022icon\">ICON: Implicit Clothed humans Obtained from Normals</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yxiu\">Xiu, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13286-13296 , IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27033\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27033\" href=\"#abstractContent27033\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27033\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tCurrent methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://icon.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/yuliangxiu/icon\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/Yuliang/ICON\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/hZd6AYin2DE\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2112.09127\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/688/01209.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/689/01209-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat.</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/690/ICON-poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01294\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Implicit Representations\" class=\"btn btn-default btn-xs\" href=\"/research_projects/implicit-representations\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27033/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/xiu2022icon&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.: https://ps.is.mpg.de/publications/xiu2022icon&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/xiu2022icon&amp;amp;title=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. &amp;amp;summary={ICON}: {I}mplicit {C}lothed humans {O}btained from {N}ormals&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. %20https://ps.is.mpg.de/publications/xiu2022icon&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn the avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair or clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (&amp;quot;Implicit Clothed humans Obtained from Normals&amp;quot;), which uses local features, instead. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation. &amp;amp;body=https://ps.is.mpg.de/publications/xiu2022icon&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "28": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/zheng-cvpr-2022\">I M Avatar: Implicit Morphable Head Avatars from Videos</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yzheng\">Zheng, Y.</a></span>, Abrevaya, V. F., Marcel C. B&#252;hler, , <span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13535-13545, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27076\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27076\" href=\"#abstractContent27076\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27076\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTraditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2112.07471\"><i class=\"fa fa-file-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ait.ethz.ch/projects/2022/IMavatar/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/915baJNX-IU\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zhengyuf/IMavatar\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dataset.ait.ethz.ch/downloads/imaOsdfvRe/\"><i class=\"fa fa-file-o\"/>  synth. data</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01318\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27076/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.: https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/zheng-cvpr-2022&amp;amp;title=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. &amp;amp;summary={I M} Avatar: Implicit Morphable Head Avatars from Videos&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. %20https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods. &amp;amp;body=https://ps.is.mpg.de/publications/zheng-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "27": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/christen-cvpr-2022\">D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tChristen, S., <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, Aksan, E., Hwangbo, J., Song, J., Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 20545-20554, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27077\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27077\" href=\"#abstractContent27077\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27077\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2112.03028.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://eth-ait.github.io/d-grasp/\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/5OqbDq-pgLc\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/christsa/dgrasp\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01992\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27077/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/christen-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.: https://ps.is.mpg.de/publications/christen-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/christen-cvpr-2022&amp;amp;title=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. &amp;amp;summary={D-Grasp}: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. %20https://ps.is.mpg.de/publications/christen-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences. &amp;amp;body=https://ps.is.mpg.de/publications/christen-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "26": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/huang2022rich\">Capturing and Inferring Dense Full-Body Human-Scene Contact</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mhoeschle\">H&#246;schle, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/msafroshkin\">Safroshkin, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/talexiadis\">Alexiadis, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/senya\">Polikovsky, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dscharstein\">Scharstein, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13264-13275, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27046\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27046\" href=\"#abstractContent27046\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27046\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tInferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://rich.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2206.09553.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/paulchhuang/bstro\"><i class=\"fa fa-github-square\"/>  BSTRO code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/IbFc12L5Kc4\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01292\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-contact\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27046/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/huang2022rich&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.: https://ps.is.mpg.de/publications/huang2022rich&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/huang2022rich&amp;amp;title=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. &amp;amp;summary=Capturing and Inferring Dense Full-Body Human-Scene Contact&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/huang2022rich&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for &#8220;Real scenes, Interaction, Contact and Humans.&#8221; RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/huang2022rich&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "25": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/rueeg-cvpr-2022\">BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, Schindler, K., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) </em>,  pages: 3876-3884, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27073\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27073\" href=\"#abstractContent27073\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27073\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tOur goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/media/upload/barc_main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/media/upload/barc_supmat-compressed-2.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat.</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/runa91/barc_release\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://barc.is.tue.mpg.de/media/upload/BARC_poster_v12.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/CSPbb1p5Hso\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://huggingface.co/spaces/runa91/barc_gradio\"><i class=\"fa fa-file-o\"/>  Demo</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27073/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.: https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&amp;amp;title=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. &amp;amp;summary={BARC}: Learning to Regress {3D} Dog Shape from Images by Exploiting Breed Information&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. %20https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Our goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes.  We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them.  We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. &amp;amp;body=https://ps.is.mpg.de/publications/rueeg-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "24": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/price-ias-2021\">Simulation and Control of Deformable Autonomous Airships in Turbulent Wind</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yliu2\">Liu, Y. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Intelligent Autonomous Systems 16</em>,  pages: 608-626, Lecture Notes in Networks and Systems, 412, <span class=\"text-muted\">(Editors:  Ang Jr, Marcelo H. and Asama, Hajime and Lin, Wei and Foong, Shaohui)</span>, Springer, Cham, 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion24598\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion24598\" href=\"#abstractContent24598\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent24598\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAbstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;\nSolutions for simulation and control of these vehicles are ubiquitous. This is&#13;\nnot the case for airships, a simulation of which needs to address unique&#13;\nproperties, i) dynamic deformation in response to aerodynamic and control&#13;\nforces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;\nhigh variability in airship designs regarding placement, direction and&#13;\nvectoring of thrusters and control surfaces. We present a flexible framework&#13;\nfor modeling, simulation and control of airships, based on the Robot operating&#13;\nsystem (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;\n(COTS) electronics, both of which are open source. Based on simulated wind and&#13;\ndeformation, we predict substantial effects on controllability, verified in&#13;\nreal world flight experiments. All our code is shared as open source, for the&#13;\nbenefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;\nresearch. https://github.com/robot-perception-group/airship_simulation\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2012.15684.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2012.15684\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/robot-perception-group/airship_simulation\"><i class=\"fa fa-github-square\"/>  project/code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-030-95892-3_46\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/24598/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/price-ias-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation: https://ps.is.mpg.de/publications/price-ias-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/price-ias-2021&amp;amp;title=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation &amp;amp;summary=Simulation and Control of Deformable Autonomous Airships in Turbulent Wind&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation %20https://ps.is.mpg.de/publications/price-ias-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Abstract. Fixed wing and multirotor UAVs are common in the field of robotics.&#13;&#10;Solutions for simulation and control of these vehicles are ubiquitous. This is&#13;&#10;not the case for airships, a simulation of which needs to address unique&#13;&#10;properties, i) dynamic deformation in response to aerodynamic and control&#13;&#10;forces, ii) high susceptibility to wind and turbulence at low airspeed, iii)&#13;&#10;high variability in airship designs regarding placement, direction and&#13;&#10;vectoring of thrusters and control surfaces. We present a flexible framework&#13;&#10;for modeling, simulation and control of airships, based on the Robot operating&#13;&#10;system (ROS), simulation environment (Gazebo) and commercial off the shelf&#13;&#10;(COTS) electronics, both of which are open source. Based on simulated wind and&#13;&#10;deformation, we predict substantial effects on controllability, verified in&#13;&#10;real world flight experiments. All our code is shared as open source, for the&#13;&#10;benefit of the community and to facilitate lighter-than-air vehicle (LTAV)&#13;&#10;research. https://github.com/robot-perception-group/airship_simulation &amp;amp;body=https://ps.is.mpg.de/publications/price-ias-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "23": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/keller-cvpr-2022\">OSSO: Obtaining Skeletal Shape from Outside</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkeller2\">Keller, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 20460-20469, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26967\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26967\" href=\"#abstractContent26967\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26967\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://osso.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/684/06883.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/685/06883-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat.</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01984\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26967/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/keller-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.: https://ps.is.mpg.de/publications/keller-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/keller-cvpr-2022&amp;amp;title=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. &amp;amp;summary={OSSO}: Obtaining Skeletal Shape from Outside&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. %20https://ps.is.mpg.de/publications/keller-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held out DXA scans, outperforming the state-of-the art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource. &amp;amp;body=https://ps.is.mpg.de/publications/keller-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "22": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/khirodkar_ochmr_2022\">Occluded Human Mesh Recovery</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tKhirodkar, R., <span class=\"default-link-ul\"><a href=\"/person/stripathi\">Tripathi, S.</a></span>, Kitani, K.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 1705-1715, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26962\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26962\" href=\"#abstractContent26962\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26962\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTop-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://rawalkhirodkar.github.io/ochmr\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2203.13349\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00176\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26962/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released.: https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&amp;amp;title=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. &amp;amp;summary=Occluded Human Mesh Recovery&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. %20https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. Code and models will be released. &amp;amp;body=https://ps.is.mpg.de/publications/khirodkar_ochmr_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "21": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/shapy-2022\">Accurate 3D Body Shape Regression using Metric and Semantic Attributes</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\t\t\t\t\t\t\t<p class=\"color-red\">(Best Paper Award Candidate)</p>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lmueller2\">M&#252;ller, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 2708-2718, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27067\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27067\" href=\"#abstractContent27067\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27067\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWhile methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://shapy.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Home</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/muelea/shapy\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/7TzXrL4eV9g\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/691/00928.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/692/00928-supp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Supplementary Material</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/693/Shapy_poster_compressed.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00274\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Regressing Humans\" class=\"btn btn-default btn-xs\" href=\"/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27067/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/shapy-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research.: https://ps.is.mpg.de/publications/shapy-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/shapy-2022&amp;amp;title=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. &amp;amp;summary=Accurate 3D Body Shape Regression using Metric and Semantic Attributes&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. %20https://ps.is.mpg.de/publications/shapy-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=While methods that regress 3D human meshes from images have progressed rapidly, the estimated body shapes often do not capture the true human shape. This is problematic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can label 2D joints, and these constrain 3D pose, it is not so easy to &#8220;label&#8221; 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of partial information: (1) we collect internet images of diverse models together with a small set of measurements; (2) we collect semantic shape attributes for a wide range of 3D body meshes and model images. Taken together, these datasets provide sufficient constraints to infer metric 3D shape. We exploit this partial and semantic data in several novel ways to train a neural network, called SHAPY, that regresses 3D human pose and shape from an RGB image. We evaluate SHAPY on public benchmarks but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for 3D human shape estimation, containing photos of people in the wild for whom we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms recent state-of-the-art methods on the task of 3D body shape estimation. This is the first demonstration that a 3D body shape regressor can be trained from sparse measurements and easy-to-obtain semantic shape attributes. Our model and data will be freely available for research. &amp;amp;body=https://ps.is.mpg.de/publications/shapy-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "20": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/xu2022gdna\">gDNA: Towards Generative Detailed Neural Avatars</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/xchen2\">Chen, X.</a></span>, Jiang, T., Song, J., <span class=\"default-link-ul\"><a href=\"/person/jyang\">Yang, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 204395-20405, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27036\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27036\" href=\"#abstractContent27036\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27036\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://xuchen-ethz.github.io/gdna/\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ait.ethz.ch/projects/2022/gdna/downloads/main.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=uOyoH7OO16I\"><i class=\"fa fa-file-video-o\"/>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/xuchen-ethz/gdna\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01978\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27036/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/xu2022gdna&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art.: https://ps.is.mpg.de/publications/xu2022gdna&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/xu2022gdna&amp;amp;title=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. &amp;amp;summary={gDNA}: Towards Generative Detailed Neural Avatars&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. %20https://ps.is.mpg.de/publications/xu2022gdna&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is challenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochastic geometric detail in clothing. Hence, current methods to represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skinning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evidence that this leads to realistic generation of local details such as clothing wrinkles. We show that our model is able to generate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, outperforming the previous state-of-the-art. &amp;amp;body=https://ps.is.mpg.de/publications/xu2022gdna&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "19": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/sun-cvpr-2022\">Putting People in their Place: Monocular Regression of 3D People in Depth</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSun, Y., Liu, W., Bao, Q., Fu, Y., Mei, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13233-13242, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27074\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27074\" href=\"#abstractContent27074\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27074\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGiven an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a \"Relative Human\" (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2112.08274\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arthur151.github.io/BEV/BEV.html\"><i class=\"fa fa-github-square\"/>  Project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Arthur151/ROMP\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Arthur151/Relative_Human\"><i class=\"fa fa-github-square\"/>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/CVPR2022/supplemental/Sun_Putting_People_in_CVPR_2022_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup Mat</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01289\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27074/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/sun-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.: https://ps.is.mpg.de/publications/sun-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/sun-cvpr-2022&amp;amp;title=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. &amp;amp;summary=Putting People in their Place: Monocular Regression of {3D} People in Depth&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. %20https://ps.is.mpg.de/publications/sun-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird&amp;#39;s-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a &amp;quot;Relative Human&amp;quot; (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/sun-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "18": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/yi_mover_2022\">Human-Aware Object Placement for Visual Environment Reconstruction</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/hyi\">Yi, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jthies\">Thies, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 3949-3960, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26938\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26938\" href=\"#abstractContent26938\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26938\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHumans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://mover.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2203.03609.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.00393\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-contact\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26938/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/yi_mover_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/yi_mover_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/yi_mover_2022&amp;amp;title=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. &amp;amp;summary=Human-Aware Object Placement for Visual Environment Reconstruction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/yi_mover_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects,  (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/yi_mover_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "17": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/taheri-cvpr-2022a\">GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/otaheri\">Taheri, O.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 13253-13263, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion27075\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion27075\" href=\"#abstractContent27075\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent27075\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tGenerating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/goal/GOAL_Taheri_CVPR2022.pdf\"><i class=\"fa fa-file-pdf-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/goal/GOAL_SuppMat_Taheri_CVPR2022.pdf\"><i class=\"fa fa-file-pdf-o\"/>  Sup. Mat</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://goal.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  Project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/A7b8DYovDZY\"><i class=\"fa fa-file-video-o\"/>  YouTube</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/otaheri/GOAL\"><i class=\"fa fa-github-square\"/>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://goal.is.tue.mpg.de/download.php\"><i class=\"fa fa-file-o\"/>  Models</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01291\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Hands-Object Interaction\" class=\"btn btn-default btn-xs\" href=\"/research_projects/hands-in-action\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-contact\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27075/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.: https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&amp;amp;title=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. &amp;amp;summary={GOAL}: Generating {4D} Whole-Body Motion for Hand-Object Grasping&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. %20https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL&#8217;s generated motions approach the realism of GRAB&#8217;s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research. &amp;amp;body=https://ps.is.mpg.de/publications/taheri-cvpr-2022a&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "16": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/emoca-cvpr-2022\">Emotion Driven Monocular Face Capture and Animation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/rdanecek\">Dan&#283;&#269;ek, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>,  pages: 20279-20290, IEEE, Piscataway, NJ, June 2022 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26969\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26969\" href=\"#abstractContent26969\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26969\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAs 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/radekd91/emoca\"><i class=\"fa fa-github-square\"/>  code </a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://emoca.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/686/EMOCA__CVPR22.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/687/EMOCA_sup_mat__CVPR2022.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supplemental</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/CVPR52688.2022.01967\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26969/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de.: https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/emoca-cvpr-2022&amp;amp;title=As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de. &amp;amp;summary=Emotion Driven Monocular Face Capture and Animation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/emoca-cvpr-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "15": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/saini-irl-2022\">AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ebonetto\">Bonetto, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, 7(2):4805-4812, IEEE, April 2022, Also accepted and presented in the 2022 IEEE International Conference on Robotics and Automation (ICRA) <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26873\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26873\" href=\"#abstractContent26873\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26873\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/9691814\"><i class=\"fa fa-file-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/robot-perception-group/AirPose\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/Ss48ICeqvnQ\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9691814\"><i class=\"fa fa-file-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2022.3145494\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26873/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/saini-irl-2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ.: https://ps.is.mpg.de/publications/saini-irl-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/saini-irl-2022&amp;amp;title=In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ. &amp;amp;summary={AirPose}: Multi-View Fusion Network for Aerial {3D} Human Pose and Shape Estimation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ. %20https://ps.is.mpg.de/publications/saini-irl-2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose calibrates the cameras relative to the person instead of in a classical way. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The persons shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post processing method (AirPose+) for offline applications that require higher mocap quality. We make code and data available for research at https://github.com/robot-perception-group/AirPose. Video describing the approach and results is available at https://youtu.be/Ss48ICeqvnQ. &amp;amp;body=https://ps.is.mpg.de/publications/saini-irl-2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "14": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/irotate2022\">iRotate: Active visual SLAM for omnidirectional robots</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ebonetto\">Bonetto, E.</a></span>, Goldschmid, P., <span class=\"default-link-ul\"><a href=\"/person/mpabst\">Pabst, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Robotics and Autonomous Systems</em>, Elsevier, 2022 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26976\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26976\" href=\"#abstractContent26976\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26976\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot&#8217;s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot&#8217;s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels&#8217; total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.sciencedirect.com/science/article/pii/S0921889022000550\"><i class=\"fa fa-external-link\"/>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1016/j.robot.2022.104102\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26976/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/irotate2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot&#8217;s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot&#8217;s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels&#8217; total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation.: https://ps.is.mpg.de/publications/irotate2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/irotate2022&amp;amp;title=In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot&#8217;s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot&#8217;s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels&#8217; total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation. &amp;amp;summary={iRotate}: Active visual {SLAM} for omnidirectional robots&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot&#8217;s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot&#8217;s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels&#8217; total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation. %20https://ps.is.mpg.de/publications/irotate2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=In this paper, we present an active visual SLAM approach for omnidirectional robots. The goal is to generate control commands that allow such a robot to simultaneously localize itself and map an unknown environment while maximizing the amount of information gained and consuming as low energy as possible. Leveraging the robot&#8217;s independent translation and rotation control, we introduce a multi-layered approach for active V-SLAM. The top layer decides on informative goal locations and generates highly informative paths to them. The second and third layers actively re-plan and execute the path, exploiting the continuously updated map and local features information. Moreover, we introduce two utility formulations to account for the presence of obstacles in the field of view and the robot&#8217;s location. Through rigorous simulations, real robot experiments, and comparisons with state-of-the-art methods, we demonstrate that our approach achieves similar coverage results with lesser overall map entropy. This is obtained while keeping the traversed distance up to 39% shorter than the other methods and without increasing the wheels&#8217; total rotation amount. Code and implementation details are provided as open-source and all the generated data is available online for consultation. &amp;amp;body=https://ps.is.mpg.de/publications/irotate2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "13": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/physact_srismithbehrens_2022\">Physical activity improves body image of sedentary adults. Exploring the roles of interoception and affective response</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSrismith, Duangkamol; Dierkes, Katja; Zipfel, Stephan; Thiel, Ansgar; Sudeck, Gorden; Giel, Katrin E.; Behrens, Simone C.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Current Psychology</em>, Springer, 2022 <small class=\"text-muted\">(article)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/s12144-022-03641-7\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/27491/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/physact_srismithbehrens_2022&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.mpg.de/publications/physact_srismithbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/physact_srismithbehrens_2022&amp;amp;title= &amp;amp;summary=Physical activity improves body image of sedentary adults. Exploring the roles of interoception and affective response&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url= %20https://ps.is.mpg.de/publications/physact_srismithbehrens_2022&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.mpg.de/publications/physact_srismithbehrens_2022&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "12": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/pixie-3dv-2021\">Collaborative Regression of Expressive Bodies using Moderation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yfeng\">Feng, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2021 International Conference on 3D Vision (3DV 2021)</em>,  pages: 792-804, IEEE, Piscataway, NJ, December 2021 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26352\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26352\" href=\"#abstractContent26352\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26352\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tRecovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X&#8217;s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer &#8220;gendered&#8221; 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2105.05301\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://pixie.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/667/PIXIE_3DV_CR.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/668/PIXIE_3DV_sup_mat.pdf\"><i class=\"fa fa-file-pdf-o\"/>  suppl</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV53792.2021.00088\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Expressive Body Models\" class=\"btn btn-default btn-xs\" href=\"/research_projects/expressive-body-models\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Regressing Humans\" class=\"btn btn-default btn-xs\" href=\"/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26352/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/pixie-3dv-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X&#8217;s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer &#8220;gendered&#8221; 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de.: https://ps.is.mpg.de/publications/pixie-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/pixie-3dv-2021&amp;amp;title=Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X&#8217;s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer &#8220;gendered&#8221; 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de. &amp;amp;summary=Collaborative Regression of Expressive Bodies using Moderation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X&#8217;s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer &#8220;gendered&#8221; 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de. %20https://ps.is.mpg.de/publications/pixie-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First,  existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X&#8217;s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer &#8220;gendered&#8221; 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de. &amp;amp;body=https://ps.is.mpg.de/publications/pixie-3dv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "11": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/fan-3dv-2021\">Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/fzicong\">Fan, Z.</a></span>, Spurr, A., <span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2021 International Conference on 3D Vision (3DV 2021)</em>,  pages: 1-10, IEEE, Piscataway, NJ, December 2021 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26353\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26353\" href=\"#abstractContent26353\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26353\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2107.00434\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://zc-alexfan.github.io/digit\"><i class=\"fa fa-github-square\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/zc-alexfan/digit-interacting\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/watch?v=et3QQNKigzc\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/3DV53792.2021.00011\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Hands-Object Interaction\" class=\"btn btn-default btn-xs\" href=\"/research_projects/hands-in-action\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26353/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/fan-3dv-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes.: https://ps.is.mpg.de/publications/fan-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/fan-3dv-2021&amp;amp;title=In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes. &amp;amp;summary=Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes. %20https://ps.is.mpg.de/publications/fan-3dv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset for both single and interacting hands across all metrics. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects single and interacting hand pose estimation. Our code will be released for research purposes. &amp;amp;body=https://ps.is.mpg.de/publications/fan-3dv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "10": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/wang2021neurips\">MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/swang\">Wang, S.</a></span>, Mihajlovic, M., <span class=\"default-link-ul\"><a href=\"/person/qma\">Ma, Q.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26258\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26258\" href=\"#abstractContent26258\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26258\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://neuralbodies.github.io/metavatar/\"><i class=\"fa fa-github-square\"/>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2106.11944\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Implicit Representations\" class=\"btn btn-default btn-xs\" href=\"/research_projects/implicit-representations\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Clothing Capture and Modeling\" class=\"btn btn-default btn-xs\" href=\"/research_projects/clothing\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26258/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/wang2021neurips&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.: https://ps.is.mpg.de/publications/wang2021neurips&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/wang2021neurips&amp;amp;title=In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. &amp;amp;summary=MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. %20https://ps.is.mpg.de/publications/wang2021neurips&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. &amp;amp;body=https://ps.is.mpg.de/publications/wang2021neurips&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "9": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/foster-neuro-2021\">The neural coding of face and body orientation in occipitotemporal cortex</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/cfoster\">Foster, C.</a></span>, Zhao, M., <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/abartels\">Bartels, A.</a></span>, B&#252;lthoff, I.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>NeuroImage</em>,  pages: 118783, December 2021 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26583\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26583\" href=\"#abstractContent26583\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26583\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tFace and body orientation convey important information for us to understand other people's actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants&#8217; brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.sciencedirect.com/science/article/pii/S1053811921010557\"><i class=\"fa fa-file-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1016/j.neuroimage.2021.118783\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26583/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/foster-neuro-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants&#8217; brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex.: https://ps.is.mpg.de/publications/foster-neuro-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/foster-neuro-2021&amp;amp;title=Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants&#8217; brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex. &amp;amp;summary=The neural coding of face and body orientation in occipitotemporal cortex&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants&#8217; brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex. %20https://ps.is.mpg.de/publications/foster-neuro-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Face and body orientation convey important information for us to understand other people&amp;#39;s actions, intentions and social interactions. It has been shown that several occipitotemporal areas respond differently to faces or bodies of different orientations. However, whether face and body orientation are processed by partially overlapping or completely separate brain networks remains unclear, as the neural coding of face and body orientation is often investigated separately. Here, we recorded participants&#8217; brain activity using fMRI while they viewed faces and bodies shown from three different orientations, while attending to either orientation or identity information. Using multivoxel pattern analysis we investigated which brain regions process face and body orientation respectively, and which regions encode both face and body orientation in a stimulus-independent manner. We found that patterns of neural responses evoked by different stimulus orientations in the occipital face area, extrastriate body area, lateral occipital complex and right early visual cortex could generalise across faces and bodies, suggesting a stimulus-independent encoding of person orientation in occipitotemporal cortex. This finding was consistent across functionally defined regions of interest and a whole-brain searchlight approach. The fusiform face area responded to face but not body orientation, suggesting that orientation responses in this area are face-specific. Moreover, neural responses to orientation were remarkably consistent regardless of whether participants attended to the orientation of faces and bodies or not. Together, these results demonstrate that face and body orientation are processed in a partially overlapping brain network, with a stimulus-independent neural code for face and body orientation in occipitotemporal cortex. &amp;amp;body=https://ps.is.mpg.de/publications/foster-neuro-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "8": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/tpose_obesity_21\">A pose-independent method for accurate and precise body composition from 3D optical scans</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tWong, M. C., Ng, B. K., Tian, I., Sobhiyeh, S., Pagano, I., Dechenaud, M., Kennedy, S. F., Liu, Y. E., Kelly, N., Chow, D., Garber, A. K., Maskarinec, G., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Curless, B., Heymsfield, S. B., Shepherd, J. A.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Obesity</em>, 29(11):1835-1847, Wiley, November 2021 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26417\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26417\" href=\"#abstractContent26417\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26417\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tObjective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.&#13;\n&#13;\nMethods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).&#13;\n&#13;\nResults: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.&#13;\n&#13;\nConclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.&#13;\n\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://onlinelibrary.wiley.com/doi/10.1002/oby.23256\"><i class=\"fa fa-file-o\"/>  Wiley online adress</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1002/oby.23256\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Bodies in Medicine\" class=\"btn btn-default btn-xs\" href=\"/research_projects/medical-diagnosis\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26417/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/tpose_obesity_21&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.&#13;&#10;&#13;&#10;Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).&#13;&#10;&#13;&#10;Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.&#13;&#10;&#13;&#10;Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.&#13;&#10;: https://ps.is.mpg.de/publications/tpose_obesity_21&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/tpose_obesity_21&amp;amp;title=Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.&#13;&#10;&#13;&#10;Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).&#13;&#10;&#13;&#10;Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.&#13;&#10;&#13;&#10;Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.&#13;&#10; &amp;amp;summary=A pose-independent method for accurate and precise body composition from 3D optical scans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.&#13;&#10;&#13;&#10;Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).&#13;&#10;&#13;&#10;Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.&#13;&#10;&#13;&#10;Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.&#13;&#10; %20https://ps.is.mpg.de/publications/tpose_obesity_21&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Objective: The aim of this study was to investigate whether digitally re-posing three-dimensional optical (3DO) whole-body scans to a standardized pose would improve body composition accuracy and precision regardless of the initial pose.&#13;&#10;&#13;&#10;Methods: Healthy adults (n = 540), stratified by sex, BMI, and age, completed whole-body 3DO and dual-energy X-ray absorptiometry (DXA) scans in the Shape Up! Adults study. The 3DO mesh vertices were represented with standardized templates and a low-dimensional space by principal component analysis (stratified by sex). The total sample was split into a training (80%) and test (20%) set for both males and females. Stepwise linear regression was used to build prediction models for body composition and anthropometry outputs using 3DO principal components (PCs).&#13;&#10;&#13;&#10;Results: The analysis included 472 participants after exclusions. After re-posing, three PCs described 95% of the shape variance in the male and female training sets. 3DO body composition accuracy compared with DXA was as follows: fat mass R2 = 0.91 male, 0.94 female; fat-free mass R2 = 0.95 male, 0.92 female; visceral fat mass R2 = 0.77 male, 0.79 female.&#13;&#10;&#13;&#10;Conclusions: Re-posed 3DO body shape PCs produced more accurate and precise body composition models that may be used in clinical or nonclinical settings when DXA is unavailable or when frequent ionizing radiation exposure is unwanted.&#13;&#10; &amp;amp;body=https://ps.is.mpg.de/publications/tpose_obesity_21&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "7": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/kalyan_fermiproblems_2021\">How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tKalyan, A., Kumar, A., <span class=\"default-link-ul\"><a href=\"/person/achandrasekaran\">Chandrasekaran, A.</a></span>, Sabharwal, A., Clark, P.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>2021 Conference on Empirical Methods in Natural Language Processing - Proceedings of the Conference</em>,  pages: 7318-7328, <span class=\"text-muted\">(Editors:  Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Wen-tau Yih, Scott)</span>, Association for Computational Linguistics, Stroudsburg, PA, November 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26154\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26154\" href=\"#abstractContent26154\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26154\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMany real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, \"How much would the sea level rise if all ice in the world melted?\" FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2110.14207.pdf\"><i class=\"fa fa-file-pdf-o\"/>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://allenai.org/data/fermi\"><i class=\"fa fa-file-o\"/>  project webpage</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/allenai/fermi\"><i class=\"fa fa-github-square\"/>  data</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.18653/v1/2021.emnlp-main.582\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26154/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason.: https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&amp;amp;title=Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason. &amp;amp;summary=How much coffee was consumed during {EMNLP} 2019? Fermi Problems: A New Reasoning Challenge for {AI}&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason. %20https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Many real-world problems require the combined application of multiple reasoning abilities -- employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, &amp;quot;How much would the sea level rise if all ice in the world melted?&amp;quot; FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason. &amp;amp;body=https://ps.is.mpg.de/publications/kalyan_fermiproblems_2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "6": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/spice-iccv-2021\">Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ssanyal\">Sanyal, S.</a></span>, Vorobiov, A., <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, Loper, M., Mohler, B., Davis, L., Romero, J., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11118-11127, IEEE, Piscataway, NJ, October 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26302\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26302\" href=\"#abstractContent26302\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26302\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tSynthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Sanyal_Learning_Realistic_Human_Reposing_Using_Cyclic_Self-Supervision_With_3D_Shape_ICCV_2021_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2110.05458v1\"><i class=\"fa fa-file-o\"/>  arxiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ICCV48922.2021.01095\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Neural Rendering\" class=\"btn btn-default btn-xs\" href=\"/research_projects/neural-rendering\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26302/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/spice-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only.: https://ps.is.mpg.de/publications/spice-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/spice-iccv-2021&amp;amp;title=Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only. &amp;amp;summary=Learning Realistic Human Reposing using Cyclic Self-Supervision with {3D} Shape, Pose, and Appearance Consistency&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only. %20https://ps.is.mpg.de/publications/spice-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only. &amp;amp;body=https://ps.is.mpg.de/publications/spice-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "5": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/soma-iccv-2021\">SOMA: Solving Optical Marker-Based MoCap Automatically</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nghorbani\">Ghorbani, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11097-11106, IEEE, Piscataway, NJ, October 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26245\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26245\" href=\"#abstractContent26245\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26245\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMarker-based optical motion capture (mocap) is the &#8220;gold standard&#8221; method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. &#8220;labelling&#8221;. Given these labels, one can then &#8220;solve&#8221; for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/nghorbani/soma\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/soma/SOMA_ICCV21.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/soma/SOMA_Suppmat.pdf\"><i class=\"fa fa-file-pdf-o\"/>  suppl</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://arxiv.org/abs/2110.04431\"><i class=\"fa fa-file-o\"/>  arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://soma.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/BEFCqIefLA8\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://download.is.tue.mpg.de/soma/SOMA_Poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ICCV48922.2021.01093\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Markers to Avatars\" class=\"btn btn-default btn-xs\" href=\"/research_projects/mosh\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26245/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/soma-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Marker-based optical motion capture (mocap) is the &#8220;gold standard&#8221; method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. &#8220;labelling&#8221;. Given these labels, one can then &#8220;solve&#8221; for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/.: https://ps.is.mpg.de/publications/soma-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/soma-iccv-2021&amp;amp;title=Marker-based optical motion capture (mocap) is the &#8220;gold standard&#8221; method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. &#8220;labelling&#8221;. Given these labels, one can then &#8220;solve&#8221; for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. &amp;amp;summary={SOMA}: Solving Optical Marker-Based MoCap Automatically&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Marker-based optical motion capture (mocap) is the &#8220;gold standard&#8221; method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. &#8220;labelling&#8221;. Given these labels, one can then &#8220;solve&#8221; for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. %20https://ps.is.mpg.de/publications/soma-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Marker-based optical motion capture (mocap) is the &#8220;gold standard&#8221; method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. &#8220;labelling&#8221;. Given these labels, one can then &#8220;solve&#8221; for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. &amp;amp;body=https://ps.is.mpg.de/publications/soma-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "4": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/actor-iccv-2021\">Action-Conditioned 3D Human Motion Synthesis with Transformer VAE</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mpetrovich\">Petrovich, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 10965-10975, IEEE, Piscataway, NJ, October 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26234\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26234\" href=\"#abstractContent26234\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26234\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2104.05670\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://imagine.enpc.fr/~petrovim/actor/\"><i class=\"fa fa-file-o\"/>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Mathux/ACTOR\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ICCV48922.2021.01080\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Language and Movement\" class=\"btn btn-default btn-xs\" href=\"/research_projects/language-and-movement\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Human Movement\" class=\"btn btn-default btn-xs\" href=\"/research_projects/modeling-human-movement\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26234/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/actor-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.: https://ps.is.mpg.de/publications/actor-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/actor-iccv-2021&amp;amp;title=We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page. &amp;amp;summary=Action-Conditioned {3D} Human Motion Synthesis with Transformer {VAE}&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page. %20https://ps.is.mpg.de/publications/actor-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page. &amp;amp;body=https://ps.is.mpg.de/publications/actor-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "3": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/romp-iccv-2021\">Monocular, One-Stage, Regression of Multiple 3D People</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSun, Y., Bao, Q., Liu, W., Fu, Y., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Mei, T.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11159-11168, IEEE, Piscataway, NJ, October 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion26330\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion26330\" href=\"#abstractContent26330\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent26330\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThis paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Monocular_One-Stage_Regression_of_Multiple_3D_People_ICCV_2021_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/ICCV2021/supplemental/Sun_Monocular_One-Stage_Regression_ICCV_2021_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://arxiv.org/abs/2008.12272\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Arthur151/ROMP\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ICCV48922.2021.01099\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Regressing Humans\" class=\"btn btn-default btn-xs\" href=\"/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/26330/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/romp-iccv-2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression.: https://ps.is.mpg.de/publications/romp-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/romp-iccv-2021&amp;amp;title=This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression. &amp;amp;summary=Monocular, One-Stage, Regression of Multiple {3D} People&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression. %20https://ps.is.mpg.de/publications/romp-iccv-2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression. &amp;amp;body=https://ps.is.mpg.de/publications/romp-iccv-2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "2": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/kocabas_pare_2021\">PARE: Part Attention Regressor for 3D Human Body Estimation</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/chuang2\">Huang, C. P.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11107-11117, IEEE, Piscataway, NJ, October 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion24865\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion24865\" href=\"#abstractContent24865\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent24865\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tDespite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  &#13;\nTo address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE's  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Kocabas_PARE_Part_Attention_Regressor_for_3D_Human_Body_Estimation_ICCV_2021_paper.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kocabas_PARE_Part_Attention_ICCV_2021_supplemental.pdf\"><i class=\"fa fa-file-pdf-o\"/>  supp</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/mkocabas/PARE\"><i class=\"fa fa-github-square\"/>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/3C9hdFajO3k\"><i class=\"fa fa-file-video-o\"/>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/2104.08527\"><i class=\"fa fa-file-o\"/>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://pare.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"/>  project website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://pare.is.tue.mpg.de/media/upload/pare_poster.pdf\"><i class=\"fa fa-file-pdf-o\"/>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ICCV48922.2021.01094\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Regressing Humans\" class=\"btn btn-default btn-xs\" href=\"/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/24865/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/kocabas_pare_2021&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  &#13;&#10;To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks.: https://ps.is.mpg.de/publications/kocabas_pare_2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/kocabas_pare_2021&amp;amp;title=Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  &#13;&#10;To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. &amp;amp;summary={PARE}: Part Attention Regressor for {3D} Human Body Estimation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  &#13;&#10;To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. %20https://ps.is.mpg.de/publications/kocabas_pare_2021&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=Despite significant progress, state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.  &#13;&#10;To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE&amp;#39;s  part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. &amp;amp;body=https://ps.is.mpg.de/publications/kocabas_pare_2021&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
    "1": "<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/samp\">Stochastic Scene-Aware Motion Prediction</a> <i class=\"fa fa-external-link\"/>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, Ceylan, D., Villegas, R., Saito, J., Yang, J., Zhou, Y., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. International Conference on Computer Vision (ICCV)</em>,  pages: 11354-11364, IEEE, Piscataway, NJ, October 2021 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion24872\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion24872\" href=\"#abstractContent24872\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"/></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent24872\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tA long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  &#13;\n&#13;\n\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://samp.is.tue.mpg.de\"><i class=\"fa fa-file-o\"/>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/652/samp.pdf\"><i class=\"fa fa-file-pdf-o\"/>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/ICCV48922.2021.01118\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Human Movement\" class=\"btn btn-default btn-xs\" href=\"/research_projects/modeling-human-movement\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Putting People into Scenes\" class=\"btn btn-default btn-xs\" href=\"/research_projects/putting-people-into-scenes\"><i class=\"fa fa-external-link\"/> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/24872/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"&lt;div style='width:800px'&gt;&lt;!-- &#10;&lt;div class=&quot;share-contianer&quot;&gt; &#10;&#10;  &lt;ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.mpg.de/publications/samp&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;&gt;&lt;/a&gt;&#10;    &#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  &#13;&#10;&#13;&#10;: https://ps.is.mpg.de/publications/samp&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;      &lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.mpg.de/publications/samp&amp;amp;title=A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  &#13;&#10;&#13;&#10; &amp;amp;summary=Stochastic Scene-Aware Motion Prediction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;&gt;&lt;/a&gt;&#10;    &lt;/li&gt;&#10;    &lt;li&gt;&#10;     &lt;a href=&quot;https://plus.google.com/share?url=A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  &#13;&#10;&#13;&#10; %20https://ps.is.mpg.de/publications/samp&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;&gt;&lt;/a&gt;&#10;   &lt;/li&gt;&#10;   &lt;li&gt;&#10;    &lt;a href=&quot;mailto:?subject=A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected mocap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions.  &#13;&#10;&#13;&#10; &amp;amp;body=https://ps.is.mpg.de/publications/samp&quot; class=&quot;social_mail&quot;&gt;&lt;/a&gt;&#10;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;/div&gt;&#10; --&gt;&#10;&#10;&#10;&lt;a href=&quot;&quot; class=&quot;facebook-share&quot;&gt;&lt;i class=&quot;fa fa-facebook-square sharing-icon sharing-icon-facebook&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;twitter-share&quot;&gt;&lt;i class=&quot;fa fa-twitter-square sharing-icon sharing-icon-twitter&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;linkedin-share&quot;&gt;&lt;i class=&quot;fa fa-linkedin-square sharing-icon sharing-icon-linkedin&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;a href=&quot;&quot; class=&quot;email-share&quot;&gt;&lt;i class=&quot;fa fa-envelope sharing-icon sharing-icon-email&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#10;&lt;/div&gt;\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"/> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t",
}