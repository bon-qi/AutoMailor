{
    "25": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://dreamfusion3d.github.io/\">\n                <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://cs.stanford.edu/~poole/\">Ben Poole</a>,\n              <a href=\"https://www.ajayj.com/\">Ajay Jain</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <strong>Ben Mildenhall</strong>\n              <br/>\n              <em>ICLR</em>, 2023   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"https://dreamfusion3d.github.io/\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2209.14988\">arXiv</a>\n              /\n              <a href=\"https://dreamfusion3d.github.io/gallery.html\">gallery</a>\n              <p/>\n              <p>\n              We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.\n              </p>\n            </td>\n          ",
    "24": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://yifanjiang.net/MalleConv.html\">\n                <papertitle>Fast and High-quality Image Denoising via Malleable Convolutions</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://yifanjiang.net/\">Yifan Jiang</a>,\n              <a href=\"https://bartwronski.com/\">Bartlomiej Wronski</a>, \n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://spark.adobe.com/page/CAdrFMJ9QeI2y/\">Zhangyang Wang</a>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>\n              <br/>\n              <em>ECCV</em>, 2022\n              <br/>\n              <a href=\"https://yifanjiang.net/MalleConv.html\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2201.00392\">arXiv</a>\n              <p/>\n              <p>\n              We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.\n              </p>\n            </td>\n          ",
    "23": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"rawnerf/index.html\">\n                <papertitle>NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</papertitle>\n              </a>\n              <br/>\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <a href=\"http://www.ricardomartinbrualla.com/\">Ricardo Martin-Brualla</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan Barron</a>\n              <br/>\n        <em>CVPR</em>, 2022   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"rawnerf/index.html\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2111.13679\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=JtBS4KBcKVc\">video</a>\n        /\n              <a href=\"https://github.com/google-research/multinerf\">code</a>\n              <p/>\n              <p>We train RawNeRF directly on linear raw camera images, enabling new HDR view synthesis applications and greatly increasing robustness to camera noise.</p>\n            </td>\n          ",
    "22": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://jonbarron.info/mipnerf360\">\n                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://scholar.harvard.edu/dorverbin/home\">Dor Verbin</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>\n              <br/>\n        <em>CVPR</em>, 2022   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"http://jonbarron.info/mipnerf360\">project page</a>\n              /\n              <a href=\"https://arxiv.org/abs/2111.12077\">arXiv</a>\n              /\n              <a href=\"https://youtu.be/YStDS2-Ln1s\">video</a>\n              /\n              <a href=\"https://github.com/google-research/multinerf\">code</a>\n              <p/>\n              <p>We extend mip-NeRF to produce photorealistic results on unbounded scenes.</p>\n            </td>\n          ",
    "21": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://dorverbin.github.io/refnerf/index.html\">\n                <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://scholar.harvard.edu/dorverbin/home\">Dor Verbin</a>,\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"Todd Zickler\">Todd Zickler</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>\n              <br/>\n        <em>CVPR</em>, 2022   <font color=\"red\"><strong>(Best Student Paper Honorable Mention)</strong></font>\n              <br/>\n              <a href=\"https://dorverbin.github.io/refnerf/index.html\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2112.03907\">arXiv</a>\n        /\n              <a href=\"https://youtu.be/qrdRH9irAlk\">video</a>\n        /\n              <a href=\"https://github.com/google-research/multinerf\">code</a>\n              <p/>\n              <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>\n            </td>\n          ",
    "20": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://waymo.com/research/block-nerf/\">\n                <papertitle>Block-NeRF: Scalable Large Scene Neural View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>,\n              <a href=\"http://casser.io/\">Vincent Casser</a>,\n              <a href=\"https://sites.google.com/site/skywalkeryxc/\">Xinchen Yan</a>,\n              <a href=\"https://scholar.google.com/citations?user=5mJUkI4AAAAJ&amp;hl=en\">Sabeek Pradhan</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://www.henrikkretzschmar.com/\">Henrik Kretzschmar</a>\n              <br/>\n        <em>CVPR</em>, 2022   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"https://waymo.com/research/block-nerf/\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2202.05263\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=6lGMCAzBzOQ\">video</a>\n              <p/>\n              <p>We build city-scale scenes from many NeRFs, trained using millions of images.</p>\n            </td>\n          ",
    "19": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://m-niemeyer.github.io/regnerf/index.html\">\n                <papertitle>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://m-niemeyer.github.io/\">Michael Niemeyer</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://msmsajjadi.github.io/\">Mehdi S. M. Sajjadi</a>,\n              <a href=\"http://www.cvlibs.net/\">Andreas Geiger</a>,\n              <a href=\"http://www2.informatik.uni-freiburg.de/~radwann/\">Noha Radwan</a>\n              <br/>\n        <em>CVPR</em>, 2022   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"https://m-niemeyer.github.io/regnerf/index.html\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2112.00724\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=QyyyvA4-Kwc\">video</a>\n              <p/>\n              <p>We regularize unseen views during optimization to enable view synthesis from as few as 3 input images.</p>\n            </td>\n          ",
    "18": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://ajayj.com/dreamfields\">\n                <papertitle>Zero-Shot Text-Guided Object Generation with Dream Fields</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://www.ajayj.com/\">Ajay Jain</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://people.eecs.berkeley.edu/~pabbeel/\">Pieter Abbeel</a>,\n              <a href=\"https://cs.stanford.edu/~poole/\">Ben Poole</a>\n              <br/>\n        <em>CVPR</em>, 2022\n              <br/>\n              <a href=\"https://ajayj.com/dreamfields\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2112.01455\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=1Fke6w46tv4\">video</a>\n        /\n              <a herf=\"https://github.com/google-research/google-research/tree/master/dreamfields\">code</a>\n              <p/>\n              <p>Supervising the CLIP embeddings of NeRF renderings allows us to generate 3D objects from text prompts alone.</p>\n            </td>\n          ",
    "17": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2112.03288\">\n                <papertitle>Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://niessnerlab.org/members/barbara_roessle/profile.html\">Barbara Roessle</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>, \n              <a href=\"https://www.niessnerlab.org/\">Matthias Nie&#223;ner</a>\n              <br/>\n        <em>CVPR</em>, 2022\n              <br/>\n              <a href=\"https://arxiv.org/abs/2112.03288\">arXiv</a>\n              /\n              <a href=\"https://www.youtube.com/watch?v=zzkvvdcvksc\">video</a>\n              <p/>\n              <p>\n              We apply dense depth completion techniques to freely-available sparse stereo data to guide NeRF reconstructions from few input images.\n              </p>\n            </td>\n          ",
    "16": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://jonbarron.info/mipnerf\">\n                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>,\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <a href=\"http://www.ricardomartinbrualla.com/\">Ricardo Martin-Brualla</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>\n              <br/>\n        <em>ICCV</em>, 2021   <font color=\"red\"><strong>(Best Paper Honorable Mention)</strong></font>\n              <br/>\n              <a href=\"http://jonbarron.info/mipnerf\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2103.13415\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=EpH175PY1A0\">video</a>\n        /\n              <a href=\"https://github.com/google/mipnerf\">code</a>\n              <p/>\n              <p>We prefilter the positional encoding function and train NeRF to generate anti-aliased renderings.</p>\n            </td>\n          ",
    "15": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://nerf.live\">\n                <papertitle>Baking Neural Radiance Fields for Real-Time View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://phogzone.com/\">Peter Hedman</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://www.pauldebevec.com/\">Paul Debevec</a>\n              <br/>\n        <em>ICCV</em>, 2021   <font color=\"red\"><strong>(oral)</strong></font> \n              <br/>\n              <a href=\"http://nerf.live\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2103.14645\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=5jKry8n5YO8\">video</a>\n        /\n              <a href=\"https://nerf.live/#demos\">demo</a>\n              <p/>\n              <p>We bake a trained NeRF into a sparse voxel grid of colors and features in order to render it in real-time.</p>\n            </td>\n          ",
    "14": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://www.matthewtancik.com/learnit\">\n                <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>,\n              <strong>Ben Mildenhall*</strong>,\n              <a href=\"https://www.linkedin.com/in/terrance-wang/\">Terrance Wang</a>,\n              <a href=\"https://www.linkedin.com/in/divi-schmidt-262044180/\">Divi Schmidt</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n        <em>CVPR</em>, 2021   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"http://www.matthewtancik.com/learnit\">project page</a>\n        /\n              <a href=\"http://arxiv.org/abs/2012.02189\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=A-r9itCzcyo\">video</a>\n        /\n              <a href=\"https://github.com/tancik/learnit\">code</a>\n              <p/>\n              <p>We use meta-learning to find weight initializations for coordinate-based MLPs that allow them to converge faster and generalize better.</p>\n            </td>\n          ",
    "13": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://pratulsrinivasan.github.io/nerv/\">\n                <papertitle>NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"https://boyangdeng.com/\">Boyang Deng</a>,\n              <a href=\"https://people.csail.mit.edu/xiuming/\">Xiuming Zhang</a>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>\n              <br/>\n        <em>CVPR</em>, 2021\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/nerv/\">project page</a> /\n              <a href=\"https://arxiv.org/abs/2012.03927\">arXiv</a> /\n              <a href=\"https://www.youtube.com/watch?v=4XyDdvhhjVo\">video</a>\n              <p/>\n              <p>We recover relightable NeRF-like models using neural approximations of expensive visibility integrals, so we can simulate complex volumetric light transport during training.</p>\n            </td>\n          ",
    "12": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"fourfeat/index.html\">\n                <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains\n</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <strong>Ben Mildenhall*</strong>,\n              <a href=\"https://people.eecs.berkeley.edu/~sfk/\">Sara Fridovich-Keil</a>,\n              <a href=\"https://www.csua.berkeley.edu/~rnithin/\">Nithin Raghavan</a>,\n              <a href=\"https://scholar.google.com/citations?user=lvA86MYAAAAJ&amp;hl=en\">Utkarsh Singhal</a>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n        <em>NeurIPS</em>, 2020  <font color=\"red\"><strong>(spotlight)</strong></font>\n              <br/>\n              <a href=\"fourfeat/index.html\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2006.10739\">arXiv</a>\n        /\n              <a href=\"https://github.com/tancik/fourier-feature-networks\">code</a>\n              <p/>\n              <p>We demonstrate that composing fully-connected networks with a simple Fourier feature mapping allows them to learn much high frequency functions.</p>\n            </td>\n          ",
    "11": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2008.03824\">\n                <papertitle>Neural Reflectance Fields for Appearance Acquisition</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://cseweb.ucsd.edu/~bisai/\">Sai Bi*</a>,\n              <a href=\"https://cseweb.ucsd.edu/~zex014/\">Zexiang Xu*</a>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"http://www.kalyans.org/\">Kalyan Sunkavalli</a>,\n              <a href=\"http://www.miloshasan.net/\">Milos Hasan</a>,\n              <a href=\"http://yannickhold.com/\">Yannick Hold-Geoffroy</a>,\n              <a href=\"https://cseweb.ucsd.edu/~kriegman/\">David Kriegman</a>,\n              <a href=\"https://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>\n              <br/>\n        <em>arXiv</em>, 2020\n              <br/>\n              <a href=\"https://arxiv.org/abs/2008.03824\">arXiv</a> \n              <p/>\n              <p>We recover relightable NeRF-like models by predicting per-location BRDFs and surface normals, and marching light rays through the NeRV volume to compute visibility.</p>\n            </td>\n          ",
    "10": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://www.matthewtancik.com/nerf\">\n                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\n</papertitle>\n              </a>\n              <br/>\n              <strong>Ben Mildenhall*</strong>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n        <em>ECCV</em>, 2020   <font color=\"red\"><strong>(Best Paper Honorable Mention)</strong></font>\n              <br/>\n              <a href=\"http://www.matthewtancik.com/nerf\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2003.08934\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=JuH79E8rdKc\">video</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=LRAqeM8EjOo\">talk</a>\n        /\n              <a href=\"https://github.com/bmild/nerf\">code</a>\n  /\n        <a href=\"https://www.youtube.com/watch?v=nCpGStnayHk\">two minute papers</a>\n  /\n        <a href=\"https://paperswithcode.com/method/nerf\">papers with code</a>\n  /\n        <a href=\"https://www.wired.com/story/new-way-ai-see-3d/\">wired</a>\n              <p/>\n              <p>We optimize a simple fully-connected network to represent a single scene as a volume, then use volume rendering to do view synthesis.</p>\n            </td>\n          ",
    "9": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2008.01815\">\n                <papertitle>Deep Multi Depth Panoramas for View Synthesis</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://www.linkedin.com/in/kaienlin2576/\">Kai-En Lin</a>,\n              <a href=\"https://cseweb.ucsd.edu/~zex014/\">Zexiang Xu</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan</a>,\n              <a href=\"http://yannickhold.com/\">Yannick Hold-Geoffroy</a>,\n              <a href=\"http://www.stephendiverdi.com/\">Stephen DiVerdi</a>,\n              <a href=\"https://qisun.me/\">Qi Sun</a>,\n              <a href=\"http://www.kalyans.org/\">Kalyan Sunkavalli</a>,\n              <a href=\"https://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>\n              <br/>\n        <em>ECCV</em>, 2020\n              <br/>\n              <a href=\"https://arxiv.org/abs/2008.01815\">arXiv</a> /\n              <a href=\"https://cseweb.ucsd.edu/~zex014/papers/2020_mdp/2020_mdp.mp4\">video</a>\n              <p/>\n              <p>We represent scenes as multi-layer panoramas with depth for VR view synthesis.</p>\n            </td>\n          ",
    "8": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://pratulsrinivasan.github.io/lighthouse/\">\n                <papertitle>Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination</papertitle>\n              </a>\n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <strong>Ben Mildenhall*</strong>,\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>,\n              <a href=\"https://research.google/people/RichardTucker/\">Richard Tucker</a>,\n              <a href=\"https://www.cs.cornell.edu/~snavely/\">Noah Snavely</a>\n              <br/>\n        <em>CVPR</em>, 2020  \n              <br/>\n              <a href=\"https://pratulsrinivasan.github.io/lighthouse/\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/2003.08367\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=KsiZpUFPqIU\">video</a>\n  /\n        <a href=\"https://github.com/pratulsrinivasan/lighthouse\">code</a>\n              <p/>\n              <p>We predict a volume from an input stereo pair that can be used to calculate incident lighting at any 3D point within a scene.</p>\n            </td>\n          ",
    "7": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://www.matthewtancik.com/stegastamp\">\n                <papertitle>StegaStamp: Invisible Hyperlinks in Physical Photographs</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://matthewtancik.com/\">Matthew Tancik*</a>,\n              <strong>Ben Mildenhall*</strong>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>\n              <br/>\n        <em>CVPR</em>, 2020  \n              <br/>\n              <a href=\"http://www.matthewtancik.com/stegastamp\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/1904.05343\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=E8OqgNDBGO0\">video</a>\n        /\n              <a href=\"https://github.com/tancik/StegaStamp\">code</a>\n              <p/>\n              <p>We can hide hyperlinks in natural images to create aesthetically pleasing barcodes.</p>\n            </td>\n          ",
    "6": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"llff/index.html\">\n                <papertitle>Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</papertitle>\n              </a>\n              <br/>\n              <strong>Ben Mildenhall*</strong>,\n              <a href=\"https://pratulsrinivasan.github.io/\">Pratul Srinivasan*</a>,\n              <a href=\"https://scholar.google.com/citations?user=yZMAlU4AAAAJ\">Rodrigo Ortiz-Cayon</a>,\n              <a href=\"http://faculty.cs.tamu.edu/nimak/\">Nima Khademi Kalantari</a>,\n              <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>,\n              <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>,\n              <a href=\"https://abhishekkar.info/\">Abhishek Kar</a>\n              <br/>\n        <em>SIGGRAPH</em>, 2019\n              <br/>\n              <a href=\"llff/index.html\">project page</a>\n        /\n              <a href=\"https://arxiv.org/abs/1905.00889\">arXiv</a>\n        /\n              <a href=\"https://www.youtube.com/watch?v=LY6MgDUzS3M\">video</a>\n        /\n              <a href=\"https://github.com/Fyusion/LLFF\">code</a>\n              <p/>\n              <p>We develop and analyze a deep learning method for rendering novel views of complex real world scenes.</p>\n            </td>\n          ",
    "5": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"http://timothybrooks.com/tech/unprocessing/\">\n                <papertitle>Unprocessing Images for Learned Raw Denoising</papertitle>\n              </a>\n              <br/>\n              <a href=\"http://timothybrooks.com/\">Tim Brooks</a>,\n              <strong>Ben Mildenhall</strong>,\n              <a href=\"https://people.csail.mit.edu/tfxue/\">Tianfan Xue</a>,\n              <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>,\n              <a href=\"http://www.dsharlet.com/\">Dillon Sharlet</a>,\n              <a href=\"https://jonbarron.info/\">Jonathan T. Barron</a>\n              <br/>\n        <em>CVPR</em>, 2019   <font color=\"red\"><strong>(oral)</strong></font>\n              <br/>\n              <a href=\"http://timothybrooks.com/tech/unprocessing/\">project page</a>\n        /\n        <a href=\"https://arxiv.org/abs/1811.11127\">arXiv</a>\n              <p/>\n              <p>We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p>\n            </td>\n          ",
    "4": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n        <p><a href=\"kpn/index.html\">\n        <papertitle>Burst Denoising with Kernel Prediction Networks</papertitle></a><br/>\n          <strong>Ben Mildenhall</strong>,\n          <a href=\"http://jonbarron.info\">Jonathan T. Barron</a>,\n          <a href=\"http://people.csail.mit.edu/jiawen/\">Jiawen Chen</a>,\n          Dillon Sharlet,\n          <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>,\n          Robert Carroll <br/>\n        <em>CVPR</em>, 2018   <font color=\"red\"><strong>(spotlight)</strong></font><br/>\n        <a href=\"kpn/index.html\">project page</a>\n        /\n        <a href=\"https://arxiv.org/abs/1712.02327\">arXiv</a>\n  /\n  <a href=\"https://github.com/google/burst-denoising\">code</a>\n        </p><p/>\n        <p>We train a network to predict linear kernels that denoise bursts of raw linear images.</p>\n      </td>\n    ",
    "3": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n        <p>\n          <a href=\"https://waller-lab.github.io/DiffuserCam/\">\n          <papertitle>DiffuserCam: Lensless Single-exposure 3D Imaging</papertitle>\n          </a>\n          <br/>\n          <a href=\"https://people.eecs.berkeley.edu/~nick.antipa/\">Nick Antipa</a>, \n          <a href=\"https://people.eecs.berkeley.edu/~gkuo/\">Grace Kuo</a>, \n          <a href=\"http://www.reinhardheckel.com/\">Reinhard Heckel</a>, \n          <strong>Ben Mildenhall</strong>, \n          <a href=\"https://emrahbostan.com/\">Emrah Bostan</a>, \n          <a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html\">Ren Ng</a>, \n            <a href=\"http://www.laurawaller.com/\">Laura Waller</a><br/>\n          <em>Optica</em>, 2018 <br/>\n          <a href=\"https://waller-lab.github.io/DiffuserCam/\">project page</a>\n          /\n          <a href=\"https://arxiv.org/abs/1710.02134\">arXiv</a>\n        </p>\n        <p>Using a diffuser instead of a lens lets you recover 3D in a single exposure.</p>\n        </td>\n      ",
    "2": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n        <p>\n          <a href=\"data/microflake.pdf\">\n          <papertitle>Approximations for the distribution of microflake normals</papertitle>\n          </a>\n          <br/>\n          <a href=\"https://faculty.engineering.ucdavis.edu/max/\">Nelson Max</a>, \n          Tom Duff, \n          <strong>Ben Mildenhall</strong>, \n          <a href=\"http://students.cec.wustl.edu/~yajieyan/\">Yajie Yan</a><br/>\n          <em>The Visual Computer</em>, 2017\n        </p>\n        <p>We precompute microflake approximations to make rendering large meshes at a distance more efficient.</p>\n        </td>\n      ",
    "1": "<td style=\"padding:20px;width:75%;vertical-align:middle\">\n        <p>\n        </p><p>\n          <a href=\"https://dritchie.github.io/pdf/sosmc.pdf\">\n          <papertitle>Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo</papertitle>\n          </a>\n          <br/>\n          <a href=\"https://dritchie.github.io/\">Daniel Ritchie</a>, \n          <strong>Ben Mildenhall</strong>, \n          <a href=\"http://cocolab.stanford.edu/ndg.html\">Noah D. Goodman</a>, \n          <a href=\"https://graphics.stanford.edu/~hanrahan\">Pat Hanrahan</a><br/>\n          <em>SIGGRAPH</em>, 2015\n        </p>\n        <p>We improve control over the output of highly-variable procedural modeling programs by using SOSMC to provide incremental feedback on partially-generated models.</p>\n        \n        </td>\n      ",
}